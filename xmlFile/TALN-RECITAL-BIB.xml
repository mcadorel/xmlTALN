<?xml version="1.0" encoding="UTF-8"?>
<conferences>
  <conference>
    <edition>
      <acronyme>TALN'2010</acronyme>
      <titre>conférence sur le Traitement Automatique des Langues Naturelles</titre>
      <ville>Montréal</ville>
      <pays>Canada</pays>
      <dateDebut>2010-07-19</dateDebut>
      <dateFin>2010-07-23</dateFin>
      <presidents>
        <nom>Philippe Langlais</nom>
        <nom>Michel Gagnon</nom>
      </presidents>
      <typeArticles>
        <type id="invite">Invités</type>
        <type id="long">Papiers longs</type>
        <type id="court">Papiers courts</type>
        <type id="démonstration">Démonstrations</type>
      </typeArticles>
      <statistiques>
        <acceptations id="long" soumissions="90">40</acceptations>
        <acceptations id="court" soumissions="68">36</acceptations>
      </statistiques>
      <siteWeb>http://www.groupes.polymtl.ca/taln2010/</siteWeb>
      <meilleurArticle>
        <articleId>taln-2010-long-033</articleId>
        <articleId>taln-2010-long-030</articleId>
      </meilleurArticle>
    </edition>
    <articles>
      <article id="taln-2010-invite-001" session="Conférence invitée ">
        <auteurs>
          <auteur>
            <nom>Igor Mel’čuk</nom>
            <email>Igor.Melcuk@umontreal.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">OLST, Université de Montréal</affiliation>
        </affiliations>
        <titre>La phraséologie en langue, en dictionnaire et en TALN</titre>
        <type>invite</type>
        <pages/>
        <resume/>
        <mots_cles/>
        <title/>
        <abstract/>
        <keywords/>
      </article>
      <article id="taln-2010-invite-002" session="Conférence invitée ">
        <auteurs>
          <auteur>
            <nom>Pierre Isabelle</nom>
            <email>Pierre.Isabelle@cnrc-nrc.gc.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">National Research Council Canada</affiliation>
        </affiliations>
        <titre>La montée en puissance des recherches en traduction automatique statistique</titre>
        <type>invite</type>
        <pages/>
        <resume/>
        <mots_cles/>
        <title/>
        <abstract/>
        <keywords/>
      </article>
      <article id="taln-2010-invite-003" session="Conférence invitée ">
        <auteurs>
          <auteur>
            <nom>Gerald Penn</nom>
            <email>frank@cs.toronto.edu</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">University of Toronto, 10 King’s College Rd., Toronto, M5S 3G4, ON, Canada</affiliation>
        </affiliations>
        <titre/>
        <type>invite</type>
        <pages/>
        <resume/>
        <mots_cles/>
        <title>The Quantitative Study of Writing Systems</title>
        <abstract/>
        <keywords/>
      </article>
      <article id="taln-2010-long-001" session="Plénière">
        <auteurs>
          <auteur>
            <nom>Holger Schwenk</nom>
            <email>Holger.Schwenk@lium.univ-lemans.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIUM, Université du Maine, 72085 Le Mans cedex, France</affiliation>
        </affiliations>
        <titre>Adaptation d’un Système de Traduction Automatique Statistique avec des Ressources monolingues</titre>
        <type>long</type>
        <pages/>
        <resume>Les performances d’un système de traduction statistique dépendent beaucoup de la qualité et de la quantité des données d’apprentissage disponibles. La plupart des textes parallèles librement disponibles proviennent d’organisations internationales. Le jargon observé dans ces textes n’est pas très adapté pour construire un système de traduction pour d’autres domaines. Nous présentons dans cet article une technique pour adapter le modèle de traduction à un domaine différent en utilisant des textes dans la langue source uniquement. Nous obtenons des améliorations significatives du score BLEU dans des systèmes de traduction de l’arabe vers le français et vers l’anglais.</resume>
        <mots_cles>Traduction statistique, adaptation du modèle de traduction, corpus monolingue, apprentissage non-supervisé</mots_cles>
        <title/>
        <abstract>The performance of a statistical machine translation system depends a lot on the quality and quantity of the available training data. Most of the existing, easily available parallel texts come from international organizations and the jargon observed in those texts is not very appropriate to build a machine translation system for other domains. In this paper, we present a technique to automatically adapt the translation model to a new domain using monolingual data in the source language only. We observe significant improvements in the BLEU score in statistical machine translation systems from Arabic to French and English respectively.</abstract>
        <keywords>Statistical machine translation, translation model adaptation, monolingual data, unsupervised training</keywords>
      </article>
      <article id="taln-2010-long-002" session="Plénière">
        <auteurs>
          <auteur>
            <nom>Julien Bourdaillet</nom>
            <email>bourdaij@iro.umontreal.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Stéphane Huet</nom>
            <email>huetstep@iro.umontreal.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Philippe Langlais</nom>
            <email>felipe@iro.umontreal.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">RALI - DIRO - Université de Montréal, C.P. 6128, succursale centre-ville, H3C 3J7, Montréal, Québec, Canada</affiliation>
        </affiliations>
        <titre>Alignement de traductions rares à l’aide de paires de phrases non alignées</titre>
        <type>long</type>
        <pages/>
        <resume>Bien souvent, le sens d’un mot ou d’une expression peut être rendu dans une autre langue par plusieurs traductions. Parmi celles-ci, certaines se révèlent très fréquentes alors que d’autres le sont beaucoup moins, conformément à une loi zipfienne. La googlisation de notre monde n’échappe pas aux mémoires de traduction, qui mettent souvent à mal ou simplement ignorent ces traductions rares qui sont souvent de bonne qualité. Dans cet article, nous nous intéressons à ces traductions rares sous l’angle du repérage de traductions. Nous argumentons qu’elles sont plus difficiles à identifier que les traductions plus fréquentes. Nous décrivons une approche originale qui permet de mieux les identifier en tirant profit de l’alignement au niveau des mots de paires de phrases qui ne sont pas alignées. Nous montrons que cette approche permet d’améliorer l’identification de ces traductions rares.</resume>
        <mots_cles>Traduction automatique statistique, alignement de mots, traduction rares, contrôle de pertinence</mots_cles>
        <title/>
        <abstract>There generally exist numerous ways to translate a word or a phrase in another language. Among these translations, some are very common while others are far less so, according to a zipfian law. As with the rest of the world, translation memories are googlized, leading to poorly handled or even simply ignored rare translations, while they are often of good quality. In this paper, we tackle this problem in a transpotting framework. We show that these rare translations are harder to identify than common translations. We describe an original approach based on the word alignment of sentences which are not aligned. We show that this approach significantly improves the identification of those rare translations.</abstract>
        <keywords>Statistical machine translation, word alignment, rare translations, relevance feedback</keywords>
      </article>
      <article id="taln-2010-long-003" session="Plénière">
        <auteurs>
          <auteur>
            <nom>Pascal Denis</nom>
            <email>pascal.denis@inria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Benoît Sagot</nom>
            <email>benoit.sagot@inria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Alpage, INRIA Paris–Rocquencourt &amp; Université Paris 7, Rocquencourt, BP 105, 78153 Le Chesnay Cedex, France</affiliation>
        </affiliations>
        <titre>Exploitation d’une ressource lexicale pour la construction d’un étiqueteur morpho-syntaxique état-de-l’art du français</titre>
        <type>long</type>
        <pages/>
        <resume>Cet article présente MEltfr, un étiqueteur morpho-syntaxique automatique du français. Il repose sur un modèle probabiliste séquentiel qui bénéficie d’informations issues d’un lexique exogène, à savoir le Lefff. Evalué sur le FTB, MEltfr atteint un taux de précision de 97.75% (91.36% sur les mots inconnus) sur un jeu de 29 étiquettes. Ceci correspond à une diminution du taux d’erreur de 18% (36.1% sur les mots inconnus) par rapport au même modèle sans couplage avec le Lefff. Nous étudions plus en détail la contribution de cette ressource, au travers de deux séries d’expériences. Celles-ci font apparaître en particulier que la contribution des traits issus du Lefff est de permettre une meilleure couverture, ainsi qu’une modélisation plus fine du contexte droit des mots.</resume>
        <mots_cles>Etiquetage morpho-syntaxique, modèles à maximisation d’entropie, français, lexique</mots_cles>
        <title/>
        <abstract>This paper presents MEltfr, an automatic POS tagger for French. This system relies on a sequential probabilistic model that exploits information extracted from an external lexicon, namely Lefff. When evaluated on the FTB corpus, MEltfr achieves an accuracy of 97.75% (91.36% on unknow words) using a tagset of 29 categories. This corresponds to an error rate decrease of 18% (36.1% on unknow words) compared to the same model without Lefff information. We investigate in more detail the contribution of this resource through two sets of experiments. These reveal in particular that the Lefff features allow for an increased coverage and a finer-grained modeling of the context at the right of a word.</abstract>
        <keywords>POS tagging, maximum entropy models, French, lexicon</keywords>
      </article>
      <article id="taln-2010-long-004" session="Analyse textuelle">
        <auteurs>
          <auteur>
            <nom>Olivier Ferret</nom>
            <email>olivier.ferret@cea.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CEA, LIST, Laboratoire Vision et Ingénierie des Contenus, Fontenay-aux-Roses, F-92265 France</affiliation>
        </affiliations>
        <titre>Similarité sémantique et extraction de synonymes à partir de corpus</titre>
        <type>long</type>
        <pages/>
        <resume>La définition de mesures sémantiques au niveau lexical a fait l’objet de nombreux travaux depuis plusieurs années. Dans cet article, nous nous focalisons plus spécifiquement sur les mesures de nature distributionnelle. Bien que différentes évaluations ont été réalisées les concernant, il reste difficile à établir si une mesure donnant de bons résultats dans un cadre d’évaluation peut être appliquée plus largement avec le même succès. Dans le travail présenté, nous commençons par sélectionner une mesure de similarité sur la base d’un test de type TOEFL étendu. Nous l’appliquons ensuite au problème de l’extraction de synonymes à partir de corpus en comparant nos résultats avec ceux de (Curran &amp; Moens, 2002). Enfin, nous testons l’intérêt pour cette tâche d’extraction de synonymes d’une méthode d’amélioration de la qualité des données distributionnelles proposée dans (Zhitomirsky-Geffet &amp; Dagan, 2009).</resume>
        <mots_cles>extraction de synonymes, similarité sémantique, méthodes distributionnelles</mots_cles>
        <title/>
        <abstract>The definition of lexical semantic measures has been the subject of lots of works for many years. In this article, we focus more specifically on distributional semantic measures. Although several evaluations about this kind of measures were already achieved, it is still difficult to determine if a measure that performs well in an evaluation framework can be applied more widely with the same success. In the work we present here, we first select a similarity measure by testing it against an extended TOEFL test. Then, we apply this measure for extracting automatically synonyms from a corpus and we compare our results to those of (Curran &amp; Moens, 2002). Finally, we test the interest for synonym extraction of a method proposed in (Zhitomirsky-Geffet &amp; Dagan, 2009) for improving the quality of distributional data.</abstract>
        <keywords>synonym extraction, semantic similarity, distributional methods</keywords>
      </article>
      <article id="taln-2010-long-005" session="Analyse textuelle">
        <auteurs>
          <auteur>
            <nom>Simon Charest</nom>
            <email>developpement@druide.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Éric Brunelle</nom>
            <email>developpement@druide.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Jean Fontaine</nom>
            <email>developpement@druide.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Druide informatique inc., 1435, rue Saint-Alexandre, bureau 1040, Montréal (Québec) H3A 2G4, Canada</affiliation>
        </affiliations>
        <titre>Au-delà de la paire de mots : extraction de cooccurrences syntaxiques multilexémiques</titre>
        <type>long</type>
        <pages/>
        <resume>Cet article décrit l’élaboration de la deuxième édition du dictionnaire de cooccurrences du logiciel d’aide à la rédaction Antidote. Cette nouvelle mouture est le résultat d’une refonte complète du processus d’extraction, ayant principalement pour but l’extraction de cooccurrences de plus de deux unités lexicales. La principale contribution de cet article est la description d’une technique originale pour l’extraction de cooccurrences de plus de deux mots conservant une structure syntaxique complète.</resume>
        <mots_cles>Antidote, cooccurrences, collocations, expressions multimots</mots_cles>
        <title/>
        <abstract>This article describes the elaboration of the second edition of the co-occurrence dictionary included in Antidote HD, a commercial software tool for writing in French. This second edition is the result of a complete overhaul of the extraction process, with the objective of extracting co-occurrences of more than two lexical units. The main contribution of this article is the description of an original method for extracting co-occurrences of more than two words retaining their full syntactic structure.</abstract>
        <keywords>Antidote, co-occurrences, collocations, multi-word expressions (MWE)</keywords>
      </article>
      <article id="taln-2010-long-006" session="Analyse textuelle">
        <auteurs>
          <auteur>
            <nom>Adil El Ghali</nom>
            <email>elghali@lutin-userlab.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Yann Vigile Hoareau</nom>
            <email>hoareau@lutin-userlab.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Lutin User Lab, Cité des Sciences, Av C. Cariou, 75019 Paris</affiliation>
          <affiliation affiliationId="2">Université Paris 8, rue de la Liberté, 93200 Saint Denis</affiliation>
        </affiliations>
        <titre>Une approche cognitive de la fouille de grandes collections de documents</titre>
        <type>long</type>
        <pages/>
        <resume>La récente éclosion du Web2.0 engendre un accroissement considérable de volumes textuels et intensifie ainsi l’importance d’une réflexion sur l’exploitation des connaissances à partir de grandes collections de documents. Dans cet article, nous présentons une approche de rechercher d’information qui s’inspire des certaines recherches issues de la psychologie cognitive pour la fouille de larges collections de documents. Nous utilisons un document comme requête permettant de récupérer des informations à partir d’une collection représentée dans un espace sémantique. Nous définissons les notions d’identité sémantique et de pollution sémantique dans un espace de documents. Nous illustrons notre approche par la description d’un système appelé BRAT (Blogosphere Random Analysis using Texts) basé sur les notions préalablement introduites d’identité et de pollution sématique appliquées à une tâche d’identification des actualités dans la blogosphère mondiale lors du concours TREC’09. Les premiers résultats produits sont tout à fait encourageant et indiquent les pistes des recherches à mettre en oeuvre afin d’améliorer les performances de BRAT.</resume>
        <mots_cles>Fouille de textes, Random-Indexing, Cognition, Marche aléatoire</mots_cles>
        <title/>
        <abstract>MiningWeb 2.0 content become nowadays an important task in Information Retrieval and Search communities. The work related in this paper present an original approach of blogs mining, inspired from researches in cognitive psychology. We define the notions of semantic identity of blogs, and the semantic pollution in a semantic space. Then, we describe a system called BRAT (Blogosphere Random Analysis using Texts) based on these notions that has been applied to the Top Stories identification task of the Blog Track at the TREC’09 contest. The performance of BRAT at TREC’09 in its preliminary stage of development are very encouraging and the results of the experiences described here-after draw the lines of the future researches that should be realized in order to upgrade its performances.</abstract>
        <keywords>Text-Mining, Random-Indexing, Cognition, Random walk</keywords>
      </article>
      <article id="taln-2010-long-007" session="Syntaxe">
        <auteurs>
          <auteur>
            <nom>Jonathan Marchand</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Bruno Guillaume</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Guy Perrier</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">INRIA Nancy-Grand Est - LORIA - Nancy-Université</affiliation>
        </affiliations>
        <titre>Motifs de graphe pour le calcul de dépendances syntaxiques complètes</titre>
        <type>long</type>
        <pages/>
        <resume>Cet article propose une méthode pour calculer les dépendances syntaxiques d’un énoncé à partir du processus d’analyse en constituants. L’objectif est d’obtenir des dépendances complètes c’est-à-dire contenant toutes les informations nécessaires à la construction de la sémantique. Pour l’analyse en constituants, on utilise le formalisme des grammaires d’interaction : celui-ci place au cœur de la composition syntaxique un mécanisme de saturation de polarités qui peut s’interpréter comme la réalisation d’une relation de dépendance. Formellement, on utilise la notion de motifs de graphes au sens de la réécriture de graphes pour décrire les conditions nécessaires à la création d’une dépendance.</resume>
        <mots_cles>Analyse syntaxique, dépendance, grammaires d’interaction, polarité</mots_cles>
        <title/>
        <abstract>This article describes a method to build syntactical dependencies starting from the phrase structure parsing process. The goal is to obtain all the information needed for a detailled semantical analysis. Interaction Grammars are used for parsing; the saturation of polarities which is the core of this formalism can be mapped to dependency relation. Formally, graph patterns are used to express the set of constraints which control dependency creations.</abstract>
        <keywords>Syntactic analysis, dependency, interaction grammars, polarity</keywords>
      </article>
      <article id="taln-2010-long-008" session="Syntaxe">
        <auteurs>
          <auteur>
            <nom>Juliette Thuilier</nom>
            <email>jthuilier@linguist.jussieu.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Gwendoline Fox</nom>
            <email/>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Benoît Crabbé</nom>
            <email>benoit.crabbe@linguist.jussieu.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université Paris VII Denis Diderot, UFRL et INRIA (Alpage)</affiliation>
          <affiliation affiliationId="2">Université Paris III Sorbonne-Nouvelle, ILPGA et EA 1483</affiliation>
        </affiliations>
        <titre>Approche quantitative en syntaxe : l’exemple de l’alternance de position de l’adjectif épithète en français</titre>
        <type>long</type>
        <pages/>
        <resume>Cet article présente une analyse statistique sur des données de syntaxe qui a pour but d’aider à mieux cerner le phénomène d’alternance de position de l’adjectif épithète par rapport au nom en français. Nous montrons comment nous avons utilisé les corpus dont nous disposons (French Treebank et le corpus de l’Est-Républicain) ainsi que les ressources issues du traitement automatique des langues, pour mener à bien notre étude. La modélisation à partir de 13 variables relevant principalement des propriétés du syntagme adjectival, de celles de l’item adjectival, ainsi que de contraintes basées sur la fréquence, permet de prédire à plus de 93% la position de l’adjectif. Nous insistons sur l’importance de contraintes relevant de l’usage pour le choix de la position de l’adjectif, notamment à travers la fréquence d’occurrence de l’adjectif, et la fréquence de contextes dans lesquels il apparaît.</resume>
        <mots_cles>Syntaxe probabiliste, linguistique de corpus, adjectif épithète, régression logistique</mots_cles>
        <title/>
        <abstract>This article presents a statistical analysis of syntactic data that aims to better understand the phenomenon of position alternation displayed by attributive adjectives with respect to nouns in French. We show how we used the corpora available for French (the French Treebank and the Est-Républicain corpus) as well as ressources provided by Natural Language Processing for our study. The proposed model contains 13 variables based on properties of the adjectival phrase, the adjectival item and on frequency constraints. This model is capable to predict the position of adjectives at more than a 93% rate. We especially focus on the importance of constraints based on usage for the choice of position for the adjective, in particular the frequency of contexts in which it appears.</abstract>
        <keywords>Probabilistic syntax, corpus linguistics, attributive adjective, logistic regression</keywords>
      </article>
      <article id="taln-2010-long-009" session="Syntaxe">
        <auteurs>
          <auteur>
            <nom>Philippe Blache</nom>
            <email>blache@lpl-aix.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire Parole et Langage, CNRS &amp; Université de Provence, 5, Avenue Pasteur, 13604 Aix en Provence - France</affiliation>
        </affiliations>
        <titre>Un modèle de caractérisation de la complexité syntaxique</titre>
        <type>long</type>
        <pages/>
        <resume>Cet article présente un modèle de la complexité syntaxique. Il réunit un ensemble d’indices de complexité et les représente à l’aide d’un cadre formel homogène, offrant ainsi la possibilité d’une quantification automatique : le modèle proposé permet d’associer à chaque phrase un indice reflétant sa complexité.</resume>
        <mots_cles>Complexité syntaxique, analyse syntaxique automatique, parser humain</mots_cles>
        <title/>
        <abstract>This paper proposes a model of syntactic complexity. It brings together a set of complexity parameters and represnt them thanks to a unique formal framework. This approach makes it possible an automatic evaluation : a complexity index can be associated to each sentence.</abstract>
        <keywords>Syntactic complexity, parsing, human parser</keywords>
      </article>
      <article id="taln-2010-long-010" session="Syntaxe">
        <auteurs>
          <auteur>
            <nom>Éric Villemonte De La Clergerie</nom>
            <email>eric.de_la_clergerie@inria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Alpage, INRIA / Univ. Paris 7, 30 rue du Château-des-rentiers, 75013 Paris</affiliation>
        </affiliations>
        <titre>Convertir des dérivations TAG en dépendances</titre>
        <type>long</type>
        <pages/>
        <resume>Les structures de dépendances syntaxiques sont importantes et bien adaptées comme point de départ de diverses applications. Dans le cadre de l’analyseur TAG FRMG, nous présentons les détails d’un processus de conversion de forêts partagées de dérivations en forêts partagées de dépendances. Des éléments d’information sont fournis sur un algorithme de désambiguisation sur ces forêts de dépendances.</resume>
        <mots_cles>dépendances, analyse syntaxique, TAG, forêt partagée</mots_cles>
        <title/>
        <abstract>Syntactic dependency structures are important and adequate as starting point for various NLP applications. In the context of the French TAG FRMG parser, we present the details of a conversion process from shared derivation forests into shared dependency forests. Some information are also provided about a disambiguisation algorithm for these dependency forests.</abstract>
        <keywords>dependencies, parsing, TAG, shared forest</keywords>
      </article>
      <article id="taln-2010-long-011" session="TALN pour les TIC">
        <auteurs>
          <auteur>
            <nom>Shamima Mithun</nom>
            <email>s_mithun@encs.concordia.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Leila Kosseim</nom>
            <email>kosseim@encs.concordia.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Concordia University, Department of Computer Science and Software Engineering, Montreal, Quebec, Canada</affiliation>
        </affiliations>
        <titre/>
        <type>long</type>
        <pages/>
        <resume/>
        <mots_cles/>
        <title>A Hybrid Approach to Utilize Rhetorical Relations for Blog Summarization</title>
        <abstract>The availability of huge amounts of online opinions has created a new need to develop effective query-based opinion summarizers to analyze this information in order to facilitate decision making at every level. To develop an effective opinion summarization approach, we have targeted to resolve specifically Question Irrelevancy and Discourse Incoherency problems which have been found to be the most frequently occurring problems for opinion summarization. To address these problems, we have introduced a hybrid approach by combining text schema and rhetorical relations to exploit intra-sentential rhetorical relations. To evaluate our approach, we have built a system called BlogSum and have compared BlogSum-generated summaries after applying rhetorical structuring to BlogSum-generated candidate sentences without utilizing rhetorical relations using the Text Analysis Conference (TAC) 2008 data for summary contents. Evaluation results show that our approach improves summary contents by reducing question irrelevant sentences.</abstract>
        <keywords>Blog Summarization, Rhetorical Relations, Text Schema</keywords>
      </article>
      <article id="taln-2010-long-012" session="TALN pour les TIC">
        <auteurs>
          <auteur>
            <nom>Richard Beaufort</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Sophie Roekhaut</nom>
            <email/>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Louise-Amélie Cougnon</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Cédrick Fairon</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CENTAL, Université catholique de Louvain, 1348 Louvain-la-Neuve, Belgique</affiliation>
          <affiliation affiliationId="2">TCTS Lab, Université de Mons, 7000 Mons, Belgique</affiliation>
        </affiliations>
        <titre>Une approche hybride traduction/correction pour la normalisation des SMS</titre>
        <type>long</type>
        <pages/>
        <resume>Cet article présente une méthode hybride de normalisation des SMS, à mi-chemin entre correction orthographique et traduction automatique. La partie du système qui assure la normalisation utilise exclusivement des modèles entraînés sur corpus. Evalué en français par validation croisée, le système obtient un taux d’erreur au mot de 9.3% et un score BLEU de 0.83.</resume>
        <mots_cles>SMS, normalisation, machines à états finis, approche hybride, orienté traduction, orienté correction, apprentissage sur corpus</mots_cles>
        <title/>
        <abstract>This paper presents a method of normalizing SMS messages that shares similarities with both spell checking and machine translation approaches. The normalization part of the system is entirely based on models trained from a corpus. Evaluated in French by ten-fold cross-validation, the system achieves a 9.3% Word Error Rate and a 0.83 BLEU score.</abstract>
        <keywords>SMS messages, normalization, finite-state machines, hybrid approach, machine translationlike, spell checking-like, corpus-based learning</keywords>
      </article>
      <article id="taln-2010-long-013" session="TALN pour les TIC">
        <auteurs>
          <auteur>
            <nom>Guillaume Wisniewski</nom>
            <email>guillaume.wisniewski@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Aurélien Max</nom>
            <email>aurelien.max@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>François Yvon</nom>
            <email>francois.yvon@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIMSI—CNRS, B.P. 133 91 403 ORSAY CEDEX, Université Paris Sud 11</affiliation>
        </affiliations>
        <titre>Recueil et analyse d’un corpus écologique de corrections orthographiques extrait des révisions de Wikipédia</titre>
        <type>long</type>
        <pages/>
        <resume>Dans cet article, nous introduisons une méthode à base de règles permettant d’extraire automatiquement de l’historique des éditions de l’encyclopédie collaborative Wikipédia des corrections orthographiques. Cette méthode nous a permis de construire un corpus d’erreurs composé de 72 483 erreurs lexicales (non-word errors) et 74 100 erreurs grammaticales (real-word errors). Il n’existe pas, à notre connaissance, de plus gros corpus d’erreurs écologiques librement disponible. En outre, les techniques mises en oeuvre peuvent être facilement transposées à de nombreuses autres langues. La collecte de ce corpus ouvre de nouvelles perspectives pour l’étude des erreurs fréquentes ainsi que l’apprentissage et l’évaluation des correcteurs orthographiques automatiques. Plusieurs expériences illustrant son intérêt sont proposées.</resume>
        <mots_cles>ressources pour le TAL, correction orthographique, Wikipédia</mots_cles>
        <title/>
        <abstract>This paper describes a French spelling error corpus we built by miningWikipedia revision history. This corpus contains 72,493 non-word errors and 74,100 real-word errors. To the best of our knowledge, this is the first time that such a large corpus of naturally occurring errors is collected and made publicly available, which opens new possibilities for the evaluation of spell checkers and the study of error patterns. In the second part of this work, a first study of french spelling error patterns and of the performance of a spell checker is presented.</abstract>
        <keywords>resources for NLP, spelling correction, Wikipedia</keywords>
      </article>
      <article id="taln-2010-long-014" session="TALN pour les TIC">
        <auteurs>
          <auteur>
            <nom>Eric Charton</nom>
            <email>eric.charton@polymtl.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Michel Gagnon</nom>
            <email>michel.gagnon@polymtl.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Benoit Ozell</nom>
            <email>benoit.ozell@polymtl.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">École Polytechnique, 2900 boul. Edouard Montpetit, Montréal, Canada H3T 1J4</affiliation>
        </affiliations>
        <titre>Extension d’un système d’étiquetage d’entités nommées en étiqueteur sémantique</titre>
        <type>long</type>
        <pages/>
        <resume>L’étiquetage sémantique consiste à associer un ensemble de propriétés à une séquence de mots contenue dans un texte. Bien que proche de la tâche d’étiquetage par entités nommées, qui revient à attribuer une classe de sens à un mot, la tâche d’étiquetage ou d’annotation sémantique cherche à établir la relation entre l’entité dans son texte et sa représentation ontologique. Nous présentons un étiqueteur sémantique qui s’appuie sur un étiqueteur d’entités nommées pour mettre en relation un mot ou un groupe de mots avec sa représentation ontologique. Son originalité est d’utiliser une ontologie intermédiaire de nature statistique pour établir ce lien.</resume>
        <mots_cles>Étiqueteur sémantique, Entités nommées, Analyse sémantique, Ontologie</mots_cles>
        <title/>
        <abstract>Semantic labelling consist to associate a set of properties to a sequence of words from a text. Although its proximity with the named entity labelling task, which is equivalent to associate a class meaning to a sequence of word, the task of semantic labelling try to establish the relation between the entity in the text and it’s ontological representation. We present a semantic labelling system based on a named entity recognition step. The originality of our system is that the link between named entity and its semantic representation is obtained trough the use of an intermediate statistical ontology.</abstract>
        <keywords>Semantic parser, Named entities, Semantic annotation</keywords>
      </article>
      <article id="taln-2010-long-015" session="Sémantique">
        <auteurs>
          <auteur>
            <nom>Jasmina Milićević</nom>
            <email>jmilicev@dal.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">French Department, Dalhousie University, 6135 University Avenue, Halifax (N.S.) CANADA B3H 4P9</affiliation>
        </affiliations>
        <titre>Extraction de paraphrases sémantiques et lexico-syntaxiques de corpus parallèles bilingues</titre>
        <type>long</type>
        <pages/>
        <resume>Nous présentons le travail en cours effectué dans le cadre d’un projet d’extraction de paraphrases à partir de textes parallèles bilingues. Nous identifions des paraphrases sémantiques et lexico-syntaxiques, qui mettent en jeu des opérations relativement complexes sur les structures sémantiques et syntaxiques de phrases, et les décrivons au moyen de règles de paraphrasage de type Sens-Texte, utilisables dans diverses applications de TALN.</resume>
        <mots_cles>paraphrase lexico-syntaxique, paraphrase sémanique, règles de paraphrasage, corpus bilingues, théorie Sens-Texte</mots_cles>
        <title/>
        <abstract>We present work in progress done within a project of extracting paraphrases from parallel bilingual texts. We identify semantic and lexical-syntactic paraphrases, which imply relatively complex operations on semantic and syntactic structures of sentences, and describe them by means of paraphrasing rules of Meaning-Text type, usable in various NLP applications.</abstract>
        <keywords>lexical-syntactic paraphrase, semantic paraphrase, paraphrasing rules, bilingual corpora, Meaning-Text theory</keywords>
      </article>
      <article id="taln-2010-long-016" session="Sémantique">
        <auteurs>
          <auteur>
            <nom>Lydia-Mai Ho-Dac</nom>
            <email>lydia.ho-dac@uclouvain.be</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Marie-Paule Péry-Woodley</nom>
            <email>pery@univ-tlse2.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Ludovic Tanguy</nom>
            <email>tanguy@univ-tlse2.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">VALIBEL, UCL et FNRS</affiliation>
          <affiliation affiliationId="2">CLLE-ERSS, Université de Toulouse</affiliation>
        </affiliations>
        <titre>Anatomie des structures énumératives</titre>
        <type>long</type>
        <pages/>
        <resume>Cet article présente les premiers résultats d’une campagne d’annotation de corpus à grande échelle réalisée dans le cadre du projet ANNODIS. Ces résultats concernent la partie descendante du dispositif d’annotation, et plus spécifiquement les structures énumératives. Nous nous intéressons à la structuration énumérative en tant que stratégie de base de mise en texte, apparaissant à différents niveaux de granularité, associée à différentes fonctions discursives, et signalée par des indices divers. Avant l’annotation manuelle, une étape de pré-traitement a permis d’obtenir le marquage systématique de traits associés à la signalisation de l’organisation du discours. Nous décrivons cette étape de marquage automatique, ainsi que la procédure d’annotation. Nous proposons ensuite une première typologie des structures énumératives basée sur la description quantitative des données annotées manuellement, prenant en compte la couverture textuelle, la composition et les types d’indices.</resume>
        <mots_cles>Annotation de corpus, organisation du discours, structure énumérative, signalisation</mots_cles>
        <title/>
        <abstract>This paper presents initial results from a large scale discourse annotation project, the ANNODIS project. These results concern the top-down part of the annotation scheme, and more specifically enumerative structures. We are interested in enumerative structures as a basic text construction strategy, occurring at different levels of granularity, associated with various discourse functions, and signalled by a broad range of cues. Before manual annotation via a purpose-built interface, a pre-processing phase produced a systematic mark-up of features associated to the signalling of discourse organisation. We describe this markup phase and the annotation procedure. We then propose a first typology of enumerative structures based on a quantitative description of the manually annotated data, taking into account textual coverage, composition, types of cues.</abstract>
        <keywords>Corpus annotation, discourse organisation, enumerative structure, signalling text structures</keywords>
      </article>
      <article id="taln-2010-long-017" session="Sémantique">
        <auteurs>
          <auteur>
            <nom>Fadila Hadouche</nom>
            <email>hadouchf@iro.umontreal.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Guy Lapalme</nom>
            <email>lapalme@iro.umontreal</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Marie-Claude L’Homme</nom>
            <email>mc.lhomme@umontreal.ca</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">RALI, Université de Montréal, C.P. 6128, succursale Centre-ville, Montréal, Québec, Canada H3C 3J7</affiliation>
          <affiliation affiliationId="2">OLST, Université de Montréal, C.P. 6128, succursale Centre-ville, Montréal, Québec, Canada H3C 3J7</affiliation>
        </affiliations>
        <titre>Identification des actants et circonstants par apprentissage machine</titre>
        <type>long</type>
        <pages/>
        <resume>Dans cet article, nous traitons de l’identification automatique des participants actants et circonstants de lexies prédicatives verbales tirées d’un corpus spécialisé en langue française. Les actants contribuent à la réalisation du sens de la lexie alors que les circonstants sont optionnels : ils ajoutent une information supplémentaire qui ne fait pas partie intégrante du sémantisme de la lexie. Nous proposons une classification de ces participants par apprentissage machine basée sur un corpus de lexies verbales du domaine de l’informatique, lexies qui ont été annotées manuellement avec des rôles sémantiques. Nous présentons des features qui nous permettent d’identifier les participants et de distinguer les actants des circonstants.</resume>
        <mots_cles>Structure actancielle, actants et circonstants, features de classification</mots_cles>
        <title/>
        <abstract>In this paper we discuss the identification of participants actants and circumstants of specialized verbal lexical units in a French specialised corpus. The actants are linguistic units that contribute to the sense of the verbal lexical unit while circumstants are optional: they add extra information that is not part of the meaning of the verbal unit. In this work, we propose a classification of participants using machine learning based on a specialized corpus of verbal lexical items in the field of computing which are annotated manually with semantic roles labels. We defined features to identify participants and distinguish actants from circumstants.</abstract>
        <keywords>Actantial structure, actants and circumstants, classification features</keywords>
      </article>
      <article id="taln-2010-long-018" session="Sémantique">
        <auteurs>
          <auteur>
            <nom>Marie-Noëlle Bessagnet</nom>
            <email>marie-noelle.bessagnet@univ-pau.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Mauro Gaio</nom>
            <email>mauro.gaio@univ-pau.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Eric Kergosien</nom>
            <email>eric.kergosien@univ-pau.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Christian Sallaberry</nom>
            <email>christian.sallaberry@univ-pau.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire LIUPPA, UPPA, IAE, Avenue du doyen Poplawski, 64012 PAU</affiliation>
          <affiliation affiliationId="2">Laboratoire LIUPPA, UPPA, Faculté des Sciences, Département Informatique, 64000 PAU</affiliation>
        </affiliations>
        <titre>Extraction automatique d'un lexique à connotation géographique à des fins ontologiques dans un corpus de récits de voyage</titre>
        <type>long</type>
        <pages/>
        <resume>Le but de ces travaux est d’extraire un lexique en analysant les relations entre des syntagmes nominaux et des syntagmes verbaux dans les textes de notre corpus, essentiellement des récits de voyage. L’hypothèse que nous émettons est de pouvoir établir une catégorisation des syntagmes nominaux associés à des Entités Nommées de type lieu à l’aide de l’analyse des relations verbales. En effet, nous disposons d’une chaine de traitement automatique qui extrait, interprète et valide des Entités Nommées de type lieu dans des documents textuels. Ce travail est complété par l’analyse des relations verbales associées à ces EN, candidates à l’enrichissement d’une ontologie.</resume>
        <mots_cles>Entité nommée, ontologie, relations verbales, patrons linguistiques</mots_cles>
        <title/>
        <abstract>The aim of this research work is to extract a lexicon by analyzing the relationship between nominal syntagms and verb construction within our corpus, namely travel stories. We would like to establish a categorization of nominal syntagms linked to Named Entity (NE) (type space) thanks to verbal relationships analysis. In fact, we develop a computerized process flow in order to extract, to interpret and to validate NE of type space in textual documents. This research work is completed by the analyze of verbal relationships linked to these EN which could enrich our ontology.</abstract>
        <keywords>Named Entity, ontology, verbal relations, language patterns</keywords>
      </article>
      <article id="taln-2010-long-019" session="Parole">
        <auteurs>
          <auteur>
            <nom>Stanislas Oger</nom>
            <email>stanislas.oger@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Mickael Rouvier</nom>
            <email>mickael.rouvier@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Georges Linarès</nom>
            <email>georges.linares@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIA, Université d’Avignon, France</affiliation>
        </affiliations>
        <titre>Classification du genre vidéo reposant sur des transcriptions automatiques</titre>
        <type>long</type>
        <pages/>
        <resume>Dans cet article nous proposons une nouvelle méthode pour l’identification du genre vidéo qui repose sur une analyse de leur contenu linguistique. Cette approche consiste en l’analyse des mots apparaissant dans les transcriptions des pistes audio des vidéos, obtenues à l’aide d’un système de reconnaissance automatique de la parole. Les expériences sont réalisées sur un corpus composé de dessins animés, de films, de journaux télévisés, de publicités, de documentaires, d’émissions de sport et de clips de musique. L’approche proposée permet d’obtenir un taux de bonne classification de 74% sur cette tâche. En combinant cette approche avec des méthodes reposant sur des paramètres acoustiques bas-niveau, nous obtenons un taux de bonne classification de 95%.</resume>
        <mots_cles>classification de genre vidéo, traitement audio de la vidéo, extraction de paramètres linguistiques</mots_cles>
        <title/>
        <abstract>In this paper, we present a new method for video genre identification based on the linguistic content analysis. This approach relies on the analysis of the words in the video transcriptions provided by an automatic speech recognition system. Experiments are conducted on a corpus composed of cartoons, movies, news, commercials, documentary, sport and music. On this 7-genre identification task, the proposed transcription-based method obtains up to 74% of correct identification. Finally, this rate is increased to 95% by combining the proposed linguistic-level features with low-level acoustic features.</abstract>
        <keywords>video genre classification, audio-based video processing, linguistic feature extraction</keywords>
      </article>
      <article id="taln-2010-long-020" session="Parole">
        <auteurs>
          <auteur>
            <nom>Christian Raymond</nom>
            <email>Christian.Raymond@irisa.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Julien Fayolle</nom>
            <email>Julien.Fayolle@irisa.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université Européenne de Bretagne, INRIA,IRISA, UMR 6074, France</affiliation>
          <affiliation affiliationId="2">INSA de Rennes, 20 Avenue des buttes de coesme, Rennes, France</affiliation>
        </affiliations>
        <titre>Reconnaissance robuste d’entités nommées sur de la parole transcrite automatiquement</titre>
        <type>long</type>
        <pages/>
        <resume>Les transcriptions automatiques de parole constituent une ressource importante, mais souvent bruitée, pour décrire des documents multimédia contenant de la parole (e.g. journaux télévisés). En vue d’améliorer la recherche documentaire, une étape d’extraction d’information à caractère sémantique, précédant l’indexation, permet de faire face au problème des transcriptions imparfaites. Parmis ces contenus informatifs, on compte les entités nommées (e.g. noms de personnes) dont l’extraction est l’objet de ce travail. Les méthodes traditionnelles de reconnaissance basées sur une définition manuelle de grammaires formelles donnent de bons résultats sur du texte ou des transcriptions propres manuellement produites, mais leurs performances se trouvent fortement affectées lorsqu’elles sont appliquées sur des transcriptions automatiques. Nous présentons, ici, trois méthodes pour la reconnaissance d’entités nommées basées sur des algorithmes d’apprentissage automatique : les champs conditionnels aléatoires, les machines à de support, et les transducteurs à états finis. Nous présentons également une méthode pour rendre consistantes les données d’entrainement lorsqu’elles sont annotées suivant des conventions légèrement différentes. Les résultats montrent que les systèmes d’étiquetage obtenus sont parmi les plus robustes sur les données d’évaluation de la campagne ESTER 2 dans les conditions où la transcription automatique est particulièrement bruitée.</resume>
        <mots_cles>étiqueteur d’entités nommées, transcription automatique de parole, apprentissage automatique, champs conditionnels aléatoires, machines à vecteurs de support, transducteurs à états finis</mots_cles>
        <title/>
        <abstract>Automatic speech transcripts are an important, but noisy, ressource to index spoken multimedia documents (e.g. broadcast news). In order to improve both indexation and information retrieval, extracting semantic information from these erroneous transcripts is an interesting challenge. Among these meaningful contents, there are named entities (e.g. names of persons) which are the subject of this work. Traditional named entity taggers are based on manual and formal grammars. They obtain correct performance on text or clean manual speech transcripts, but they have a lack of robustness when applied on automatic transcripts. We are introducing, in this work, three methods for named entity recognition based on machine learning algorithms, namely conditional random fields, support vector machines, and finitestate transducers. We are also introducing a method to make consistant the training data when they are annotated with slightly different conventions. We show that our tagger systems are among the most robust when applied to the evaluation data of the French ESTER 2 campaign in the most difficult conditions where transcripts are particularly noisy.</abstract>
        <keywords>named entity tagger, automatic speech recognition transcripts, machine learning, conditionnal random fields, support vector machines, finite-state transducers</keywords>
      </article>
      <article id="taln-2010-long-021" session="Parole">
        <auteurs>
          <auteur>
            <nom>Younès Bahou</nom>
            <email>bahou_younes@yahoo.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Abir Masmoudi</nom>
            <email>masmoudiabir@gmail.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Lamia Hadrich Belguith</nom>
            <email>l.belguith@fsegs.rnu.tn</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">ANLP Research Group – Laboratoire MIRACL, Faculté des Sciences Economiques et de Gestion de Sfax, B.P. 1088, 3018 - Sfax – TUNISIE</affiliation>
        </affiliations>
        <titre>Traitement des disfluences dans le cadre de la compréhension automatique de l’oral arabe spontané</titre>
        <type>long</type>
        <pages/>
        <resume>Les disfluences inhérents de toute parole spontanée sont un vrai défi pour les systèmes de compréhension de la parole. Ainsi, nous proposons dans cet article, une méthode originale pour le traitement des disfluences (plus précisément, les autocorrections, les répétitions, les hésitations et les amorces) dans le cadre de la compréhension automatique de l’oral arabe spontané. Notre méthode est basée sur une analyse à la fois robuste et partielle, des énoncés oraux arabes. L’idée consiste à combiner une technique de reconnaissance de patrons avec une analyse sémantique superficielle par segments conceptuels. Cette méthode a été testée à travers le module de compréhension du système SARF, un serveur vocal interactif offrant des renseignements sur le transport ferroviaire tunisien (Bahou et al., 2008). Les résultats d’évaluation de ce module montrent que la méthode proposée est très prometteuse. En effet, les mesures de rappel, de précision et de F-Measure sont respectivement de 79.23%, 74.09% et 76.57%.</resume>
        <mots_cles>disfluences, segment conceptuel, reconnaissance de patrons, parole arabe spontanée</mots_cles>
        <title/>
        <abstract>The disfluencies inherent in spontaneous speaking are a real challenge for speech understanding systems. Thus, we propose in this paper, an original method for processing disfluencies (more precisely, self-corrections, repetitions, hesitations and word-fragments) in the context of automatic spontaneous Arabic speech understanding. Our method is based on a robust and partial analysis of Arabic oral utterances. The main idea is to combine a pattern matching technique and a surface semantic analysis with conceptual segments. This method has been evaluated through the understanding module of SARF system, an interactive vocal server offering Tunisian railway information (Bahou et al., 2008). The evaluation results of this module show that the proposed method is very promising. Indeed, the measures of recall, precision and F-Measure are respectively 79.23%, 74.09% and 76.57%.</abstract>
        <keywords>disfluencies, conceptual segment, pattern matching, spontaneous Arabic speech</keywords>
      </article>
      <article id="taln-2010-long-022" session="Segmentation">
        <auteurs>
          <auteur>
            <nom>Camille Guinaudeau</nom>
            <email>camille.guinaudeau@irisa.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Guillaume Gravier</nom>
            <email>guillaume.gravier@irisa.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Pascale Sébillot</nom>
            <email>pascale.sebillot@irisa.fr</email>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">INRIA Rennes, France</affiliation>
          <affiliation affiliationId="2">IRISA (CNRS), France</affiliation>
          <affiliation affiliationId="3">IRISA (INSA), France</affiliation>
        </affiliations>
        <titre>Utilisation de relations sémantiques pour améliorer la segmentation thématique de documents télévisuels</titre>
        <type>long</type>
        <pages/>
        <resume>Les méthodes de segmentation thématique exploitant une mesure de la cohésion lexicale peuvent être appliquées telles quelles à des transcriptions automatiques de programmes télévisuels. Cependant, elles sont moins efficaces dans ce contexte, ne prenant en compte ni les particularités des émissions TV, ni celles des transcriptions. Nous étudions ici l’apport de relations sémantiques pour rendre les techniques de segmentation thématique plus robustes. Nous proposons une méthode pour exploiter ces relations dans une mesure de la cohésion lexicale et montrons qu’elles permettent d’augmenter la F1-mesure de +1.97 et +11.83 sur deux corpus composés respectivement de 40h de journaux télévisés et de 40h d’émissions de reportage. Ces améliorations démontrent que les relations sémantiques peuvent rendre les méthodes de segmentation moins sensibles aux erreurs de transcription et au manque de répétitions constaté dans certaines émissions télévisées.</resume>
        <mots_cles>Segmentation thématique, documents oraux, cohésion lexicale, relations sémantiques</mots_cles>
        <title/>
        <abstract>Topic segmentation methods based on a measure of the lexical cohesion can be applied as is to automatic transcripts of TV programs. However, these methods are less effective in this context as neither the specificities of TV contents, nor those of automatic transcripts are considered. The aim of this paper is to study the use of semantic relations to make segmentation techniques more robust.We propose a method to account for semantic relations in a measure of the lexical cohesion.We show that such relations increase the F1-measure by +1.97 and +11.83 for two data sets consisting of respectively 40h of news and 40h of longer reports on current affairs. These results demonstrate that semantic relations can make segmentation methods less sensitive to transcription errors or to the lack of repetitions in some television programs.</abstract>
        <keywords>Topic segmentation, spoken document, lexical cohesion, semantic relations</keywords>
      </article>
      <article id="taln-2010-long-023" session="Segmentation">
        <auteurs>
          <auteur>
            <nom>Clémentine Adam</nom>
            <email>adam@univ-tlse2.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Philippe Muller</nom>
            <email>muller@irit.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Cécile Fabre</nom>
            <email>cfabre@univ-tlse2.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CLLE / Université de Toulouse</affiliation>
          <affiliation affiliationId="2">IRIT / Université de Toulouse &amp; Alpage / INRIA</affiliation>
        </affiliations>
        <titre>Une évaluation de l’impact des types de textes sur la tâche de segmentation thématique</titre>
        <type>long</type>
        <pages/>
        <resume>Cette étude a pour but de contribuer à la définition des objectifs de la segmentation thématique (ST), en incitant à prendre en considération le paramètre du type de textes dans cette tâche. Notre hypothèse est que, si la ST est certes pertinente pour traiter certains textes dont l’organisation est bien thématique, elle n’est pas adaptée à la prise en compte d’autres modes d’organisation (temporelle, rhétorique), et ne peut pas être appliquée sans précaution à des textes tout-venants. En comparant les performances d’un système de ST sur deux corpus, à organisation thématique "forte" et "faible", nous montrons que cette tâche est effectivement sensible à la nature des textes.</resume>
        <mots_cles>Segmentation thématique, organisation textuelle, cohésion lexicale, voisins distributionnels</mots_cles>
        <title/>
        <abstract>This paper aims to contribute to a better definition of the requirements of the text segmentation task, by stressing the need for taking into account the types of texts that can be appropriately considered. Our hypothesis is that while TS is indeed relevant to analyse texts with a thematic organisation, this task is ill-fitted to deal with other modes of text organisation (temporal, rhetorical, etc.). By comparing the performance of a TS system on two corpora, with either a "strong" or a "weak" thematic organisation, we show that TS is sensitive to text types.</abstract>
        <keywords>Text segmentation, textual organisation, lexical cohesion, distributional neighbours</keywords>
      </article>
      <article id="taln-2010-long-024" session="Segmentation">
        <auteurs>
          <auteur>
            <nom>Ludovic Jean-Louis</nom>
            <email>ludovic.jean-louis@cea.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Romaric Besançon</nom>
            <email>romaric.besancon@cea.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Olivier Ferret</nom>
            <email>olivier.ferret@cea.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CEA, LIST, Laboratoire Vision et Ingénierie des Contenus, Fontenay-aux-Roses, F-92265, France</affiliation>
        </affiliations>
        <titre>Utilisation d’indices temporels pour la segmentation événementielle de textes</titre>
        <type>long</type>
        <pages/>
        <resume>Dans le domaine de l’Extraction d’Information, une place importante est faite à l’extraction d’événements dans des dépêches d’actualité, particulièrement justifiée dans le contexte d’applications de veille. Or il est fréquent qu’une dépêche d’actualité évoque plusieurs événements de même nature pour les comparer. Nous proposons dans cet article d’étudier des méthodes pour segmenter les textes en séparant les événements, dans le but de faciliter le rattachement des informations pertinentes à l’événement principal. L’idée est d’utiliser des modèles d’apprentissage statistique exploitant les marqueurs temporels présents dans les textes pour faire cette segmentation. Nous présentons plus précisément deux modèles (HMM et CRF) entraînés pour cette tâche et, en faisant une évaluation de ces modèles sur un corpus de dépêches traitant d’événements sismiques, nous montrons que les méthodes proposées permettent d’obtenir des résultats au moins aussi bons que ceux d’une approche ad hoc, avec une approche beaucoup plus générique.</resume>
        <mots_cles>Extraction d’information, extraction d’événements, segmentation de textes, indices temporels, apprentissage statistique</mots_cles>
        <title/>
        <abstract>One of the early application of Information Extraction, motivated by the needs for intelligence tools, is the detection of events in news articles. But this detection may be difficult when news articles mention several occurrences of events of the same kind, which is often done for comparison purposes. We propose in this article new approaches to segment the text of news articles in units relative to only one event, in order to help the identification of relevant information associated to the main event of the news. We present two approaches that use statistical machine learning models (HMM and CRF) exploiting temporal information extracted from the texts as a basis for this segmentation. The evaluation of these approaches in the domain of seismic events show that with a robust and generic approach, we can achieve results at least as good as results obtained with an ad hoc approach.</abstract>
        <keywords>Information extraction, event extraction, text segmentation, temporal cues, statistical machine learning</keywords>
      </article>
      <article id="taln-2010-long-025" session="Résumé/Extraction">
        <auteurs>
          <auteur>
            <nom>Juan-Manuel Torres-Moreno</nom>
            <email>juan-manuel.torres@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Horacio Saggion</nom>
            <email>horacio.saggion@upf.edu</email>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Iria da Cunha</nom>
            <email>iria.dacunha@upf.edu</email>
            <affiliationId>1</affiliationId>
            <affiliationId>4</affiliationId>
          </auteur>
          <auteur>
            <nom>Patricia Velázquez-Morales</nom>
            <email/>
            <affiliationId>5</affiliationId>
          </auteur>
          <auteur>
            <nom>Eric Sanjuan</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIA, Université d’Avignon et des Pays de Vaucluse, Avignon, France</affiliation>
          <affiliation affiliationId="2">Ecole Polytechnique de Montréal, (Québec) Canada</affiliation>
          <affiliation affiliationId="3">DTIC, Universtitat Pompeu Fabra, Barcelona, Espagne</affiliation>
          <affiliation affiliationId="4">IULA, Universitat Pompeu Fabra, Barcelona, Espagne</affiliation>
          <affiliation affiliationId="5">VM Labs, Avignon, France</affiliation>
        </affiliations>
        <titre>Évaluation automatique de résumés avec et sans référence</titre>
        <type>long</type>
        <pages/>
        <resume>Nous étudions différentes méthodes d’évaluation de résumé de documents basées sur le contenu. Nous nous intéressons en particulier à la corrélation entre les mesures d’évaluation avec et sans référence humaine. Nous avons développé FRESA, un nouveau système d’évaluation fondé sur le contenu qui calcule les divergences entre les distributions de probabilité. Nous appliquons notre système de comparaison aux diverses mesures d’évaluation bien connues en résumé de texte telles que la Couverture, Responsiveness, Pyramids et Rouge en étudiant leurs associations dans les tâches du résumé multi-document générique (francais/anglais), focalisé (anglais) et résumé mono-document générique (français/espagnol).</resume>
        <mots_cles>Mesures d’évaluation, Résumé automatique de textes</mots_cles>
        <title/>
        <abstract>We study document-summary content-based evaluation methods in text summarization and we investigate the correlation among evaluation measures with and without human models. We apply our comparison framework to various well-established content-based evaluation measures in text summarization such as Coverage, Responsiveness, Pyramids and Rouge studying their associations in various text summarization tasks including generic (English/French) and focus-based (English) multi-document summarization and generic multi and single-document summarization (French/Spanish). The research is carried out using the new content-based evaluation framework FRESA to compute the divergences among probability distributions.</abstract>
        <keywords>Evaluation measures, Text Automatic Summarization</keywords>
      </article>
      <article id="taln-2010-long-026" session="Résumé/Extraction">
        <auteurs>
          <auteur>
            <nom>Pierre-Etienne Genest</nom>
            <email>genestpe@iro.umontreal.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Guy Lapalme</nom>
            <email>lapalme@iro.umontreal.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Mehdi Yousfi-Monod</nom>
            <email>yousfim@iro.umontreal.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">RALI-DIRO, Université de Montréal, B.P. 6128, Centre-Ville, Montréal, Québec, Canada, H3C 3J7</affiliation>
        </affiliations>
        <titre>Jusqu’où peut-on aller avec les méthodes par extraction pour la rédaction de résumés?</titre>
        <type>long</type>
        <pages/>
        <resume>La majorité des systèmes de résumés automatiques sont basés sur l’extraction de phrases, or on les compare le plus souvent avec des résumés rédigés manuellement par abstraction. Nous avons mené une expérience dans le but d’établir une limite supérieure aux performances auxquelles nous pouvons nous attendre avec une approche par extraction. Cinq résumeurs humains ont composé 88 résumés de moins de 100 mots, en extrayant uniquement des phrases présentes intégralement dans les documents d’entrée. Les résumés ont été notés sur la base de leur contenu, de leur niveau linguistique et de leur qualité globale par les évaluateurs de NIST dans le cadre de la compétition TAC 2009. Ces résumés ont obtenus de meilleurs scores que l’ensemble des 52 systèmes automatiques participant à la compétition, mais de nettement moins bons que ceux obtenus par les résumeurs humains pouvant formuler les phrases de leur choix dans le résumé. Ce grand écart montre l’insuffisance des méthodes par extraction pure.</resume>
        <mots_cles>Résumés automatiques, résumés par extraction, résumés manuels</mots_cles>
        <title/>
        <abstract>The majority of automatic summarization systems are based on sentence extraction, whereas they are usually compared with human-written, abstractive summaries. We have thus conducted an experiment to establish an upper bound on the expected performance of extractive summarization. 5 human summarizers completed 88 summaries of no more than 100 words from unedited sentences of the source documents. The summaries were scored based on their content, linguistic quality and overall responsiveness by NIST annotators in the context of the TAC 2009 competition. The human extracts received better scores than all of the 52 participating automatic systems, but much lower scores than those obtained by human summarizers free to use abstraction. This large gap shows that pure extraction methods are insufficient for summarization.</abstract>
        <keywords>Automatic summarization, extractive summarization, manual summarization</keywords>
      </article>
      <article id="taln-2010-long-027" session="Résumé/Extraction">
        <auteurs>
          <auteur>
            <nom>Anne Garcia-Fernandez</nom>
            <email>annegf@limsi.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Sophie Rosset</nom>
            <email>rosset@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Anne Vilnat</nom>
            <email>vilnat@limsi.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIMSI-CNRS B.P. 133 91403 Orsay Cedex</affiliation>
          <affiliation affiliationId="2">Université Paris Sud 11 Orsay</affiliation>
        </affiliations>
        <titre>Comment formule-t-on une réponse en langue naturelle ?</titre>
        <type>long</type>
        <pages/>
        <resume>Cet article présente l’étude d’un corpus de réponses formulées par des humains à des questions factuelles. Des observations qualitatives et quantitatives sur la reprise d’éléments de la question dans les réponses sont exposées. La notion d’information-réponse est introduite et une étude de la présence de cet élément dans le corpus est proposée. Enfin, les formulations des réponses sont étudiées.</resume>
        <mots_cles>systèmes de réponse à une question, variations linguistiques, réponse en langue naturelle, oral et écrit</mots_cles>
        <title/>
        <abstract>This paper presents the study of a corpus of human answers to factual questions. Observations of how and how much question elements are used in the answer are done. We define the concept of “information-answer” and study its presence in the corpus. Finally, answer formulations are shown.</abstract>
        <keywords>question-answering systems, linguistics variations, natural language answer, oral and written</keywords>
      </article>
      <article id="taln-2010-long-028" session="Traduction">
        <auteurs>
          <auteur>
            <nom>Do Thi Ngoc Diep</nom>
            <email>thi-ngoc-diep.do@imag.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Laurent Besacier</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Eric Castelli</nom>
            <email/>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire LIG, GETALP, Grenoble, France</affiliation>
          <affiliation affiliationId="2">Centre MICA, CNRS/UMI-2954, Hanoi, Vietnam</affiliation>
        </affiliations>
        <titre>Apprentissage non supervisé pour la traduction automatique : application à un couple de langues peu doté</titre>
        <type>long</type>
        <pages/>
        <resume>Cet article présente une méthode non-supervisée pour extraire des paires de phrases parallèles à partir d’un corpus comparable. Un système de traduction automatique est utilisé pour exploiter le corpus comparable et détecter les paires de phrases parallèles. Un processus itératif est exécuté non seulement pour augmenter le nombre de paires de phrases parallèles extraites, mais aussi pour améliorer la qualité globale du système de traduction. Une comparaison avec une méthode semi-supervisée est présentée également. Les expériences montrent que la méthode non-supervisée peut être réellement appliquée dans le cas où on manque de données parallèles. Bien que les expériences préliminaires soient menées sur la traduction français-anglais, cette méthode non-supervisée est également appliquée avec succès à un couple de langues peu doté : vietnamien-français.</resume>
        <mots_cles>apprentissage non-supervisé, système de traduction automatique, corpus comparable, paires de phrases parallèles</mots_cles>
        <title/>
        <abstract>This paper presents an unsupervised method for extracting parallel sentence pairs from a comparable corpus. A translation system is used to mine and detect the parallel sentence pairs from the comparable corpus. An iterative process is implemented not only to increase the number of extracted parallel sentence pairs but also to improve the overall quality of the translation system. A comparison between this unsupervised method and a semi-supervised method is also presented. The experiments conducted show that the unsupervised method can be really applied in cases where parallel data are not available. While preliminary experiments are conducted on French-English translation, this unsupervised method is also applied successfully to a low e-resourced language pair (Vietnamese-French).</abstract>
        <keywords>unsupervised training, machine translation, comparable corpus, parallel sentence pairs</keywords>
      </article>
      <article id="taln-2010-long-029" session="Traduction">
        <auteurs>
          <auteur>
            <nom>Ahmed El Kholy</nom>
            <email>akholy@ccls.columbia.edu</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Nizar Habash</nom>
            <email>habash@ccls.columbia.edu</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Center for Computational Learning Systems, Columbia University</affiliation>
        </affiliations>
        <titre/>
        <type>long</type>
        <pages/>
        <resume>De nombreux travaux en Traduction Automatique Statistique (TAS) pour des langues d’entrée morphologiquement riches montrent que la ségmentation morphologique et la normalisation orthographique améliorent la qualité des traductions en diminuant la sparsité des données. Dans cet article, nous étudions l’impact de ce prétraitement pour la TAS vers une langue de sortie riche morphologiquement, comme l’Arabe. Nous explorons l’espace des schémas de segmentation et des options de normalisation possibles. Nous évaluons seulement la sortie sous une forme désegmentée et enrichie orthographiquement. Nos résultats montrent d’une part que le meilleur schéma pour la ségmentation est celui de la Penn Arabic Treebank. D’autre part, la meilleure procédure de prétraitement consiste à entraîner le système sur des données normalisées orthographiquement, puis à enrichir et désegmenter les traductions en sortie.</resume>
        <mots_cles>Langue Arabe, Morphologie, Ségmentation, Désegmentation, La Traduction Automatique Statistique</mots_cles>
        <title>Orthographic and Morphological Processing for English-Arabic Statistical Machine Translation</title>
        <abstract>Much of the work on Statistical Machine Translation (SMT) from morphologically rich languages has shown that morphological tokenization and orthographic normalization help improve SMT quality because of the sparsity reduction they contribute. In this paper, we study the effect of these processes on SMT when translating into a morphologically rich language, namely Arabic.We explore a space of tokenization schemes and normalization options. We only evaluate on detokenized and orthographically correct (enriched) output. Our results show that the best performing tokenization scheme is that of the Penn Arabic Treebank. Additionally, training on orthographically normalized (reduced) text then jointly enriching and detokenizing the output outperforms training on enriched text.</abstract>
        <keywords>Arabic Language, Morphology, Tokenization, Detokenization, Statistical Machine Translation</keywords>
      </article>
      <article id="taln-2010-long-030" session="Traduction">
        <auteurs>
          <auteur>
            <nom>Marine Carpuat</nom>
            <email>marine@ccls.columbia.edu</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Yuval Marton</nom>
            <email>ymarton@ccls.columbia.edu</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Nizar Habash</nom>
            <email>habash@ccls.columbia.edu</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Columbia University, Center for Computational Learning Systems, 475 Riverside Drive, New York, NY 10115</affiliation>
        </affiliations>
        <titre/>
        <type>long</type>
        <pages/>
        <resume>Distinguer les constructions verbe-sujet (VS) des propositions principales (“matrice”) et subordonnées (“non-matrice”) améliore notre nouveau modèle de réordonnancement pour l’alignement des mots en Traduction Automatique Statistique (TAS) arabe-anglais (Carpuat et al., 2010). D’une part, la majorité des constructions verbe-sujet (VS) dans les propositions principales doivent être réordonnancées en anglais, alors que l’ordre du verbe et du sujet est préservé dans la moitié des cas de constructions VS subordonnées. D’autre part, nous constatons que notre analyseur syntaxique parvient à mieux identifier les constructions VS des propositions principales. Ces observations nous amènent à limiter le réordonnancement des constructions VS à celles des propositions principales lors de l’alignement des mots. Cette technique améliore substantiellement la performance d’un système de TAS conventionnel, et d’un système qui réordonnance toutes les constructions VS. L’amélioration des mesures BLEU et TER obtenue par simple réordonnancement représente presque la moitié de l’amélioration obtenue lorsque le modèle d’alignement des mots est entraîné sur un corpus parallèle d’une taille cinq fois supérieure.</resume>
        <mots_cles>Analyse morpho-syntaxique de l’arabe, Traduction automatique statistique, VS, VSO</mots_cles>
        <title>Reordering Matrix Post-verbal Subjects for Arabic-to-English SMT</title>
        <abstract>We improve our recently proposed technique for integrating Arabic verb-subject constructions in SMT word alignment (Carpuat et al., 2010) by distinguishing between matrix (or main clause) and non-matrix Arabic verb-subject constructions. In gold translations, most matrix VS (main clause verb-subject) constructions are translated in inverted SV order, while non-matrix (subordinate clause) VS constructions are inverted in only half the cases. In addition, while detecting verbs and their subjects is a hard task, our syntactic parser detects VS constructions better in matrix than in non-matrix clauses. As a result, reordering only matrix VS for word alignment consistently improves translation quality over a phrase-based SMT baseline, and over reordering all VS constructions, in both medium- and large-scale settings. In fact, the improvements obtained by reordering matrix VS on the medium-scale setting remarkably represent 44% of the gain in BLEU and 51% of the gain in TER obtained with a word alignment training bitext that is 5 times larger.</abstract>
        <keywords/>
      </article>
      <article id="taln-2010-long-031" session="Table ronde">
        <auteurs>
          <auteur>
            <nom>Michael Zock</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Guy Lapalme</nom>
            <email/>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Equipe TALEP, LIF, CNRS, UMR 6166, Case 901 - 163 Avenue de Luminy, F-13288 Marseille Cedex 9</affiliation>
          <affiliation affiliationId="2">RALI-DIRO, Université de Montréal, CP 6128, Succ. Centre-Ville, Montréal, Québec, Canada H3C 3J7</affiliation>
        </affiliations>
        <titre>Du TAL au TIL</titre>
        <type>long</type>
        <pages/>
        <resume>Historiquement deux types de traitement de la langue ont été étudiés: le traitement par le cerveau (approche psycholinguistique) et le traitement par la machine (approche TAL). Nous pensons qu’il y a place pour un troisième type: le traitement interactif de la langue (TIL), l’ordinateur assistant le cerveau. Ceci correspond à un besoin réel dans la mesure où les gens n’ont souvent que des connaissances partielles par rapport au problème à résoudre. Le but du TIL est de construire des ponts entre ces connaissances momentanées d’un utilisateur et la solution recherchée. À l'aide de quelques exemples, nous essayons de montrer que ceci est non seulement faisable et souhaitable, mais également d’un coût très raisonnable.</resume>
        <mots_cles>traitement interactif de la langue, prise en compte de l'usager, outils de traitement de la langue, apprentissage des langues, dictionnaires, livres de phrases, concordanciers, traduction</mots_cles>
        <title/>
        <abstract>Historically two types of NLP have been investigated: fully automated processing of language by machines (NLP) and autonomous processing of natural language by people, i.e. the human brain (psycholinguistics). We believe that there is room and need for another kind, INLP: interactive natural language processing. This intermediate approach starts from peoples’ needs, trying to bridge the gap between their actual knowledge and a given goal. Given the fact that peoples’ knowledge is variable and often incomplete, the aim is to build bridges linking a given knowledge state to a given goal. We present some examples, trying to show that this goal is worth pursuing, achievable and at a reasonable cost.</abstract>
        <keywords>interactive NLP (INLP), user interaction, tools for NLP, language learning, dictionaries, phrase books, concordancers, translation</keywords>
      </article>
      <article id="taln-2010-long-032" session="Plénière">
        <auteurs>
          <auteur>
            <nom>Piet Mertens</nom>
            <email>Piet.Mertens@arts.kuleuven.be</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Département de Linguistique, Université de Leuven, Belgique</affiliation>
        </affiliations>
        <titre>Restrictions de sélection et réalisations syntagmatiques dans DICOVALENCE Conversion vers un format utilisable en TAL</titre>
        <type>long</type>
        <pages/>
        <resume>Cet article décrit des modifications du dictionnaire de valence des verbes du français DICOVALENCE qui visent à le rendre neutre par rapport aux modèles syntaxiques, à expliciter certaines informations sur le cadre de sous-catégorisation et à le rendre ainsi directement utilisable en TAL. Les informations explicitées sont les suivantes : (a) les fonctions syntaxiques des arguments verbaux, (b) les restrictions de sélection portant sur ces arguments et (c) leurs réalisations syntagmatiques possibles. Les restrictions sont exprimées à l’aide de traits sémantiques. L’article décrit aussi le calcul de ces traits sémantiques à partir des paradigmes des pronoms (et d’éléments similaires) associés aux arguments. On obtient un format indépendant du modèle syntaxique, dont l’interprétation est transparente.</resume>
        <mots_cles>lexiques syntaxiques, restrictions de sélection, traits sémantiques</mots_cles>
        <title/>
        <abstract>This paper describes modifications to the verbal valency dictionary for French, known as DICOVALENCE, in order to obtain a theory-independent syntactic lexicon, to make explicit certain information about the slots in the valency frame, to facilitate the use of the lexicon in natural language processing. The modifications make explicit the following aspects: (a) the syntactic function of the slots, (b) the selection restrictions on these verbal arguments, (c) their possible phrasal realizations. Selection restrictions are expressed using semantic features. The article describes how these features are obtained from the paradigms of pronouns (and similar elements) associated with the valency slots. This results in a format which is theoryindependent, with a transparent interpretation.</abstract>
        <keywords>lexical databases, selection restrictions, semantic features</keywords>
      </article>
      <article id="taln-2010-long-033" session="Plénière">
        <auteurs>
          <auteur>
            <nom>Caroline Barrière</nom>
            <email>caroline.barriere@nrc-cnrc.gc.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">ITI-CNR, Gatineau, Canada</affiliation>
        </affiliations>
        <titre>Recherche contextuelle d’équivalents en banque de terminologie</titre>
        <type>long</type>
        <pages/>
        <resume>Notre recherche démontre que l’utilisation du contenu d’un texte à traduire permet de mieux cibler dans une banque de terminologie les équivalents terminologiques pertinents à ce texte. Une banque de terminologie a comme particularité qu’elle catégorise ses entrées (fiches) en leur assignant un ou des domaines provenant d’une liste de domaines préétablie. La stratégie ici présentée repose sur l’utilisation de cette information sur les domaines. Un algorithme a été développé pour l’assignation automatique d’un profil de domaines à un texte. Celui-ci est combiné à un algorithme d’appariement entre les domaines d’un terme présent dans la banque de terminologie et le profil de domaines du texte. Pour notre expérimentation, des résumés bilingues (français et anglais) provenant de huit revues scientifiques nous fournissent un ensemble de 1130 paires d’équivalents terminologiques et le Grand Dictionnaire Terminologique (Office Québécois de la Langue Française) nous sert de ressource terminologique. Sur notre ensemble, nous démontrons une réduction de 75% du rang moyen de l’équivalent correct en comparaison avec un choix au hasard.</resume>
        <mots_cles>recherche contextuelle, équivalents terminologiques, banque de terminologie, désambiguïsation par domaine</mots_cles>
        <title/>
        <abstract>Our research shows the usefulness of taking into account the context of a term within a text to be translated to better find an appropriate term equivalent for it in a term bank. A term bank has the particularity of categorising its records by assigning them one or more domains from a pre-established list of domains. The strategy presented here uses this domain information. An algorithm has been developed to automatically assign a domain profile to a source text. It is then combined with another algorithm which finds a match between a term’s domains (as found in the term bank) and the text’s domain profile. For our experimentation, bilingual abstracts (French-English) from eight scientific journals provide 1130 pairs of term equivalents. The Grand Dictionnaire Terminologique (Office Québécois de la Langue Française) is used as a terminological ressource. On our data set, we show a reduction of 75% in the average rank of the correct equivalent, in comparison to a random choice.</abstract>
        <keywords>contextual search, term equivalents, term bank, domain-based disambiguation</keywords>
      </article>
      <article id="taln-2010-long-034" session="Plénière">
        <auteurs>
          <auteur>
            <nom>Guillaume Bonfante</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Bruno Guillaume</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Mathieu Morey</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Guy Perrier</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">INRIA Nancy-Grand Est - LORIA - Nancy-Université</affiliation>
        </affiliations>
        <titre>Réécriture de graphes de dépendances pour l’interface syntaxe-sémantique</titre>
        <type>long</type>
        <pages/>
        <resume>Nous définissons le beta-calcul, un calcul de réécriture de graphes, que nous proposons d’utiliser pour étudier les liens entre différentes représentations linguistiques. Nous montrons comment transformer une analyse syntaxique en une représentation sémantique par la composition de deux jeux de règles de beta-calcul. Le premier souligne l’importance de certaines informations syntaxiques pour le calcul de la sémantique et explicite le lien entre syntaxe et sémantique sous-spécifiée. Le second décompose la recherche de modèles pour les représentations sémantiques sous-spécifiées.</resume>
        <mots_cles>Dépendances, réécriture de graphes, interface syntaxe-sémantique, DMRS</mots_cles>
        <title/>
        <abstract>We define the beta-calculus, a graph-rewriting calculus, which we propose to use to study the links between different linguistic representations. We show how to transform a syntactic analysis into a semantic analysis via the composition of two sets of beta-calculus rules. The first one underlines the importance of some syntactic information to compute the semantics and clearly expresses the link between syntax and underspecified semantics. The second one breaks up the search for models of underspecified semantic representations.</abstract>
        <keywords>Dependencies, graph rewriting, syntax-semantics interface, DMRS</keywords>
      </article>
      <article id="taln-2010-long-035" session="Corpus">
        <auteurs>
          <auteur>
            <nom>Karën Fort</nom>
            <email>karen.fort@inist.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Claire François</nom>
            <email>claire.francois@inist.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Maha Ghribi</nom>
            <email>maha.ghribi@inist.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">INIST / CNRS, 2 allée de Brabois, 54500 Vandoeuvre-lès-Nancy</affiliation>
          <affiliation affiliationId="2">LIPN, Université Paris 13 &amp; CNRS, 99 av. J.B. Clément, 93430 Villetaneuse</affiliation>
        </affiliations>
        <titre>Évaluer des annotations manuelles dispersées : les coefficients sont-ils suffisants pour estimer l’accord inter-annotateurs ?</titre>
        <type>long</type>
        <pages/>
        <resume>L’objectif des travaux présentés dans cet article est l’évaluation de la qualité d’annotations manuelles de relations de renommage de gènes dans des résumés scientifiques, annotations qui présentent la caractéristique d’être très dispersées. Pour cela, nous avons calculé et comparé les coefficients les plus communément utilisés, entre autres kappa (Cohen, 1960) et pi (Scott, 1955), et avons analysé dans quelle mesure ils sont adaptés à nos données. Nous avons également étudié les différentes pondérations applicables à ces coefficients permettant de calculer le kappa pondéré (Cohen, 1968) et l’alpha (Krippendorff, 1980, 2004). Nous avons ainsi étudié le biais induit par la grande prévalence d’une catégorie et défini un mode de calcul des distances entre catégories reposant sur les annotations réalisées.</resume>
        <mots_cles>Annotation manuelle, évaluation, accord inter-annotateurs</mots_cles>
        <title/>
        <abstract>This article details work aiming at evaluating the quality of the manual annotation of gene renaming relations in scientific abstracts, which generates sparse annotations. To evaluate these annotations, we computed and compared the results obtained using the commonly advocated inter-annotator agreement coefficients such as kappa (Cohen, 1960) or pi (Scott, 1955) and analyzed to which extent they are relevant for our data.We also studied the different weighting computations applicable to kappa! (Cohen, 1968) and alpha (Krippendorff, 1980, 2004) and estimated the bias introduced by prevalence. We then define a way to compute distances between categories based on the produced annotations.</abstract>
        <keywords>Manual annotation, evaluation, inter-annotator agreement</keywords>
      </article>
      <article id="taln-2010-long-036" session="Corpus">
        <auteurs>
          <auteur>
            <nom>Phuong Le-Hong</nom>
            <email>lehong@loria.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Azim Roussanaly</nom>
            <email>azim@loria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Thi Minh Huyen Nguyen</nom>
            <email>huyenntm@vnu.edu.vn</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Mathias Rossignol</nom>
            <email>mathias.rossignol@gmail.com</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LORIA, Nancy, France</affiliation>
          <affiliation affiliationId="2">Hanoi University of Science, Hanoi, Vietnam</affiliation>
        </affiliations>
        <titre/>
        <type>long</type>
        <pages/>
        <resume>Nous présentons dans cet article une étude empirique de l’application de l’approche de l’entropie maximale pour l’étiquetage syntaxique de textes vietnamiens. Le vietnamien est une langue qui possède des caractéristiques spéciales qui la distinguent largement des langues occidentales. Notremeilleur étiqueteur explore et inclut des connaissances utiles qui, en terme de performance pour l’étiquetage de textes vietnamiens, fournit un taux de précision globale de 93.40% et de 80.69% pour les mots inconnus sur un ensemble de test du corpus arboré vietnamien. Notre étiqueteur est nettement supérieur à celui qui est en train d’être utilisé pour développer le corpus arboré vietnamien, et à l’heure actuelle c’est le meilleur résultat obtenu pour l’étiquetage de textes vietnamiens.</resume>
        <mots_cles>Etiqueteur syntaxique, entropie maximale, texte, vietnamien</mots_cles>
        <title>An empirical study of maximum entropy approach for part-of-speech tagging of Vietnamese texts</title>
        <abstract>This paper presents an empirical study on the application of the maximum entropy approach for part-of-speech tagging of Vietnamese text, a language with special characteristics which largely distinguish it from occidental languages. Our best tagger explores and includes useful knowledge sources for tagging Vietnamese text and gives a 93.40%overall accuracy and a 80.69%unknown word accuracy on a test set of the Vietnamese treebank. Our tagger significantly outperforms the tagger that is being used for building the Vietnamese treebank, and as far as we are aware, this is the best tagging result ever published for the Vietnamese language.</abstract>
        <keywords>Part-of-speech tagger, maximum entropy, text, Vietnamese</keywords>
      </article>
      <article id="taln-2010-long-037" session="Corpus">
        <auteurs>
          <auteur>
            <nom>Lyne Da Sylva</nom>
            <email>lyne.da.sylva@umontreal.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Ecole de bibliothéconomie et des sciences de l'information, Université de Montréal, C.P. 6128, succ. Centre-ville, Montréal, Canada H3C 3J7</affiliation>
        </affiliations>
        <titre>Extraction semi-automatique d’un vocabulaire savant de base pour l’indexation automatique</titre>
        <type>long</type>
        <pages/>
        <resume>Le projet décrit vise à soutenir les efforts de constitution de ressources lexicales utiles à l’indexation automatique. Un type de vocabulaire utile à l’indexation est défini, le vocabulaire savant de base, qui peut s’articuler avec le vocabulaire spécialisé pour constituer des entrées d’index structurées. On présente les résultats d’ une expérimentation d’ extraction (semi-)automatique des mots du vocabulaire savant de base à partir d’un corpus ciblé, constitué de résumés d’articles scientifiques en français et en anglais. La tâche d’extraction a réussi à doubler une liste originale constituée manuellement pour le français. La comparaison est établie avec une expérimentation similaire effectuée pour l’anglais sur un corpus plus grand et contenant des résumés d’articles non seulement en sciences pures mais aussi en sciences humaines et sociales.</resume>
        <mots_cles>classes de vocabulaire, indexation automatique, extraction automatique, corpus, approche basée sur les corpus, vocabulaire savant de base, ressources lexicales, français</mots_cles>
        <title/>
        <abstract>This project aims to help develop lexical resources useful for automatic indexing. A type of useful vocabulary for indexing is defined, the basic scholarly vocabulary, which can combine with specialized vocabulary items to form evocative, structured index entries. The article presents the results of an experiment of (semi-)automatic extraction of the basic scholarly vocabulary lexical items from a large corpus. The corpus is especially suited to the task; it consists of abstracts of scientific articles in French and English. The extraction task was successful in doubling the size of a previously manually compiled list. A comparison is made with a similar experiment conducted for English on a larger corpus which also contained summaries of articles in the humanities and social sciences.</abstract>
        <keywords>vocabulary classes, automatic indexing, automatic extraction, corpus, corpus-based approach, basic scholarly vocabulary, lexical resources, French</keywords>
      </article>
      <article id="taln-2010-long-038" session="Morphologie">
        <auteurs>
          <auteur>
            <nom>Jean-François Lavallée</nom>
            <email>lavalljf@iro.umontreal.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Philippe Langlais</nom>
            <email>felipe@iro.umontreal.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université de Montréal, C.P. 6128, succ. Centre-ville, Montréal, Qc, Canada, H3C 3J7</affiliation>
        </affiliations>
        <titre>Apprentissage non supervisé de la morphologie d’une langue par généralisation de relations analogiques</titre>
        <type>long</type>
        <pages/>
        <resume>Bien que les approches fondées sur la théorie de l’information sont prédominantes dans le domaine de l’analyse morphologique non supervisée, depuis quelques années, d’autres approches ont gagné en popularité, dont celles basées sur l’analogie formelle. Cette dernière reste tout de même marginale due notamment à son coût de calcul élevé. Dans cet article, nous proposons un algorithme basé sur l’analogie formelle capable de traiter les lexiques volumineux. Nous introduisons pour cela le concept de règle de cofacteur qui permet de généraliser l’information capturée par une analogie tout en contrôlant les temps de traitement. Nous comparons notre système à 2 systèmes : Morfessor (Creutz &amp; Lagus, 2005), un système de référence dans de nombreux travaux sur l’analyse morphologique et le système analogique décrit par Langlais (2009). Nous en montrons la supériorité pour 3 des 5 langues étudiées ici : le finnois, le turc, et l’allemand.</resume>
        <mots_cles>Analyse morphologique non supervisée, Analogie formelle, Approche à base de graphe</mots_cles>
        <title/>
        <abstract>Although approaches based on information theory are prominent in the field of unsupervised morphological analysis, in recent years, other approaches have gained in popularity. Those based on formal analogy remain marginal partly because of their high computational cost. In this paper we propose an algorithm based on formal analogy able to handle large lexicons. We introduce the concept of cofactor rule which allows the generalization of the information captured by analogy, while controlling the processing time. We compare our system to 2 others : Morfessor (Creutz &amp; Lagus, 2005), a reference in many studies on morphological analysis and the analogical system described by Langlais (2009). We show the superiority of our approach for 3 out of the 5 languages studied here : Finnish, Turkish, and German.</abstract>
        <keywords>Unsupervised Learning of Morphology, Formal Analogy, Graph-Based Approach</keywords>
      </article>
      <article id="taln-2010-long-039" session="Morphologie">
        <auteurs>
          <auteur>
            <nom>Vincent Claveau</nom>
            <email>Vincent.Claveau@irisa.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Ewa Kijak</nom>
            <email>Ewa.Kijak@irisa.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">IRISA-CNRS, Campus de Beaulieu, F-35042 Rennes, France</affiliation>
          <affiliation affiliationId="2">IRISA-Univ. Rennes 1, Campus de Beaulieu, F-35042 Rennes, France</affiliation>
        </affiliations>
        <titre>Analyse morphologique en terminologie biomédicale par alignement et apprentissage non-supervisé</titre>
        <type>long</type>
        <pages/>
        <resume>Dans le domaine biomédical, beaucoup de termes sont des composés savants (composés de plusieurs racines gréco-latines). L’étude de leur morphologie est importante pour de nombreuses applications puisqu’elle permet de structurer ces termes, de les rechercher efficacement, de les traduire... Dans cet article, nous proposons de suivre une démarche originale mais fructueuse pour mener cette analyse morphologique sur des termes simples en français, en nous appuyant sur une langue pivot, le japonais, et plus précisément sur les termes écrits en kanjis. Pour cela nous avons développé un algorithme d’alignement de termes spécialement adapté à cette tâche. C’est cet alignement d’un terme français avec sa traduction en kanjis qui fournit en même temps une décomposition en morphe et leur étiquetage par les kanjis correspondants. Évalué sur un jeu de données conséquent, notre approche obtient une précision supérieure à 70% et montrent son bien fondé en comparaison avec les techniques existantes. Nous illustrons également l’intérêt de notre démarche au travers de deux applications directes de ces alignements : la traduction de termes inconnus et la découverte de relations entre morphes pour la tructuration terminologique.</resume>
        <mots_cles>Alignement, terminologie, morphologie, analogie, traduction de terme, kanji</mots_cles>
        <title/>
        <abstract>In the biomedical domain, many terms are neoclassical compounds (composed of several Greek or Latin roots). The study of their morphology is important for numerous applications since it makes it possible to structure them, retrieve them efficiently, translate them... In this paper, we propose an original yet fruitful approach to carry out this morphological analysis by relying on Japanese, more precisely on terms written in kanjis, as a pivot language. In order to do so, we have developed a specially crafted alignment algorithm. This alignment process of French terms with their kanji-based counterparts provides at the same time a decomposition of the French term into morphs, and a kanji label for each morph. Evaluated on a big dataset, our approach yields a precision greater than 70% and shows its the relevance compared with existing techniques. We also illustrate the validity of our reasoning through two direct applications of the produced alignments: translation of unknown terms and discovering of relationships between morphs for terminological structuring.</abstract>
        <keywords>Alignment, terminology, morphology, analogy, term translation, kanji</keywords>
      </article>
      <article id="taln-2010-long-040" session="Morphologie">
        <auteurs>
          <auteur>
            <nom>Benoît Sagot</nom>
            <email>benoit.sagot@inria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Géraldine Walther</nom>
            <email>geraldine.walther@linguist.jussieu.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Alpage, INRIA Paris–Rocquencourt &amp; Université Paris 7, Rocquencourt, BP 105, 78153 Le Chesnay Cedex, France</affiliation>
          <affiliation affiliationId="2">LLF, Université Paris 7, 30 rue du Château des Rentiers, 75013 Paris, France</affiliation>
        </affiliations>
        <titre>Développement de ressources pour le persan: lexique morphologique et chaîne de traitements de surface</titre>
        <type>long</type>
        <pages/>
        <resume>Nous présentons PerLex, un lexique morphologique du persan à large couverture et librement disponible, accompagné d’une chaîne de traitements de surface pour cette langue. Nous décrivons quelques caractéristiques de la morphologie du persan, et la façon dont nous l’avons représentée dans le formalisme lexical Alexina, sur lequel repose PerLex. Nous insistons sur la méthodologie que nous avons employée pour construire les entrées lexicales à partir de diverses sources, ainsi que sur les problèmes liés à la normalisation typographique. Le lexique obtenu a une couverture satisfaisante sur un corpus de référence, et devrait donc constituer un bon point de départ pour le développement d’un lexique syntaxique du persan.</resume>
        <mots_cles>Lexique morphologique, Persan, Développement de lexiques, Traitements de surface</mots_cles>
        <title/>
        <abstract>We introduce PerLex, a large-coverage and freely-available morphological lexicon for the Persian language, as well as a corresponding surface processing chain. We describe the main features of the Persian morphology, and the way we have represented it within the Alexina formalism, on which PerLex is based. We focus on the methodology we used for constructing lexical entries from various sources, as well as on the problems related to typographic normalisation. The resulting lexicon shows a satisfying coverage on a reference corpus and should therefore be a good starting point for developing a syntactic lexicon for the Persian language.</abstract>
        <keywords>Morphological lexicon, Persian language, Lexical development, Surface processing.</keywords>
      </article>
      <article id="taln-2010-court-001" session="Poster">
        <auteurs>
          <auteur>
            <nom>Louise Deléger</nom>
            <email>louise.deleger@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Bruno Cartoni</nom>
            <email>bruno.cartoni@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIMSI-CNRS, BP 133, 91403 Orsay Cedex, France</affiliation>
        </affiliations>
        <titre>Adjectifs relationnels et langue de spécialité : vérification d’une hypothèse linguistique en corpus comparable médical</titre>
        <type>court</type>
        <pages/>
        <resume>Cet article présente une étude en corpus comparable médical pour confirmer la préférence d’utilisation des adjectifs relationnels dans les langues de spécialité et examiner plus finement l’alternance entre syntagmes nominaux avec adjectifs relationnels et syntagmes avec complément prépositionnel.</resume>
        <mots_cles>corpus comparables monolingues, morphologie constructionnelle, langue de spécialité</mots_cles>
        <title/>
        <abstract>This paper presents a study in medical comparable corpora that aims to confirm the preferred use of relational adjectives in specialised languages and to examine in a more fine-grained manner the alternance between phrases with adjective and noun phrases with prepositional complement.</abstract>
        <keywords>monolingual comparable corpora, constructional morphology, specialised language</keywords>
      </article>
      <article id="taln-2010-court-002" session="Poster">
        <auteurs>
          <auteur>
            <nom>Mathieu Lafourcade</nom>
            <email>lafourcade@lirmm.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Alain Joubert</nom>
            <email>joubert@lirmm.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIRMM – Univ. Montpellier 2 - CNRS, Laboratoire d’Informatique, de Robotique et de Microélectronique de Montpellier, 161, rue Ada – 34392 Montpellier Cédex 5 – France</affiliation>
        </affiliations>
        <titre>Détermination et pondération des raffinements d’un terme à partir de son arbre des usages nommés</titre>
        <type>court</type>
        <pages/>
        <resume>Grâce à la participation d’un grand nombre de personnes via des jeux accessibles sur le web, nous avons construit un réseau lexical évolutif de grande taille pour le Français. A partir de cette ressource, nous avons abordé la question de la détermination des sens d’usage d’un terme, puis après avoir introduit la notion de similarité entre ces différents usages, nous avons pu obtenir pour un terme son arbre des usages : la racine regroupe tous les usages du terme et une descente dans l’arbre correspond à un raffinement de ces usages. Le nommage des différents noeuds est effectué lors d’une descente en largeur. En simplifiant l’arbre des usages nommés, nous déterminons les différents sens d’un terme, sens que nous introduisons dans le réseau lexical en tant que noeuds de raffinement du terme considéré. Nous terminons par une évaluation empirique des résultats obtenus.</resume>
        <mots_cles>réseau lexical, arbre des usages nommés d’un terme, pondération des sens d’un terme</mots_cles>
        <title/>
        <abstract>Thanks to the participation of a large number of persons via web-based games, a largesized evolutionary lexical network is available for French. With this resource, we approached the question of the determination of the word usages of a term, and after introducing the notion of similarity between these various word usages, we were able to build for a term its word usage tree: the root groups together all possible usages of this term and a search in the tree corresponds to a refinement of these word usages. The labelling of the various nodes is made during a width-first search. From its labelled word usage tree, we obtain the different meanings of a term, which can be inserted in the lexical network as refinement nodes for this term. Lastly, we present an evaluation of the results we obtain.</abstract>
        <keywords>lexical network, tree of labelled word usages for a term, weighting of the meanings</keywords>
      </article>
      <article id="taln-2010-court-003" session="Poster">
        <auteurs>
          <auteur>
            <nom>Jonathan Chevelu</nom>
            <email>jonathan.chevelu@orange-ftgroup.com</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Yves Lepage</nom>
            <email>yves.lepage@info.unicaen.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Thierry Moudenc</nom>
            <email>thierry.moudenc@orange-ftgroup.com</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Ghislain Putois</nom>
            <email>ghislain.putois@orange-ftgroup.com</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université de Caen, France</affiliation>
          <affiliation affiliationId="2">Orange Labs, France</affiliation>
        </affiliations>
        <titre>L’évaluation des paraphrases : pour une prise en compte de la tâche</titre>
        <type>court</type>
        <pages/>
        <resume>Les définitions des paraphrases privilégient généralement la conservation du sens. Cet article démontre par l’absurde qu’une évaluation uniquement basée sur la conservation du sens permet à un système inutile de production de paraphrase d’être jugé meilleur qu’un système au niveau de l’état de l’art. La conservation du sens n’est donc pas l’unique critère des paraphrases. Nous exhibons les trois objectifs des paraphrases : la conservation du sens, la naturalité et l’adaptation à la tâche. La production de paraphrase est alors un compromis dépendant de la tâche entre ces trois critères et ceux-ci doivent être pris en compte lors des évaluations.</resume>
        <mots_cles>Générateur de paraphrase, évaluation des paraphrases</mots_cles>
        <title/>
        <abstract>Meaning preservation is generally rooted in the paraphrase definitions. This article proves by reductio ad absurdum that an evaluation based only on the meaning preservation can rank a dummy and useless system better than a state-of-the-art system. Meaning preservation is therefore not the one and only criterion for a paraphrase system. We exhibit the three objectives of paraphrase : meaning preservation, sentence naturalness and adequacy for the task. Paraphrase generation consists actually in reaching a taskdependent compromise between these three criteria, and they have to be taken into account during each evaluation process.</abstract>
        <keywords>Paraphrase generator, paraphrase evaluation</keywords>
      </article>
      <article id="taln-2010-court-004" session="Poster">
        <auteurs>
          <auteur>
            <nom>Olivier Collin</nom>
            <email>olivier.collin@orange-ftgroup.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Benoît Gaillard</nom>
            <email>benoit.gaillard@orange-ftgroup.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Jean-Léon Bouraoui</nom>
            <email>jeanleon.bouraoui@orange-ftgroup.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Orange Labs – Av Pierre Marzin, 22300 Lannion</affiliation>
        </affiliations>
        <titre>Constitution d'une ressource sémantique issue du treillis des catégories de Wikipedia</titre>
        <type>court</type>
        <pages/>
        <resume>Le travail présenté dans cet article s'inscrit dans le thème de l'acquisition automatique de ressources sémantiques s'appuyant sur les données de Wikipedia. Nous exploitons le graphe des catégories associées aux pages de Wikipedia à partir duquel nous extrayons une hiérarchie de catégories parentes, sémantiquement et thématiquement liées. Cette extraction est le résultat d'une stratégie de plus court chemin appliquée au treillis global des catégories. Chaque page peut ainsi être représentée dans l'espace de ses catégories propres, ainsi que des catégories parentes. Nous montrons la possibilité d'utiliser cette ressource pour deux applications. La première concerne l'indexation et la classification des pages de Wikipedia. La seconde concerne la désambiguïsation dans le cadre d'un traducteur de requêtes français/anglais. Ce dernier travail a été réalisé en exploitant les catégories des pages anglaises.</resume>
        <mots_cles>Wikipedia, plus court chemin, désambiguïsation, classification, traduction de requête</mots_cles>
        <title/>
        <abstract>This work is closely related to the domain of automatic acquisition of semantic resources exploiting Wikipedia data. More precisely, we exploit the graph of parent categories linked to each Wikipedia page to extract the semantically and thematically related parent categories. This extraction is the result of a shortest path length calculus applied to the global lattice of Wikipedia categories. So, each page can be projected within its first level categories, and in addition their parent categories. This resource has been used for two kinds of applications. The first one concerns the indexation and classification of Wikipedia pages. The second one concerns a disambiguation task applied to a query translator for cross lingual search engine. This last work has been performed by using English categories lattice.</abstract>
        <keywords>Wikipedia, shortest path, disambiguation, classification, query translation</keywords>
      </article>
      <article id="taln-2010-court-005" session="Poster">
        <auteurs>
          <auteur>
            <nom>Laurence Danlos</nom>
            <email>laurence.danlos@linguist.jussieu.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Benoît Sagot</nom>
            <email>benoit.sagot@inria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Alpage, INRIA Paris–Rocquencourt &amp; Université Paris 7, Rocquencourt, BP 105, 78153 Le Chesnay Cedex, France</affiliation>
        </affiliations>
        <titre>Ponctuations fortes abusives</titre>
        <type>court</type>
        <pages/>
        <resume>Certaines ponctuations fortes sont « abusivement » utilisées à la place de ponctuations faibles, débouchant sur des phrases graphiques qui ne sont pas des phrases grammaticales. Cet article présente une étude sur corpus de ce phénomène et une ébauche d’outil pour repérer automatiquement les ponctuations fortes abusives.</resume>
        <mots_cles>pseudo-phrase, phrase averbale, analyse syntaxique et sémantique</mots_cles>
        <title/>
        <abstract>Some strong punctuation signs are “wrongly” used instead of weak punctuation signs, leading to graphic sentences which are not grammatical sentences. This paper presents a corpus study of this phenomenon and a tool in the early stages to automatically detect wrong strong punctuation signs.</abstract>
        <keywords>pseudo-sentence, verbless utterance, syntactic and semantic analysis</keywords>
      </article>
      <article id="taln-2010-court-006" session="Poster">
        <auteurs>
          <auteur>
            <nom>Véronique Moriceau</nom>
            <email>Veronique.Moriceau@limsi.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Xavier Tannier</nom>
            <email>Xavier.Tannier@limsi.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Mathieu Falco</nom>
            <email>Mathieu.Falco@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIMSI-CNRS, B.P. 133, 91403 ORSAY, FRANCE</affiliation>
          <affiliation affiliationId="2">Université Paris-Sud 11, 91405 ORSAY</affiliation>
        </affiliations>
        <titre>Une étude des questions “complexes” en question-réponse</titre>
        <type>court</type>
        <pages/>
        <resume>La plupart des systèmes de question-réponse ont été conçus pour répondre à des questions dites “factuelles” (réponses précises comme des dates, des lieux), et peu se sont intéressés au traitement des questions complexes. Cet article présente une typologie des questions en y incluant les questions complexes, ainsi qu’une typologie des formes de réponses attendues pour chaque type de questions. Nous présentons également des expériences préliminaires utilisant ces typologies pour les questions complexes, avec de bons résultats.</resume>
        <mots_cles>Système de question-réponse, questions complexes</mots_cles>
        <title/>
        <abstract>Most question-answering systems have been designed to answer “factual” questions (short and precise answers as dates, locations), and only a few researches concern complex questions. This article presents a typology of questions, including complex questions, as well as a typology of answers that should be expected for each type of questions. We also present preliminary experiments using these typologies for answering complex questions and leading to good results.</abstract>
        <keywords>Question-answering systems, complex questions</keywords>
      </article>
      <article id="taln-2010-court-007" session="Poster">
        <auteurs>
          <auteur>
            <nom>Muhammad Ghulam Abbas Malik</nom>
            <email>Abbas.Malik@imag.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Christian Boitet</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Pushpak Bhattacharyya</nom>
            <email/>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Laurent Besacier</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">GETALP-LIG, Université de Grenoble (UJF), FranceIndian Institute of Technology Bombay (IITB), India</affiliation>
          <affiliation affiliationId="2">Indian Institute of Technology Bombay (IITB), India</affiliation>
        </affiliations>
        <titre/>
        <type>court</type>
        <pages/>
        <resume>La TA généraliste de haute qualité et totalement automatique est considérée comme impossible. Nous nous intéressons aux problèmes de traduction scripturale, qui sont des sous-problèmes faibles du problème général de la traduction. Nous présentons les caractéristiques des problèmes faibles de traduction et les problèmes de traduction scripturale, décrivons différentes approches computationnelles (à états finis, statistiques, et hybrides) et présentons nos résultats sur différentes combinaisons de langues et systèmes d’écriture Indo-Pak.</resume>
        <mots_cles>problèmes faibles de traduction, traduction scripturale, traduction interdialectal, transcriptions, translittérations</mots_cles>
        <title>Weak Translation Problems – a case study of Scriptural Translation</title>
        <abstract>General purpose, high quality and fully automatic MT is believed to be impossible. We are interested in scriptural translation problems, which are weak sub-problems of the general problem of translation. We introduce the characteristics of the weak problems of translation and of the scriptural translation problems, describe different computational approaches (finite-state, statistical and hybrid) to solve these problems, and report our results on several combinations of Indo-Pak languages and writing systems.</abstract>
        <keywords>weak problems of translation, scriptural translation, interdialectal translation, transcription, transliteration</keywords>
      </article>
      <article id="taln-2010-court-008" session="Poster">
        <auteurs>
          <auteur>
            <nom>Fadoua Ataa Allah</nom>
            <email>ataaallah@ircam.ma</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Siham Boulaknadel</nom>
            <email>boulaknadel@ircam.ma</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CEISIC, IRCAM, Avenue Allal El Fassi, Madinat Al Irfane, Rabat, Morocco</affiliation>
        </affiliations>
        <titre>Pseudo-racinisation de la langue amazighe</titre>
        <type>court</type>
        <pages/>
        <resume>Dans le cadre de la promotion de la langue amazighe, nous avons voulu lui apporter des ressources et outils linguistiques pour son traitement automatique et son intégration dans le domaine des nouvelles technologies de l'information et de la communication. Partant de ce principe, nous avons opté, au sein de l’Institut Royal de la Culture Amazighe, pour une démarche innovante de réalisations progressives de ressources linguistiques et d’outils de base de traitement automatique, qui permettront de préparer le terrain pour d’éventuelles recherches scientifiques. Dans cette perspective, nous avons entrepris de développer, dans un premier temps, un outil de pseudoracinisation basé sur une approche relevant du cas de la morphologie flexionnelle et reposant sur l’élimination d’une liste de suffixes et de préfixes de la langue amazighe. Cette approche permettra de regrouper les mots sémantiquement proches à partir de ressemblances afin d’être exploités dans des applications tel que la recherche d’information et la classification.</resume>
        <mots_cles>Langue amazighe, Pseudo-racinisation, Morphologie flexionnelle</mots_cles>
        <title/>
        <abstract>In the context of promoting the Amazigh language, we would like to provide this language with linguistic resources and tools in the aim to enable its automatic processing and its integration in the field of Information and Communication Technology. Thus, we have opted, in the Royal Institute of Amazigh Culture, for an innovative approach of progressive realizations of linguistic resources and basic natural language processing tools that will pave the way for further scientific researches. In this perspective, we are trying initially to develop a light stemmer based on an approach dealing with inflectional morphology, and on stripping a list of Amazigh suffixes and prefixes. This approach will conflate word variants into a common stem that will be used in many applications such as information retrieval and classification.</abstract>
        <keywords>Amazigh language, Light stemming, Inflectional morphology</keywords>
      </article>
      <article id="taln-2010-court-009" session="Poster">
        <auteurs>
          <auteur>
            <nom>François-Régis Chaumartin</nom>
            <email>frc@proxem.com</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Sylvain Kahane</nom>
            <email>sylvain@kahane.fr</email>
            <affiliationId>2</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Proxem, 7 impasse Dumur, 92110 Clichy</affiliation>
          <affiliation affiliationId="2">Alpage, Université Paris 7 &amp; INRIA</affiliation>
          <affiliation affiliationId="3">Modyco, Université Paris Ouest Nanterre &amp; CNRS</affiliation>
        </affiliations>
        <titre>Une approche paresseuse de l’analyse sémantique ou comment construire une interface syntaxe-sémantique à partir d’exemples</titre>
        <type>court</type>
        <pages/>
        <resume>Cet article montre comment calculer une interface syntaxe-sémantique à partir d’un analyseur en dépendance quelconque et interchangeable, de ressources lexicales variées et d’une base d’exemples associés à leur représentation sémantique. Chaque exemple permet de construire une règle d’interface. Nos représentations sémantiques sont des graphes hiérarchisés de relations prédicat-argument entre des acceptions lexicales et notre interface syntaxe-sémantique est une grammaire de correspondance polarisée. Nous montrons comment obtenir un système très modulaire en calculant certaines règles par « soustraction » de règles moins modulaires.</resume>
        <mots_cles>Interface syntaxe-sémantique, graphe sémantique, grammaires de dépendance, GUP (Grammaire d’unification polarisée), GUST (Grammaire d’unification Sens-Texte)</mots_cles>
        <title/>
        <abstract>This article shows how to extract a syntax-semantics interface starting from an interchangeable dependency parser, various lexical resources and from samples associated with their semantic representations. Each example allows us to build an interface rule. Our semantic representations are hierarchical graphs of predicate-argument relations between lexical meanings and our syntax-semantics interface is a polarized unification grammar. We show how to obtain a very modular system by computing some rules by “subtraction” of less modular rules.</abstract>
        <keywords>Syntax-semantics Interface, Semantic Graph, Dependency Grammar, PUG (Polarized Unification Grammar), MTUG (Meaning-Text Unification Grammar)</keywords>
      </article>
      <article id="taln-2010-court-010" session="Poster">
        <auteurs>
          <auteur>
            <nom>Lorenza Russo</nom>
            <email>Lorenza.Russo@unige.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire d’Analyse et de Technologie du Langage, Département de linguistique – Université de Genève, 2, rue de Candolle – CH-1211 Genève 4</affiliation>
        </affiliations>
        <titre>La traduction automatique des pronoms clitiques. Quelle approche pour quels résultats?</titre>
        <type>court</type>
        <pages/>
        <resume>Dans cet article, nous abordons la problématique de la traduction automatique des pronoms clitiques, en nous focalisant sur la traduction de l’italien vers le français et en comparant les résultats obtenus par trois systèmes : Its-2, développé au LATL (Laboratoire d’Analyse et de Technologie du Langage) et basé sur un analyseur syntaxique profond ; Babelfish, basé sur des règles linguistiques ; et Google Translate, caractérisé par une approche statistique.</resume>
        <mots_cles>Analyseur syntaxique, traduction automatique, pronoms clitiques, proclise, enclise</mots_cles>
        <title/>
        <abstract>In this article, we discuss the problem of automatic translation of clitic pronouns, focalysing our attention on the translation from Italian to French and comparing the results obtained by three MT systems : Its-2, developed at LATL (Language Technology Laboratory) and based on a syntactic parser ; Babelfish, a rule-based system ; and Google Translate, caracterised by a statistical approach.</abstract>
        <keywords>Syntactic parser, automatic translation, clitic pronouns, proclisis, enclisis</keywords>
      </article>
      <article id="taln-2010-court-011" session="Poster">
        <auteurs>
          <auteur>
            <nom>François Morlane-Hondère</nom>
            <email>francois.morlane@univ-tlse2.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Cécile Fabre</nom>
            <email>cecile.fabre@univ-tlse2.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CLLE-ERSS, Université de Toulouse</affiliation>
        </affiliations>
        <titre>L’antonymie observée avec des méthodes de TAL : une relation à la fois syntagmatique et paradigmatique ?</titre>
        <type>court</type>
        <pages/>
        <resume>Cette étude utilise des outils de TAL pour tester l’hypothèse avancée par plusieurs études linguistiques récentes selon laquelle la relation antonymique, classiquement décrite comme une relation paradigmatique, a la particularité de fonctionner également sur le plan syntagmatique, c’est-à-dire de réunir des mots qui sont non seulement substituables mais qui apparaissent également régulièrement dans des relations contextuelles. Nous utilisons deux méthodes – l’analyse distributionnelle pour le plan paradigmatique, la recherche par patrons antonymiques pour le plan syntagmatique. Les résultats montrent que le diagnostic d’antonymie n’est pas significativement meilleur lorsqu’on croise les deux méthodes, puisqu’une partie des antonymes identifiés ne répondent pas au test de substituabilité, ce qui semble confirmer la prépondérance du plan syntagmatique pour l’étude et l’acquisition de cette relation.</resume>
        <mots_cles>sémantique lexicale, antonymie, analyse distributionnelle, patrons lexico-syntaxiques</mots_cles>
        <title/>
        <abstract>In this paper, we use NLP methods to test the hypothesis, suggested by several linguistic studies, that antonymy is not only a paradigmatic but also a syntagmatic relation : antonym pairs, that have been classically described by their ability to be substituted for each other, also tend to frequently co-occur in texts. We use two methods – distributional analysis on the paradigmatic level, lexico-syntactic pattern recognition on the syntagmatic level. Results show that antonym detection is not significantly improved by combining the two methods : a set of antonyms do not satisfy the test for substitutability, which tends to confirm the predominance of the syntagmatic level for studying and identifying antonymy.</abstract>
        <keywords>lexical semantics, antonymy, distributional analysis, lexico-grammatical patterns</keywords>
      </article>
      <article id="taln-2010-court-012" session="Poster">
        <auteurs>
          <auteur>
            <nom>Ludivine Kuznik</nom>
            <email>ludivine-externe.kuznik@edf.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Anne-Laure Guénet</nom>
            <email>anne-laure.guenet@edf.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Anne Peradotto</nom>
            <email>anne.peradotto@edf.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Chloé Clavel</nom>
            <email>chloe.clavel@edf.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Electricité de France, Direction Commerce, 92050 Paris La Défense, France</affiliation>
          <affiliation affiliationId="2">Electricité de France R&amp;D, 92141 Clamart, France</affiliation>
          <affiliation affiliationId="3">Société Lincoln , 92570 Boulogne-Billancourt, France</affiliation>
        </affiliations>
        <titre>L’apport des concepts métiers pour la classification des questions ouvertes d’enquête</titre>
        <type>court</type>
        <pages/>
        <resume>EDF utilise les techniques de Text Mining pour optimiser sa relation client, en analysant des réponses aux questions ouvertes d'enquête de satisfaction, et des retranscriptions de conversations issues des centres d'appels. Dans cet article, nous présentons les différentes contraintes applicatives liées à l’utilisation d’outils de text mining pour l’analyse de données clients. Après une analyse des différents outils présents sur le marché, nous avons identifié la technologie Skill CartridgeTM fournie par la société TEMIS comme la plus adaptée à nos besoins. Cette technologie nous permet une modélisation sémantique de concepts liés au motif d’insatisfaction. L’apport de cette modélisation est illustrée pour une tâche de classification de réponses d’enquêtes de satisfaction chargée d’évaluer la fidélité des clients EDF. La modélisation sémantique a permis une nette amélioration des scores de classification (F-mesure = 75,5%) notamment pour les catégories correspondant à la satisfaction et au mécontentement.</resume>
        <mots_cles>outils de text mining, modélisation de concepts métier, classification supervisée</mots_cles>
        <title/>
        <abstract>The French power supply company EDF uses text mining tools to improve customer insight by analysing satisfaction inquiries or transcriptions of call-centre conversations. In this paper, we present the various application needs for text mining tools. After an analysis of the various existing industrial tools, we identify the Skill Cartridge tool provided by TEMIS company as the more relevant to our needs. This tool offers the capability to model expressions linked to reason for satisfaction/dissatisfaction. The contribution of this modelling is illustrated here for the classification of satisfaction inquiries dedicated to the evaluation of customer loyalty. The semantic models provide a marked improvement of classification scores (F-mesure = 75.5%) for the satisfaction/dissatisfaction categories in particular.</abstract>
        <keywords>text mining tools, business concept modelling, supervised classification</keywords>
      </article>
      <article id="taln-2010-court-013" session="Poster">
        <auteurs>
          <auteur>
            <nom>Charles Dejean</nom>
            <email/>
            <affiliationId/>
          </auteur>
          <auteur>
            <nom>Manoel Fortun</nom>
            <email/>
            <affiliationId/>
          </auteur>
          <auteur>
            <nom>Clotilde Massot</nom>
            <email/>
            <affiliationId/>
          </auteur>
          <auteur>
            <nom>Vincent Pottier</nom>
            <email/>
            <affiliationId/>
          </auteur>
          <auteur>
            <nom>Fabien Poulard</nom>
            <email>Fabien.Poulard@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Matthieu Vernier</nom>
            <email>Matthieu.Vernier@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LINA, UMR6241, 44322 Nantes</affiliation>
        </affiliations>
        <titre>Un étiqueteur de rôles grammaticaux libre pour le français intégré à Apache UIMA</titre>
        <type>court</type>
        <pages/>
        <resume>L’étiquetage des rôles grammaticaux est une tâche de pré-traitement récurrente. Pour le français, deux outils sont majoritairement utilisés : TreeTagger et Brill. Nous proposons une démarche, ne nécessitant aucune ressource, pour la création d’un modèle de Markov caché (HMM) pour palier les problèmes de ces outils, et de licences notamment. Nous distribuons librement toutes les ressources liées à ce travail.</resume>
        <mots_cles>étiquetage grammatical, Modèle de Markov caché, UIMA, Brill, TreeTagger</mots_cles>
        <title/>
        <abstract>Part-of-speech tagging is a common preprocessing task. For the French language, Brill and TreeTagger are the most often used tools. We propose a method, requiring no resource, to create a Hidden Markov Model to get rid of the problems and licences of these tools. We freely distribute all the resources related to this work.</abstract>
        <keywords>grammatical tagging, Hidden Markov Model, UIMA, Brill, TreeTagger</keywords>
      </article>
      <article id="taln-2010-court-014" session="Poster">
        <auteurs>
          <auteur>
            <nom>Michel Généreux</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Rita Marquilhas</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Iris Hendrickx</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Centro de Linguistica da Universidade de Lisboa, Av. Prof. Gama Pinto, 2, 1649-003 Lisboa - Portugal</affiliation>
        </affiliations>
        <titre>Segmentation Automatique de Lettres Historiques</titre>
        <type>court</type>
        <pages/>
        <resume>Cet article présente une approche basée sur la comparaison fréquentielle de modèles lexicaux pour la segmentation automatique de textes historiques Portugais. Cette approche traite d’abord le problème de la segmentation comme un problème de classification, en attribuant à chaque élément lexical présent dans la phase d’apprentissage une valeur de saillance pour chaque type de segment. Ces modèles lexicaux permettent à la fois de produire une segmentation et de faire une analyse qualitative de textes historiques. Notre évaluation montre que l’approche adoptée permet de tirer de l’information sémantique que des approches se concentrant sur la détection des frontières séparant les segments ne peuvent acquérir.</resume>
        <mots_cles>Corpus comparables, Saillance, Segmentation, Textes historiques</mots_cles>
        <title/>
        <abstract>This article presents an approach based on the frequency comparison of lexical models for the automatic segmentation of historical texts. This approach first addresses the problem of segmentation as a classification problem by assigning each token present in the learning phase a value of salience for each type of segment. These lexical patterns can both produce a segmentation and make possible a qualitative analysis of historical texts. Our evaluation shows that the approach can extract semantic information that approaches focusing on the detection of boundaries between segments cannot capture.</abstract>
        <keywords>Comparable corpora, Salience, Segmentation, Historical Texts</keywords>
      </article>
      <article id="taln-2010-court-015" session="Poster">
        <auteurs>
          <auteur>
            <nom>Helena Blancafort</nom>
            <email/>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Gaëlle Recourcé</nom>
            <email/>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Javier Couto</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Benoît Sagot</nom>
            <email/>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Rosa Stern</nom>
            <email/>
            <affiliationId>3</affiliationId>
            <affiliationId>4</affiliationId>
          </auteur>
          <auteur>
            <nom>Denis Teyssou</nom>
            <email/>
            <affiliationId>4</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Syllabs, 15, rue Jean-Baptiste Berlier, 75013 Paris, France</affiliation>
          <affiliation affiliationId="2">Universitat Pompeu Fabra, Roc Boronat, 08013 Barcelona, Espagne</affiliation>
          <affiliation affiliationId="3">Alpage, INRIA Paris–Rocquencourt &amp; Université Paris 7, Rocquencourt, BP 105, 78153 Le Chesnay Cedex, France</affiliation>
          <affiliation affiliationId="4">Agence France-Presse – Medialab, 2 place de la Bourse, 75002 Paris, France</affiliation>
        </affiliations>
        <titre>Traitement des inconnus : une approche systématique de l’incomplétude lexicale</titre>
        <type>court</type>
        <pages/>
        <resume>Cet article aborde le phénomène de l’incomplétude des ressources lexicales, c’est-à-dire la problématique des inconnus, dans un contexte de traitement automatique. Nous proposons tout d’abord une définition opérationnelle de la notion d’inconnu. Nous décrivons ensuite une typologie des différentes classes d’inconnus, motivée par des considérations linguistiques et applicatives ainsi que par l’annotation des inconnus d’un petit corpus selon notre typologie. Cette typologie sera mise en oeuvre et validée par l’annotation d’un corpus important de l’Agence France-Presse dans le cadre du projet EDyLex.</resume>
        <mots_cles>mots inconnus, incomplétude lexicale, acquisition dynamique des ressources lexicales</mots_cles>
        <title/>
        <abstract>This paper addresses the incompleteness of lexical resources, i.e., the problem of unknown words, in the context of natural language processing. First, we put forward an operational definition of the notion of unknown words. Next, we describe a typology of the various classes of unknown words, motivated by linguistic and applicative considerations as well as the annotation of unknown words in a small-scale corpus w.r.t. our typology. This typology shall be applied and validated through the annotation of a large corpus from the Agence France-Presse as part of the EDyLex project.</abstract>
        <keywords>unknown words, lexical incompleteness, dynamic acquisition of lexical information</keywords>
      </article>
      <article id="taln-2010-court-016" session="Poster">
        <auteurs>
          <auteur>
            <nom>Evelyne Jacquey</nom>
            <email>Evelyne.Jacquey@atilf.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Laurence Kister</nom>
            <email>Laurence.Kister@atilf.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Mick Grzesitchak</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Bertrand Gaiffe</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Coralie Reutenauer</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Sandrine Ollinger</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Mathieu Valette</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">UMR ATILF-CNRS-Nancy Université, 44 avenue de la libération 54000 NANCY</affiliation>
        </affiliations>
        <titre>Thésaurus et corpus de spécialité sciences du langage : approches lexicométriques appliquées à l’analyse de termes en corpus</titre>
        <type>court</type>
        <pages/>
        <resume>Cet article s'inscrit dans les recherches sur l'exploitation de ressources terminologiques pour l'analyse de textes de spécialité, leur annotation et leur indexation. Les ressources en présence sont, d'une part, un thesaurus des Sciences du Langage, le Thesaulangue et, d'autre part, un corpus d’échantillons issus de cinq ouvrages relevant du même domaine. L'article a deux objectifs. Le premier est de déterminer dans quelle mesure les termes de Thesaulangue sont représentés dans les textes. Le second est d'évaluer si les occurrences des unités lexicales correspondant aux termes de Thesaulangue relèvent majoritairement d'emplois terminologiques ou de langue courante. A cette fin, les travaux présentés utilisent une mesure de richesse lexicale telle qu'elle a été définie par Brunet (rapporté dans Muller, 1992) dans le domaine de la lexicométrie, l'indice W. Cette mesure est adaptée afin de mesurer la richesse terminologie (co-occurrents lexicaux et sémantiques qui apparaissent dans Thesaulangue).</resume>
        <mots_cles>sémantique lexicale, terminologie, corpus, richesse lexicale, lexicométrie</mots_cles>
        <title/>
        <abstract>This article aims to contribute to the field of the exploitation of terminological resources for the analysis of technical and scientific texts, their annotation and their indexation. The available resources are on one hand a thesaurus, Thesaulangue, which deals with Linguistics, and on the other hand, a corpus made of samples extracted from five books about Linguistics. More precisely, the article has two goals: first, studying how to determine which terms of Thesaulangue occur in texts. Second, attempting to measure if the lexical units which correspond to terms of Thesaulangue are used in texts in a terminological way or not. In this perspective, the presented work uses and adapts the Brunet’s W-index designed in the area of lexicometry.</abstract>
        <keywords>lexical semantics, terminology, corpora, lexical richness, lexicometry</keywords>
      </article>
      <article id="taln-2010-court-017" session="Poster">
        <auteurs>
          <auteur>
            <nom>Marc Le Tallec</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Jeanne Villaneau</nom>
            <email/>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Jean-Yves Antoine</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Agata Savary</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Arielle Syssau-Vaccarella</nom>
            <email/>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université François Rabelais Tours – LI</affiliation>
          <affiliation affiliationId="2">Université Européenne de Bretagne – VALORIA</affiliation>
          <affiliation affiliationId="3">Université Montpellier 3</affiliation>
        </affiliations>
        <titre>Détection hors contexte des émotions à partir du contenu linguistique d’énoncés oraux : le système EmoLogus</titre>
        <type>court</type>
        <pages/>
        <resume>Le projet EmotiRob, soutenu par l'agence nationale de la recherche, s'est donné pour objectif de détecter des émotions dans un contexte d'application original : la réalisation d'un robot compagnon émotionnel pour des enfants fragilisés. Nous présentons dans cet article le système qui caractérise l'émotion induite par le contenu linguistique des propos de l'enfant. Il se base sur un principe de compositionnalité des émotions, avec une valeur émotionnelle fixe attribuée aux mots lexicaux, tandis que les verbes et les adjectifs agissent comme des fonctions dont le résultat dépend de la valeur émotionnelle de leurs arguments. L'article présente la méthode de calcul utilisée, ainsi que la norme lexicale émotionnelle correspondante. Une analyse quantitative et qualitative des premières expérimentations présente les différences entre les sorties du module de détection et l'annotation d'experts, montrant des résultats satisfaisants, avec la bonne détection de la valence émotionnelle dans plus de 90% des cas.</resume>
        <mots_cles>Emotion, valence émotionnelle, norme lexicale émotionnelle, robot compagnon, compréhension de parole</mots_cles>
        <title/>
        <abstract>The ANR Emotirob project aims at detecting emotions in an original application context: realizing an emotional companion robot for weakened children. This paper presents a system which aims at characterizing emotions by only considering linguistic content. It is based on the assumption that emotions can be compound: simple lexical words have an intrinsic emotional value, while verbal and adjectival predicates act as a function on the emotional values of their arguments. The paper describes the algorithm of compositional computation of the emotion and the lexical emotional norm used by this algorithm. A quantitative and qualitative analysis of the differences between system outputs and expert annotations is given, which shows satisfactory results, with a good detection of emotional valency in 90.0% of the test utterances.</abstract>
        <keywords>Emotion, Emotional valency, Emotional lexical standard, companion robot, spoken language understanding</keywords>
      </article>
      <article id="taln-2010-court-018" session="Poster">
        <auteurs>
          <auteur>
            <nom>Houda Bouamor</nom>
            <email>Houda.Bouamor@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Aurélien Max</nom>
            <email>Aurelien.Max@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Anne Vilnat</nom>
            <email>Anne.Vilnat@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIMSI-CNRS, Univ. Paris-Sud, Orsay, F-91403, France</affiliation>
        </affiliations>
        <titre>Acquisition de paraphrases sous-phrastiques depuis des paraphrases d’énoncés</titre>
        <type>court</type>
        <pages/>
        <resume>Dans cet article, nous présentons la tâche d’acquisition de paraphrases sous-phrastiques (impliquant des paires de mots ou de groupes de mots), et décrivons plusieurs techniques opérant à différents niveaux. Nous décrivons une évaluation visant à comparer ces techniques et leurs combinaisons sur deux corpus de paraphrases d’énoncés obtenus par traduction multiple. Les conclusions que nous tirons peuvent servir de guide pour améliorer des techniques existantes.</resume>
        <mots_cles>Paraphrase, Patrons de correspondances de segments monolingues</mots_cles>
        <title/>
        <abstract>In this article, the task of acquiring sub-sentential paraphrases (word or phrase pairs) is discussed and several automatic techniques operating at different levels are presented. We describe an evaluation methodology to compare these techniques and their combination that is applied on two corpora of sentential paraphrases obtained by multiple translation. The conclusions that are drawn can be used to guide future work for improving existing techniques.</abstract>
        <keywords>Paraphrase, Monolingual bi-phrase patterns</keywords>
      </article>
      <article id="taln-2010-court-019" session="Poster">
        <auteurs>
          <auteur>
            <nom>Claire Mouton</nom>
            <email>claire.mouton@cea.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Gaël de Chalendar</nom>
            <email>gael.de-chalendar@cea.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CEA, LIST, Laboratoire Vision et Ingénierie des Contenus, Fontenay aux Roses, F-92265, France</affiliation>
          <affiliation affiliationId="2">Exalead, 10 place de la Madeleine, 75008 Paris</affiliation>
        </affiliations>
        <titre>JAWS : Just Another WordNet Subset</titre>
        <type>court</type>
        <pages/>
        <resume>WordNet, une des ressources lexicales les plus utilisées aujourd’hui a été constituée en anglais et les chercheurs travaillant sur d’autres langues souffrent du manque d’une telle ressource. Malgré les efforts fournis par la communauté française, les différents WordNets produits pour la langue française ne sont toujours pas aussi exhaustifs que le WordNet de Princeton. C’est pourquoi nous proposons une méthode novatrice dans la production de termes nominaux instanciant les différents synsets de WordNet en exploitant les propriétés syntaxiques distributionnelles du vocabulaire français. Nous comparons la ressource que nous obtenons avecWOLF et montrons que notre approche offre une couverture plus large.</resume>
        <mots_cles>ressources lexicales françaises, WordNet, relations sémantiques, distributions syntaxiques</mots_cles>
        <title/>
        <abstract>WordNet, one of the most used lexical resource until today has been made up for the English language and scientists working on other languages suffer from the lack of such a resource. Despite the efforts performed by the French community, the differentWordNets produced for the French language are still not as exhaustive as the original Princeton WordNet. We propose a new approach in the way of producing nominal terms filling the synset slots. We use syntactical distributional properties of French vocabulary to determine which of the candidates given by a bilingual dictionary matches the best. We compare the resource we obtain withWOLF and show that our approach provides a much larger coverage.</abstract>
        <keywords>French lexical resources, WordNet, semantic relations, syntactical distributionality</keywords>
      </article>
      <article id="taln-2010-court-020" session="Poster">
        <auteurs>
          <auteur>
            <nom>Caroline Brun</nom>
            <email>Caroline.Brun@xrce.xerox.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Maud Ehrmann</nom>
            <email>maud.ehrmann@jrc.ec.europa.eu</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">XRCE, 6, Chemins de Maupertuis, Meylan, France</affiliation>
          <affiliation affiliationId="2">JRC – European Commission, Ispra, Italie</affiliation>
        </affiliations>
        <titre>Un système de détection d’entités nommées adapté pour la campagne d’évaluation ESTER 2</titre>
        <type>court</type>
        <pages/>
        <resume>Dans cet article nous relatons notre participation à la campagne d’évaluation ESTER 2 (Evaluation des Systèmes de Transcription Enrichie d’Emissions Radiophoniques). Après avoir décrit les objectifs de cette campagne ainsi que ses spécificités et difficultés, nous présentons notre système d’extraction d’entités nommées en nous focalisant sur les adaptations réalisées dans le cadre de cette campagne. Nous décrivons ensuite les résultats obtenus lors de la compétition, ainsi que des résultats originaux obtenus par la suite. Nous concluons sur les leçons tirées de cette expérience.</resume>
        <mots_cles>entités nommées, évaluation, extraction d’information</mots_cles>
        <title/>
        <abstract>In this paper, we report our participation to the ESTER 2 (Evaluation des Systèmes de Transcription Enrichie d’Emissions Radiophoniques) evaluation campaign. After describing the goals, specificities and challenges of the campaign, we present our named entity detection system and focus on the adaptations made in the framework of the campaign. We present the results obtained during the competition and then new results obtained afterward. We then conclude by the lessons we learned from this experiment.</abstract>
        <keywords>named entities, evaluation, information extraction</keywords>
      </article>
      <article id="taln-2010-court-021" session="Poster">
        <auteurs>
          <auteur>
            <nom>Philippe Muller</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Philippe Langlais</nom>
            <email/>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">IRIT, Université de Toulouse &amp; Alpage, INRIA</affiliation>
          <affiliation affiliationId="2">DIRO, Université de Montr´eal</affiliation>
        </affiliations>
        <titre>Comparaison de ressources lexicales pour l’extraction de synonymes</titre>
        <type>court</type>
        <pages/>
        <resume/>
        <mots_cles/>
        <title/>
        <abstract/>
        <keywords/>
      </article>
      <article id="taln-2010-court-022" session="Poster">
        <auteurs>
          <auteur>
            <nom>Laurence Longo</nom>
            <email>longo@unistra.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Amalia Todiraşcu</nom>
            <email>todiras@unistra.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire LiLPa, EA 1339, Université de Strasbourg, 67000 Strasbourg Cedex, France</affiliation>
        </affiliations>
        <titre>RefGen : un module d’identification des chaînes de référence dépendant du genre textuel</titre>
        <type>court</type>
        <pages/>
        <resume>Dans cet article, nous présentons RefGen, un module d’identification des chaînes de référence pour le français. RefGen effectue une annotation automatique des expressions référentielles puis identifie les relations de coréférence établies entre ces expressions pour former des chaînes de référence. Le calcul de la référence utilise des propriétés des chaînes de référence dépendantes du genre textuel, l’échelle d’accessibilité d’(Ariel, 1990) et une série de filtres lexicaux, morphosyntaxiques et sémantiques. Nous évaluons les premiers résultats de RefGen sur un corpus issu de rapports publics.</resume>
        <mots_cles>Chaînes de référence, relation de coréférence, saillance, genre textuel</mots_cles>
        <title/>
        <abstract>We present RefGen, a reference chain identification module for French. RefGen automatically annotates referential expressions then identifies coreference relations between these expressions to make reference chains. Reference calculus uses textual genre specific properties of reference chains, (Ariel, 1990)’s accessibility theory and applies lexical, morphosyntactic and semantic filters. We evaluate the first results obtained by RefGen from a public reports corpus.</abstract>
        <keywords>Reference chain identification, coreference relation, salience, genre</keywords>
      </article>
      <article id="taln-2010-court-023" session="Poster">
        <auteurs>
          <auteur>
            <nom>Rosa Stern</nom>
            <email>rosa.stern@afp.com</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Benoît Sagot</nom>
            <email>benoit.sagot@inria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Alpage, INRIA Paris–Rocquencourt &amp; Université Paris 7, Rocquencourt, BP 105, 78153 Le Chesnay Cedex, France</affiliation>
          <affiliation affiliationId="2">Agence France-Presse – Medialab, 2 place de la Bourse, 75002 Paris, France</affiliation>
        </affiliations>
        <titre>Détection et résolution d’entités nommées dans des dépêches d’agence</titre>
        <type>court</type>
        <pages/>
        <resume>Nous présentons NP, un système de reconnaissance d’entités nommées. Comprenant un module de résolution, il permet d’associer à chaque occurrence d’entité le référent qu’elle désigne parmi les entrées d’un référentiel dédié. NP apporte ainsi des informations pertinentes pour l’exploitation de l’extraction d’entités nommées en contexte applicatif. Ce système fait l’objet d’une évaluation grâce au développement d’un corpus annoté manuellement et adapté aux tâches de détection et de résolution.</resume>
        <mots_cles>résolution d’entités nommées, détection d’entités nommées, extraction d’information</mots_cles>
        <title/>
        <abstract>We introduce NP, a system for named entity recognition. It includes a resolution module for linking each entity occurrence to its matching entry in a dedicated reference base. NP thus brings information relevant for using named entity extraction in an applicative context. We have evaluated NP by the means of a manually annotated corpus designed for the tasks of recognition and resolution.</abstract>
        <keywords>named entity resolution, named entity recognition, information extraction</keywords>
      </article>
      <article id="taln-2010-court-024" session="Poster">
        <auteurs>
          <auteur>
            <nom>Marie-Jean Meurs</nom>
            <email>marie-jean.meurs@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Fabrice Lefèvre</nom>
            <email>fabrice.lefevre@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire Informatique d’Avignon (EA 931), F-84911 Avignon, France.</affiliation>
        </affiliations>
        <titre>Processus de décision à base de SVM pour la composition d’arbres de frames sémantiques</titre>
        <type>court</type>
        <pages/>
        <resume>Cet article présente un processus de décision basé sur des classifieurs à vaste marge (SVMDP) pour extraire l’information sémantique dans un système de dialogue oral. Dans notre composant de compréhension, l’information est représentée par des arbres de frames sémantiques définies selon le paradigme FrameNet. Le processus d’interprétation est réalisé en deux étapes. D’abord, des réseaux bayésiens dynamiques (DBN) sont utilisés comme modèles de génération pour inférer des fragments d’arbres de la requête utilisateur. Ensuite, notre SVMDP dépendant du contexte compose ces fragments afin d’obtenir la représentation sémantique globale du message. Les expériences sont menées sur le corpus de dialogue MEDIA. Une procédure semi-automatique fournit une annotation de référence en frames sur laquelle les paramètres des DBN et SVMDP sont appris. Les résultats montrent que la méthode permet d’améliorer les performances d’identification de frames pour les exemples de test les plus complexes par rapport à un processus de décision déterministe ad hoc.</resume>
        <mots_cles>système de dialogue oral, compréhension de la parole, composition sémantique, frame sémantique, séparateur à vaste marge</mots_cles>
        <title/>
        <abstract>This paper presents a decision process based on Support Vector Machines to extract the semantic information from the user’s input in a spoken dialog system. In our interpretation component, the information is represented by means of trees of semantic frames, as defined in the Berkeley FrameNet paradigm, and the understanding process is performed in two steps. First Dynamic Bayesian Networks are used as generative models to sequentially infer tree fragments from the users’ inputs. Then the contextsensitive SVMDP introduced in this paper is applied to detect the relations between the frames hypothesized in the fragments and compose them to obtain the overall semantic representation of the user’s request. Experiments are reported on the French MEDIA dialogue corpus. A semi-automatic process provides a reference frame annotation of the speech training data. The parameters of DBNs and SVMDP are learned from these data. The method is shown to outperform an ad-hoc deterministic decision process on the most complex test examples for frame identification.</abstract>
        <keywords>spoken dialogue system, spoken language understanding, semantic composition, semantic frame, support vector machines</keywords>
      </article>
      <article id="taln-2010-court-025" session="Poster">
        <auteurs>
          <auteur>
            <nom>Béatrice Arnulphy</nom>
            <email>Beatrice.Arnulphy@limsi.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Xavier Tannier</nom>
            <email>Xavier.Tannier@limsi.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Anne Vilnat</nom>
            <email>Anne.Vilnat@limsi.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Univ. Paris-Sud, Orsay, France</affiliation>
          <affiliation affiliationId="2">LIMSI-CNRS, B.P. 133, 91403 Orsay Cedex, France</affiliation>
        </affiliations>
        <titre>Les entités nommées événement et les verbes de cause-conséquence</titre>
        <type>court</type>
        <pages/>
        <resume>L’extraction des événements désignés par des noms est peu étudiée dans des corpus généralistes. Si des lexiques de noms déclencheurs d’événements existent, les problèmes de polysémie sont nombreux et beaucoup d’événements ne sont pas introduits par des déclencheurs. Nous nous intéressons dans cet article à une hypothèse selon laquelle les verbes induisant la cause ou la conséquence sont de bons indices quant à la présence d’événements nominaux dans leur cotexte.</resume>
        <mots_cles>Entité nommée, événement, rapports de cause et conséquence</mots_cles>
        <title/>
        <abstract>Few researches focus on nominal event extraction in open-domain corpora. Lists of cue words for events exist, but raise many problems of polysemy. In this article, we focus on the following hypothesis : verbs introducing cause or consequence links have good chances to have an event noun around them.</abstract>
        <keywords>Named entity, event, cause and consequence links</keywords>
      </article>
      <article id="taln-2010-court-026" session="Poster">
        <auteurs>
          <auteur>
            <nom>Alexander Pak</nom>
            <email>alexpak@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Patrick Paroubek</nom>
            <email>pap@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université de Paris-Sud, Laboratoire LIMSI-CNRS, Bâtiment 508, F-91405 Orsay Cedex, France</affiliation>
        </affiliations>
        <titre>Construction d’un lexique affectif pour le français à partir de Twitter</titre>
        <type>court</type>
        <pages/>
        <resume>Un lexique affectif est un outil utile pour l’étude des émotions ainsi que pour la fouille d’opinion et l’analyse des sentiments. Un tel lexique contient des listes de mots annotés avec leurs évaluations émotionnelles. Il existe un certain nombre de lexiques affectifs pour la langue anglaise, espagnole, allemande, mais très peu pour le français. Un travail de longue haleine est nécessaire pour construire et enrichir un lexique affectif. Nous proposons d’utiliser Twitter, la plateforme la plus populaire de microblogging de nos jours, pour recueillir un corpus de textes émotionnels en français. En utilisant l’ensemble des données recueillies, nous avons estimé les normes affectives de chaque mot. Nous utilisons les données de la Norme Affective desMots Anglais (ANEW, Affective Norms of EnglishWords) que nous avons traduite en français afin de valider nos résultats. Les valeurs du coefficient tau de Kendall et du coefficient de corrélation de rang de Spearman montrent que nos scores estimés sont en accord avec les scores ANEW.</resume>
        <mots_cles>Analyse de sentiments, ANEW, Twitter</mots_cles>
        <title/>
        <abstract>Affective lexicons are a useful tool for emotion studies as well as for opinion mining and sentiment analysis. Such lexicons contain lists of words annotated with their emotional assessments. There exist a number of affective lexicons for English, Spanish, German and other languages. However, only a few of such resources are available for French. A lot of human efforts are needed to build and extend an affective lexicon. We propose to use Twitter, the most popular microblogging platform nowadays, to collect a dataset of emotional texts in French. Using the collected dataset, we estimated the affective norms of words present in our corpus. We used the dataset of Affective Norms of English Words (ANEW) that we translated into French to validate our results. Values of Kendall’s tau coefficient and Spearman’s rank correlation coefficient show that our estimated scores correlate well with the ANEW scores.</abstract>
        <keywords>Sentiment analysis, ANEW, Twitter</keywords>
      </article>
      <article id="taln-2010-court-027" session="Poster">
        <auteurs>
          <auteur>
            <nom>Lei Zhang</nom>
            <email>Lei.Zhang@unicaen.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Stéphane Ferrari</nom>
            <email>Stephane.Ferrari@unicaen.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">GREYC - Département Informatique, Université de Caen - Campus 2, 14032 Caen</affiliation>
        </affiliations>
        <titre>Analyse d’opinion : annotation sémantique de textes chinois</titre>
        <type>court</type>
        <pages/>
        <resume>Notre travail concerne l’analyse automatique des énoncés d’opinion en chinois. En nous inspirant de la théorie linguistique de l’Appraisal, nous proposons une méthode fondée sur l’usage de lexiques et de règles locales pour déterminer les caractéristiques telles que la Force (intensité), le Focus (prototypicalité) et la polarité de tels énoncés. Nous présentons le modèle et sa mise en oeuvre sur un corpus journalistique. Si pour la détection d’énoncés d’opinion, la précision est bonne (94 %), le taux de rappel (67 %) pose cependant des questions sur l’enrichissement des ressources actuelles.</resume>
        <mots_cles>Analyse d’opinion, théorie de l’Appraisal</mots_cles>
        <title/>
        <abstract>Our work concerns automatic analysis of opinion in texts. Based on the Appraisal linguistic theory, our method uses lexical and syntactic resources to process such properties as the Force, the Focus and the polarity of an opinion. We present our model and its implementation on a journalistic corpus. The precision for detecting opinion expressions is high (94%), but the recall (67%) raises the question of how to enhance the resources.</abstract>
        <keywords>Opinion analysis, Appraisal theory</keywords>
      </article>
      <article id="taln-2010-court-028" session="Poster">
        <auteurs>
          <auteur>
            <nom>Peggy Cellier</nom>
            <email>Peggy.Cellier@info.unicaen.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Thierry Charnois</nom>
            <email>Thierry.Charnois@info.unicaen.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">GREYC – CNRS UMR 6072, Université de Caen – Bd Mal Juin 14032 Caen, France</affiliation>
        </affiliations>
        <titre>Fouille de données séquentielles d’itemsets pour l’apprentissage de patrons linguistiques</titre>
        <type>court</type>
        <pages/>
        <resume>Dans cet article nous présentons une méthode utilisant l’extraction de motifs séquentiels d’itemsets pour l’apprentissage automatique de patrons linguistiques. De plus, nous proposons de nous appuyer sur l’ordre partiel existant entre les motifs pour les énumérer de façon structurée et ainsi faciliter leur validation en tant que patrons linguistiques.</resume>
        <mots_cles>Fouille de données, motifs séquentiels, extraction d’information, apprentissage de patrons linguistiques</mots_cles>
        <title/>
        <abstract>In this paper, we present a method based on the extraction of itemset sequential patterns in order to automatically generate linguistic patterns. In addition, we propose to use the partial ordering between sequential patterns to enumerate and validate them.</abstract>
        <keywords>Data mining, sequential patterns, information extraction, linguistic pattern learning</keywords>
      </article>
      <article id="taln-2010-court-029" session="Poster">
        <auteurs>
          <auteur>
            <nom>Anouar Ben Hassena</nom>
            <email>benhasse@enssat.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Laurent Miclet</nom>
            <email>miclet@enssat.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">ENSSAT / IRISA, Lannion, France</affiliation>
        </affiliations>
        <titre/>
        <type>court</type>
        <pages/>
        <resume>En intelligence artificielle, l’analogie est utilisée comme une technique de raisonnement non exact pour la résolution de problèmes, la compréhension du langage naturel, l’apprentissage des règles de classification, etc. Cet article s’intéresse à la proportion analogique, une forme simple du raisonnement par analogie, et présente son application en apprentissage automatique pour le TALN. La proportion analogique est une relation entre quatre objets qui exprime que la manière de transformer le premier objet en le second est la même que la façon de transformer le troisième en le quatrième. Premièrement, nous définissons formellement la proportion analogique entre quatre objets. Nous nous intéressons particulièrement aux objets structurés que sont les arbres ordonnés et étiquetés, avec une définition originale de l’analogie fondée sur l’alignement optimal. Ensuite, nous présentons deux algorithmes qui calculent la dissemblance analogique entre quatre arbres et qui trouvent des solutions, éventuellement approchées, à une équation analogique entre arbres. Nous montrons leur utilisation dans deux applications : l’apprentissage de l’arbre syntaxique d’une phrase et la génération de la prosodie dans la synthèse de parole.</resume>
        <mots_cles>Proportion analogique, arbre syntaxique, analyseur syntaxique analogique</mots_cles>
        <title>Tree analogical learning. Application in NLP</title>
        <abstract>In Artificial Intelligence, analogy is used as a non exact reasoning technique to solve problems, for natural language processing, for learning classification rules, etc. This paper is interested in the analogical proportion, a simple form of the reasoning by analogy, and presents some of its uses in machine learning for NLP. The analogical proportion is a relation between four objects that expresses that the way to transform the first object into the second is the same as the way to transform the third in the fourth. We firstly give definitions about the general notion of analogical proportion between four objects. We give a special focus on objects structured as ordered and labeled trees, with an original definition of analogy based on optimal alignment. Secondly, we present two algorithms which deal with tree analogical matching and solving analogical equations between trees. We show their use in two applications : the learning of the syntactic tree (parsing) of a sentence and the generation of prosody for synthetic speech.</abstract>
        <keywords>Analogical proportion, syntactic tree, analogical syntactic parser</keywords>
      </article>
      <article id="taln-2010-court-030" session="Poster">
        <auteurs>
          <auteur>
            <nom>Mehdi Embarek</nom>
            <email>mehdi.embarek@cea.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Olivier Ferret</nom>
            <email>olivier.ferret@cea.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CEA, LIST, Laboratoire Vision et Ingénierie des Contenus, Fontenay-aux-Roses, F-92265 France.</affiliation>
        </affiliations>
        <titre>Adapter un système de question-réponse en domaine ouvert au domaine médical</titre>
        <type>court</type>
        <pages/>
        <resume>Dans cet article, nous présentons Esculape, un système de question-réponse en français dédié aux médecins généralistes et élaboré à partir d’OEdipe, un système de question-réponse en domaine ouvert. Esculape ajoute à OEdipe la capacité d’exploiter la structure d’un modèle du domaine, le domaine médical dans le cas présent. Malgré l’existence d’un grand nombre de ressources dans ce domaine (UMLS, MeSH ...), il n’est pas possible de se reposer entièrement sur ces ressources, et plus spécifiquement sur les relations qu’elles abritent, pour répondre aux questions. Nous montrons comment surmonter cette difficulté en apprenant de façon supervisée des patrons linguistiques d’extraction de relations et en les appliquant à l’extraction de réponses.</resume>
        <mots_cles>systèmes de question-réponse, extraction de relations, domaine médical</mots_cles>
        <title/>
        <abstract>In this article, we present Esculape, a question-answering system for French dedicated to family doctors and built from OEdipe, an open-domain system. Esculape adds to OEdipe the capability to exploit the concepts and relations of a domain model, the medical domain in the present case. Although a large number of resources exist in this domain (UMLS, MeSH ...), it is not possible to rely only on them, and more specifically on the relations they contain, to answer questions. We show how this difficulty can be overcome by learning linguistic patterns for identifying relations and applying them to extract answers.</abstract>
        <keywords>question-answering systems, relation extraction, medical domain</keywords>
      </article>
      <article id="taln-2010-court-031" session="Poster">
        <auteurs>
          <auteur>
            <nom>Inès Zribi</nom>
            <email>ineszribi@gmail.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Souha Mezghani Hammami</nom>
            <email>souha.mezghani@fsegs.rnu.tn</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Lamia Hadrich Belguith</nom>
            <email>l.belguith@fsegs.rnu.tn</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">ANLP Research Group – Laboratoire MIRACL, Faculté des Sciences Economiques et de Gestion de Sfax, B.P. 1088, 3018 - Sfax – TUNISIE</affiliation>
        </affiliations>
        <titre>L’apport d’une approche hybride pour la reconnaissance des entités nommées en langue arabe</titre>
        <type>court</type>
        <pages/>
        <resume>Dans cet article, nous proposons une méthode hybride pour la reconnaissance des entités nommées pour la langue arabe. Cette méthode profite, d’une part, des avantages de l’utilisation d’une méthode d’apprentissage pour extraire des règles permettant l’identification et la classification des entités nommées. D’autre part, elle repose sur un ensemble de règles extraites manuellement pour corriger et améliorer le résultat de la méthode d’apprentissage. Les résultats de l’évaluation de la méthode proposée sont encourageants. Nous avons obtenu un taux global de F-mesure égal à 79.24%.</resume>
        <mots_cles>Traitement de la langue arabe, reconnaissance des entités nommées, méthode d’apprentissage</mots_cles>
        <title/>
        <abstract>In this paper, we propose a hybrid method for Arabic named entities recognition. This method takes advantage of the use of a learning method to extract rules for the identification and classification of named entities. Moreover, it is based on a set of rules extracted manually to correct and improve the outcome of the learning method. The evaluation results are encouraging as we get an overall F-measure equal to 79.24%.</abstract>
        <keywords>Arabic language processing, named entity recognition, learning method</keywords>
      </article>
      <article id="taln-2010-court-032" session="Poster">
        <auteurs>
          <auteur>
            <nom>Richard Moot</nom>
            <email>Richard.Moot@labri.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LaBRI (CNRS, Bordeaux) &amp; SIGNES (INRIA Bordeaux SW), 351 cours de la Libération, 33405 Talence, FRANCE</affiliation>
        </affiliations>
        <titre/>
        <type>court</type>
        <pages/>
        <resume>Cet article décrit le développement d’une grammaire catégorielle à large couverture du Français, extraite à partir du corpus arboré de Paris 7 et vérifiée et corrigée manuellement. Le grammaire catégorielle résultant est évaluée en utilisant un supertagger et obtient des résultats comparables aux meilleurs supertaggers pour l’Anglais.</resume>
        <mots_cles>Extraction de grammaires, grammaires catégorielles, supertagging</mots_cles>
        <title>Semi-automated Extraction of a Wide-Coverage Type-Logical Grammar for French</title>
        <abstract>The paper describes the development of a wide-coverage type-logical grammar for French, which has been extracted from the Paris 7 treebank and received a significant amount of manual verification and cleanup. The resulting treebank is evaluated using a supertagger and performs at a level comparable to the best supertagging results for English.</abstract>
        <keywords>Categorial grammar, grammar extraction, supertagging, type-logical grammar</keywords>
      </article>
      <article id="taln-2010-court-033" session="Poster">
        <auteurs>
          <auteur>
            <nom>Yayoi Nakamura-Delloye</nom>
            <email>yayoi@yayoi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Éric Villemonte De La Clergerie</nom>
            <email>eric.de_la_clergerie@inria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">ALPAGE, INRIA-Rocquencourt, Domaine de Voluceau Rocquencourt B.P.105, 78153 Le Chesnay</affiliation>
        </affiliations>
        <titre>Exploitation de résultats d’analyse syntaxique pour extraction semi-supervisée des chemins de relations</titre>
        <type>court</type>
        <pages/>
        <resume>Le présent article décrit un travail en cours sur l’acquisition des patrons de relations entre entités nommées à partir de résultats d’analyse syntaxique. Sans aucun patron prédéfini, notre méthode fournit des chemins syntaxiques susceptibles de représenter une relation donnée à partir de quelques exemples de couples d’entités nommées entretenant la relation en question.</resume>
        <mots_cles>Extraction des connaissances, extraction des patrons, relation des entités nommées, arbre syntaxique dépendanciel</mots_cles>
        <title/>
        <abstract>This paper describes our current work on the acquisition of named entity relation patterns from parsing results. Without any predefined pattern, our method provides candidate syntactic paths that represent a given relationship with a small seed set of named entity pairs on this relationship.</abstract>
        <keywords>Knowledge extraction, pattern extraction, named entity relation, syntactic dependency tree</keywords>
      </article>
      <article id="taln-2010-court-034" session="Poster">
        <auteurs>
          <auteur>
            <nom>Damien Nouvel</nom>
            <email>Damien.Nouvel@univ-tours.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Arnaud Soulet</nom>
            <email>Arnaud.Soulet@univ-tours.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Jean-Yves Antoine</nom>
            <email>Jean-Yves.Antoine@univ-tours.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Nathalie Friburger</nom>
            <email>Nathalie.Friburger@univ-tours.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Denis Maurel</nom>
            <email>Denis.Maurel@univ-tours.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université François Rabelais Tours, LI, Antenne Universitaire de Blois, 3 place Jean Jaurès, F-41000 Blois, France</affiliation>
        </affiliations>
        <titre>Reconnaissance d’entités nommées : enrichissement d’un système à base de connaissances à partir de techniques de fouille de textes</titre>
        <type>court</type>
        <pages/>
        <resume>Dans cet article, nous présentons et analysons les résultats du système de reconnaissance d’entités nommées CasEN lors de sa participation à la campagne d’évaluation Ester2. Nous identifions quelles ont été les difficultés pour notre système, essentiellement : les mots hors-vocabulaire, la métonymie, les frontières des entités nommées. Puis nous proposons une approche pour améliorer les performances de systèmes à base de connaissances, en utilisant des techniques exhaustives de fouille de données séquentielles afin d’extraire des motifs qui représentent les structures linguistiques en jeu lors de la reconnaissance d’entités nommées. Enfin, nous décrivons l’expérimentation menée à cet effet, donnons les résultats obtenus à ce jour et en faisons une première analyse.</resume>
        <mots_cles>Reconnaissance d’Entités Nommées, Séquences Hiérarchiques, Motifs, Ester2</mots_cles>
        <title/>
        <abstract>In this paper, we present and analyze the results obtained by our named entity recognition system, CasEN, during the Ester2 evaluation campaign.We identify on what difficulties our system was the most challenged, which mainly are : out-of-vocabulary words, metonymy and detection of the boundaries of named entities. Next, we propose a direction which may help us for improving performances of our system, by using exhaustive hierarchical and sequential data mining algorithms. This approach aims at extracting patterns corresponding to useful linguistic constructs for recognizing named entities. Finaly, we describe our experiments, give the results we currently obtain and analyze those results.</abstract>
        <keywords>Named Entity Recognition, Hierarchical Sequences, Patterns, Ester2</keywords>
      </article>
      <article id="taln-2010-court-035" session="Poster">
        <auteurs>
          <auteur>
            <nom>Benoît Gaillard</nom>
            <email>benoit.gaillard@univ-tlse2.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Olivier Collin</nom>
            <email>olivier.collin@univ-tlse2.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Malek Boualem</nom>
            <email>malek.boualem@orange-ftgroup.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Orange Labs – 2, Avenue Pierre Marzin, 22300 Lannion, France</affiliation>
        </affiliations>
        <titre>Traduction de requêtes basée sur Wikipédia</titre>
        <type>court</type>
        <pages/>
        <resume>Cet article s'inscrit dans le domaine de la recherche d'information multilingue. Il propose une méthode de traduction automatique de requêtes basée sur Wikipédia. Une phase d'analyse permet de segmenter la requête en syntagmes ou unités lexicales à traduire en s'appuyant sur les liens multilingues entre les articles de Wikipédia. Une deuxième phase permet de choisir, parmi les traductions possibles, celle qui est la plus cohérente en s'appuyant sur les informations d'ordre sémantique fournies par les catégories associées à chacun des articles de Wikipédia. Cet article justifie que les données issues de Wikipédia sont particulièrement pertinentes pour la traduction de requêtes, détaille l'approche proposée et son implémentation, et en démontre le potentiel par la comparaison du taux d'erreur du prototype de traduction avec celui d'autres services de traduction automatique.</resume>
        <mots_cles>recherche d'information multilingue, traduction de requêtes, Wikipédia</mots_cles>
        <title/>
        <abstract>This work investigates query translation using only Wikipedia-based resources in a two steps approach: analysis and disambiguation. After arguing that data mined from Wikipedia is particularly relevant to query translation, we detail the implementation of the approach. In the analysis phase, queries are segmented into lexical units that are associated to several possible translations using a bilingual dictionary extracted from Wikipedia. During the second phase, one translation is chosen amongst the various candidates, based on consistency, asserted with the help of semantic information carried by categories associated to Wikipedia articles. These two steps take advantage of data mined from Wikipedia, which is very rich and detailed, constantly updated but also easy and free to access. We report promising results regarding translation accuracy.</abstract>
        <keywords>cross language information retrieval, query translation, Wikipedia</keywords>
      </article>
      <article id="taln-2010-court-036" session="Poster">
        <auteurs>
          <auteur>
            <nom>Husam Ali</nom>
            <email>ali@cs.uleth.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Yllias Chali</nom>
            <email>chali@cs.uleth.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Sadid A. Hasan</nom>
            <email>hasan@cs.uleth.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">University of Lethbridge, Lethbridge, AB, Canada</affiliation>
        </affiliations>
        <titre/>
        <type>court</type>
        <pages/>
        <resume/>
        <mots_cles>Génération de questions, Analyseur syntaxique, Phrases élémentaires, POS Tagging</mots_cles>
        <title>Automatic Question Generation from Sentences</title>
        <abstract>Question Generation (QG) and Question Answering (QA) are some of the many challenges for natural language understanding and interfaces. As humans need to ask good questions, the potential benefits from automated QG systems may assist them in meeting useful inquiry needs. In this paper, we consider an automatic Sentence-to-Question generation task, where given a sentence, the Question Generation (QG) system generates a set of questions for which the sentence contains, implies, or needs answers. To facilitate the question generation task, we build elementary sentences from the input complex sentences using a syntactic parser. A named entity recognizer and a part of speech tagger are applied on each of these sentences to encode necessary information.We classify the sentences based on their subject, verb, object and preposition for determining the possible type of questions to be generated. We use the TREC-2007 (Question Answering Track) dataset for our experiments and evaluation.</abstract>
        <keywords>Question Generation, Syntactic Parsing, Elementary Sentence, POS Tagging</keywords>
      </article>
      <article id="taln-2010-demo-001" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Éric Brunelle</nom>
            <email>developpement@druide.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Simon Charest</nom>
            <email>developpement@druide.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Druide informatique inc., 1435, rue St-Alexandre, bureau 1040, Montréal (Québec) H3A 2G4, Canada</affiliation>
        </affiliations>
        <titre>Présentation du logiciel Antidote HD</titre>
        <type>démonstration</type>
        <pages/>
        <resume/>
        <mots_cles/>
        <title/>
        <abstract/>
        <keywords/>
      </article>
      <article id="taln-2010-demo-002" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Caroline Barrière</nom>
            <email>caroline.barriere@nrc-cnrc.gc.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">ITI-CNR, Gatineau, Canada</affiliation>
        </affiliations>
        <titre>TerminoWeb : recherche et analyse d’information thématique</titre>
        <type>démonstration</type>
        <pages/>
        <resume>Notre démonstration porte sur le prototype TerminoWeb, une plateforme Web qui permet (1) la construction automatique d’un corpus thématique à partir d’une recherche de documents sur le Web, (2) l’extraction de termes du corpus, et (3) la recherche d’information définitionnelle sur ces termes en corpus. La plateforme intégrant les trois modules, elle aidera un langagier (terminologue, traducteur, rédacteur) à découvrir un nouveau domaine (thème) en facilitant la recherche et l’analyse de documents informatifs pertinents à ce domaine.</resume>
        <mots_cles>information thématique, construction de corpus, extraction de termes, découverte de contextes définitionnels</mots_cles>
        <title/>
        <abstract>Our demonstration shows the TerminoWeb prototype, a Web platform which can (1) automatically assemble a thematic corpus from Web documents, (2) extract terms from that corpus, and (3) find definitional information in the corpus about terms of interest. As the platform integrates all three modules, it can help a language worker (terminologist, translator, writer) to explore a new domain (theme) as it facilitates the gathering and analysis of informative documents about that domain.</abstract>
        <keywords>thematic information, corpus construction, term extraction, definitional contexts discovery</keywords>
      </article>
      <article id="taln-2010-demo-003" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Christian Boitet</nom>
            <email>Christian.Boitet@imag.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Cong Phap Huynh</nom>
            <email>Cong-Phap.Huynh@imag.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Hong Thai Nguyen</nom>
            <email>Hong-Thai.Nguyen@imag.fr</email>
            <affiliationId/>
          </auteur>
          <auteur>
            <nom>Valérie Bellynck</nom>
            <email>Valerie.Bellynck@imag.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">GETALP, LIG-campus (UJF, CNRS, INPG, INRIA, UPMF), BP 53, 38041 GRENOBLE cedex 9, France</affiliation>
        </affiliations>
        <titre/>
        <type>démonstration</type>
        <pages/>
        <resume/>
        <mots_cles/>
        <title>The iMAG concept: multilingual access gateway to an elected Web sites with incremental quality increase through collaborative post-edition of MT pretranslations</title>
        <abstract>We will demonstrate iMAGs (interactive Multilingual Access Gateways), in particular on a scientific laboratory web site and on the Greater Grenoble (La Métro) web site.</abstract>
        <keywords>Interactive translation gateway, iMAG, MT post-editing, collaborative translation</keywords>
      </article>
      <article id="taln-2010-demo-004" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Wajdi Zaghouani</nom>
            <email>wajdiz@ldc.upenn.edu</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Linguistic Data Consortium, 3600 Market street suite 810, Philadelphia, USA</affiliation>
        </affiliations>
        <titre>L'intégration d'un outil de repérage d'entités nommées pour la langue arabe dans un système de veille</titre>
        <type>démonstration</type>
        <pages/>
        <resume>Dans cette démonstration, nous présentons l'implémentation d'un outil de repérage d'entités nommées à base de règle pour la langue arabe dans le système de veille médiatique EMM (Europe Media Monitor).</resume>
        <mots_cles>Étiquetage des entités nommées, langue arabe, système de veille médiatique</mots_cles>
        <title/>
        <abstract>We will present in this demo an Arabic rule-based named entity recogntion tool which is integrated within the news monitoring system EMM (Europe Media Montior).</abstract>
        <keywords>Named entity recognition, Arabic language, news monitoring system</keywords>
      </article>
      <article id="taln-2010-demo-005" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Olivier Blanc</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Noémi Boubel</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Jean-Philippe Goldman</nom>
            <email/>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Sophie Roekhaut</nom>
            <email/>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Anne Catherine Simon</nom>
            <email/>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Cédrick Fairon</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Richard Beaufort</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CENTAL, Université catholique de Louvain, 1348 Louvain-la-Neuve, Belgique</affiliation>
          <affiliation affiliationId="2">Valibel, Université catholique de Louvain, 1348 Louvain-la-Neuve, Belgique</affiliation>
          <affiliation affiliationId="3">TCTS Lab, Université de Mons, 7000 Mons, Belgique</affiliation>
        </affiliations>
        <titre>Expressive : Génération automatique de parole expressive à partir de données non linguistiques</titre>
        <type>démonstration</type>
        <pages/>
        <resume>Nous présentons Expressive, un système de génération de parole expressive à partir de données non linguistiques. Ce système est composé de deux outils distincts : Taittingen, un générateur automatique de textes d’une grande variété lexico-syntaxique produits à partir d’une représentation conceptuelle du discours, et StyloPhone, un système de synthèse vocale multi-styles qui s’attache à rendre le discours produit attractif et naturel en proposant différents styles vocaux.</resume>
        <mots_cles>Génération de texte, synthèse vocale, expressivité</mots_cles>
        <title/>
        <abstract>We present Expressive, a system that converts non-linguistic data into expressive speech. This system is made of two distinct parts : Taittingen, a natural language generation tool able to produce lexically and syntactically rich texts from a discourse abstract representation, and StyloPhone, a text-to-speech synthesis system that proposes varying speaking styles, to make the speech sound both more attractive and natural.</abstract>
        <keywords>Natural language generation, text-to-speech synthesis, expressiveness</keywords>
      </article>
      <article id="taln-2010-demo-006" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Fatiha Sadat</nom>
            <email>sadat.fatiha@uqam.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Alexandre Terrasa</nom>
            <email>alexandre.terrasa@viacesi.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université du Québec à Montréal, 201 av. President Kennedy, Montréal, QC, H3X 2Y3, Canada</affiliation>
          <affiliation affiliationId="2">École Supérieure d’Informatique Appliquée EXIA.CESI, 11 avenue Neil Armstrong, 33700 Mérignac, Bordeaux, France</affiliation>
        </affiliations>
        <titre>Exploitation de Wikipédia pour l’Enrichissement et la Construction des Ressources Linguistiques</titre>
        <type>démonstration</type>
        <pages/>
        <resume>Cet article présente une approche et des résultats utilisant l'encyclopédie en ligne Wikipédia comme ressource semi-structurée de connaissances linguistiques et en particulier comme un corpus comparable pour l’extraction de terminologie bilingue. Cette approche tend à extraire d’abord des paires de terme et traduction à partir de types des informations, liens et textes de Wikipédia. L’étape suivante consiste à l’utilisation de l’information linguistique afin de ré-ordonner les termes et leurs traductions pertinentes et ainsi éliminer les termes cibles inutiles. Les évaluations préliminaires utilisant les paires de langues français-anglais, japonais-français et japonais-anglais ont montré une bonne qualité des paires de termes extraits. Cette étude est très favorable pour la construction et l’enrichissement des ressources linguistiques tels que les dictionnaires et ontologies multilingues. Aussi, elle est très utile pour un système de recherche d’information translinguistique (RIT).</resume>
        <mots_cles>Terminologie bilingue, corpus comparable, Wikipédia, ontologie multilingue</mots_cles>
        <title/>
        <abstract>Multilingual linguistic resources are usually constructed from parallel corpora, but since these corpora are available only for selected text domains and language pairs, the potential of other resources is being explored as well. This article seeks to explore and exploit the idea of using multilingual web-based encyclopaedias such as Wikipedia as comparable corpora for bilingual terminology extraction. We propose an approach to extract terms and their translations from different types of Wikipedia link information and texts. The next step will be using a linguistic-based information to re-rank and filter the extracted term candidates in the target language. Preliminary evaluations using the combined statisticsbased and linguistic-based approaches were applied on Japanese-French, French-English and Japanese- French. These evaluations showed a real open improvement and good quality of the extracted term candidates for building or enriching multilingual ontologies, dictionaries or feeding a cross-language information retrieval system with the related expansion terms of the source query.</abstract>
        <keywords>Bilingual terminology, comparable corpora, Wikipedia, mulilingual ontologies</keywords>
      </article>
      <article id="taln-2010-demo-007" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Annelies Braffort</nom>
            <email>annelies.braffort@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Michael Filhol</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Jérémie Segouat</nom>
            <email/>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIMSI-CNRS, Campus d'Orsay bat. 508, BP 133, F691403 Orsay cx, France</affiliation>
          <affiliation affiliationId="2">WebSourd, 99 route d’Espagne, F631100 Toulouse, France</affiliation>
        </affiliations>
        <titre>Traitement automatique des langues des signes : le projet Dicta-Sign, des corpus aux applications</titre>
        <type>démonstration</type>
        <pages/>
        <resume>Cet article présente Dicta-Sign, un projet de recherche sur le traitement automatique des langues des signes (LS), qui aborde un grand nombre de questions de recherche : linguistique de corpus, modélisation linguistique, reconnaissance et génération automatique. L’objectif de ce projet est de réaliser trois applications prototypes destinées aux usagers sourds : un traducteur de termes de LS à LS, un outil de recherche par l’exemple et un Wiki en LS. Pour cela, quatre corpus comparables de cinq heures de dialogue seront produits et analysés. De plus, des avancées significatives sont attendues dans le domaine des outils d’annotation. Dans ce projet, le LIMSI est en charge de l’élaboration des modèles linguistiques et participe aux aspects corpus et génération automatique. Nous nous proposons d’illustrer l’état d’avancement de Dicta-Sign au travers de vidéos extraites du corpus et de démonstrations des outils de traitement et de génération d’animations de signeur virtuel.</resume>
        <mots_cles>Langue des signes, corpus vidéo comparables, reconnaissance automatique, génération automatique</mots_cles>
        <title/>
        <abstract>This paper presents Dicta-Sign, a research project related to sign language (SL) processing. It covers numerous research topics: corpus linguistics, linguistic modelling, automatic recognition and generation. The aim of this project is to design three proof-of-concept end user applications: an SL-to-SL term translator, a search-by-example tool, and a SL wiki. For that, four comparable corpora of five hours will be produced and analysed. Aside from these applications, we also expect major improvements to be integrated to annotation tools. In this project, LIMSI is in charge of the linguistic modelling, and participates in building the corpus and in the generation efforts. We propose to illustrate the current work with excerpts from the corpus and demonstrations of the processing and generation tools.</abstract>
        <keywords>Sign languages, comparable video corpora, automatic recognition, automatic generation</keywords>
      </article>
      <article id="taln-2010-demo-008" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Jean-Philippe Goldman</nom>
            <email>Jean-Philippe.Goldman@unige.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Kamel Nebhi</nom>
            <email>Kamel.Nebhi@unige.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Christopher Laenzlinger</nom>
            <email>Christopher.Laenzlinger@unige.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LATL - Laboratoire d’Analyse et de Technologie du Langage, Département de Linguistique, Faculté des Lettres, Université de Genève</affiliation>
        </affiliations>
        <titre>FipsColor : grammaire en couleur interactive pour l’apprentissage du français</titre>
        <type>démonstration</type>
        <pages/>
        <resume>L'analyseur multilingue FiPS permet de transformer une phrase en une structure syntaxique riche et accompagnée d'informations lexicales, grammaticales et thématiques. On décrit ici une application qui adapte les structures en constituants de l’analyseur FiPS à une nomenclature grammaticale permettant la représentation en couleur. Cette application interactive et disponible en ligne (http://latl.unige.ch/fipscolor) peut être utilisée librement par les enseignants et élèves de primaire.</resume>
        <mots_cles>analyse syntaxique, grammaire générative, services web, tei</mots_cles>
        <title/>
        <abstract>The FiPS parser analyzes a sentence into a syntactic structure reflecting lexical, grammatical and thematic information. The present paper describes the adaptation of the structures in terms of constituents as existent in FiPS to a grammatical annotation, as well as its coloured representation. This online interactive application (available at http://latl.unige.ch/fipscolor) can be freely used by teachers and pupils of primary education.</abstract>
        <keywords>chart parser, generative grammar, web services, tei</keywords>
      </article>
      <article id="taln-2010-demo-009" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Yves Scherrer</nom>
            <email>yves.scherrer@unige.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université de Genève, Rue de Candolle 5, 1211 Genève 4, Suisse</affiliation>
        </affiliations>
        <titre>Des cartes dialectologiques numérisées pour le TALN</titre>
        <type>démonstration</type>
        <pages/>
        <resume>Cette démonstration présente une interface web pour des données numérisées de l’atlas linguistique de la Suisse allemande. Nous présentons d’abord l’intégration des données brutes et des données interpolées de l’atlas dans une interface basée sur Google Maps. Ensuite, nous montrons des prototypes de systèmes de traduction automatique et d’identification de dialectes qui s’appuient sur ces données dialectologiques numérisées.</resume>
        <mots_cles>Dialectologie, atlas linguistique, traduction automatique, identification de dialectes</mots_cles>
        <title/>
        <abstract>This demonstration presents a web interface for digitized data of the linguistic atlas of German-speaking Switzerland. First, we present the integration of raw and interpolated atlas data with an interface based on Google Maps. Then, we show prototypes of machine translation and dialect identification systems which rely on the digitized dialectological data.</abstract>
        <keywords>Dialectology, linguistic atlas, machine translation, dialect identification</keywords>
      </article>
      <article id="taln-2010-demo-010" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Richard Beaufort</nom>
            <email>richard.beaufort@uclouvain.be</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Kévin Macé</nom>
            <email>kevin.mace@uclouvain.be</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Cédrick Fairon</nom>
            <email>cedrick.fairon@uclouvain.be</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CENTAL, Université catholique de Louvain, 1348 Louvain-la-Neuve, Belgique</affiliation>
        </affiliations>
        <titre>Text-it /Voice-it Une application mobile de normalisation des SMS</titre>
        <type>démonstration</type>
        <pages/>
        <resume>Cet article présente Text-it / Voice-it, une application de normalisation des SMS pour téléphone mobile. L’application permet d’envoyer et de recevoir des SMS normalisés, et offre le choix entre un résultat textuel (Text-it) et vocal (Voice-it).</resume>
        <mots_cles>SMS, normalisation, application, plugin, serveur</mots_cles>
        <title/>
        <abstract>This paper presents Text-it / Voice-it, an application that makes it possible to normalize text messages directly from mobile phones. The application allows the user to both send and receive normalized text messages, and gives the choice between a textual (Text-it) and a vocal (Voice-it) result.</abstract>
        <keywords>Text messages, normalization, application, plugin, server</keywords>
      </article>
      <article id="taln-2010-demo-011" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Richard Moot</nom>
            <email>Richard.Moot@labri.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LaBRI (CNRS, Bordeaux) &amp; SIGNES (INRIA Bordeaux SW), 351 cours de la Libération, 33405 Talence, FRANCE</affiliation>
        </affiliations>
        <titre/>
        <type>démonstration</type>
        <pages/>
        <resume>Cette démonstration décrit Grail : un analyseur syntaxique pour grammaires catégorielles. Elle met l’accent sur les recherches récentes qui ont permis à Grail de donner des analyses syntaxiques et sémantiques du Français. Ces développements sont possibles grâce à une grammaire extraite semiautomatiquement du corpus de Paris 7 ainsi qu’un lexique sémantique qui traduit des combinaisons de mots, des étiquettes syntaxiques et des formules en Discourse Representation Structures.</resume>
        <mots_cles>Discourse Representation Theory, grammaires catégorielles</mots_cles>
        <title>Wide-Coverage French Syntax and Semantics using Grail</title>
        <abstract>The system demo introduces Grail, a general-purpose parser for multimodal categorial grammars, with special emphasis on recent research which makes Grail suitable for wide-coverage French syntax and semantics. These developments have been possible thanks to a categorial grammar which has been extracted semi-automatically from the Paris 7 treebank and a semantic lexicon which maps word, part-of-speech tags and formulas combinations to Discourse Representation Structures.</abstract>
        <keywords>Categorial grammar, Discourse Representation Theory, type-logical grammar</keywords>
      </article>
      <article id="taln-2010-demo-012" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Asma Ben Abacha</nom>
            <email>asma.benabacha@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Pierre Zweigenbaum</nom>
            <email>pz@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIMSI - CNRS, B.P. 133 91403 ORSAY CEDEX FRANCE</affiliation>
        </affiliations>
        <titre>MeTAE : Plate-forme d’annotation automatique et d’exploration sémantiques pour le domaine médical</titre>
        <type>démonstration</type>
        <pages/>
        <resume>Nous présentons une plate-forme d’annotation sémantique et d’exploration de textes médicaux, appelée « MeTAE ». Le processus d’annotation automatique comporte une première étape de reconnaissance des entités médicales présentes dans les textes suivie d’une étape d’identification des relations sémantiques qui les relient. Cette identification se fonde sur des patrons linguistiques construits manuellement pour chaque type de relation. MeTAE génère des annotations RDF à partir des informations extraites et offre une interface d’exploration des textes annotés avec des requêtes sous forme de formulaire. La plate-forme peut être utilisée pour analyser sémantiquement les textes médicaux ou interroger la base d’annotation disponible pour avoir une/des réponses à une requête donnée (e.g. « ?X prévient maladie d’Alzheimer », équivalent à la question « comment prévenir la maladie d’Alzheimer ? »). Cette application peut être la base d’un système de questions-réponses pour le domaine médical.</resume>
        <mots_cles>Annotation sémantique, interrogation sémantique, domaine médical</mots_cles>
        <title/>
        <abstract>This paper presents MeTAE, a platform for semantic annotation and exploration of medical texts. The annotation process encompasses medical entity recognition and semantic relationship identification between the retrieved entities. This identification is based on linguistic patterns constructed manually for each type of relation. MeTAE generates RDF annotations from the extracted information and allows semantic exploration of the annotated texts through a form-based interface. The platform can be used to semantically analyze medical texts or to explore the available annotation base through structured queries (e.g. “?X Prevents Alzheimer’s disease” for its natural-language equivalent: “how to prevent Alzheimer’s disease?”). MeTAE can be a basis for a medical question-answering system.</abstract>
        <keywords>Semantic annotation, semantic querying, medical domain</keywords>
      </article>
      <article id="taln-2010-demo-013" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Bruno Guillaume</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Guy Perrier</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">INRIA Nancy-Grand Est - LORIA - Nancy-Université</affiliation>
        </affiliations>
        <titre>LEOPAR, un analyseur syntaxique pour les grammaires d’interaction</titre>
        <type>démonstration</type>
        <pages/>
        <resume>Nous présentons ici l’analyseur syntaxique LEOPAR basé sur les grammaires d’interaction ainsi que d’autres outils utiles pour notre chaîne de traitement syntaxique.</resume>
        <mots_cles>Analyse syntaxique, grammaires d’interaction, polarités</mots_cles>
        <title/>
        <abstract>We present the parser LEOPAR which is based on the Interaction Grammars formalism. We present also other tools used in our framework for parsing.</abstract>
        <keywords>Parsing, Interaction Grammars, polarities</keywords>
      </article>
      <article id="taln-2010-demo-014" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Julien Bourdaillet</nom>
            <email>bourdaij@iro.umontreal.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Fabrizio Gotti</nom>
            <email>gottif@iro.umontreal.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Stéphane Huet</nom>
            <email>huetstep@iro.umontreal.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Philippe Langlais</nom>
            <email>felipe@iro.umontreal.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Guy Lapalme</nom>
            <email>lapalme@iro.umontreal.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">RALI - DIRO - Université de Montréal, C.P. 6128, succursale centre-ville, H3C 3J7, Montréal, Québec, Canada</affiliation>
        </affiliations>
        <titre>TransSearch : un moteur de recherche de traductions</titre>
        <type>démonstration</type>
        <pages/>
        <resume>Malgré les nombreuses études visant à améliorer la traduction automatique, la traduction assistée par ordinateur reste la solution préférée des traducteurs lorsqu’une sortie de qualité est recherchée. Cette démonstration vise à présenter le moteur de recherche de traductions TransSearch. Cetteapplication commerciale, accessible sur leWeb, repose d’une part sur l’exploitation d’un bitexte aligné au niveau des phrases, et d’autre part sur des modèles statistiques d’alignement de mots.</resume>
        <mots_cles>Traduction automatique statistique, repérage de traductions, alignement de mots, requêtes linguistiques</mots_cles>
        <title/>
        <abstract>Despite the impressive amount of studies devoted to improving the state of the art of machine translation, computer assisted translation tools remain the preferred solution of human translators when publication quality is of concern. This demonstration presents the translation search engine TransSearch. This web-based commercial application relies on a sentence-aligned bitext and a statistical word alignment techniques.</abstract>
        <keywords>Statistical machine translation, translation spotting, word alignment, linguistic queries</keywords>
      </article>
      <article id="taln-2010-demo-015" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Graham Russell</nom>
            <email>grussell@onscope.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Onscope Group Inc., 651 Notre-Dame Ouest, Montréal QC, Canada H3C 1J1</affiliation>
        </affiliations>
        <titre/>
        <type>démonstration</type>
        <pages/>
        <resume>Description de Moz, un système d’aide à la traduction conçu pour le traitement de textes structurés ou semi-structurés avec une forte proportion de contenu terminologique. Le système comporte une mémoire de traduction collaborative, qui atteint un niveau élevé de rappel grâce à l’analyse sousphrastique ; il fournit également des dispositifs de communication et de révision. Le système est en production et traduit 140 000 mots par semaine.</resume>
        <mots_cles>Aides à la traduction, sous-langage, analyse conceptuelle</mots_cles>
        <title>Moz: Translation of Structured Terminology-Rich Text</title>
        <abstract>Description of Moz, a translation support system designed for texts exhibiting a high proportion of structured and semi-structured terminological content. The system comprises a web-based collaborative translation memory, with high recall via subsentential linguistic analysis and facilities for messaging and quality assurance. It is in production use, translating some 140,000 words per week.</abstract>
        <keywords>Translation aids, sublanguage, conceptual analysis</keywords>
      </article>
      <article id="taln-2010-demo-016" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Alexis Nasr</nom>
            <email>alexis.nasr@lif.univ-mrs.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Frédéric Béchet</nom>
            <email>frederic.bechet@lif.univ-mrs.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Jean-François Rey</nom>
            <email>jean-francois.rey@lif.univ-mrs.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire d’Informatique Fondamentale de Marseille, Université Aix-Marseille</affiliation>
        </affiliations>
        <titre>MACAON Une chaîne linguistique pour le traitement de graphes de mots</titre>
        <type>démonstration</type>
        <pages/>
        <resume/>
        <mots_cles/>
        <title/>
        <abstract/>
        <keywords/>
      </article>
    </articles>
  </conference>
  <conference>
    <edition>
      <acronyme>TALN'2011</acronyme>
      <titre>conférence sur le Traitement Automatique des Langues Naturelles</titre>
      <ville>Montpellier</ville>
      <pays>France</pays>
      <dateDebut>2011-06-27</dateDebut>
      <dateFin>2011-07-01</dateFin>
      <presidents>
        <nom>Mathieu Lafourcade</nom>
        <nom>Violaine Prince</nom>
      </presidents>
      <typeArticles>
        <type id="invite">Invités</type>
        <type id="long">Papiers longs</type>
        <type id="court">Papiers courts</type>
        <type id="démonstration">Démonstrations</type>
      </typeArticles>
      <statistiques>
        <acceptations id="long" soumissions="94">38</acceptations>
        <acceptations id="court" soumissions="95">52</acceptations>
      </statistiques>
      <siteWeb>http://www.lirmm.fr/~lopez/TALN2011/</siteWeb>
      <meilleurArticle>
        <articleId>taln-2011-long-026</articleId>
      </meilleurArticle>
    </edition>
    <articles>
      <article id="taln-2011-invite-001" session="Conférenciers invités">
        <auteurs>
          <auteur>
            <nom>Vladimir A. Fomichov</nom>
            <email>vfomichov@hse.ru</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Department of Innovations and Business in the Sphere of Informational Technologies, Faculty of Business Informatics, National Research University “Higher School of Economics”, Kirpichnaya str. 33, 105679 Moscow, Russia</affiliation>
        </affiliations>
        <titre/>
        <type>invite</type>
        <pages/>
        <resume>L’article décrit la structure et les applications possibles de la théorie des K-représentations (représentation des connaissances) dans la bioinformatique afin de développer un Réseau Sémantique d’une génération nouvelle. La théorie des K-répresentations est une théorie originale du développement des analyseurs sémantico–syntactiques avec l’utilisation large des moyens formels pour décrire les données d’entrée, intermédiaires et de sortie. Cette théorie est décrit dans la monographie de V. Fomichov (Springer, 2010). La première partie de la théorie est un modèle formel d’un système qui est composé de dix opérations sur les structures conceptuelles. Ce modèle définit une classe nouvelle des langages formels – la classe des SK-langages. Les possibilités larges de construire des répresentations sémantiques des discours compliqués en rapport à la biologie sont manifestes. Une approche formelle nouvelle de l’élaboration des analysateurs multilinguistiques sémantico-syntactiques est décrite. Cet approche a été implémentée sous la forme d'un programme en langage PYTHON.</resume>
        <mots_cles>dialogue homme-machine en langage naturel, algorithme de l‟analyse sémantico-syntactique, sémantique intégrale formelle, théorie des K-représentations, SK-langues, représentation sémantique, bases de données linguistiques, réseau sémantique d’une génération nouvelle, réseau sémantique multilingue, bioinformatique</mots_cles>
        <title>The prospects revealed by the theory of K-representations for bioinformatics and Semantic Web</title>
        <abstract>The paper describes the structure and possible applications of the theory of K-representations (knowledge representations) in bioinformatics and in the development of a Semantic Web of a new generation. It is an original theory of designing semantic-syntactic analyzers of natural language (NL) texts with the broad use of formal means for representing input, intermediary, and output data. The current version of the theory is set forth in a monograph by V. Fomichov (Springer, 2010). The first part of the theory is a formal model describing a system consisting of ten operations on conceptual structures. This model defines a new class of formal languages – the class of SK-languages. The broad possibilities of constructing semantic representations of complex discourses pertaining to biology are shown. A new formal approach to developing multilingual algorithms of semantic-syntactic analysis of NL-texts is outlined. This approach is realized by means of a program in the language PYTHON.</abstract>
        <keywords>man-machine natural language dialogue, algorithm of semantic-syntactic analysis, integral formal semantics, theory of K-representations, SK-languages, semantic representation, text meaning representation, linguistic database, Semantic Web of a new generation, multilingual Semantic Web, bioinformatics</keywords>
      </article>
      <article id="taln-2011-invite-002" session="Conférenciers invités">
        <auteurs>
          <auteur>
            <nom>Nicholas Asher</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LILac, IRIT, Université Paul Sabatier</affiliation>
        </affiliations>
        <titre>Theorie et Praxis Une optique sur les travaux en TAL sur le discours et le dialogue</titre>
        <type>invite</type>
        <pages/>
        <resume/>
        <mots_cles/>
        <title/>
        <abstract>Discourse parsing is a relatively new field and it differs from parsing in syntax in its pedegree. Parsing and computational models of syntax have the benefit of 50 years of research in generative syntax and reactions to it. Discourse parsing has on the other hand little conceptual help from linguistics or philosophy. Though impressive gains have been registered in discourse parsing with superficial features, theoretical not really come to grips with the theoretical underpinnings of text interpretation, and its interaction especially with lexical semantics, a rather neglected branch of formal semantics. In my talk I will assess the interaction between theoretical linguistics, formal methods, and experimental work on discourse structure and interpretation. Sounding a note of optimism, I will then turn to assessing the situation for the computational analysis of dialogue. I will argue that the view that we are saddled with from Grice and the philosophy of the seventies is inadequate and is great need of revision from work on communication from economics and theoretical computer science</abstract>
        <keywords/>
      </article>
      <article id="taln-2011-invite-003" session="Conférenciers invités">
        <auteurs>
          <auteur>
            <nom>Claire Gardent</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CNRS/LORIA, Nancy (France)</affiliation>
          <affiliation affiliationId="2"/>
          <affiliation affiliationId="3"/>
        </affiliations>
        <titre/>
        <type>invite</type>
        <pages/>
        <resume/>
        <mots_cles/>
        <title>Sentence Generation: Input, Algorithms and Applications</title>
        <abstract>Sentence Generation maps abstract linguistic representations into sentences. A necessary part of any natural language generation system, sentence generation has also recently received increasing attention in applications such as transfer based machine translation (cf. the LOGON project) and natural language interfaces to knowledge bases (e.g., to verbalise, to author and/or to query ontologies). One outstanding issue in Sentence Generation is what it starts from. What is the abstract linguistic representation it generates from? In my talk, I will explore sentence generation from two main input formats (flat semantic formulae and dependency structures) and discuss their impact on efficiency, algorithms and applications. I will start by describing an algorithm that generates from flat semantic formulae, explain why it is computationally intractable and presenting ways of optimising it to make it usable in practice. I will then show how this algorithm can be used to generate paraphrases; to support error mining and to generate teaching material for language learners from an ontology. In the second part of the talk, I will focus on generation from dependency structures. Based on the input data recently made available by the Generation Challenges Surface Realisation Shared Task, I will show how the algorithm previously used to generate from flat semantic formulae can be adapted to generate from dependency structures. I will moreover discuss various issues raised by the GenChal data such as, missing lexical entries and mismatches between dependency and grammar structures.</abstract>
        <keywords/>
      </article>
      <article id="taln-2011-long-001" session="Fouille de textes et applications">
        <auteurs>
          <auteur>
            <nom>Michael Zock</nom>
            <email>michael.zock@lif.univ-mrs.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Guy Lapalme</nom>
            <email>2</email>
            <affiliationId>lapalme@iro.umontreal.ca</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CNRS – LIF (Aix-Marseille II), Laboratoire d’Informatique Fondamentale, Case 901, 163 avenue de Luminy,, F-13288 Marseille Cedex 9</affiliation>
          <affiliation affiliationId="2">RALI-DIRO, Université de Montréal, CP 6128, Succ. Centre-Ville, Montréal, QC Canada H3C 3J7</affiliation>
        </affiliations>
        <titre>Patrons de phrase, raccourcis pour apprendre rapidement à parler une nouvelle langue</titre>
        <type>long</type>
        <pages/>
        <resume>Nous décrivons la création d'un environnement web pour aider des apprenants (adolescents ou adultes) à acquérir les automatismes nécessaires pour produire à un débit “normal” les structures fondamentales d’une langue. Notre point de départ est une base de données de phrases, glanées sur le web ou issues de livres scolaires ou de livres de phrases. Ces phrases ont été généralisées (remplacement de mots par des variables) et indexées en termes de buts pour former une arborescence de patrons. Ces deux astuces permettent de motiver l'usage des patrons et de crééer des phrases structurellement identiques à celles rencontrées, tout en étant sémantiquement différentes. Si les notions de 'patrons' ou de 'phrases à trou implicitement typées' ne sont pas nouvelles, le fait de les avoir portées sur ordinateur pour apprendre des langues l'est. Le système étant conçu pour être ouvert, il permet aux utilisateurs, concepteurs ou apprenants, des changements sur de nombreux points importants : le nom des variables, leurs valeurs, le laps de temps entre une question et sa réponse, etc. La version initiale a été développée pour l’anglais et le japonais. Pour tester la généricité de notre approche nous y avons ajouté relativement facilement le français et le chinois.</resume>
        <mots_cles>apprentissage de langues, production de langage, livres de phrases, patrons, schéma de phrase, structures fondamentales</mots_cles>
        <title/>
        <abstract>We describe a web application to assist language learners (teenagers or adults) to acquire the needed skills to produce at a ‘normal’ rate the fundamental structures of a new language, the scope being the survival level. The starting point is a database of sentences gleaned in textbooks, phrasebooks, or the web. We propose to extend the applicability of these structures by generalizing them: concrete sentences becoming productive sentence patterns. In order to produce such generic structures (schemata), we index the sentences in terms of goals, replacing specific elements (words) of the chain by more general terms (variables). This allows the user not only to acquire these structures, but also to express his/her own thoughts. Starting from a communicative goal, he instantiates the variables of the associated schema with words of his choice. We have developed a prototype for English and Japanese, adding Chinese and French without too many problems.</abstract>
        <keywords>foreign language learning, language production, phrasebook, sentence patterns, basic structure</keywords>
      </article>
      <article id="taln-2011-long-002" session="Fouille de textes et applications">
        <auteurs>
          <auteur>
            <nom>Eric Charton</nom>
            <email>eric.charton@polymtl.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Michel Gagnon</nom>
            <email>michel.gagnon@polymtl.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Benoit Ozell</nom>
            <email>benoit.ozell@polymtl.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">École Polytechnique, 2900 boul. Edouard Montpetit, Montréal, Canada</affiliation>
        </affiliations>
        <titre>Génération automatique de motifs de détection d’entités nommées en utilisant des contenus encyclopédiques</titre>
        <type>long</type>
        <pages/>
        <resume>Les encyclopédies numériques contiennent aujourd’hui de vastes inventaires de formes d’écritures pour des noms de personnes, de lieux, de produits ou d’organisation. Nous présentons un système hybride de détection d’entités nommées qui combine un classifieur à base de Champs Conditionnel Aléatoires avec un ensemble de motifs de détection extraits automatiquement d’un contenu encyclopédique. Nous proposons d’extraire depuis des éditions en plusieurs langues de l’encyclopédie Wikipédia de grandes quantités de formes d’écriture que nous utilisons en tant que motifs de détection des entités nommées. Nous décrivons une méthode qui nous assure de ne conserver dans cette ressources que des formes non ambiguës susceptibles de venir renforcer un système de détection d’entités nommées automatique. Nous procédons à un ensemble d’expériences qui nous permettent de comparer un système d’étiquetage à base de CRF avec un système utilisant exclusivement des motifs de détection. Puis nous fusionnons les résultats des deux systèmes et montrons qu’un gain de performances est obtenu grâce à cette proposition.</resume>
        <mots_cles>Étiqueteur, Entités nommées, Lexiques</mots_cles>
        <title/>
        <abstract>Encyclopedic content can provide numerous samples of surface writing forms for persons, places, products or organisations names. In this paper we present an hybrid named entities recognition system based on a gazetteer automatically extracted. We propose to extract it from various language editions ofWikipedia encyclopedia. The wide amount of surface forms extracted from this encyclopedic content is then used as detection pattern of named entities.We build a labelling tool using those patterns. This labelling tool is used as simple pattern detection component, to combine with a Conditional Random Field tagger.We compare the performances of each component of our system with the results previously obtained by various systems in the French NER campaign ESTER 2. Finally, we show that the fusion of a CRF label tool with a pattern based ones, can improve the global performances of a named entity recognition system.</abstract>
        <keywords>Tagger, Named entities, Gazetteer</keywords>
      </article>
      <article id="taln-2011-long-003" session="Fouille de textes et applications">
        <auteurs>
          <auteur>
            <nom>Cédric Lopez</nom>
            <email>lopez@lirmm.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Mathieu Roche</nom>
            <email>mroche@lirmm.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIRMM, 161, rue ADA 34392 Montpellier Cedex 5</affiliation>
        </affiliations>
        <titre>Approche de construction automatique de titres courts par des méthodes de Fouille du Web</titre>
        <type>long</type>
        <pages/>
        <resume>Le titrage automatique de documents textuels est une tâche essentielle pour plusieurs applications (titrage de mails, génération automatique de sommaires, synthèse de documents, etc.). Cette étude présente une méthode de construction de titres courts appliquée à un corpus d’articles journalistiques via des méthodes de Fouille du Web. Il s’agit d’une première étape cruciale dans le but de proposer une méthode de construction de titres plus complexes. Dans cet article, nous présentons une méthode proposant des titres tenant compte de leur cohérence par rapport au texte, par rapport au Web, ainsi que de leur contexte dynamique. L’évaluation de notre approche indique que nos titres construits automatiquement sont informatifs et/ou accrocheurs.</resume>
        <mots_cles>Traitement Automatique du Langage Naturel, Fouille du Web, Titrage automatique</mots_cles>
        <title/>
        <abstract>The automatic titling of text documents is an essential task for several applications (automatic titling of e-mails, summarization, and so forth). This study presents a method of generation of short titles applied to a corpus of journalistic articles using methods ofWeb Mining. It is a first crucial stage with the aim of proposing a method of generation of more complex titles. In this article, we present a method that proposes titles taking into account their coherence in connection with the text and the Web, as well as their dynamic context. The evaluation of our approach indicates that our titles generated automatically are informative and/or catchy.</abstract>
        <keywords>Natural Language Processing, Web Mining, Automatic Titling</keywords>
      </article>
      <article id="taln-2011-long-004" session="Fouille de textes et applications">
        <auteurs>
          <auteur>
            <nom>Ludovic Jean-Louis</nom>
            <email>ludovic.jean-louis@cea.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Romaric Besançon</nom>
            <email>romaric.besancon@cea.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Olivier Ferret</nom>
            <email>olivier.ferret@cea.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Adrien Durand</nom>
            <email>adrien.durand@cea.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CEA, LIST, Laboratoire Vision et Ingénierie des Contenus, Fontenay-aux-Roses, F-92265, France</affiliation>
        </affiliations>
        <titre>Une approche faiblement supervisée pour l’extraction de relations à large échelle</titre>
        <type>long</type>
        <pages/>
        <resume>Les systèmes d’extraction d’information traditionnels se focalisent sur un domaine spécifique et un nombre limité de relations. Les travaux récents dans ce domaine ont cependant vu émerger la problématique des systèmes d’extraction d’information à large échelle. À l’instar des systèmes de question-réponse en domaine ouvert, ces systèmes se caractérisent à la fois par le traitement d’un grand nombre de relations et par une absence de restriction quant aux domaines abordés. Dans cet article, nous présentons un système d’extraction d’information à large échelle fondé sur un apprentissage faiblement supervisé de patrons d’extraction de relations. Cet apprentissage repose sur la donnée de couples d’entités en relation dont la projection dans un corpus de référence permet de constituer la base d’exemples de relations support de l’induction des patrons d’extraction. Nous présentons également les résultats de l’application de cette approche dans le cadre d’évaluation défini par la tâche KBP de l’évaluation TAC 2010.</resume>
        <mots_cles>extraction d’information, extraction de relations</mots_cles>
        <title/>
        <abstract>Standard Information Extraction (IE) systems are designed for a specific domain and a limited number of relations. Recent work has been undertaken to deal with large-scale IE systems. Such systems are characterized by a large number of relations and no restriction on the domain, which makes difficult the definition of manual resources or the use of supervised techniques. In this paper, we present a large-scale IE system based on a weakly supervised method of pattern learning. This method uses pairs of entities known to be in relation to automatically extract example sentences from which the patterns are learned. We present the results of this system on the data from the KBP task of the TAC 2010 evaluation campaign.</abstract>
        <keywords>information extraction, relation extraction</keywords>
      </article>
      <article id="taln-2011-long-005" session="Fouille de textes et applications">
        <auteurs>
          <auteur>
            <nom>Stéphane Huet</nom>
            <email>stephane.huet,@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Florian Boudin</nom>
            <email>florian.boudin@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Juan-Manuel Torres-Moreno</nom>
            <email>juan-manuel.torres@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIA, Université d’Avignon, France</affiliation>
          <affiliation affiliationId="2">École Polytechnique de Montréal, Canada</affiliation>
          <affiliation affiliationId="3">GIL-IINGEN, Universidad Nacional Autónoma de México, Mexique</affiliation>
        </affiliations>
        <titre>Utilisation d’un score de qualité de traduction pour le résumé multi-document cross-lingue</titre>
        <type>long</type>
        <pages/>
        <resume>Le résumé automatique cross-lingue consiste à générer un résumé rédigé dans une langue différente de celle utilisée dans les documents sources. Dans cet article, nous proposons une approche de résumé automatique multi-document, basée sur une représentation par graphe, qui prend en compte des scores de qualité de traduction lors du processus de sélection des phrases. Nous évaluons notre méthode sur un sous-ensemble manuellement traduit des données utilisées lors de la campagne d’évaluation internationale DUC 2004. Les résultats expérimentaux indiquent que notre approche permet d’améliorer la lisibilité des résumés générés, sans pour autant dégrader leur informativité.</resume>
        <mots_cles>Résumé cross-lingue, qualité de traduction, graphe</mots_cles>
        <title/>
        <abstract>Cross-language summarization is the task of generating a summary in a language different from the language of the source documents. In this paper, we propose a graph-based approach to multi-document summarization that integrates machine translation quality scores in the sentence selection process. We evaluate our method on a manually translated subset of the DUC 2004 evaluation campaign. Results indicate that our approach improves the readability of the generated summaries without degrading their informativity.</abstract>
        <keywords>Cross-lingual summary, translation quality, graph</keywords>
      </article>
      <article id="taln-2011-long-006" session="Fouille de textes et applications">
        <auteurs>
          <auteur>
            <nom>Cyril Grouin</nom>
            <email>cyril.grouin@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Louise Deléger</nom>
            <email>louise.delegeR@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Bruno Cartoni</nom>
            <email>bruno.cartoni@unige.ch</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Sophie Rosset</nom>
            <email>sophie.rosset@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Pierre Zweigenbaum</nom>
            <email>pierre.zweigenbaum@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIMSI-CNRS, BP133, 91403 Orsay Cedex, France</affiliation>
          <affiliation affiliationId="2">Département de Linguistique, Université de Genève, Suisse</affiliation>
        </affiliations>
        <titre>Accès au contenu sémantique en langue de spécialité : extraction des prescriptions et concepts médicaux</titre>
        <type>long</type>
        <pages/>
        <resume>Pourtant essentiel pour appréhender rapidement et globalement l’état de santé des patients, l’accès aux informations médicales liées aux prescriptions médicamenteuses et aux concepts médicaux par les outils informatiques se révèle particulièrement difficile. Ces informations sont en effet généralement rédigées en texte libre dans les comptes rendus hospitaliers et nécessitent le développement de techniques dédiées. Cet article présente les stratégies mises en oeuvre pour extraire les prescriptions médicales et les concepts médicaux dans des comptes rendus hospitaliers rédigés en anglais. Nos systèmes, fondés sur des approches à base de règles et d’apprentissage automatique, obtiennent une F1-mesure globale de 0,773 dans l’extraction des prescriptions médicales et dans le repérage et le typage des concepts médicaux.</resume>
        <mots_cles>Extraction d’information, Indexation contrôlée, Informatique médicale, Concepts médicaux, Prescriptions</mots_cles>
        <title/>
        <abstract>While essential for rapid access to patient health status, computer-based access to medical information related to prescriptions key medical expressed and concepts proves to be difficult. This information is indeed generally in free text in the clinical records and requires the development of dedicated techniques. This paper presents the strategies implemented to extract medical prescriptions and concepts in clinical records written in English language. Our systems, based upon linguistic patterns and machine-learning approaches, achieved a global F1-measure of 0.773 for extraction of medical prescriptions, and of clinical concepts.</abstract>
        <keywords>Information extraction, Controled indexing, Medical informatics, Clinical concepts, Prescriptions</keywords>
      </article>
      <article id="taln-2011-long-007" session="Parole">
        <auteurs>
          <auteur>
            <nom>Bassam Jabaian</nom>
            <email>bassam.jabaian@imag.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Laurent Besacier</nom>
            <email>laurent.besacier@imag.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Fabrice Lefèvre</nom>
            <email>fabrice.lefevre@univ-avignon.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIG, University Joseph Fourier, Grenoble - France</affiliation>
          <affiliation affiliationId="2">LIA, University of Avignon, Avignon - France</affiliation>
        </affiliations>
        <titre>Comparaison et combinaison d’approches pour la portabilité vers une nouvelle langue d’un système de compréhension de l’oral</titre>
        <type>long</type>
        <pages/>
        <resume>Dans cet article, nous proposons plusieurs approches pour la portabilité du module de compréhension de la parole (SLU) d’un système de dialogue d’une langue vers une autre. On montre que l’utilisation des traductions automatiques statistiques (SMT) aide à réduire le temps et le cout de la portabilité d’un tel système d’une langue source vers une langue cible. Pour la tache d’étiquetage sémantique on propose d’utiliser soit les champs aléatoires conditionnels (CRF), soit l’approche à base de séquences (PH-SMT). Les résultats expérimentaux montrent l’efficacité des méthodes proposées pour une portabilité rapide du SLU vers une nouvelle langue. On propose aussi deux méthodes pour accroître la robustesse du SLU aux erreurs de traduction. Enfin on montre que la combinaison de ces approches réduit les erreurs du système. Ces travaux sont motivés par la disponibilité du corpus MEDIA français et de la traduction manuelle vers l’italien d’une sous partie de ce corpus.</resume>
        <mots_cles>Système de dialogue, compréhension de la parole, portabilité à travers les langues, traduction automatique statistique</mots_cles>
        <title/>
        <abstract>In this paper we investigate several approaches for language portability of the spoken language understanding (SLU) module of a dialogue system. We show that the use of statistical machine translation (SMT) can reduce the time and the cost of porting a system from a source to a target language. For conceptual decoding we propose to use even conditional random fields (CRF) or phrase based statistical machine translation PB-SMT). The experimental results show the efficiency of the proposed methods for a fast and low cost SLU language portability. Also we proposed two methods to increase SLU robustness to translation errors. Overall we show that the combination of all these approaches reduce the concept error rate. This work was motivated by the availability of the MEDIA French corpus and the manual translation of a subset of this corpus into Italian.</abstract>
        <keywords>Spoken Dialogue Systems, Spoken Language Understanding, Language Portability, Statistical Machine Translation</keywords>
      </article>
      <article id="taln-2011-long-008" session="Parole">
        <auteurs>
          <auteur>
            <nom>Thierry Bazillon</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Benjamin Maza</nom>
            <email/>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Mickael Rouvier</nom>
            <email/>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Frédéric Béchet</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Alexis Nasr</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Aix Marseille Université, LIF-CNRS, Marseille, France</affiliation>
          <affiliation affiliationId="2">Université d’Avignon, LIA-CERI, Avignon, France</affiliation>
        </affiliations>
        <titre>Qui êtes-vous ? Catégoriser les questions pour déterminer le rôle des locuteurs dans des conversations orales</titre>
        <type>long</type>
        <pages/>
        <resume>La fouille de données orales est un domaine de recherche visant à caractériser un flux audio contenant de la parole d’un ou plusieurs locuteurs, à l’aide de descripteurs liés à la forme et au contenu du signal. Outre la transcription automatique en mots des paroles prononcées, des informations sur le type de flux audio traité ainsi que sur le rôle et l’identité des locuteurs sont également cruciales pour permettre des requêtes complexes telles que : « chercher des débats sur le thème X », « trouver toutes les interviews de Y », etc. Dans ce cadre, et en traitant des conversations enregistrées lors d’émissions de radio ou de télévision, nous étudions la manière dont les locuteurs expriment des questions dans les conversations, en partant de l’intuition initiale que la forme des questions posées est une signature du rôle du locuteur dans la conversation (présentateur, invité, auditeur, etc.). En proposant une classification du type des questions et en utilisant ces informations en complément des descripteurs généralement utilisés dans la littérature pour classer les locuteurs par rôle, nous espérons améliorer l’étape de classification, et valider par la même occasion notre intuition initiale.</resume>
        <mots_cles>Fouille de données orales, Traitement Automatique de la Parole, Annotation de corpus oraux, Classification en rôles de locuteurs</mots_cles>
        <title/>
        <abstract>Speech Data Mining is an area of research dedicated to characterize audio streams containing speech of one or more speakers, using descriptors related to the form and the content of the speech signal. Besides the automatic word transcription process, information about the type of audio stream and the role and identity of speakers is also crucial to allow complex queries such as : “ seek debates on X ,”“ find all the interviews of Y”, etc. In this framework we present a study done on broadcast conversations on how speakers express questions in conversations, starting with the initial intuition that the form of the questions uttered is a signature of the role of the speakers in the conversation (anchor, guest, expert, etc.). By classifying these questions thanks to a set of labels and using this information in addition to the commonly used descriptors to classify users’ role in broadcast conversations, we want to improve the role classification accuracy and validate our initial intuition.</abstract>
        <keywords>Speech data mining, Automatic Speech Processing, Speech Corpus Annotation, Speaker role classification</keywords>
      </article>
      <article id="taln-2011-long-009" session="Sémantique">
        <auteurs>
          <auteur>
            <nom>Charles Teissèdre</nom>
            <email>charles.teissedre@ u-paris10.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Delphine Battistelli</nom>
            <email>delphine.battistelli@paris-sorbonne.fr</email>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Jean-Luc Minel</nom>
            <email>jean-luc.minel@u-paris10.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">MoDyCo - UMR 7114 CNRS, Paris Ouest Nanterre La Défense, 200, av. de la République, 92001 Nanterre</affiliation>
          <affiliation affiliationId="2">&gt;Mondeca, 3, cité Nollez, 75018 Paris</affiliation>
          <affiliation affiliationId="3">STIH, Université Paris Sorbonne, 28, rue Serpente, 75006 Paris</affiliation>
        </affiliations>
        <titre>Recherche d’information et temps linguistique : une heuristique pour calculer la pertinence des expressions calendaires</titre>
        <type>long</type>
        <pages/>
        <resume>A rebours de bon nombre d’applications actuelles offrant des services de recherche d’information selon des critères temporels - applications qui reposent, à y regarder de près, sur une approche consistant à filtrer les résultats en fonction de leur inclusion dans une fenêtre de temps, nous souhaitons illustrer dans cet article l’intérêt d’un service s’appuyant sur un calcul de similarité entre des expressions adverbiales calendaires. Nous décrivons une heuristique pour mesurer la pertinence d’un fragment de texte en prenant en compte la sémantique des expressions calendaires qui y sont présentes. A travers la mise en oeuvre d’un système de recherche d’information, nous montrons comment il est possible de tirer profit de l’indexation d’expressions calendaires présentes dans les textes en définissant des scores de pertinence par rapport à une requête. L’objectif est de faciliter la recherche d’information en offrant la possibilité de croiser des critères de recherche thématique avec des critères temporels.</resume>
        <mots_cles>Indexation d’informations calendaires, Recherche d’information, Annotation et extraction d’expressions calendaires</mots_cles>
        <title/>
        <abstract>Unlike many nowadays applications providing Information Retrieval services able to handle temporal criteria - applications which usually filter results after testing their inclusion in a time span, this paper illustrates the interest of a service based on a calculation of similarity between calendar adverbial phrases. We describe a heuristic to measure the relevance of a fragment of text by taking into account the semantics of calendar expressions. Through the implementation of an Information Retrieval system, we show how it is possible to take advantage of the indexing of calendar expressions found in texts by setting scores of relevance with respect to a query. The objective is to ease Information Retrieval by offering the possibility of crossing thematic research criteria with temporal criteria.</abstract>
        <keywords>Calendar information indexing, Information Retrieval, Annotation and extraction of calendar expressions</keywords>
      </article>
      <article id="taln-2011-long-010" session="Sémantique">
        <auteurs>
          <auteur>
            <nom>Ismaïl El Maarouf</nom>
            <email>ismail.el-maarouf@univ-ubs.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Jeanne Villaneau</nom>
            <email>jeanne.villaneau@univ-ubs.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Sophie Rosset</nom>
            <email>sophie.rosset@limsi.fr</email>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">HCTI UBS-UEB, Centre de Recherche Christiaan Huygens, 56321 Lorient</affiliation>
          <affiliation affiliationId="2">Valoria UBS-UEB, Rue Yves Mainguy, Campus de Tohannic 56017 Vannes cedex</affiliation>
          <affiliation affiliationId="3">LIMSI-CNRS, F-91403 Orsay Cedex</affiliation>
        </affiliations>
        <titre>Extraction de patrons sémantiques appliquée à la classification d'Entités Nommées</titre>
        <type>long</type>
        <pages/>
        <resume>La variabilité des corpus constitue un problème majeur pour les systèmes de reconnaissance d'entités nommées. L'une des pistes possibles pour y remédier est l'utilisation d'approches linguistiques pour les adapter à de nouveaux contextes : la construction de patrons sémantiques peut permettre de désambiguïser les entités nommées en structurant leur environnement syntaxico-sémantique. Cet article présente une première réalisation sur un corpus de presse d'un système de correction. Après une étape de segmentation sur des critères discursifs de surface, le système extrait et pondère les patrons liés à une classe d'entité nommée fournie par un analyseur. Malgré des modèles encore relativement élémentaires, les résultats obtenus sont encourageants et montrent la nécessité d'un traitement plus approfondi de la classe Organisation.</resume>
        <mots_cles>entités nommées, patrons sémantiques, segmentation discursive de surface</mots_cles>
        <title/>
        <abstract>Corpus variation is a major problem for named entity recognition systems. One possible direction to tackle this problem involves using linguistic approaches to adapt them to unseen contexts : building semantic patterns may help for their disambiguation by structuring their syntactic and semantic environment. This article presents a preliminary implementation on a press corpus of a correction system. After a segmentation step based on surface discourse clues, the system extracts and weights the patterns linked to a named entity class provided by an analyzer. Despite relatively elementary models, the results obtained are promising and point on the necessary treatment of the Organisation class.</abstract>
        <keywords>named entities, semantic patterns, surface discourse segmentation</keywords>
      </article>
      <article id="taln-2011-long-011" session="Sémantique">
        <auteurs>
          <auteur>
            <nom>Didier Schwab</nom>
            <email>didier.schwab@imag.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Jérôme Goulian</nom>
            <email>jerome.goulian@imag.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Nathan Guillaume</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIG-GETALP (Laboratoire d’Informatique de Grenoble, Groupe d’Étude pour la Traduction/le Traitement Automatique des Langues et de la Parole), Université Pierre Mendès France, Grenoble 2</affiliation>
        </affiliations>
        <titre>Désambiguïsation lexicale par propagation de mesures sémantiques locales par algorithmes à colonies de fourmis</titre>
        <type>long</type>
        <pages/>
        <resume>Effectuer une tâche de désambiguïsation lexicale peut permettre d’améliorer de nombreuses applications du traitement automatique des langues comme l’extraction d’informations multilingues, ou la traduction automatique. Schématiquement, il s’agit de choisir quel est le sens le plus approprié pour chaque mot d’un texte. Une des approches classiques consiste à estimer la proximité sémantique qui existe entre deux sens de mots puis de l’étendre à l’ensemble du texte. La méthode la plus directe donne un score à toutes les paires de sens de mots puis choisit la chaîne de sens qui a le meilleur score. La complexité de cet algorithme est exponentielle et le contexte qu’il est calculatoirement possible d’utiliser s’en trouve réduit. Il ne s’agit donc pas d’une solution viable. Dans cet article, nous nous intéressons à une autre méthode, l’adaptation d’un algorithme à colonies de fourmis. Nous présentons ses caractéristiques et montrons qu’il permet de propager à un niveau global les résultats des algorithmes locaux et de tenir compte d’un contexte plus long et plus approprié en un temps raisonnable.</resume>
        <mots_cles>Désambiguïsation lexicale, Algorithmes à colonies de fourmis, Mesures sémantiques</mots_cles>
        <title/>
        <abstract>Word sense disambiguation can lead to significant improvement in many Natural Language Processing applications as Machine Translation or Multilingual Information Retrieval. Basically, the aim is to choose for each word in a text its best sense. One of the most popular method estimates local semantic relatedness between two word senses and then extends it to the whole text. The most direct method computes a rough score for every pair of word senses and chooses the lexical chain that has the best score. The complexity of this algorithm is exponential and the context that it is computationally possible to use is reduced. Brute force is therefore not a viable solution. In this paper, we focus on another method : the adaptation of an ant colony algorithm. We present its features and show that it can spread at a global level the results of local algorithms and consider a longer and more appropriate context in a reasonable time.</abstract>
        <keywords>Lexical Disambiguation, Ant colony algorithms, Semantic relatedness</keywords>
      </article>
      <article id="taln-2011-long-012" session="Lexique et Corpus">
        <auteurs>
          <auteur>
            <nom>Benoît Sagot</nom>
            <email>benoit.sagot@inria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Karën Fort</nom>
            <email>karen.fort@inist.fr</email>
            <affiliationId>2</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Gilles Adda</nom>
            <email>gilles.adda@limsi.fr</email>
            <affiliationId>4</affiliationId>
          </auteur>
          <auteur>
            <nom>Joseph Mariani</nom>
            <email>joseph.mariani@limsi.fr</email>
            <affiliationId>4</affiliationId>
            <affiliationId>5</affiliationId>
          </auteur>
          <auteur>
            <nom>Bernard Lang</nom>
            <email>bernard.lang@inria.fr</email>
            <affiliationId>6</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Alpage, INRIA Paris–Rocquencourt &amp; Université Paris 7, Rocquencourt, BP 105, 78153 Le Chesnay Cedex, France</affiliation>
          <affiliation affiliationId="2">INIST-CNRS, 2 allée de Brabois, 54500 Vandoeuvre-lès-Nancy, France</affiliation>
          <affiliation affiliationId="3">LIPN, Université Paris Nord, 99 av J-B Clément, 93430 Villetaneuse, France</affiliation>
          <affiliation affiliationId="4">LIMSI-CNRS, Bât. 508, rue John von Neumann, Université Paris-Sud BP 133, 91403 Orsay Cedex, France</affiliation>
          <affiliation affiliationId="5">IMMI-CNRS, Bât. 508, rue John von Neumann, Université Paris-Sud BP 133, 91403 Orsay Cedex, France</affiliation>
          <affiliation affiliationId="6">INRIA Paris–Rocquencourt, Rocquencourt, BP 105, 78153 Le Chesnay Cedex, France</affiliation>
        </affiliations>
        <titre>Un turc mécanique pour les ressources linguistiques : critique de la myriadisation du travail parcellisé</titre>
        <type>long</type>
        <pages/>
        <resume>Cet article est une prise de position concernant les plate-formes de type Amazon Mechanical Turk, dont l’utilisation est en plein essor depuis quelques années dans le traitement automatique des langues. Ces plateformes de travail en ligne permettent, selon le discours qui prévaut dans les articles du domaine, de faire développer toutes sortes de ressources linguistiques de qualité, pour un prix imbattable et en un temps très réduit, par des gens pour qui il s’agit d’un passe-temps. Nous allons ici démontrer que la situation est loin d’être aussi idéale, que ce soit sur le plan de la qualité, du prix, du statut des travailleurs ou de l’éthique. Nous rappellerons ensuite les solutions alternatives déjà existantes ou proposées. Notre but est ici double : informer les chercheurs, afin qu’ils fassent leur choix en toute connaissance de cause, et proposer des solutions pratiques et organisationnelles pour améliorer le développement de nouvelles ressources linguistiques en limitant les risques de dérives éthiques et légales, sans que cela se fasse au prix de leur coût ou de leur qualité.</resume>
        <mots_cles>Amazon Mechanical Turk, ressources linguistiques</mots_cles>
        <title/>
        <abstract>This article is a position paper concerning Amazon Mechanical Turk-like systems, the use of which has been steadily growing in natural language processing in the past few years. According to the mainstream opinion expressed in the articles of the domain, these online working platforms allow to develop very quickly all sorts of quality language resources, for a very low price, by people doing that as a hobby. We shall demonstrate here that the situation is far from being that ideal, be it from the point of view of quality, price, workers’ status or ethics. We shall then bring back to mind already existing or proposed alternatives. Our goal here is twofold : to inform researchers, so that they can make their own choices with all the elements of the reflection in mind, and propose practical and organizational solutions in order to improve new language resources development, while limiting the risks of ethical and legal issues without letting go price or quality.</abstract>
        <keywords>Amazon Mechanical Turk, language resources</keywords>
      </article>
      <article id="taln-2011-long-013" session="Lexique et Corpus">
        <auteurs>
          <auteur>
            <nom>Bo Li</nom>
            <email>bo.li@imag.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Eric Gaussier</nom>
            <email>eric.gaussier@imag.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Emmanuel Morin</nom>
            <email>emmanuel.morin@univ-nantes.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Amir Hazem</nom>
            <email>amir.hazem@univ-nantes.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université Grenoble I, LIG UMR 5217</affiliation>
          <affiliation affiliationId="2">LINA, UMR 6241, Université de Nantes</affiliation>
        </affiliations>
        <titre>Degré de comparabilité, extraction lexicale bilingue et recherche d’information interlingue</titre>
        <type>long</type>
        <pages/>
        <resume>Nous étudions dans cet article le problème de la comparabilité des documents composant un corpus comparable afin d’améliorer la qualité des lexiques bilingues extraits et les performances des systèmes de recherche d’information interlingue. Nous proposons une nouvelle approche qui permet de garantir un certain degré de comparabilité et d’homogénéité du corpus tout en préservant une grande part du vocabulaire du corpus d’origine. Nos expériences montrent que les lexiques bilingues que nous obtenons sont d’une meilleure qualité que ceux obtenus avec les approches précédentes, et qu’ils peuvent être utilisés pour améliorer significativement les systèmes de recherche d’information interlingue.</resume>
        <mots_cles>Corpus comparables, comparabilité, lexiques bilingues, recherche d’information interlingue</mots_cles>
        <title/>
        <abstract>We study in this paper the problem of enhancing the comparability of bilingual corpora in order to improve the quality of bilingual lexicons extracted from comparable corpora and the performance of crosslanguage information retrieval (CLIR) systems. We introduce a new method for enhancing corpus comparability which guarantees a certain degree of comparability and homogeneity, and still preserves most of the vocabulary of the original corpus. Our experiments illustrate the well-foundedness of this method and show that the bilingual lexicons obtained are of better quality than the lexicons obtained with previous approaches, and that they can be used to significantly improve CLIR systems.</abstract>
        <keywords>Comparable corpora, comparability, bilingual lexicon, cross-language information retrieval</keywords>
      </article>
      <article id="taln-2011-long-014" session="Lexique et Corpus">
        <auteurs>
          <auteur>
            <nom>Nadja Vincze</nom>
            <email>nadja.vincze@uclouvain.be</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Yves Bestgen</nom>
            <email>yves.bestgen@uclouvain.be</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">UCLouvain, Cental, Place Blaise Pascal, 1, B-1348 Louvain-la-Neuve, Belgique</affiliation>
          <affiliation affiliationId="2">UCLouvain, CECL, B-1348 Louvain-la-Neuve, Belgique</affiliation>
        </affiliations>
        <titre>Identification de mots germes pour la construction d'un lexique de valence au moyen d'une procédure supervisée</titre>
        <type>long</type>
        <pages/>
        <resume>De nombreuses méthodes automatiques de classification de textes selon les sentiments qui y sont exprimés s'appuient sur un lexique dans lequel à chaque entrée est associée une valence. Le plus souvent, ce lexique est construit à partir d'un petit nombre de mots, choisis arbitrairement, qui servent de germes pour déterminer automatiquement la valence d'autres mots. La question de l'optimalité de ces mots germes a bien peu retenu l'attention. Sur la base de la comparaison de cinq méthodes automatiques de construction de lexiques de valence, dont une qui, à notre connaissance, n'a jamais été adaptée au français et une autre développée spécifiquement pour la présente étude, nous montrons l'importance du choix de ces mots germes et l'intérêt de les identifier au moyen d'une procédure d'apprentissage supervisée.</resume>
        <mots_cles>Analyse de sentiments, lexique de valence, apprentissage supervisé, analyse sémantique latente</mots_cles>
        <title/>
        <abstract>Many methods of automatic sentiment classification of texts are based on a lexicon in which each entry is associated with a semantic orientation. These entries serve as seeds for automatically determining the semantic orientation of other words. Most often, this lexicon is built from a small number of words, chosen arbitrarily. The optimality of these seed words has received little attention. In this study, we compare five automatic methods to build a semantic orientation lexicon. One among them, to our knowledge, has never been adapted to French and another was developed specifically for this study. Based on them, we show that choosing good seed words is very important and identifying them with a supervised learning procedure brings a benefit.</abstract>
        <keywords>Sentiment analysis, semantic orientation lexicon, supervised learning, latent semantic analysis</keywords>
      </article>
      <article id="taln-2011-long-015" session="Lexique et Corpus">
        <auteurs>
          <auteur>
            <nom>Philippe Muller</nom>
            <email>muller@irit.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Philippe Langlais</nom>
            <email>felipe@iro.umontreal.ca</email>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">IRIT, Université Paul Sabatier</affiliation>
          <affiliation affiliationId="2">Alpage, INRIA Paris-Rocquencourt</affiliation>
          <affiliation affiliationId="3">RALI / DIRO / Université de Montréal</affiliation>
        </affiliations>
        <titre>Comparaison d’une approche miroir et d’une approche distributionnelle pour l’extraction de mots sémantiquement reliés</titre>
        <type>long</type>
        <pages/>
        <resume>Dans (Muller &amp; Langlais, 2010), nous avons comparé une approche distributionnelle et une variante de l’approche miroir proposée par Dyvik (2002) sur une tâche d’extraction de synonymes à partir d’un corpus en français. Nous présentons ici une analyse plus fine des relations extraites automatiquement en nous intéressant cette fois-ci à la langue anglaise pour laquelle de plus amples ressources sont disponibles. Différentes façons d’évaluer notre approche corroborent le fait que l’approche miroir se comporte globalement mieux que l’approche distributionnelle décrite dans (Lin, 1998), une approche de référence dans le domaine.</resume>
        <mots_cles>Sémantique lexicale, similarité distributionnelle, similarité traductionnelle</mots_cles>
        <title/>
        <abstract>In (Muller &amp; Langlais, 2010), we compared a distributional approach to a variant of the mirror approach described by Dyvik (2002) on a task of synonym extraction. This was conducted on a corpus of the French language. In the present work, we propose a more precise and systematic evaluation of the relations extracted by a mirror and a distributional approaches. This evaluation is conducted on the English language for which widespread resources are available. All the evaluations we conducted in this study concur to the observation that our mirror approach globally outperforms the distributional one described by Lin (1998), which we believe to be a fair reference in the domain.</abstract>
        <keywords>Lexical Semantics, distributional similarity, mirror approach</keywords>
      </article>
      <article id="taln-2011-long-016" session="Lexique et Corpus">
        <auteurs>
          <auteur>
            <nom>Yann Mathet</nom>
            <email>Yann.Mathet@unicaen.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Antoine Widlöcher</nom>
            <email>Antoine.Widlocher@unicaen.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">GREYC, UMR CNRS 6072, Université de Caen, 14032 Caen Cedex</affiliation>
        </affiliations>
        <titre>Une approche holiste et unifiée de l’alignement et de la mesure d’accord inter-annotateurs</titre>
        <type>long</type>
        <pages/>
        <resume>L’alignement et la mesure d’accord sur des textes multi-annotés sont des enjeux majeurs pour la constitution de corpus de référence. Nous défendons dans cet article l’idée que ces deux tâches sont par essence interdépendantes, la mesure d’accord nécessitant de s’appuyer sur des annotations alignées, tandis que les choix d’alignements ne peuvent se faire qu’à l’aune de la mesure qu’ils induisent. Nous proposons des principes formels relevant cette gageure, qui s’appuient notamment sur la notion de désordre du système constitué par l’ensemble des jeux d’annotations d’un texte. Nous posons que le meilleur alignement est celui qui minimise ce désordre, et que la valeur de désordre obtenue rend compte simultanément du taux d’accord. Cette approche, qualifiée d’holiste car prenant en compte l’intégralité du système pour opérer, est algorithmiquement lourde, mais nous sommes parvenus à produire une implémentation d’une version légèrement dégradée de cette dernière, et l’avons intégrée à la plate-forme d’annotation Glozz.</resume>
        <mots_cles>Alignement d’annotations, mesure d’accord inter-annotateurs, linguistique de corpus</mots_cles>
        <title/>
        <abstract>Building reference corpora makes it necessary to align annotations and to measure agreement among annotators, in order to test the reliability of the annotated ressources. In this paper, we argue that alignment and agreement measure are interrelated : agreement measure applies to pre-aligned data and alignment assumes a prior agreement measure. We describe here a formal and computational framework which takes this interrelation into account, and relies on the notion of disorder of annotation sets available for a text. In this framework, the best alignment is the one which has the minimal disorder, and this disorder reflects an agreement measure of these data. This approach is said to be holistic insofar as alignment and measure depend on the system as a whole and cannot be locally determined. This holism introduces a computational cost which has been reduced by a heuristic strategy, implemented within the Glozz annotation platform.</abstract>
        <keywords>Alignment, inter-coder agreement measure, corpus linguistics</keywords>
      </article>
      <article id="taln-2011-long-017" session="Lexique et Corpus">
        <auteurs>
          <auteur>
            <nom>André Bittar</nom>
            <email>andre.bittar@xrce.xerox.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Pascal Amsili</nom>
            <email>pascal.amsili@linguist.jussieu.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Pascal Denis</nom>
            <email>pascal.denis@inria.fr</email>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Xerox Research Centre Europe</affiliation>
          <affiliation affiliationId="2">LLF, Université Paris Diderot, UMR CNRS 7110</affiliation>
          <affiliation affiliationId="3">EPI Alpage, INRIA Rocquencourt et Université Paris Diderot</affiliation>
        </affiliations>
        <titre>French TimeBank : un corpus de référence sur la temporalité en français</titre>
        <type>long</type>
        <pages/>
        <resume>Cet article a un double objectif : d’une part, il s’agit de présenter à la communauté un corpus récemment rendu public, le French Time Bank (FTiB), qui consiste en une collection de textes journalistiques annotés pour les temps et les événements selon la norme ISO-TimeML ; d’autre part, nous souhaitons livrer les résultats et réflexions méthodologiques que nous avons pu tirer de la réalisation de ce corpus de référence, avec l’idée que notre expérience pourra s’avérer profitable au-delà de la communauté intéressée par le traitement de la temporalité.</resume>
        <mots_cles>Annotation temporelle, corpus, ISO-TimeML</mots_cles>
        <title/>
        <abstract>This article has two objectives. Firstly, it presents the French TimeBank (FTiB) corpus, which has recently been made public. The corpus consists of a collection of news texts annotated for times and events according to the ISO-TimeML standard. Secondly, we wish to present the results and methodological conclusions that we have drawn from the creation of this reference corpus, with the hope that our experience may also prove useful to others outside the community of those interested in temporal processing.</abstract>
        <keywords>Temporal annotation, corpus, ISO-TimeML</keywords>
      </article>
      <article id="taln-2011-long-018" session="Lexique et Corpus">
        <auteurs>
          <auteur>
            <nom>Edmond Lassalle</nom>
            <email>edmond.lassalle@orange-ftgroup.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Orange Labs, 2 avenue Pierre Marzin, 22 307 Lannion - France</affiliation>
        </affiliations>
        <titre>Acquisition automatique de terminologie à partir de corpus de texte</titre>
        <type>long</type>
        <pages/>
        <resume>Les applications de recherche d'informations chez Orange sont confrontées à des flux importants de données textuelles, recouvrant des domaines larges et évoluant très rapidement. Un des problèmes à résoudre est de pouvoir analyser très rapidement ces flux, à un niveau élevé de qualité. Le recours à un modèle d'analyse sémantique, comme solution, n'est viable qu'en s'appuyant sur l'apprentissage automatique pour construire des grandes bases de connaissances dédiées à chaque application. L'extraction terminologique décrite dans cet article est un composant amont de ce dispositif d'apprentissage. Des nouvelles méthodes d'acquisition, basée sur un modèle hybride (analyse par grammaires de chunking et analyse statistique à deux niveaux), ont été développées pour répondre aux contraintes de performance et de qualité.</resume>
        <mots_cles>Apprentissage automatique, acquisition terminologique, entropie, grammaires de chunking</mots_cles>
        <title/>
        <abstract>Information retrieval applications by Orange must process tremendous textual dataflows which cover large domains and evolve rapidly. One problem to solve is to analyze these dataflows very quickly, with a high quality level. Having a semantic analysis model as a solution is reliable only if unsupervised learning is used to build large knowledge databases dedicated to each application. The terminology extraction described in this paper is a prior component of the learning architecture. New acquisition methods, based on hybrid model (chunking analysis coupled with two-level statistical analysis) have been developed to meet the constraints of both performance and quality.</abstract>
        <keywords>Unsupervised learning, terminology acquisition, entropy, chunking analysis</keywords>
      </article>
      <article id="taln-2011-long-019" session="Lexique et Corpus">
        <auteurs>
          <auteur>
            <nom>Amir Hazem</nom>
            <email>amir.hazem@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Emmanuel Morin</nom>
            <email>emmanuel.morin@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Sebastián Peña Saldarriaga</nom>
            <email>spena@synchromedia.ca</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université de Nantes, LINA - UMR CNRS 6241, 2 rue de la Houssinière, BP 92208, 44322 Nantes Cedex 03</affiliation>
          <affiliation affiliationId="2">Synchromedia, École de technologie supérieure, 1100 rue Notre-Dame Ouest, Montréal, Québec, Canada H3C 1K3</affiliation>
        </affiliations>
        <titre>Métarecherche pour l’extraction lexicale bilingue à partir de corpus comparables</titre>
        <type>long</type>
        <pages/>
        <resume>Nous présentons dans cet article une nouvelle manière d’aborder le problème de l’acquisition automatique de paires de mots en relation de traduction à partir de corpus comparables. Nous décrivons tout d’abord les approches standard et par similarité interlangue traditionnellement dédiées à cette tâche. Nous réinterprétons ensuite la méthode par similarité interlangue et motivons un nouveau modèle pour reformuler cette approche inspirée par les métamoteurs de recherche d’information. Les résultats empiriques que nous obtenons montrent que les performances de notre modèle sont toujours supérieures à celles obtenues avec l’approche par similarité interlangue, mais aussi comme étant compétitives par rapport à l’approche standard.</resume>
        <mots_cles>Corpus comparables, lexiques bilingues, métarecherche</mots_cles>
        <title/>
        <abstract>In this article we present a novel way of looking at the problem of automatic acquisition of pairs of translationally equivalent words from comparable corpora.We first describe the standard and extended approaches traditionally dedicated to this task. We then re-interpret the extended method, and motivate a novel model to reformulate this approach inspired by the metasearch engines in information retrieval. The empirical results show that performances of our model are always better than the baseline obtained with the extended approach and also competitive with the standard approach.</abstract>
        <keywords>Comparable corpora, bilingual lexicon, metasearch</keywords>
      </article>
      <article id="taln-2011-long-020" session="Lexique et Corpus">
        <auteurs>
          <auteur>
            <nom>Alain Joubert</nom>
            <email>alain.joubert@lirmm.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Mathieu Lafourcade</nom>
            <email>mathieu.lafourcade@lirmm.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Didier Schwab</nom>
            <email>didier.schwab@imag.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Michael Zock</nom>
            <email>michael.zock@lif.univ-mrs.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIRMM, Université Montpellier II</affiliation>
          <affiliation affiliationId="2">LIG, Université Grenoble II</affiliation>
          <affiliation affiliationId="3">LIF-CNRS, Marseille</affiliation>
        </affiliations>
        <titre>Évaluation et consolidation d’un réseau lexical via un outil pour retrouver le mot sur le bout de la langue</titre>
        <type>long</type>
        <pages/>
        <resume>Depuis septembre 2007, un réseau lexical de grande taille pour le Français est en cours de construction à l'aide de méthodes fondées sur des formes de consensus populaire obtenu via des jeux (projet JeuxDeMots). L’intervention d’experts humains est marginale en ce qu'elle représente moins de 0,5% des relations du réseau et se limite à des corrections, à des ajustements ainsi qu’à la validation des sens de termes. Pour évaluer la qualité de cette ressource construite par des participants de jeu (utilisateurs non experts) nous adoptons une démarche similaire à celle de sa construction, à savoir, la ressource doit être validée sur un vocabulaire de classe ouverte, par des non-experts, de façon stable (persistante dans le temps). Pour ce faire, nous proposons de vérifier si notre ressource est capable de servir de support à la résolution du problème nommé 'Mot sur le Bout de la Langue' (MBL). A l'instar de JeuxdeMots, l'outil développé peut être vu comme un jeu en ligne. Tout comme ce dernier, il permet d’acquérir de nouvelles relations, constituant ainsi un enrichissement de notre réseau lexical.</resume>
        <mots_cles>Réseau lexical, JeuxDeMots, évaluation, outil de MBL, mot sur le bout de la langue</mots_cles>
        <title/>
        <abstract>Since September 2007, a large scale lexical network for French is under construction through methods based on some kind of popular consensus by means of games (JeuxDeMots project). Human intervention can be considered as marginal. It is limited to corrections, adjustments and validation of the senses of terms, which amounts to less than 0,5 % of the relations in the network. To appreciate the quality of this resource built by non-expert users (players of the game), we use a similar approach to its construction. The resource must be validated by laymen, persistent in time, on open class vocabulary. We suggest to check whether our tool is able to solve the Tip of the Tongue (TOT) problem. Just like JeuxDeMots, our tool can be considered as an on-line game. Like the former, it allows the acquisition of new relations, enriching thus the (existing) network.</abstract>
        <keywords>Lexical network, JeuxDeMots, evaluation, TOT software, tip of the tongue</keywords>
      </article>
      <article id="taln-2011-long-021" session="Morphologie et Segmentation">
        <auteurs>
          <auteur>
            <nom>Matthieu Vernier</nom>
            <email>Matthieu.Vernier@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Laura Monceaux</nom>
            <email>Laura.Monceaux@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Béatrice Daille</nom>
            <email>Beatrice.Daille@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université de Nantes, LINA, 2, rue de la Houssinière 44322 Nantes</affiliation>
        </affiliations>
        <titre>Identifier la cible d’un passage d’opinion dans un corpus multithématique</titre>
        <type>long</type>
        <pages/>
        <resume>L’identification de la cible d’une d’opinion fait l’objet d’une attention récente en fouille d’opinion. Les méthodes existantes ont été testées sur des corpus monothématiques en anglais. Elles permettent principalement de traiter les cas où la cible se situe dans la même phrase que l’opinion. Dans cet article, nous abordons cette problématique pour le français dans un corpus multithématique et nous présentons une nouvelle méthode pour identifier la cible d’une opinion apparaissant hors du contexte phrastique. L’évaluation de la méthode montre une amélioration des résultats par rapport à l’existant.</resume>
        <mots_cles>Fouille d’opinions, Identification des cibles, Méthode RankSVM</mots_cles>
        <title/>
        <abstract>Recent works on opinion mining deal with the problem of finding the semantic relation between sentiment expressions and their target. Existing methods have been evaluated on monothematic english corpora. These methods are only able to solve intrasentential relationships. In this article, we focus on this task apply to french and we present a new method for solving intrasentential and intersentential relationships in a multithematic corpus. We show that our method is able to improve results on the intra- and intersential relationships.</abstract>
        <keywords>Opinion mining, Targeting sentiment expressions, RankSVM</keywords>
      </article>
      <article id="taln-2011-long-022" session="Morphologie et Segmentation">
        <auteurs>
          <auteur>
            <nom>Matthieu Constant</nom>
            <email>mconstan@univ-mlv.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Isabelle Tellier</nom>
            <email>isabelle.tellier@univ-orleans.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Denys Duchier</nom>
            <email>denys.duchier@univ-orleans.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Yoann Dupont</nom>
            <email>yoann.dupont@etu.univ-orleans.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Anthony Sigogne</nom>
            <email>sigogne@univ-mlv.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Sylvie Billot</nom>
            <email>sylvie.billot@univ-orleans.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université Paris-Est, LIGM, CNRS, 5 bd Descartes, Champs-sur-Marne 77454, Marne-la-Vallée cedex 2</affiliation>
          <affiliation affiliationId="2">LIFO, université d’Orléans, 6 rue Léonard de Vinci, BP 6759, 45067 Orléans cedex 2</affiliation>
        </affiliations>
        <titre>Intégrer des connaissances linguistiques dans un CRF : application à l’apprentissage d’un segmenteur-étiqueteur du français</titre>
        <type>long</type>
        <pages/>
        <resume>Dans cet article, nous synthétisons les résultats de plusieurs séries d’expériences réalisées à l’aide de CRF (Conditional Random Fields ou “champs markoviens conditionnels”) linéaires pour apprendre à annoter des textes français à partir d’exemples, en exploitant diverses ressources linguistiques externes. Ces expériences ont porté sur l’étiquetage morphosyntaxique intégrant l’identification des unités polylexicales. Nous montrons que le modèle des CRF est capable d’intégrer des ressources lexicales riches en unités multi-mots de différentes manières et permet d’atteindre ainsi le meilleur taux de correction d’étiquetage actuel pour le français.</resume>
        <mots_cles>Etiquetagemorphosyntaxique,Modèle CRF, Ressources lexicales, Segmentation, Unités polylexicales</mots_cles>
        <title/>
        <abstract>In this paper, we synthesize different experiments using a linear CRF (Conditional Random Fields) to annotate French texts from examples, by exploiting external linguistic resources. These experiments especially dealt with part-of-speech tagging including multiword units identification. We show that CRF models allow to integrate, in different ways, large-coverage lexical resources including multiword units and reach stateof- the-art tagging results for French.</abstract>
        <keywords>Part-of-speech tagging, CRF model, Lexical resources, Segmentation, Multiword units</keywords>
      </article>
      <article id="taln-2011-long-023" session="Morphologie et Segmentation">
        <auteurs>
          <auteur>
            <nom>Pierre Magistry</nom>
            <email>pierre.magistry@inria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Benoît Sagot</nom>
            <email>benoit.sagot@inria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Alpage, INRIA Paris–Rocquencourt &amp; Université Paris 7, Rocquencourt, BP 105, 78153 Le Chesnay Cedex, France</affiliation>
        </affiliations>
        <titre>Segmentation et induction de lexique non-supervisées du mandarin</titre>
        <type>long</type>
        <pages/>
        <resume>Pour la plupart des langues utilisant l'alphabet latin, le découpage d'un texte selon les espaces et les symboles de ponctuation est une bonne approximation d'un découpage en unités lexicales. Bien que cette approximation cache de nombreuses difficultés, elles sont sans comparaison avec celles que l'on rencontre lorsque l'on veut traiter des langues qui, comme le chinois mandarin, n'utilisent pas l'espace. Un grand nombre de systèmes de segmentation ont été proposés parmi lesquels certains adoptent une approche non-supervisée motivée linguistiquement. Cependant les méthodes d'évaluation communément utilisées ne rendent pas compte de toutes les propriétés de tels systèmes. Dans cet article, nous montrons qu'un modèle simple qui repose sur une reformulation en termes d'entropie d'une hypothèse indépendante de la langue énoncée par Harris (1955), permet de segmenter un corpus et d'en extraire un lexique. Testé sur le corpus de l'Academia Sinica, notre système permet l'induction d'une segmentation et d'un lexique qui ont de bonnes propriétés intrinsèques et dont les caractéristiques sont similaires à celles du lexique sous-jacent au corpus segmenté manuellement. De plus, on constate une certaine corrélation entre les résultats du modèle de segmentation et les structures syntaxiques fournies par une sous-partie arborée corpus.</resume>
        <mots_cles>Segmentation non-supervisée, entropie, induction de lexique, unité lexicale, chinois mandarin</mots_cles>
        <title/>
        <abstract>For most languages using the Latin alphabet, tokenizing a text on spaces and punctuation marks is a good approximation of a segmentation into lexical units. Although this approximation hides many difficulties, they do not compare with those arising when dealing with languages that do not use spaces, such as Mandarin Chinese. Many segmentation systems have been proposed, some of them use linguistitically motivated unsupervized algorithms. However, standard evaluation practices fail to account for some properties of such systems. In this paper, we show that a simple model, based on an entropy-based reformulation of a language-independent hypothesis put forward by Harris (1955), allows for segmenting a corpus and extracting a lexicon from the results. Tested on the Academia Sinica Corpus, our system allows for inducing a segmentation and a lexicon with good intrinsic properties and whose characteristics are similar to those of the lexicon underlying the manually-segmented corpus. Moreover, the results of the segmentation model correlate with the syntactic structures provided by the syntactically annotated subpart of the corpus.</abstract>
        <keywords>Non-supervized segmentation, entropy, lexicon induction, Mandarin Chinese</keywords>
      </article>
      <article id="taln-2011-long-024" session="Morphologie et Segmentation">
        <auteurs>
          <auteur>
            <nom>Delphine Bernhard</nom>
            <email>bernhard@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Bruno Cartoni</nom>
            <email>bruno.cartoni@unige.ch</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Delphine Tribout</nom>
            <email>tribout@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIMSI-CNRS, 91403 Orsay, France</affiliation>
          <affiliation affiliationId="2">Département de linguistique, Université de Genève, Suisse</affiliation>
        </affiliations>
        <titre>Évaluer la pertinence de la morphologie constructionnelle dans les systèmes de Question-Réponse</titre>
        <type>long</type>
        <pages/>
        <resume>Les connaissances morphologiques sont fréquemment utilisées en Question-Réponse afin de faciliter l’appariement entre mots de la question et mots du passage contenant la réponse. Il n’existe toutefois pas d’étude qualitative et quantitative sur les phénomènes morphologiques les plus pertinents pour ce cadre applicatif. Dans cet article, nous présentons une analyse détaillée des phénomènes de morphologie constructionnelle permettant de faire le lien entre question et réponse. Pour ce faire, nous avons constitué et annoté un corpus de paires de questions-réponses, qui nous a permis de construire une ressource de référence, utile pour l’évaluation de la couverture de ressources et d’outils d’analyse morphologique. Nous détaillons en particulier les phénomènes de dérivation et de composition et montrons qu’il reste un nombre important de relations morphologiques dérivationnelles pour lesquelles il n’existe pas encore de ressource exploitable pour le français.</resume>
        <mots_cles>Évaluation, Morphologie, Ressources, Système de Question-Réponse</mots_cles>
        <title/>
        <abstract>Morphological knowledge is often used in Question Answering systems to facilitate the matching between question words and words in the passage containing the answer. However, there is no qualitative and quantitative study about morphological phenomena which are most relevant to this application. In this paper, we present a detailed analysis of the constructional morphology phenomena found in question and answer pairs. To this aim, we gathered and annotated a corpus of question and answer pairs. We relied on this corpus to build a gold standard for evaluating the coverage of morphological analysis tools and resources. We detail in particular the phenomena of derivation and composition and show that a significant number of derivational morphological relations are still not covered by any existing resource for the French language.</abstract>
        <keywords>Evaluation, Morphology, Resources, Question-answering system</keywords>
      </article>
      <article id="taln-2011-long-025" session="Morphologie et Segmentation">
        <auteurs>
          <auteur>
            <nom>Julien Gosme</nom>
            <email>Julien.Gosme@unicaen.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Yves Lepage</nom>
            <email>Yves.Lepage@aoni.waseda.jp</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">GREYC, université de Caen Basse-Normandie, France</affiliation>
          <affiliation affiliationId="2">IPS, université Waseda, Japon</affiliation>
        </affiliations>
        <titre>Structure des trigrammes inconnus et lissage par analogie</titre>
        <type>long</type>
        <pages/>
        <resume>Nous montrons dans une série d’expériences sur quatre langues, sur des échantillons du corpus Europarl, que, dans leur grande majorité, les trigrammes inconnus d’un jeu de test peuvent être reconstruits par analogie avec des trigrammes hapax du corpus d’entraînement. De ce résultat, nous dérivons une méthode de lissage simple pour les modèles de langue par trigrammes et obtenons de meilleurs résultats que les lissages de Witten-Bell, Good-Turing et Kneser-Ney dans des expériences menées en onze langues sur la partie commune d’Europarl, sauf pour le finnois et, dans une moindre mesure, le français.</resume>
        <mots_cles>analogie, trigrammes inconnus, trigrammes hapax, modèle de langue trigrammes, Europarl</mots_cles>
        <title/>
        <abstract>In a series of experiments in four languages on subparts of the Europarl corpus, we show that a large number of unseen trigrams can be reconstructed by proportional analogy using only hapax trigrams. We derive a simple smoothing scheme from this empirical result and show that it outperforms Witten-Bell, Good-Turing and Kneser-Ney smoothing schemes on trigram models built on the common part of the Europarl corpus, in all 11 languages except Finnish and French.</abstract>
        <keywords>proportional analogy, unseen trigrams, hapax trigrams, trigram language models, Europarl</keywords>
      </article>
      <article id="taln-2011-long-026" session="Syntaxe">
        <auteurs>
          <auteur>
            <nom>Joseph Le Roux</nom>
            <email>joseph.le-roux@lif.univ-mrs.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Benoît Favre</nom>
            <email>benoit.favre@lif.univ-mrs.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Seyed Abolghasem Mirroshandel</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Alexis Nasr</nom>
            <email>alexis.nasr@lif.univ-mrs.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIF - CNRS UMR 6166 - Université Aix Marseille</affiliation>
        </affiliations>
        <titre>Modèles génératif et discriminant en analyse syntaxique : expériences sur le corpus arboré de Paris 7</titre>
        <type>long</type>
        <pages/>
        <resume>Nous présentons une architecture pour l’analyse syntaxique en deux étapes. Dans un premier temps un analyseur syntagmatique construit, pour chaque phrase, une liste d’analyses qui sont converties en arbres de dépendances. Ces arbres sont ensuite réévalués par un réordonnanceur discriminant. Cette méthode permet de prendre en compte des informations auxquelles l’analyseur n’a pas accès, en particulier des annotations fonctionnelles. Nous validons notre approche par une évaluation sur le corpus arboré de Paris 7. La seconde étape permet d’améliorer significativement la qualité des analyses retournées, quelle que soit la métrique utilisée.</resume>
        <mots_cles>analyse syntaxique, corpus arboré, apprentissage automatique, réordonnancement discriminant</mots_cles>
        <title/>
        <abstract>We present an architecture for parsing in two steps. First, a phrase-structure parser builds for each sentence an n-best list of analyses which are converted to dependency trees. Then these trees are rescored by a discriminative reranker. This method enables the incorporation of additional linguistic information, more precisely functional annotations. We test our approach on the French Treebank. The evaluation shows a significative improvement on different parse metrics.</abstract>
        <keywords>parsing, treebank, machine learning, discriminative reranking</keywords>
      </article>
      <article id="taln-2011-long-027" session="Syntaxe">
        <auteurs>
          <auteur>
            <nom>Anne-Lyse Minard</nom>
            <email>Anne-Lyse.Minard@limsi.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Anne-Laure Ligozat</nom>
            <email>Anne-Laure.Ligozat@limsi.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Brigitte Grau</nom>
            <email>Brigitte.Grau@limsi.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIMSI-CNRS, BP 133, 91403 Orsay cedex</affiliation>
          <affiliation affiliationId="2">Université Paris-Sud, 91400 Orsay</affiliation>
          <affiliation affiliationId="3">ENSIIE, 1 square de la résistance, 91000 Évry</affiliation>
        </affiliations>
        <titre>Apport de la syntaxe pour l’extraction de relations en domaine médical</titre>
        <type>long</type>
        <pages/>
        <resume>Dans cet article, nous nous intéressons à l’identification de relations entre entités en domaine de spécialité, et étudions l’apport d’informations syntaxiques. Nous nous plaçons dans le domaine médical, et analysons des relations entre concepts dans des comptes-rendus médicaux, tâche évaluée dans la campagne i2b2 en 2010. Les relations étant exprimées par des formulations très variées en langue, nous avons procédé à l’analyse des phrases en extrayant des traits qui concourent à la reconnaissance de la présence d’une relation et nous avons considéré l’identification des relations comme une tâche de classification multi-classes, chaque catégorie de relation étant considérée comme une classe. Notre système de référence est celui qui a participé à la campagne i2b2, dont la F-mesure est d’environ 0,70. Nous avons évalué l’apport de la syntaxe pour cette tâche, tout d’abord en ajoutant des attributs syntaxiques à notre classifieur, puis en utilisant un apprentissage fondé sur la structure syntaxique des phrases (apprentissage à base de tree kernels) ; cette dernière méthode améliore les résultats de la classification de 3%.</resume>
        <mots_cles>extraction de relation, domaine médical, apprentissage multi-classes, tree kernel</mots_cles>
        <title/>
        <abstract>In this paper, we study relation identification between concepts in medical reports, a task that was evaluated in the i2b2 campaign in 2010, and evaluate the usefulness of syntactic information. As relations are expressed in natural language with a great variety of forms, we proceeded to sentence analysis by extracting features that enable to identify a relation and we modeled this task as a multiclass classification task based on SVM, each category of relation representing a class. This method obtained an F-measure of 0.70 at i2b2 evaluation. We then evaluated the introduction of syntactic information in the classification process, by adding syntactic features, and by using tree kernels. This last method improves the classification up to 3%.</abstract>
        <keywords>relation identification, medical domain, multiclass learning, tree kernel</keywords>
      </article>
      <article id="taln-2011-long-028" session="Syntaxe">
        <auteurs>
          <auteur>
            <nom>Guillaume Bonfante</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Bruno Guillaume</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Mathieu Morey</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Guy Perrier</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">INRIA Nancy-Grand Est - LORIA - Nancy-Université</affiliation>
        </affiliations>
        <titre>Enrichissement de structures en dépendances par réécriture de graphes</titre>
        <type>long</type>
        <pages/>
        <resume>Nous montrons comment enrichir une annotation en dépendances syntaxiques au format du French Treebank de Paris 7 en utilisant la réécriture de graphes, en vue du calcul de sa représentation sémantique. Le système de réécriture est composé de règles grammaticales et lexicales structurées en modules. Les règles lexicales utilisent une information de contrôle extraite du lexique des verbes français Dicovalence.</resume>
        <mots_cles>dépendance, French Treebank, réécriture de graphes, Dicovalence</mots_cles>
        <title/>
        <abstract>We show how to enrich a syntactic dependency annotation of the French Paris 7 Treebank format, using graph rewriting, in order to compute its semantic representation. The rewriting system is composed of grammatical and lexical rules structured in modules. The lexical rules use a control information extracted from Dicovalence, a lexicon of French verbs.</abstract>
        <keywords>dependency, French Treebank, graph rewriting, Dicovalence</keywords>
      </article>
      <article id="taln-2011-long-029" session="Syntaxe">
        <auteurs>
          <auteur>
            <nom>Alexander Pak</nom>
            <email>alexpak@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Patrick Paroubek</nom>
            <email>pap@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université de Paris-Sud, Laboratoire LIMSI-CNRS, Bâtiment 508, F-91405 Orsay Cedex, France</affiliation>
        </affiliations>
        <titre>Classification en polarité de sentiments avec une représentation textuelle à base de sous-graphes d’arbres de dépendances</titre>
        <type>long</type>
        <pages/>
        <resume>Les approches classiques à base de n-grammes en analyse supervisée de sentiments ne peuvent pas correctement identifier les expressions complexes de sentiments à cause de la perte d’information induite par l’approche « sac de mots » utilisée pour représenter les textes. Dans notre approche, nous avons recours à des sous-graphes extraits des graphes de dépendances syntaxiques comme traits pour la classification de sentiments. Nous représentons un texte par un vecteur composé de ces sous-graphes syntaxiques et nous employons un classifieurs SVM état-de-l’art pour identifier la polarité d’un texte. Nos évaluations expérimentales sur des critiques de jeux vidéo montrent que notre approche à base de sous-graphes est meilleure que les approches standard à modèles « sac de mots » et n-grammes. Dans cet article nous avons travaillé sur le français, mais notre approche peut facilement être adaptée à d’autres langues.</resume>
        <mots_cles>analyse de sentiments, analyse syntaxique, arbre de dépendances, SVM</mots_cles>
        <title/>
        <abstract>A standard approach for supervised sentiment analysis with n-grams features cannot correctly identify complex sentiment expressions due to the loss of information incurred when representing texts with bagof- words models. In our research, we propose to use subgraphs from sentence dependency parse trees as features for sentiment classification.We represent a text by a feature vector made from extracted subgraphs and use a state of the art SVM classifier to identify the polarity of a text. Our experimental evaluations on video game reviews show that using our dependency subgraph features outperforms standard bag-of-words and n-gram models. In this paper, we worked with French, however our approach can be easily adapted to other languages.</abstract>
        <keywords>sentiment analysis, parsing, dependency tree, SVM</keywords>
      </article>
      <article id="taln-2011-long-030" session="Syntaxe">
        <auteurs>
          <auteur>
            <nom>Sylvain Kahane</nom>
            <email>sylvain@kahane.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Modyco, Université Paris Ouest Nanterre &amp; CNRS / Alpage, INRIA</affiliation>
        </affiliations>
        <titre>Une modélisation des dites alternances de portée des quantifieurs par des opérations de combinaison des groupes nominaux</titre>
        <type>long</type>
        <pages/>
        <resume>Nous montrons que les différentes interprétations d’une combinaison de plusieurs GN peuvent être modélisées par deux opérations de combinaison sur les référents de ces GN, appelées combinaison cumulative et combinaison distributive. Nous étudions aussi bien les GN définis et indéfinis que les GN quantifiés ou pluriels et nous montrons comment la combinaison d’un GN avec d’autres éléments peut induire des interprétations collective ou individualisante. Selon la façon dont un GN se combine avec d’autres GN, le calcul de son référent peut être fonction de ces derniers ; ceci définit une relation d’ancrage de chaque GN, qui induit un ordre partiel sur les GN. Considérer cette relation plutôt que la relation converse de portée simplifie le calcul de l’interprétation des GN et des énoncés. Des représentations sémantiques graphiques et algébriques sans considération de la portée sont proposées pour les dites alternances de portée.</resume>
        <mots_cles>portée des quantifieurs, cumulatif, collectif, distributif, référent de discours, ancrage</mots_cles>
        <title/>
        <abstract>We show that the various interpretations of a combination of several Noun Phrases can be modeled by two operations of combination on the referent of these NPs, called cumulative and distributive combinations. We study definite and indefinite NPs as well as quantified and plural NPs and we show how the combination of an NP with other NPs can induce collective or individualizing interpretations. According to the way a NP combine with another NP, the calculation of its referent can be a function of the latter; this defines an anchoring relation for each NP, which induces a partial order on NPs. Considering this relation rather than the converse scope relation simplifies the calculation of the interpretation of NPs and utterances. Graphic and algebraic semantic representations without considering scope are proposed for the so-called scope alternations.</abstract>
        <keywords>quantifier scope alternation, cumulative, collective, distributive, discourse referent, anchoring</keywords>
      </article>
      <article id="taln-2011-long-031" session="Discours">
        <auteurs>
          <auteur>
            <nom>Delphine Bernhard</nom>
            <email>bernhard@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Anne-Laure Ligozat</nom>
            <email>annlor@limsi.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIMSI-CNRS, B.P. 133, 91403 Orsay Cedex</affiliation>
          <affiliation affiliationId="2">ENSIIE, 1 square de la résistance, 91000 Évry</affiliation>
        </affiliations>
        <titre>Analyse automatique de la modalité et du niveau de certitude : application au domaine médical</titre>
        <type>long</type>
        <pages/>
        <resume>De nombreux phénomènes linguistiques visent à exprimer le doute ou l’incertitude de l’énonciateur, ainsi que la subjectivité potentielle du point de vue. La prise en compte de ces informations sur le niveau de certitude est primordiale pour de nombreuses applications du traitement automatique des langues, en particulier l’extraction d’information dans le domaine médical. Dans cet article, nous présentons deux systèmes qui analysent automatiquement les niveaux de certitude associés à des problèmes médicaux mentionnés dans des compte-rendus cliniques en anglais. Le premier système procède par apprentissage supervisé et obtient une f-mesure de 0,93. Le second système utilise des règles décrivant des déclencheurs linguistiques spécifiques et obtient une f-mesure de 0,90.</resume>
        <mots_cles>Modalité épistémique, Niveau de certitude, Domaine médical</mots_cles>
        <title/>
        <abstract>Many linguistic phenomena aim at expressing the speaker’s doubt or incertainty, as well as the potential subjectivity of the point of view. Most natural language processing applications, and in particular knowledge extraction in the medical domain, need to take this type of information into account. In this article, we describe two systems which automatically analyse the levels of certainty associated with medical problems mentioned in English clinical reports. The first system uses supervised machine learning and obtains an f-measure of 0.93. The second system relies on a set of rules decribing specific linguistic triggers and reaches an f-measure of 0.90.</abstract>
        <keywords>Epistemic modality, Certainty level, Medical domain</keywords>
      </article>
      <article id="taln-2011-long-032" session="Discours">
        <auteurs>
          <auteur>
            <nom>Laurence Danlos</nom>
            <email>Laurence.Danlos@linguist.jussieu.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">ALPAGE, Université Paris Diderot (Paris 7), 175 rue du Chevaleret, 750013 Paris</affiliation>
        </affiliations>
        <titre>Analyse discursive et informations de factivité</titre>
        <type>long</type>
        <pages/>
        <resume>Les annotations discursives proposées dans le cadre de théories discursives comme RST (Rhetorical Structure Theory) ou SDRT (Segmented Dicourse Representation Theory) ont comme point fort de construire une structure discursive globale liant toutes les informations données dans un texte. Les annotations discursives proposées dans le PDTB (Penn Discourse Tree Bank) ont comme point fort d’identifier la “source” de chaque information du texte—répondant ainsi à la question qui a dit ou pense quoi ? Nous proposons une approche unifiée pour les annotations discursives alliant les points forts de ces deux courants de recherche. Cette approche unifiée repose crucialement sur des information de factivité, telles que celles qui sont annotées dans le corpus (anglais) FactBank.</resume>
        <mots_cles>Discours, Analyse discursive, Factivité (véracité), Interface syntaxe-sémantique, RST, SDRT, PDTB, FactBank</mots_cles>
        <title/>
        <abstract>Discursive annotations proposed in theories of discourse such as RST (Rhetorical Structure Theory) or SDRT (Segmented Representation Theory Dicourse) have the advatange of building a global discourse structure linking all the information in a text. Discursive annotations proposed in PDTB (Penn Discourse Tree Bank) have the advatange of identifying the “source” of each information—thereby answering to questions such as who says or thinks what ? We propose a unified approach for discursive annotations combining the strengths of these two streams of research. This unified approach relies crucially on factivity information, as encoded in the English corpus FactBank.</abstract>
        <keywords>Discourse, Discursive analysis, Factuality (vericity), Syntax-semantic interface, RST, SDRT, PDTB, FactBank</keywords>
      </article>
      <article id="taln-2011-long-033" session="Discours">
        <auteurs>
          <auteur>
            <nom>Camille Dutrey</nom>
            <email>camille@dutrey.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Houda Bouamor</nom>
            <email>Houda.Bouamor@limsi.fr</email>
            <affiliationId>2</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Delphine Bernhard</nom>
            <email>Delphine.Bernhard@limsi.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Aurélien Max</nom>
            <email>Aurelien.Max@limsi.fr</email>
            <affiliationId>2</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">INALCO, Paris, France</affiliation>
          <affiliation affiliationId="2">LIMSI-CNRS, Orsay, France</affiliation>
          <affiliation affiliationId="3">Univ. Paris-Sud, Orsay, France</affiliation>
        </affiliations>
        <titre>Paraphrases et modifications locales dans l’historique des révisions deWikipédia</titre>
        <type>long</type>
        <pages/>
        <resume>Dans cet article, nous analysons les modifications locales disponibles dans l’historique des révisions de la version française de Wikipédia. Nous définissons tout d’abord une typologie des modifications fondée sur une étude détaillée d’un large corpus de modifications. Puis, nous détaillons l’annotation manuelle d’une partie de ce corpus afin d’évaluer le degré de complexité de la tâche d’identification automatique de paraphrases dans ce genre de corpus. Enfin, nous évaluons un outil d’identification de paraphrases à base de règles sur un sous-ensemble de notre corpus.</resume>
        <mots_cles>Wikipédia, révisions, identification de paraphrases</mots_cles>
        <title/>
        <abstract>In this article, we analyse the modifications available in the French Wikipédia revision history. We first define a typology of modifications based on a detailed study of a large corpus of modifications. Moreover, we detail a manual annotation study of a subpart of the corpus aimed at assessing the difficulty of automatic paraphrase identification in such a corpus. Finally, we assess a rule-based paraphrase identification tool on a subset of our corpus.</abstract>
        <keywords>Wikipedia, revisions, paraphrase identification</keywords>
      </article>
      <article id="taln-2011-long-034" session="Discours">
        <auteurs>
          <auteur>
            <nom>Patrick Saint-Dizier</nom>
            <email>stdizier@irit.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">IRIT-CNRS, Toulouse</affiliation>
        </affiliations>
        <titre>&lt;TextCoop&gt;: un analyseur de discours basé sur les grammaires logiques</titre>
        <type>long</type>
        <pages/>
        <resume>Dans ce document, nous présentons les principales caractéristiques de &lt;TextCoop&gt;, un environnement basé sur les grammaires logiques dédié à l’analyse de structures discursives. Nous étudions en particulier le langage DisLog qui fixe la structure des règles et des spécifications qui les accompagnent. Nous présentons la structure du moteur de &lt;TextCoop&gt; en indiquant au fur et à mesure du texte l’état du travail, les performances et les orientations en particulier en matière d’environnement, d’aide à l’écriture de règles et de développement applicatif.</resume>
        <mots_cles>grammaire du discours, programmation en logique, grammaires logiques</mots_cles>
        <title/>
        <abstract>In this paper, we introduce the main features of &lt;TextCoop&gt;, an environment dedicated to discourse analysis within a logic-based grammar framework.We focus on the structure of discourse rules (DisLog language) and on the features of the engine, while outlining the results, the performances and the orientations for future work.</abstract>
        <keywords>discourse structure, logic programming, logic-based grammars</keywords>
      </article>
      <article id="taln-2011-long-035" session="Discours">
        <auteurs>
          <auteur>
            <nom>Katya Alahverdzhieva</nom>
            <email>K.Alahverdzhieva@sms.ed.ac.uk</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Alex Lascarides</nom>
            <email>alex@inf.ed.ac.uk</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">School of Informatics, University of Edinburgh</affiliation>
        </affiliations>
        <titre>Integration of Speech and Deictic Gesture in a Multimodal Grammar</titre>
        <type>long</type>
        <pages/>
        <resume>Dans cet article, nous présentons une analyse à base de contraintes de la relation forme-sens des gestes déictiques et de leur signal de parole synchrone. En nous basant sur une étude empirique de corpus multimodaux, nous définissons quels énoncés multimodaux sont bien formés, et lesquels ne pourraient jamais produire le sens voulu dans la situation communicative. Plus précisément, nous formulons une grammaire multimodale dont les règles de construction utilisent la prosodie, la syntaxe et la sémantique de la parole, la forme et le sens du signal déictique, ainsi que la performance temporelle de la parole et la deixis afin de contraindre la production d’un arbre de syntaxe combinant parole et gesture déictique ainsi que la représentation unifiée du sens pour l’action multimodale correspondant à cet arbre. La contribution de notre projet est double : nous ajoutons aux ressources existantes pour le TAL un corpus annoté de parole et de gestes, et nous créons un cadre théorique pour la grammaire au sein duquel la composition sémantique d’un énoncé découle de la synchronie entre geste et parole.</resume>
        <mots_cles>Deixis, parole et geste, grammaires multimodales</mots_cles>
        <title/>
        <abstract>In this paper we present a constraint-based analysis of the form-meaning relation of deictic gesture and its synchronous speech signal. Based on an empirical study of multimodal corpora, we capture generalisations about which multimodal utterances are well-formed, and which would never produce the intended meaning in the communicative situation. More precisely, we articulate a multimodal grammar whose construction rules use the prosody, syntax and semantics of speech, the form and meaning of the deictic signal, as well as the relative temporal performance of the speech and deixis to constrain the production of a single syntactic tree of speech and deictic gesture and its corresponding meaning representation for the multimodal action. In so doing, the contribution of our project is two-fold: it augments the existing NLP resources with annotated speech and gesture corpora, and it also provides the theoretical grammar framework where the semantic composition of an utterance results from its gestural and speech synchrony.</abstract>
        <keywords>Deixis, speech and gesture, multimodal grammars</keywords>
      </article>
      <article id="taln-2011-long-036" session="Traduction et Alignement">
        <auteurs>
          <auteur>
            <nom>Adrien Lardilleux</nom>
            <email>Adrien.Lardilleux@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>François Yvon</nom>
            <email>Francois.Yvon@limsi.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Yves Lepage</nom>
            <email>Yves.Lepage@aoni.waseda.jp</email>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIMSI-CNRS, BP 133, 91403 Orsay Cedex</affiliation>
          <affiliation affiliationId="2">Université Paris-Sud</affiliation>
          <affiliation affiliationId="3">IPS, université Waseda, Japon</affiliation>
        </affiliations>
        <titre>Généralisation de l’alignement sous-phrastique par échantillonnage</titre>
        <type>long</type>
        <pages/>
        <resume>L’alignement sous-phrastique consiste à extraire des traductions d’unités textuelles de grain inférieur à la phrase à partir de textes multilingues parallèles alignés au niveau de la phrase. Un tel alignement est nécessaire, par exemple, pour entraîner des systèmes de traduction statistique. L’approche standard pour réaliser cette tâche implique l’estimation successive de plusieurs modèles probabilistes de complexité croissante et l’utilisation d’heuristiques qui permettent d’aligner des mots isolés, puis, par extension, des groupes de mots. Dans cet article, nous considérons une approche alternative, initialement proposée dans (Lardilleux &amp; Lepage, 2008), qui repose sur un principe beaucoup plus simple, à savoir la comparaison des profils d’occurrences dans des souscorpus obtenus par échantillonnage. Après avoir analysé les forces et faiblesses de cette approche, nous montrons comment améliorer la détection d’unités de traduction longues, et évaluons ces améliorations sur des tâches de traduction automatique.</resume>
        <mots_cles>alignement sous-phrastique, traduction automatique par fragments</mots_cles>
        <title/>
        <abstract>Sub-sentential alignment is the process by which multi-word translation units are extracted from sentence-aligned multilingual parallel texts. Such alignment is necessary, for instance, to train statistical machine translation systems. Standard approaches typically rely on the estimation of several probabilistic models of increasing complexity and on the use of various heuristics that make it possible to align, first isolated words, then, by extension, groups of words. In this paper, we explore an alternative approach, originally proposed in (Lardilleux &amp; Lepage, 2008), that relies on a much simpler principle, which is the comparison of occurrence profiles in subcorpora obtained by sampling. After analyzing the strengths and weaknesses of this approach, we show how to improve the detection of long translation units, and evaluate these improvements on machine translation tasks.</abstract>
        <keywords>sub-sentential alignment, phrase-based machine translation</keywords>
      </article>
      <article id="taln-2011-long-037" session="Traduction et Alignement">
        <auteurs>
          <auteur>
            <nom>Nadi Tomeh</nom>
            <email>nadi@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Alexandre Allauzen</nom>
            <email>allauzen@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>François Yvon</nom>
            <email>yvon@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université Paris Sud et LIMSI/CNRS</affiliation>
        </affiliations>
        <titre>Estimation d’un modèle de traduction à partir d’alignements mot-à-mot non-déterministes</titre>
        <type>long</type>
        <pages/>
        <resume>Dans les systèmes de traduction statistique à base de segments, le modèle de traduction est estimé à partir d’alignements mot-à-mot grâce à des heuristiques d’extraction et de valuation. Bien que ces alignements mot-à-mot soient construits par des modèles probabilistes, les processus d’extraction et de valuation utilisent ces modèles en faisant l’hypothèse que ces alignements sont déterministes. Dans cet article, nous proposons de lever cette hypothèse en considérant l’ensemble de la matrice d’alignement, d’une paire de phrases, chaque association étant valuée par sa probabilité. En comparaison avec les travaux antérieurs, nous montrons qu’en utilisant un modèle exponentiel pour estimer de manière discriminante ces probabilités, il est possible d’obtenir des améliorations significatives des performances de traduction. Ces améliorations sont mesurées à l’aide de la métrique BLEU sur la tâche de traduction de l’arabe vers l’anglais de l’évaluation NIST MT’09, en considérant deux types de conditions selon la taille du corpus de données parallèles utilisées.</resume>
        <mots_cles>traduction statistique, modèles de traduction à base de segments, modèles d’alignement mot-à-mot</mots_cles>
        <title/>
        <abstract>In extant phrase-based statistical translation systems, the translation model relies on word-to-word alignments, which serve as constraints for further heuristic extraction and scoring processes. These word alignments are infered in a probabilistic framework ; yet, only one single best word alignment is used as if alignments were deterministically produced. In this paper, we propose to take the full probabilistic alignment matrix into account, where each alignment link is scored by its probability score. By comparison with previous attempts, we show that using an exponential model to compute these probabilities is an effective way to achieve significant improvements in translation accuracy on the NIST MT’09 Arabic to English translation task, where the accuracy is measured in terms of BLEU scores.</abstract>
        <keywords>statistical machine translation, phrase based translation models, word alignment models</keywords>
      </article>
      <article id="taln-2011-long-038" session="Traduction et Alignement">
        <auteurs>
          <auteur>
            <nom>Houda Bouamor</nom>
            <email>Houda.Bouamor@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Aurélien Max</nom>
            <email>Aurelien.Max@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Anne Vilnat</nom>
            <email>Anne.Vilnat@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIMSI-CNRS, Univ. Paris-Sud, Orsay, F-91403, France</affiliation>
        </affiliations>
        <titre>Combinaison d’informations pour l’alignement monolingue</titre>
        <type>long</type>
        <pages/>
        <resume>Dans cet article, nous décrivons une nouvelle méthode d’alignement automatique de paraphrases d’énoncés. Nous utilisons des méthodes développées précédemment afin de produire différentes approches hybrides (hybridations). Ces différentes méthodes permettent d’acquérir des équivalences textuelles à partir d’un corpus monolingue parallèle. L’hybridation combine des informations obtenues par diverses techniques : alignements statistiques, approche symbolique, fusion d’arbres syntaxiques et alignement basé sur des distances d’édition. Nous avons évalué l’ensemble de ces résultats et nous constatons une amélioration sur l’acquisition de paraphrases sous-phrastiques.</resume>
        <mots_cles>Paraphrase sous-phrastique, corpus parallèle monolingue, hybridation</mots_cles>
        <title/>
        <abstract>In this paper, we detail a new method to automatic alignment of paraphrase of statements.We also use previously developed methods to produce different hybrid approaches. These methods allow the acquisition of textual equivalence from a parallel monolingual corpus. Hybridization combines information obtained by using advanced statistical alignments, symbolic approach, syntax tree based alignment and edit distances technique. We evaluated all these results and we see an improvement on the acquisition of sub-sentential paraphrases.</abstract>
        <keywords>Phrasal paraphrase, monolingual parallel corpora, hybridization</keywords>
      </article>
      <article id="taln-2011-court-001" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Jean-Yves Antoine</nom>
            <email>Jean-Yves.Antoine@univ-tours.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Marc Le Tallec</nom>
            <email>Marc.Le-Tallec@univ-tours.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Jeanne Villaneau</nom>
            <email>Jeanne.Villaneau@univ-ubs.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université François Rabelais de Tours, LI, 37000 Blois</affiliation>
          <affiliation affiliationId="2">Université Européenne de Bretagne, VALORIA, 56100 Lorient</affiliation>
        </affiliations>
        <titre>Evaluation de la détection des émotions, des opinions ou des sentiments : dictature de la majorité ou respect de la diversité d’opinions ?</titre>
        <type>court</type>
        <pages/>
        <resume>Détection d’émotion, fouille d’opinion et analyse des sentiments sont généralement évalués par comparaison des réponses du système concerné par rapport à celles contenues dans un corpus de référence. Les questions posées dans cet article concernent à la fois la définition de la référence et la fiabilité des métriques les plus fréquemment utilisées pour cette comparaison. Les expérimentations menées pour évaluer le système de détection d’émotions EmoLogus servent de base de réflexion pour ces deux problèmes. L’analyse des résultats d’EmoLogus et la comparaison entre les différentes métriques remettent en cause le choix du vote majoritaire comme référence. Par ailleurs elles montrent également la nécessité de recourir à des outils statistiques plus évolués que ceux généralement utilisés pour obtenir des évaluations fiables de systèmes qui travaillent sur des données intrinsèquement subjectives et incertaines.</resume>
        <mots_cles>Détection d’émotion, analyse de sentiments, fouille d’opinion ; Evaluation : métrique d’évaluation, constitution de référence, analyse statistique des résultats</mots_cles>
        <title/>
        <abstract>Emotion detection, opinion identification and sentiment analysis are generally assessed by means of the comparison of a reference corpus with the answers of the system. This paper addresses the problem of the definition of the reference and the reliability of the metrics which are commonly used for this comparison. We present some experiments led with EmoLogus, a system of emotion detection, to investigate these two problems. A detailed analysis of the quantitative results obtained by EmoLogus on various metrics questions the choice of a majority vote among several human judgments to build a reference. Besides, it shows the necessity of using more sophisticated statistical tools to obtain a reliable evaluation of such systems which are working on intrinsically subjective and uncertain data.</abstract>
        <keywords>Detection of emotion, sentiment analysis, opinion mining, Evaluation: objective measures, test reference, statistical analysis of the results</keywords>
      </article>
      <article id="taln-2011-court-002" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Violeta Seretan</nom>
            <email>violeta.seretan@gmail.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Institute for Language, Cognition and Computation, Human Communication Research Centre, University of Edinburgh, 10 Crichton Street, Edinburgh EH8 9AB, United Kingdom</affiliation>
        </affiliations>
        <auteurs/>
        <titre>A Collocation-Driven Approach to Text Summarization</titre>
        <type>court</type>
        <pages/>
        <resume>Dans cet article, nous décrivons une nouvelle approche pour la création de résumés extractifs – tâche qui consiste à créer automatiquement un résumé pour un document en sélectionnant un sous-ensemble de ses phrases – qui exploite des informations collocationnelles spécifiques à un domaine, acquises préalablement à partir d’un corpus de développement. Un extracteur de collocations fondé sur l’analyse syntaxique est utilisé afin d’inférer un modèle de contenu qui est ensuite appliqué au document à résumer. Cette approche a été utilisée pour la création des versions simples pour les articles de Wikipedia en anglais, dans le cadre d’un projet visant la création automatique d’articles simplifiées, similaires aux articles recensées dans Simple English Wikipedia. Une évaluation du système développé reste encore à faire. Toutefois, les résultats préalables obtenus pour les articles sur des villes montrent le potentiel de cette approche guidée par collocations pour la sélection des phrases pertinentes.</resume>
        <mots_cles>résumé de texte automatique, résumé extractif, statistiques de co-occurrence, collocations, analyse syntaxique, Wikipedia</mots_cles>
        <title/>
        <abstract>We present a novel approach to extractive summarization – the task of producing an abstract for an input document by selecting a subset of the original sentences – which relies on domain-specific collocation information automatically acquired from a development corpus. A syntax-based collocation extractor is used to infer a content template and then to match this template against the document to summarize. The approach has been applied to generate simplified versions of Wikipedia articles in English, as part of a larger project on automatically generating Simple EnglishWikipedia articles starting from their standard counterpart. An evaluation of the developed system has yet to be performed; nonetheless, the preliminary results obtained in summarizing Wikipedia articles on cities already indicated the potential of our collocation-driven method to select relevant sentences.</abstract>
        <keywords>text summarization, extractive summarization, co-occurrence statistics, collocations, syntactic parsing, Wikipedia</keywords>
      </article>
      <article id="taln-2011-court-003" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Thomas François</nom>
            <email>thomas.francois@uclouvain.be</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Patrick Watrin</nom>
            <email>patrick.watrin@uclouvain.be</email>
            <affiliationId>2</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Aspirant FNRS</affiliation>
          <affiliation affiliationId="2">Centre de traitement automatique du langage (CENTAL), UCLouvain</affiliation>
          <affiliation affiliationId="3">Institut Langage et Communication (IL&amp;C), UCLouvain</affiliation>
        </affiliations>
        <auteurs> and </auteurs>
        <titre>Quel apport des unités polylexicales dans une formule de lisibilité pour le français langue étrangère</titre>
        <type>court</type>
        <pages/>
        <resume>Cette étude envisage l’emploi des unités polylexicales (UPs) comme prédicteurs dans une formule de lisibilité pour le français langue étrangère. À l’aide d’un extracteur d’UPs combinant une approche statistique à un filtre linguistique, nous définissons six variables qui prennent en compte la densité et la probabilité des UPs nominales, mais aussi leur structure interne. Nos expérimentations concluent à un faible pouvoir prédictif de ces six variables et révèlent qu’une simple approche basée sur la probabilité moyenne des n-grammes des textes est plus efficace.</resume>
        <mots_cles>Lisibilité du FLE, unités polylexicales nominales, modèles N-grammes</mots_cles>
        <title/>
        <abstract>This study considers the use of multi-words expressions (MWEs) as predictors for a readability formula for French as a foreign language. Using a MWEs extractor combining a statistical approach with a linguistic filter, we define six variables. These take into account the density and the probability of MWEs, but also their internal structure. Our experiments conclude that the predictive power of these six variables is low. Moreover, we show that a simple approach based on the average probability of n-grams is a more effective predictor.</abstract>
        <keywords>Readability of FFL, nominal MWEs, N-grams models</keywords>
      </article>
      <article id="taln-2011-court-004" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Frédéric Béchet</nom>
            <email>frederic.bechet@lif.univ-mrs.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Benoît Sagot</nom>
            <email>benoit.sagot@inria.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Rosa Stern</nom>
            <email>rosa.stern@afp.com</email>
            <affiliationId>2</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Aix Marseille Université, LIF-CNRS, route de Luminy, Marseille</affiliation>
          <affiliation affiliationId="2">Alpage, INRIA &amp; Univ. Paris 7, Rocquencourt, BP 105, 78153 Le Chesnay Cedex, France</affiliation>
          <affiliation affiliationId="3">Agence France-Presse – Medialab, 2 place de la Bourse, 75002 Paris, France</affiliation>
        </affiliations>
        <titre>Coopération de méthodes statistiques et symboliques pour l’adaptation non-supervisée d’un système d’étiquetage en entités nommées</titre>
        <type>court</type>
        <pages/>
        <resume>La détection et le typage des entités nommées sont des tâches pour lesquelles ont été développés à la fois des systèmes symboliques et probabilistes. Nous présentons les résultats d’une expérience visant à faire interagir le système à base de règles NP, développé sur des corpus provenant de l’AFP, intégrant la base d’entités Aleda et qui a une bonne précision, et le système LIANE, entraîné sur des transcriptions de l’oral provenant du corpus ESTER et qui a un bon rappel. Nous montrons qu’on peut adapter à un nouveau type de corpus, de manière non supervisée, un système probabiliste tel que LIANE grâce à des corpus volumineux annotés automatiquement par NP. Cette adaptation ne nécessite aucune annotation manuelle supplémentaire et illustre la complémentarité des méthodes numériques et symboliques pour la résolution de tâches linguistiques.</resume>
        <mots_cles>Détection d’entités nommées, adaptation à un nouveau domaine, coopération entre approches probabilistes et symboliques</mots_cles>
        <title/>
        <abstract>Named entity recognition and typing is achieved both by symbolic and probabilistic systems. We report on an experiment for making the rule-based system NP, a high-precision system developed on AFP news corpora and relies on the Aleda named entity database, interact with LIANE, a high-recall probabilistic system trained on oral transcriptions from the ESTER corpus.We show that a probabilistic system such as LIANE can be adapted to a new type of corpus in a non-supervized way thanks to large-scale corpora automatically annotated by NP. This adaptation does not require any additional manual anotation and illustrates the complementarity between numeric and symbolic techniques for tackling linguistic tasks.</abstract>
        <keywords>Named entity recognition, domain adaptation, cooperation between probabilistic and symbolic approaches</keywords>
      </article>
      <article id="taln-2011-court-005" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Nuria Gala</nom>
            <email>nuria.gala@lif.univ-mrs.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Nabil Hathout</nom>
            <email>nabil.hathout@univ-tlse2.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Alexis Nasr</nom>
            <email>alexis.nasr@lif.univ-mrs.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Véronique Rey</nom>
            <email>veronique.rey-lafay@univmed.fr</email>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Selja Seppälä</nom>
            <email>selja.seppala@lif.univ-mrs.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIF-TALEP, 163, Av. de Luminy case 901, 13288 Marseille Cedex 9</affiliation>
          <affiliation affiliationId="2">CLLE-ERSS, 5, allées Antonio Machado, 31058 Toulouse Cedex 9</affiliation>
          <affiliation affiliationId="3">EHESS, 2, rue de la Charité, 13002 Marseille</affiliation>
        </affiliations>
        <titre>Création de clusters sémantiques dans des familles morphologiques à partir du TLFi</titre>
        <type>court</type>
        <pages/>
        <resume>La constitution de ressources linguistiques est une tâche longue et coûteuse. C’est notamment le cas pour les ressources morphologiques. Ces ressources décrivent de façon approfondie et explicite l’organisation morphologique du lexique complétée d’informations sémantiques exploitables dans le domaine du TAL. Le travail que nous présentons dans cet article s’inscrit dans cette perspective et, plus particulièrement, dans l’optique d’affiner une ressource existante en s’appuyant sur des informations sémantiques obtenues automatiquement. Notre objectif est de caractériser sémantiquement des familles morpho-phonologiques (des mots partageant une même racine et une continuité de sens). Pour ce faire, nous avons utilisé des informations extraites du TLFi annoté morpho-syntaxiquement. Les premiers résultats de ce travail seront analysés et discutés.</resume>
        <mots_cles>Ressources lexicales, familles morphologiques, clusters sémantiques, mesure de Lesk</mots_cles>
        <title/>
        <abstract>Building lexical resources is a time-consuming and expensive task, mainly when it comes to morphological lexicons. Such resources describe in depth and explicitly the morphological organization of the lexicon, completed with semantic information to be used in NLP applications. The work we present here goes on such direction, and especially, on refining an existing resource with automatically acquired semantic information. Our goal is to semantically characterize morpho-phonological families (words sharing a same base form and semantic continuity). To this end, we have used data from the TLFi which has been morpho-syntactically annotated. The first results of such a task will be analyzed and discussed.</abstract>
        <keywords>Lexical resources, morphological families, semantic clusters, Lesk measure</keywords>
      </article>
      <article id="taln-2011-court-006" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Louis de Viron</nom>
            <email>louis.deviron@student.uclouvain.be</email>
            <affiliationId>1</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Delphine Bernhard</nom>
            <email>delphine.bernhard@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Véronique Moriceau</nom>
            <email>moriceau@limsi.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Xavier Tannier</nom>
            <email>xtannier@limsi.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIMSI-CNRS, 91403 Orsay, France</affiliation>
          <affiliation affiliationId="2">Université Paris Sud, 91405 Orsay, France</affiliation>
          <affiliation affiliationId="3">Université Catholique de Louvain, Belgique</affiliation>
        </affiliations>
        <titre>Génération automatique de questions à partir de textes en français</titre>
        <type>court</type>
        <pages/>
        <resume>Nous présentons dans cet article un générateur automatique de questions pour le français. Le système de génération procède par transformation de phrases déclaratives en interrogatives et se base sur une analyse syntaxique préalable de la phrase de base. Nous détaillons les différents types de questions générées. Nous présentons également une évaluation de l’outil, qui démontre que 41 % des questions générées par le système sont parfaitement bien formées.</resume>
        <mots_cles>génération de questions, analyse syntaxique, transformation syntaxique</mots_cles>
        <title/>
        <abstract>In this article, we present an automatic question generation system for French. The system proceeds by transforming declarative sentences into interrogative sentences, based on a preliminary syntactic analysis of the base sentence. We detail the different types of questions generated. We also present an evaluation of the tool, which shows that 41 % of the questions generated by the system are perfectly well-formed.</abstract>
        <keywords>question generation, syntactic analysis, syntactic transformation</keywords>
      </article>
      <article id="taln-2011-court-007" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Arnaud Grappy</nom>
            <email>Arnaud.Grappy@limsi.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Brigitte Grau</nom>
            <email>Brigitte.Grau@limsi.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Mathieu-Henri Falco</nom>
            <email>Mathieu-Henri.Falco@limsi.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Anne-Laure Ligozat</nom>
            <email>Anne-Laure.Ligozat@limsi.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Isabelle Robba</nom>
            <email>Isabelle.Robba@limsi.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>4</affiliationId>
          </auteur>
          <auteur>
            <nom>Anne Vilnat</nom>
            <email>Anne.Vilnat@limsi.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIMSI-CNRS</affiliation>
          <affiliation affiliationId="2">Université Paris 11</affiliation>
          <affiliation affiliationId="3">ENSIIE</affiliation>
          <affiliation affiliationId="3">UVSQ</affiliation>
        </affiliations>
        <titre>Sélection de réponses à des questions dans un corpus Web par validation</titre>
        <type>court</type>
        <pages/>
        <resume>Les systèmes de questions réponses recherchent la réponse à une question posée en langue naturelle dans un ensemble de documents. Les collectionsWeb diffèrent des articles de journaux de par leurs structures et leur style. Pour tenir compte de ces spécificités nous avons développé un système fondé sur une approche robuste de validation où des réponses candidates sont extraites à partir de courts passages textuels puis ordonnées par apprentissage. Les résultats montrent une amélioration du MRR (Mean Reciprocal Rank) de 48% par rapport à la baseline.</resume>
        <mots_cles>systèmes de questions réponses, validation de réponses, analyse de documents Web</mots_cles>
        <title/>
        <abstract>Question answering systems look for the answer of a question given in natural language in a large collection of documents. Web documents have a structure and a style different from those of newspaper articles. We developed a QA system based on an answer validation process able to handle Web specificity. Large number of candidate answers are extracted from short passages in order to be validated according to question and passage characteristics. The validation module is based on a machine learning approach.We show that our system outperforms a baseline by up to 48% in MRR (Mean Reciprocal Rank).</abstract>
        <keywords>question-answering system, answer validation, Web document analysis</keywords>
      </article>
      <article id="taln-2011-court-008" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Wei Wang</nom>
            <email>wei.wang@cea.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Romaric Besançon</nom>
            <email>romaric.besancon@cea.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Olivier Ferret</nom>
            <email>olivier.ferret@cea.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Brigitte Grau</nom>
            <email>brigitte.grau@limsi.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CEA, LIST, 18 route du Panorama, BP 6, 92265 Fontenay-aux-Roses</affiliation>
          <affiliation affiliationId="2">LIMSI, UPR-3251 CNRS-DR4, Bat. 508, BP 133, 91403 Orsay Cedex</affiliation>
        </affiliations>
        <titre>Filtrage de relations pour l’extraction d’information non supervisée</titre>
        <type>court</type>
        <pages/>
        <resume>Le domaine de l’extraction d’information s’est récemment développé en limitant les contraintes sur la définition des informations à extraire, ouvrant la voie à des applications de veille plus ouvertes. Dans ce contexte de l’extraction d’information non supervisée, nous nous intéressons à l’identification et la caractérisation de nouvelles relations entre des types d’entités fixés. Un des défis de cette tâche est de faire face à la masse importante de candidats pour ces relations lorsque l’on considère des corpus de grande taille. Nous présentons dans cet article une approche pour le filtrage des relations combinant méthode heuristique et méthode par apprentissage. Nous évaluons ce filtrage de manière intrinsèque et par son impact sur un regroupement sémantique des relations.</resume>
        <mots_cles>Extraction d’information non supervisée, filtrage, apprentissage automatique, clustering</mots_cles>
        <title/>
        <abstract>Information Extraction have recently been extended to new areas, by loosening the constraints on the strict definition of the information extracted, thus allowing to design more open information extraction systems. In this new domain of unsupervised information extraction, we focus on the task of extracting and characterizing new relations between a given set of entity types. One of the challenges of this task is to deal with the large amount of candidate relations when extracting them from a large corpus. We propose in this paper an approach for filtering such candidate relations, based on heuristic and machine learning methods. We present an evaluation of this filtering phase and an evaluation of the impact of the filtering on the semantic clustering of relations.</abstract>
        <keywords>Unsupervised information extraction, filtering, machine learning, clustering</keywords>
      </article>
      <article id="taln-2011-court-009" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Béatrice Arnulphy</nom>
            <email>Beatrice.Arnulphy@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Xavier Tannier</nom>
            <email>Xavier.Tannier@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Anne Vilnat</nom>
            <email>Anne.Vilnat@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Univ. Paris-Sud 11, 91405 Orsay</affiliation>
          <affiliation affiliationId="2">LIMSI-CNRS, 91403 Orsay</affiliation>
        </affiliations>
        <titre>Un lexique pondéré des noms d’événements en français</titre>
        <type>court</type>
        <pages/>
        <resume>Cet article décrit une étude sur l’annotation automatique des noms d’événements dans les textes en français. Plusieurs lexiques existants sont utilisés, ainsi que des règles syntaxiques d’extraction, et un lexique composé de façon automatique, permettant de fournir une valeur sur le niveau d’ambiguïté du mot en tant qu’événement. Cette nouvelle information permettrait d’aider à la désambiguïsation des noms d’événements en contexte.</resume>
        <mots_cles>extraction d’information, événements nominaux, lexiques</mots_cles>
        <title/>
        <abstract>This article describes a study on automatic extraction of event nominals in French texts. Some existing lexicons are used, as well as some syntactic extraction rules, and a new, automatically built lexicon is presented. This lexicon gives a value concerning the level of ambiguity of each word as an event.</abstract>
        <keywords>information extraction, nominal events, lexicons</keywords>
      </article>
      <article id="taln-2011-court-010" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Stéphane Huet</nom>
            <email>stephane.huet@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Fabrice Lefèvre</nom>
            <email>fabrice.lefevre@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université d’Avignon, LIA-CERI, France</affiliation>
        </affiliations>
        <titre>Alignement automatique pour la compréhension littérale de l’oral par approche segmentale</titre>
        <type>court</type>
        <pages/>
        <resume>Les approches statistiques les plus performantes actuellement pour la compréhension automatique du langage naturel nécessitent une annotation segmentale des données d’entraînement. Nous étudions dans cet article une alternative permettant d’obtenir de façon non-supervisée un alignement segmental d’unités conceptuelles sur les mots. L’impact de l’alignement automatique sur les performances du système de compréhension est évalué sur une tâche de dialogue oral.</resume>
        <mots_cles>Alignement non-supervisé, compréhension de la parole</mots_cles>
        <title/>
        <abstract>Most recent efficient statistical approaches for language understanding require a segmental annotation of the training data. In this paper we study an alternative that obtains a segmental alignment of conceptual units with words in an unsupervised way. The impact of the automatic alignment on the understanding system performance is evaluated on a spoken dialogue task.</abstract>
        <keywords>Unsupervised alignment, spoken language understanding</keywords>
      </article>
      <article id="taln-2011-court-011" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Romain Deveaud</nom>
            <email>romain.deveaud@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Eric Sanjuan</nom>
            <email>eric.sanjuan@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Patrice Bellot</nom>
            <email>patrice.bellot@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIA - Université d’Avignon, 339, chemin des Meinajariès Agroparc BP 91228, 84 911 Avignon Cedex 9</affiliation>
        </affiliations>
        <titre>Ajout d’informations contextuelles pour la recherche de passages au sein de Wikipédia</titre>
        <type>court</type>
        <pages/>
        <resume>La recherche de passages consiste à extraire uniquement des passages pertinents par rapport à une requête utilisateur plutôt qu’un ensemble de documents entiers. Cette récupération de passages est souvent handicapée par le manque d’informations complémentaires concernant le contexte de la recherche initiée par l’utilisateur. Des études montrent que l’ajout d’informations contextuelles par l’utilisateur peut améliorer les performances des systèmes de recherche de passages. Nous confirmons ces observations dans cet article, et nous introduisons également une méthode d’enrichissement de la requête à partir d’informations contextuelles issues de documents encyclopédiques. Nous menons des expérimentations en utilisant la collection et les méthodes d’évaluation proposées par la campagne INEX. Les résultats obtenus montrent que l’ajout d’informations contextuelles permet d’améliorer significativement les performances de notre système de recherche de passages. Nous observons également que notre approche automatique obtient les meilleurs résultats parmi les différentes approches que nous évaluons.</resume>
        <mots_cles>Recherche de passages, enrichissement de requêtes, contexte, Wikipedia, INEX, entropie</mots_cles>
        <title/>
        <abstract>Traditional Information Retrieval aims to present whole documents that are relevant to a user request. However, there is sometimes only one sentence that is relevant in the document. The purpose of Focused Information Retrieval is to find and extract relevant passages instead of entire documents. This retrieval task often lacks of complement concerning the context of the information need of the user. Studies show that the performance of focused retrieval systems are improved when user manually add contextual information. In this paper we confirm these observation, and we also introduce a query expansion approach using contextual information taken from encyclopedic documents. We use the INEX workshop collection and evaluation framework in our experiments. Results show that adding contextual information significantly improves the performance of our focused retrieval system. We also see that our automatic approach obtains the best results among the different approach we evaluate.</abstract>
        <keywords>Focused retrieval, query expansion, context, Wikipedia, INEX, entropy</keywords>
      </article>
      <article id="taln-2011-court-012" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Jana Strnadová</nom>
            <email>strnadjana13@gmail.com</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Benoît Sagot</nom>
            <email>benoit.sagot@inria.fr</email>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LLF, CNRS &amp; Univ. Paris 7, 5 rue Thomas Mann, 75205 Paris Cedex 13, France</affiliation>
          <affiliation affiliationId="2">Univerzita Karlova, Filozofická Fakulta, nám. J. Palacha 2, 116 38 Prague, Rép. Tchèque</affiliation>
          <affiliation affiliationId="3">Alpage, INRIA &amp; Univ. Paris 7, Rocquencourt, BP 105, 78153 Le Chesnay Cedex, France</affiliation>
        </affiliations>
        <titre>Construction d’un lexique des adjectifs dénominaux</titre>
        <type>court</type>
        <pages/>
        <resume>Après une brève analyse linguistique des adjectifs dénominaux en français, nous décrivons le processus automatique que nous avons mis en place à partir de lexiques et de corpus volumineux pour construire un lexique d’adjectifs dénominaux dérivés de manière régulière. Nous estimons à la fois la précision et la couverture du lexique dérivationnel obtenu. À terme, ce lexique librement disponible aura été validé manuellement et contiendra également les adjectifs dénominaux à base supplétive.</resume>
        <mots_cles>Adjectifs dénominaux, dérivation morphologique, lexique dérivationnel</mots_cles>
        <title/>
        <abstract>After a brief linguistic analysis of French denominal adjectives, we describe the automatic technique based on large-scale lexicons and corpora that we developed for building a lexicon of regular denominal adjectives. We evaluate both the precision and coverage of the resulting derivational lexicon. This freely available lexicon should eventually be fully manually validated and contain denominal adjectives with a suppletive base.</abstract>
        <keywords>Denominal adjectives, morphological derivation, derivational lexicon</keywords>
      </article>
      <article id="taln-2011-court-013" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Benoît Sagot</nom>
            <email>benoit.sagot@inria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Géraldine Walther</nom>
            <email>geraldine.walther@linguist.jussieu.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Pegah Faghiri</nom>
            <email>pegah.faghiri@etud.sorbonne-nouvelle.fr</email>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Pollet Samvelian</nom>
            <email>pollet.samvelian@univ-paris3.fr</email>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Alpage, INRIA &amp; Univ. Paris 7, Rocquencourt, BP 105, 78153 Le Chesnay Cedex, France</affiliation>
          <affiliation affiliationId="2">LLF, CNRS &amp; Univ. Paris 7, 5 rue Thomas Mann, 75205 Paris Cedex 13, France</affiliation>
          <affiliation affiliationId="3">MII, CNRS &amp; Univ. Paris 3, 27 rue Paul Bert, 94204 Ivry-sur-Seine, France</affiliation>
        </affiliations>
        <titre>Développement de ressources pour le persan : PerLex 2, nouveau lexique morphologique et MEltfa, étiqueteur morphosyntaxique</titre>
        <type>court</type>
        <pages/>
        <resume>Nous présentons une nouvelle version de PerLex, lexique morphologique du persan, une version corrigée et partiellement réannotée du corpus étiqueté BijanKhan (BijanKhan, 2004) et MEltfa, un nouvel étiqueteur morphosyntaxique librement disponible pour le persan. Après avoir développé une première version de PerLex (Sagot &amp; Walther, 2010), nous en proposons donc ici une version améliorée. Outre une validation manuelle partielle, PerLex 2 repose désormais sur un inventaire de catégories linguistiquement motivé. Nous avons également développé une nouvelle version du corpus BijanKhan : elle contient des corrections significatives de la tokenisation ainsi qu'un réétiquetage à l'aide des nouvelles catégories. Cette nouvelle version du corpus a enfin été utilisée pour l'entraînement de MEltfa, notre étiqueteur morphosyntaxique pour le persan librement disponible, s'appuyant à la fois sur ce nouvel inventaire de catégories, sur PerLex 2 et sur le système d'étiquetage MElt (Denis &amp; Sagot, 2009).</resume>
        <mots_cles>Ressource lexicale, validation, étiqueteur morphosyntaxique, persan, catégories, PerLex, MElt</mots_cles>
        <title/>
        <abstract>We present a new version of PerLex, the morphological lexicon for the Persian language, a corrected and partially re-annotated version of the BijanKhan corpus (BijanKhan, 2004) and MEltfa, a new freely available POS-tagger for the Persian language. After PerLex's first version (Sagot &amp; Walther, 2010), we propose an improved version of our morphological lexicon. Apart from a partial manual validation, PerLex 2 now relies on a set of linguistically motivated POS. Based on these POS, we also developped a new version of the BijanKhan corpus with significant corrections of the tokenisation. It has been re-tagged according to the new set of POS. The new version of the BijanKhan corpus has been used to develop MEltfa, our new freely-available POS-tagger for the Persian language, based on the new POS set, PerLex 2 and the MElt tagging system (Denis &amp; Sagot, 2009).</abstract>
        <keywords>Lexical resource, validation, tagger, Persian, POS, PerLex, MElt</keywords>
      </article>
      <article id="taln-2011-court-014" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Mirabela Navlea</nom>
            <email>navlea@unistra.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Amalia Todiraşcu</nom>
            <email>todiras@unistra.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université de Strasbourg, 22 rue René Descartes, BP, 80010, 67084 Strasbourg, cedex</affiliation>
        </affiliations>
        <titre>Identification de cognats à partir de corpus parallèles français-roumain</titre>
        <type>court</type>
        <pages/>
        <resume>Cet article présente une méthode hybride d’identification de cognats français - roumain. Cette méthode exploite des corpus parallèles alignés au niveau propositionnel, lemmatisés et étiquetés (avec des propriétés morphosyntaxiques). Notre méthode combine des techniques statistiques et des informations linguistiques pour améliorer les résultats obtenus. Nous évaluons le module d’identification de cognats et nous faisons une comparaison avec des méthodes statistiques pures, afin d’étudier l’impact des informations linguistiques utilisées sur la qualité des résultats obtenus. Nous montrons que l’utilisation des informations linguistiques augmente significativement la performance de la méthode.</resume>
        <mots_cles>cognat, identification de cognats, corpus parallèles alignés au niveau propositionnel</mots_cles>
        <title/>
        <abstract>This paper describes a hybrid French - Romanian cognate identification method. This method uses lemmatized, tagged (POS tags) and sentence-aligned parallel corpora. Our method combines statistical techniques and linguistic information in order to improve the results. We evaluate the cognate identification method and we compare it to other methods using pure statistical techniques to study the impact of the used linguistic information on the quality of the results. We show that the use of linguistic information in the cognate identification method significantly improves the results.</abstract>
        <keywords>cognate, cognate identification, sentence-aligned parallel corpora</keywords>
      </article>
      <article id="taln-2011-court-015" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Richard Beaufort</nom>
            <email>richard.beaufort@uclouvain.be</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Sophie Roekhaut</nom>
            <email>sophie.roekhaut@uclouvain.be</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CENTAL, UCLouvain, Place Blaise Pascal 1, B-1348 Louvain-la-Neuve</affiliation>
        </affiliations>
        <titre>Le TAL au service de l’ALAO/ELAO L’exemple des exercices de dictée automatisés</titre>
        <type>court</type>
        <pages/>
        <resume>Ce papier s’inscrit dans le cadre général de l’Apprentissage et de l’Enseignement des Langues Assistés par Ordinateur, et concerne plus particulièrement l’automatisation des exercices de dictée. Il présente une méthode de correction des copies d’apprenants qui se veut originale en deux points. Premièrement, la méthode exploite la composition d’automates à états finis pour détecter et pour analyser les erreurs. Deuxièmement, elle repose sur une analyse morphosyntaxique automatique de l’original de la dictée, ce qui facilite la production de diagnostics.</resume>
        <mots_cles>ALAO/ELAO, exercices de dictée, alignement, diagnostic, machines à états finis</mots_cles>
        <title/>
        <abstract>This paper comes within the scope of the Computer Assisted Language Learning framework, and addresses more especially the automation of dictation exercises. It presents a correction method of learners’ copies that is original in two ways. First, the method exploits the composition of finite-state automata, to both detect and analyze the errors. Second, it relies on an automatic morphosyntactic analysis of the original dictation, which makes it easier to produce diagnoses.</abstract>
        <keywords>CALL, dictation exercises, alignment, diagnosis, finite-state machines</keywords>
      </article>
      <article id="taln-2011-court-016" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Maxime Amblard</nom>
            <email>maxime.amblard@univ-nancy2.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>4</affiliationId>
          </auteur>
          <auteur>
            <nom>Michel Musiol</nom>
            <email>michel.musiol@univ-nancy2.fr</email>
            <affiliationId>2</affiliationId>
            <affiliationId>4</affiliationId>
          </auteur>
          <auteur>
            <nom>Manuel Rebuschi</nom>
            <email>manuel.rebuschi@univ-nancy2.fr</email>
            <affiliationId>3</affiliationId>
            <affiliationId>4</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LORIA - UMR 7503</affiliation>
          <affiliation affiliationId="2">InterPSY - EA 4432 / MSH Lorraine USR 3261</affiliation>
          <affiliation affiliationId="3">Archives Poincaré - UMR 7117 / MSH Lorraine USR 3261</affiliation>
          <affiliation affiliationId="4">Université Nancy 2 – 54000 Nancy</affiliation>
        </affiliations>
        <titre>Une analyse basée sur la S-DRT pour la modélisation de dialogues pathologiques</titre>
        <type>court</type>
        <pages/>
        <resume>Dans cet article, nous présentons la définition et l’étude d’un corpus de dialogues entre un schizophrène et un interlocuteur ayant pour objectif la conduite et le maintien de l’échange. Nous avons identifié des discontinuités significatives chez les schizophrènes paranoïdes. Une représentation issue de la S-DRT (sa partie pragmatique) permet de rendre compte des ces usages non standards.</resume>
        <mots_cles>S-DRT, interaction verbale, schizophrénie, dialogue pathologique, incohérence pragmatique</mots_cles>
        <title/>
        <abstract>In this article, we present a corpus of dialogues between a schizophrenic speaker and an interlocutor who drives the dialogue. We had identified specific discontinuities for paranoid schizophrenics. We propose a modeling of these discontinuities with S-DRT (its pragmatic part).</abstract>
        <keywords>S-DRT, verbal interaction, schizophrenia, pathological dialogue, pragmatical incoherence</keywords>
      </article>
      <article id="taln-2011-court-017" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Anne Göhring</nom>
            <email>Gohring@cl.uzh.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Martin Volk</nom>
            <email>Volk@cl.uzh.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">UZH, Institute of Computational Linguistics, University of Zurich, Switzerland</affiliation>
        </affiliations>
        <titre>The Text+Berg Corpus An Alpine French-German Parallel Resource</titre>
        <type>court</type>
        <pages/>
        <resume>Cet article présente un corpus parallèle français-allemand de plus de 4 millions de mots issu de la numérisation d’un corpus alpin multilingue. Ce corpus est une précieuse ressource pour de nombreuses études de linguistique comparée et du patrimoine culturel ainsi que pour le développement d’un système statistique de traduction automatique dans un domaine spécifique. Nous avons annoté un échantillon de ce corpus parallèle et aligné les structures arborées au niveau des mots, des constituants et des phrases. Cet “alpine treebank” est le premier corpus arboré parallèle français-allemand de haute qualité (manuellement contrôlé), de libre accès et dans un domaine et un genre nouveau : le récit d’alpinisme.</resume>
        <mots_cles>corpus alpin français-allemand, structures arborées parallèles, annotation morphosyntaxique du français</mots_cles>
        <title/>
        <abstract>This article presents a French-German parallel corpus of more than 4 million tokens which we have compiled as part of the digitization of a large multilingual heritage corpus of alpine texts. This corpus is a valuable resource for cultural heritage and cross-linguistic studies as well as for the development of domain-specific machine translation systems. We have turned a small fraction of the parallel corpus into a high-quality parallel treebank with manually checked syntactic annotations and cross-language word and phrase alignments. This alpine treebank is the first freely available French-German parallel treebank. It complements other treebanks with texts in a new domain and genre : mountaineering reports.</abstract>
        <keywords>French-German alpine corpus, parallel treebank, French morphosyntactic annotation, Text+Berg, e-Humanities</keywords>
      </article>
      <article id="taln-2011-court-018" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Aurélien Bossard</nom>
            <email>Aurelien.Bossard@orange-ftgroup.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Émilie Guimier De Neef</nom>
            <email>emilie;GuimierDeNeef@orange-ftgroup.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Orange Labs, 2 av. Pierre Marzin, 22300 Lannion, France</affiliation>
        </affiliations>
        <titre>Ordonner un résumé automatique multi-documents fondé sur une classification des phrases en classes lexicales</titre>
        <type>court</type>
        <pages/>
        <resume>Nous présentons différentes méthodes de réordonnancement de phrases pour le résumé automatique fondé sur une classification des phrases à résumer en classes thématiques. Nous comparons ces méthodes à deux baselines : ordonnancement des phrases selon leur pertinence et ordonnancement selon la date et la position dans le document d’origine. Nous avons fait évaluer les résumés obtenus sur le corpus RPM2 par 4 annotateurs et présentons les résultats.</resume>
        <mots_cles>Résumé automatique, ordonnancement de phrases</mots_cles>
        <title/>
        <abstract>We present several sentence ordering methods for automatic summarization which are specific to multi-document summarizers, based on sentences subtopic clustering. These methods are compared to two baselines : sentence ordering according to pertinence and according to publication date and inner document position. The resulting summaries on RPM2 corpus have been evaluated by four judges.</abstract>
        <keywords>Automatic summarization, sentence ordering</keywords>
      </article>
      <article id="taln-2011-court-019" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Fériel Ben Fraj</nom>
            <email>Feriel.BenFraj@riadi.rnu.tn</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire RIADI, École Nationale des Sciences de l'Informatique, 2010 Manouba, Tunisie</affiliation>
        </affiliations>
        <titre>Construction d’une grammaire d’arbres adjoints pour la langue arabe</titre>
        <type>court</type>
        <pages/>
        <resume>La langue arabe présente des spécificités qui la rendent plus ambigüe que d’autres langues naturelles. Sa morphologie, sa syntaxe ainsi que sa sémantique sont en corrélation et se complètent l’une l’autre. Dans le but de construire une grammaire qui soit adaptée à ces spécificités, nous avons conçu et développé une application d’aide à la création des règles syntaxiques licites suivant le formalisme d’arbres adjoints. Cette application est modulaire et enrichie par des astuces de contrôle de la création et aussi d’une interface conviviale pour assister l’utilisateur final dans la gestion des créations prévues.</resume>
        <mots_cles>Outil semi-automatique, grammaire d’arbres adjoints, langue arabe, traits d’unification</mots_cles>
        <title/>
        <abstract>The Arabic language consists of a set of specificities. Thus, it is more ambiguous than other natural languages. Its morphology, syntax and semantic are correlated to each other. We have constructed an application for the construction of a tree adjoining grammar which respects the characteristics of the Arabic language. This tool allows constructing the grammatical rules as elementary trees enriched by different feature structures. It helps the user by its interface and control system to manage correct and uniform rules.</abstract>
        <keywords>semi-automatic tool, tree adjoining grammar, Arabic language, feature structures</keywords>
      </article>
      <article id="taln-2011-court-020" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Enrique Henestroza Anguiano</nom>
            <email>henestro@inria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Pascal Denis</nom>
            <email>pascal.denis@inria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Alpage, INRIA Paris-Rocquencourt &amp; Université Paris 7, Rocquencourt, BP 105, 78153 Le Chesnay Cedex, France</affiliation>
        </affiliations>
        <titre>FreDist : Automatic construction of distributional thesauri for French</titre>
        <type>court</type>
        <pages/>
        <resume>Dans cet article, nous présentons FreDist, un logiciel libre pour la construction automatique de thésaurus distributionnels à partir de corpus de texte, ainsi qu’une évaluation des différents ressources ainsi produites. Suivant les travaux de (Lin, 1998) et (Curran, 2004), nous utilisons un corpus journalistique de grande taille et implémentons différentes options pour : le type de relation contexte lexical, la fonction de poids, et la fonction de mesure de similarité. Prenant l’EuroWordNet français et le WOLF comme références, notre évaluation révèle, de manière originale, que c’est l’approche qui combine contextes linéaires (ici, de type bigrammes) et contextes syntaxiques qui semble fournir le meilleur thésaurus. Enfin, nous espérons que notre logiciel, distribué avec nos meilleurs thésaurus pour le français, seront utiles à la communauté TAL.</resume>
        <mots_cles>thésaurus distributionnel, similarité sémantique, méthodes non supervisées, lexique</mots_cles>
        <title/>
        <abstract>In this article we present FreDist, a freely available software package for the automatic construction of distributional thesauri from text corpora, as well as an evaluation of various distributional similarity metrics for French. Following from the work of (Lin, 1998) and (Curran, 2004), we use a large corpus of journalistic text and implement different choices for the type of lexical context relation, the weight function, and the measure function needed to build a distributional thesaurus. Using the EuroWordNet and WOLF wordnet resources for French as gold-standard references for our evaluation, we obtain the novel result that combining bigram and syntactic dependency context relations results in higher quality distributional thesauri. In addition, we hope that our software package and a joint release of our best thesauri for French will be useful to the NLP community.</abstract>
        <keywords>distributional thesaurus, semantic similarity, unsupervised methods, lexicon</keywords>
      </article>
      <article id="taln-2011-court-021" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Ali Reza Ebadat</nom>
            <email>ali_reza.ebadat@inria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Vincent Claveau</nom>
            <email>vincent.claveau@irisa.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Pascale Sébillot</nom>
            <email>pascale.sebillot@irisa.fr</email>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">INRIA-INSA</affiliation>
          <affiliation affiliationId="2">IRISA-CNRS</affiliation>
          <affiliation affiliationId="3">IRISA-INSA</affiliation>
        </affiliations>
        <titre>Using shallow linguistic features for relation extraction in bio-medical texts</titre>
        <type>court</type>
        <pages/>
        <resume>Dans cet article, nous proposons de modéliser la tâche d’extraction de relations à partir de corpus textuels comme un problème de classification. Nous montrons que, dans ce cadre, des représentations fondées sur des informations linguistiques de surface sont suffisantes pour que des algorithmes d’apprentissage artificiel standards les exploitant rivalisent avec les meilleurs systèmes d’extraction de relations reposant sur des connaissances issues d’analyses profondes (analyses syntaxiques ou sémantiques). Nous montrons également qu’en prenant davantage en compte les spécificités de la tâche d’extraction à réaliser et des données disponibles, il est possible d’obtenir des méthodes encore plus efficaces tout en exploitant ces informations simples. La technique originale à base d’apprentissage « paresseux » et de modèles de langue que nous évaluons en extraction d’interactions géniques sur les données du challenge LLL2005 dépasse les résultats de l’état de l’art.</resume>
        <mots_cles>Extraction de relations, classification, apprentissage paresseux, modèle de langue, analyse linguistique de surface</mots_cles>
        <title/>
        <abstract>In this paper, we model the corpus-based relation extraction task as a classification problem. We show that, in this framework, standard machine learning systems exploiting representations simply based on shallow linguistic information can rival state-of-the-art systems that rely on deep linguistic analysis. Even more effective systems can be obtained, still using these easy and reliable pieces of information, if the specifics of the extraction task and the data are taken into account. Our original method combining lazy learning and language modeling out-performs the existing systems when evaluated on the LLL2005 protein-protein interaction extraction task data.</abstract>
        <keywords>Relation extraction, classification, lazy learning, langage model, shallow linguistic analysis</keywords>
      </article>
      <article id="taln-2011-court-022" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Julien Lebranchu</nom>
            <email>Julien.Lebranchu@unicaen.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Yann Mathet</nom>
            <email>Yann.Mathet@unicaen.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université de Caen Basse-Normandie, UMR 6072 GREYC, F-14032 Caen, France</affiliation>
        </affiliations>
        <titre>Vers une prise en charge approfondie des phénomènes itératifs par TimeML</titre>
        <type>court</type>
        <pages/>
        <resume>Les travaux menés ces dernières années autour de l’itération en langue, tant par la communauté linguistique que par celle du TAL, ont mis au jour des phénomènes particuliers, non réductibles aux représentations temporelles classiques. En particulier, une itération ne saurait structurellement être réduite à une simple énumération de procès, et du point de vue de l’aspect, met en jeu simultanément deux visées aspectuelles indépendantes. Le formalisme TimeML, qui a vocation à annoter les informations temporelles portées par un texte, intègre déjà des éléments relatifs aux itérations, mais ne prend pas en compte ces dernières avancées. C’est ce que nous entreprenons de faire dans cet article, en proposant une extension à ce formalisme.</resume>
        <mots_cles>TimeML, discours, sémantique, phénomènes itératifs</mots_cles>
        <title/>
        <abstract>The work that has recently been done concerning the iterative phenomena in language, which was performed by the linguistic and TAL communities, has illuminated specific phenomena, not reducible to classical time representations. In particular, an iteration can not structurally be reduced to a simple listing of process, and involves simultaneously two independent referred aspectual. The TimeML formalism, which aims to annotate temporal information of a given text, includes already relative elements to iterations but does not take into account recent advances. That is the reason why in this paper, we propose to extend this formalism.</abstract>
        <keywords>TimeML, discourse, semantics, iterative phenomena</keywords>
      </article>
      <article id="taln-2011-court-023" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Noémi Boubel</nom>
            <email>noemi.boubel@uclouvain.be</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Yves Bestgen</nom>
            <email>yves.bestgen@uclouvain.be</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">UCLouvain, Cental, Place Blaise Pascal, 1, B-1348 Louvain-la-Neuve, Belgique</affiliation>
          <affiliation affiliationId="2">UCLouvain, CECL, B-1348 Louvain-la-Neuve, Belgique</affiliation>
        </affiliations>
        <titre>Une procédure pour identifier les modifieurs de la valence affective d'un mot dans des textes</titre>
        <type>court</type>
        <pages/>
        <resume>Cette recherche s'inscrit dans le champ de la fouille d'opinion et, plus particulièrement, dans celui de l'analyse de la polarité d’une phrase ou d'un syntagme. Dans ce cadre, la prise en compte du contexte linguistique dans lequel apparaissent les mots porteurs de valence est particulièrement importante. Nous proposons une méthodologie pour extraire automatiquement de corpus de textes de telles expressions linguistiques. Cette approche s'appuie sur un corpus de textes, ou d'extraits de textes, dont la valence est connue, sur un lexique de valence construit à partir de ce corpus au moyen d'une procédure automatique et sur un analyseur syntaxique. Une étude exploratoire, limitée à la seule relation syntaxique associant un adverbe à un adjectif, laisse entrevoir les potentialités de l'approche.</resume>
        <mots_cles>modifieurs de valence, fouille d’opinion, lexique de valence</mots_cles>
        <title/>
        <abstract>This research is situated within the field of opinion mining and focuses more particularly on the analysis of the opinion expressed in a sentence or a syntagm. Within this frame of research, taking into account the linguistic context in which words which carry valence appear is particularly important. We propose a methodology to automatically extract such linguistic expressions from text corpora. This approach is based on (a) a corpus of texts, or text excerpts, the valence of which is known, (b) on a valence lexicon built from this corpus using an automatic procedure and (c) on a parser. An exploratory study, focusing on the syntactic relation associating an adverb to an adjective, shows the potential of the approach.</abstract>
        <keywords>contextual valence shifter, opinion mining, semantic orientation lexicon</keywords>
      </article>
      <article id="taln-2011-court-024" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Yann Mathet</nom>
            <email>Yann.Mathet@unicaen.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Antoine Widlöcher</nom>
            <email>Antoine.Widlocher@unicaen.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">GREYC, UMR CNRS 6072, Université de Caen, 14032 Caen Cedex</affiliation>
        </affiliations>
        <titre>Stratégie d’exploration de corpus multi-annotés avec GlozzQL</titre>
        <type>court</type>
        <pages/>
        <resume>La multiplication des travaux sur corpus, en linguistique computationnelle et en TAL, conduit à la multiplication des campagnes d’annotation et des corpus multi-annotés, porteurs d’informations relatives à des phénomènes variés, envisagés par des annotateurs multiples, parfois automatiques. Pour mieux comprendre les phénomènes que ces campagnes prennent pour objets, ou pour contrôler les données en vue de l’établissement d’un corpus de référence, il est nécessaire de disposer d’outils permettant d’explorer les annotations. Nous présentons une stratégie possible et son opérationalisation dans la plate-forme Glozz par le langage GlozzQL.</resume>
        <mots_cles>Corpus, Annotation, Exploration, GlozzQL</mots_cles>
        <title/>
        <abstract>More and more works in compuational linguistics and NLP rely on corpora. They lead to an increasing number of annotation campaigns and multi-annotated corpora, providing informations on various linguistic phenomena, annotated by several annotators or computational processes. In order to understand these linguistic phenomena, or to control annotated data, tools dedicated to annotated data mining are needed. We present here an exploration strategy and its implementation within the Glozz platform, GlozzQL.</abstract>
        <keywords>Corpus, Annotation, Exploration, GlozzQL</keywords>
      </article>
      <article id="taln-2011-court-025" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Fadila Hadouche</nom>
            <email>hadouchf@iro.umontreal.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Guy Lapalme</nom>
            <email>lapalme@iro.umontreal.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Marie-Claude L’Homme</nom>
            <email>mc.lhomme@umontreal.ca</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">RALI, Université de Montréal, C.P 6128 Succursale Centre-ville, Montréal, Québec, Canada H3C 3J7</affiliation>
          <affiliation affiliationId="2">OLST, Université de Montréal, C.P 6128 Succursale Centre-ville, Montréal, Québec, Canada H3C 3J7</affiliation>
        </affiliations>
        <titre>Attribution de rôles sémantiques aux actants des lexies verbales</titre>
        <type>court</type>
        <pages/>
        <resume>Dans cet article, nous traitons de l’attribution des rôles sémantiques aux actants de lexies verbales en corpus spécialisé en français. Nous proposons une classification de rôles sémantiques par apprentissage machine basée sur un corpus de lexies verbales annotées manuellement du domaine de l’informatique et d’Internet. Nous proposons également une méthode de partitionnement semi-supervisé pour prendre en compte l’annotation de nouvelles lexies ou de nouveaux rôles sémantiques et de les intégrés dans le système. Cette méthode de partitionnement permet de regrouper les instances d’actants selon les valeurs communes correspondantes aux traits de description des actants dans des groupes d’instances d’actants similaires. La classification de rôles sémantique a obtenu une F-mesure de 93% pour Patient, de 90% pour Agent, de 85% pour Destination et de 76% pour les autres rôles pris ensemble. Quand au partitionnement en regroupant les instances selon leur similarité donne une F-mesure de 88% pour Patient, de 81% pour Agent, de 58% pour Destination et de 46% pour les autres rôles.</resume>
        <mots_cles>Rôles sémantiques, traits syntaxiques, classification, partitionnement semi-supervisé</mots_cles>
        <title/>
        <abstract>In this paper, we discuss assigning semantic roles to actants of verbal lexical units in French specialized corpus. We propose a machine learning classification of semantic roles based on a corpus of verbal lexical units, which are annotated manually in the Informatics and Internet domain. We also propose a semi supervised clustering method to consider the annotation of new verbal lexical units or new semantic roles and integrated them in the system. Clustering is used to group instances of actants according to their common values corresponding to the features describing these actants into groups of similar instances of actants. The classification model give an F-measure of 93% for Patient, 90% for Agent, 85% for Destination and 76% for other roles. When partitioning by grouping instances according to their similarity gives an F-measure of 88% for Patient, 81% for Agent, 58% for Destination and 46% for other roles.</abstract>
        <keywords>Semantic roles, syntactic features, classification, semi supervised partitioning</keywords>
      </article>
      <article id="taln-2011-court-026" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Olivier Ferret</nom>
            <email>olivier.ferret@cea.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CEA, LIST, Laboratoire Vision et Ingénierie des Contenus, Fontenay-aux-Roses, F-92265 France</affiliation>
        </affiliations>
        <auteurs/>
        <titre>Utiliser l’amorçage pour améliorer une mesure de similarité sémantique</titre>
        <type>court</type>
        <pages/>
        <resume>Les travaux sur les mesures de similarité sémantique de nature distributionnelle ont abouti à un certain consensus quant à leurs performances et ont montré notamment que leurs résultats sont surtout intéressants pour des mots de forte fréquence et une similarité sémantique étendue, non restreinte aux seuls synonymes. Dans cet article, nous proposons une méthode d’amélioration d’une mesure de similarité classique permettant de rééquilibrer ses résultats pour les mots de plus faible fréquence. Cette méthode est fondée sur un mécanisme d’amorçage : un ensemble d’exemples et de contre-exemples de mots sémantiquement liés sont sélectionnés de façon non supervisée à partir des résultats de la mesure initiale et servent à l’entraînement d’un classifieur supervisé. Celui-ci est ensuite utilisé pour réordonner les voisins sémantiques initiaux. Nous évaluons l’intérêt de ce réordonnancement pour un large ensemble de noms anglais couvrant différents domaines fréquentiels.</resume>
        <mots_cles>Extraction de voisins sémantiques, similarité sémantique, méthodes distributionnelles</mots_cles>
        <title/>
        <abstract>Work about distributional semantic similarity measures has now widely shown that such measures are mainly reliable for high frequency words and for capturing semantic relatedness rather than strict semantic similarity. In this article, we propose a method for improving such a measure for middle and low frequency words. This method is based on a bootstrapping mechanism : a set of examples and counter-examples of semantically related words are selected in an unsupervised way from the results of the initial measure and used for training a supervised classifier. This classifier is then applied for reranking the initial semantic neighbors. We evaluate the interest of this reranking for a large set of english nouns with various frequencies.</abstract>
        <keywords>Semantic neighbor extraction, semantic similarity, distributional methods</keywords>
      </article>
      <article id="taln-2011-court-027" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Richard Moot</nom>
            <email>richard.moot@labri.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Laurent Prévot</nom>
            <email>laurent.prevot@lpl-aix.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Christian Retoré</nom>
            <email>christian.retore@labri.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université de Bordeaux, LaBRI &amp; INRIA</affiliation>
          <affiliation affiliationId="2">Université de Provence, LPL</affiliation>
        </affiliations>
        <titre>Un calcul de termes typés pour la pragmatique lexicale: chemins et voyageurs fictifs dans un corpus de récits de voyage</titre>
        <type>court</type>
        <pages/>
        <resume>Ce travail s’inscrit dans l’analyse automatique d’un corpus de récits de voyage. À cette fin, nous raffinons la sémantique de Montague pour rendre compte des phénomènes d’adaptation du sens des mots au contexte dans lequel ils apparaissent. Ici, nous modélisons les constructions de type ’le chemin descend pendant une demi-heure’ où ledit chemin introduit un voyageur fictif qui le parcourt, en étendant des idées que le dernier auteur a développé avec Bassac et Mery. Cette introduction du voyageur utilise la montée de type afin que le quantificateur introduisant le voyageur porte sur toute la phrase et que les propriétés du chemin ne deviennent pas des propriétés du voyageur, fût-il fictif. Cette analyse sémantique (ou plutôt sa traduction en lambda-DRT) est d’ores et déjà implantée pour une partie du lexique de Grail.</resume>
        <mots_cles>Sémantique lexicale, pragmatique, sémantique compositionnelle</mots_cles>
        <title/>
        <abstract>This work is part of the automated analysis of travel stories corpus. To do so, we refine Montague semantics, to model the adaptation of word meaning to the context in which they appear. Here we study construction like ’the path goes down for half an hour’ in which the path introduces a virtual traveller following it, extending ideas of the last author with Bassac, Mery. The introduction of a traveller relies on type raising satisfies the following requirements : the quantification binding the traveller has the widest scope, and properties of the path do not apply to the traveller, be it virtual. This semantical analysis (actually its translation in lambda-DRT) is already implemented for a part of the Grail lexicon.</abstract>
        <keywords>Lexical semantics, pragmatics, compositional semantics</keywords>
      </article>
      <article id="taln-2011-court-028" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Brigitte Bigi</nom>
            <email>brigitte.bigi@lpl-aix.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Cristel Portes</nom>
            <email>cristel.portes@lpl-aix.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Agnès Steuckardt</nom>
            <email>Agnes.Steuckardt@univ-provence.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Marion Tellier</nom>
            <email>marion.tellier@lpl-aix.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire Parole &amp; Langage, CNRS &amp; Aix-Marseille Universités, 5, avenue Pasteur, BP 80975, 13604 Aix en Provence, France</affiliation>
        </affiliations>
        <titre>Catégoriser les réponses aux interruptions dans les débats politiques</titre>
        <type>court</type>
        <pages/>
        <resume>Cet article traite de l’analyse de débats politiques selon une orientation multimodale. Nous étudions plus particulièrement les réponses aux interruptions lors d’un débat à l’Assemblée nationale. Nous proposons de procéder à l’analyse via des annotations systématiques de différentes modalités. L’analyse argumentative nous a amenée à proposer une typologie de ces réponses. Celle-ci a été mise à l’épreuve d’une classification automatique. La difficulté dans la construction d’un tel système réside dans la nature même des données : multimodales, parfois manquantes et incertaines.</resume>
        <mots_cles>corpus, annotations, multimodalité, classification supervisée</mots_cles>
        <title/>
        <abstract>This work was conducted to analyze political debates, with a multimodal point of view. Particularly, we focus on the answers produced by a main speakers after he was disrupted. Our approach relies on the annotations of each modality and on their review. We propose a manual categorization of the observed disruptions. A categorization method was applied to validate the manual one. The difficulty is to deal with multimodality, missing values and uncertainty in the automatic classification system.</abstract>
        <keywords>corpus, annotations, multimodality, classification</keywords>
      </article>
      <article id="taln-2011-court-029" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Ludovic Bonnefoy</nom>
            <email>ludovic.bonnefoy@ismart.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Patrice Bellot</nom>
            <email>patrice.bellot@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Michel Benoit</nom>
            <email>michel.benoit@ismart.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université d’Avignon - CERI/LIA, Agroparc – B.P. 1228, 84911 Avignon Cedex 9</affiliation>
          <affiliation affiliationId="2">iSmart, Le Mercure A, 13851 Aix-en-Provence Cedex 3</affiliation>
        </affiliations>
        <titre>Mesure non-supervisée du degré d’appartenance d’une entité à un type</titre>
        <type>court</type>
        <pages/>
        <resume>La recherche d’entités nommées a été le sujet de nombreux travaux. Cependant, la construction des ressources nécessaires à de tels systèmes reste un problème majeur. Dans ce papier, nous proposons une méthode complémentaire aux outils capables de reconnaître des entités de types larges, dont l’objectif est de déterminer si une entité est d’un type donné, et ce de manière non-supervisée et quel que soit le type. Nous proposons pour cela une approche basée sur la comparaison de modèles de langage estimés à partir du Web. L’intérêt de notre approche est validé par une évaluation sur 100 entités et 273 types différents.</resume>
        <mots_cles>typage d’entités nommées, comparaison de distribution de mots, divergence de Kullback-Leibler</mots_cles>
        <title/>
        <abstract>Searching for named entities has been the subject of many researches. In this paper, we seek to determine whether a named entity is of a given type and in what extent it is. We propose to address this issue by an unsupervised Web oriented language modeling approach. The interest of it is demonstrated by our evaluation on 100 entities and 273 different types.</abstract>
        <keywords>named entity identification, language modeling approach, Kullback-Leibler divergence</keywords>
      </article>
      <article id="taln-2011-court-030" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Laurence Danlos</nom>
            <email>Laurence.Danlos@linguist.jussieu.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Charlotte Roze</nom>
            <email>Charlotte.Roze@linguist.jussieu.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">ALPAGE, Université Paris Diderot (Paris 7), 175 rue du Chevaleret, F-750013 Paris</affiliation>
        </affiliations>
        <titre>Traduction (automatique) des connecteurs de discours</titre>
        <type>court</type>
        <pages/>
        <resume>En nous appuyant sur des données fournies par le concordancier bilingue TransSearch qui intègre un alignement statistique au niveau des mots, nous avons effectué une annotation semi-manuelle de la traduction anglaise de deux connecteurs du français. Les résultats de cette annotation montrent que les traductions de ces connecteurs ne correspondent pas aux « transpots » identifiés par TransSearch et encore moins à ce qui est proposé dans les dictionnaires bilingues.</resume>
        <mots_cles>Traduction (automatique), TransSearch, Discours</mots_cles>
        <title/>
        <abstract>On the basis of data provided by the bilingual concordancer TransSearch which propose a statistical word alignment, we made a semi-manual annotation of the English translation of two French connectives. The results of this annotation show that the translations of these connectives do not correspond to the “transpots” identified by TransSearch and even less to the translations proposed in bilingual dictionaries.</abstract>
        <keywords>(Machine) Translation, TransSearch, Discourse</keywords>
      </article>
      <article id="taln-2011-court-031" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Bruno Cartoni</nom>
            <email>bruno.cartoni@unige.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Louise Deléger</nom>
            <email>louise.deleger@cchmc.org</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Département de Linguistique, Université de Genève</affiliation>
          <affiliation affiliationId="2">Division of Biomedical Informatics, Cincinnati Children’s Hospital Medical Center</affiliation>
        </affiliations>
        <titre>Découverte de patrons paraphrastiques en corpus comparable: une approche basée sur les n-grammes</titre>
        <type>court</type>
        <pages/>
        <resume>Cet article présente l’utilisation d’un corpus comparable pour l’extraction de patrons de paraphrases. Nous présentons une méthode empirique basée sur l’appariement de n-grammes, permettant d’extraire des patrons de paraphrases dans des corpus comparables d’une même langue (le français), du même domaine (la médecine) mais de registres de langues différents (spécialisé ou grand public). Cette méthode confirme les résultats précédents basés sur des méthodes à base de patrons, et permet d’identifier de nouveaux patrons, apportant également un regard nouveau sur les différences entre les discours de langue générale et spécialisée.</resume>
        <mots_cles>Identification de paraphrases, extraction de patrons, type de discours, domaine médical, corpus comparable monolingue</mots_cles>
        <title/>
        <abstract>This paper presents the use of a comparable corpus for extracting paraphrase patterns.We present an empirical method based on n-gram matching and ordering, to extract paraphrase pattern in comparable corpora of the same language (French) and the same domaine, but of two different registers (lay and specialised). This method confirms previous results from pattern-based methods, and identify new patterns, giving fresh look on the difference between specialised and lay discourse.</abstract>
        <keywords>paraphrase identification, lexico-syntactic pattern discovery, discourse type, medical domain, monolingual comparable corpora</keywords>
      </article>
      <article id="taln-2011-court-032" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Alexis Kauffmann</nom>
            <email>alexis.kauffmann@unige.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LATL, Université de Genève, 2, Rue de Candolle, 1211 Genève, Suisse</affiliation>
        </affiliations>
        <titre>Prise en compte de la sous-catégorisation verbale dans un lexique bilingue anglais-japonais</titre>
        <type>court</type>
        <pages/>
        <resume>Dans cet article, nous présentons une méthode de détection des correspondances bilingues de sous-catégorisation verbale à partir de données lexicales monolingues. Nous évoquons également la structure de ces lexiques et leur utilisation en traduction automatique (TA) à base linguistique anglais-japonais. Les lexiques sont utilisés par un programme de TA fonctionnant selon une architecture classique dite "à transfert", et leur structure permet une classification précise des sous-catégorisations verbales. Nos travaux ont permis une amélioration des données de sous-catégorisation des lexiques pour les verbes japonais et leurs équivalents anglais, en utilisant des données linguistiques compilées à partir d'un corpus de textes extrait du web. De plus, le fonctionnement du programme de TA a pu ^etre amélioré en utilisant ces données.</resume>
        <mots_cles>bases de données lexicales, sous-catégorisation verbale, traduction automatique à base linguistique, japonais</mots_cles>
        <title/>
        <abstract>In this paper, we present a method for the detection of bilingual correspondences of verb subcategorization from monolingual lexical data. We also mention the structure of the lexicons and examples making use of such data in linguistics-based English-Japanese machine translation (MT). The lexicons are used by a MT system with a classical transfer-based architecture, and their structure allow an accurate classification of verb subcategorization. Our work has improved the lexical data about subcategorization of Japanese verbs and their English equivalents, using linguistic data compiled from a corpus of web extracted texts. Furthermore, the MT system could also be improved by the use of this data.</abstract>
        <keywords>lexical databases, verb subcategorisation, linguistics-based machine translation, Japanese</keywords>
      </article>
      <article id="taln-2011-court-033" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Yayoi Nakamura-Delloye</nom>
            <email>yayoi@yayoi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">ALPAGE, INRIA-Rocquencourt, Domaine de Voluceau Rocquencourt B.P.105 78153 Le Chesnay</affiliation>
        </affiliations>
        <titre>Extraction non-supervisée de relations basée sur la dualité de la représentation</titre>
        <type>court</type>
        <pages/>
        <resume>Nous proposons dans cet article une méthode non-supervisée d’extraction des relations entre entités nommées. La méthode proposée se caractérise par l’utilisation de résultats d’analyses syntaxiques, notamment les chemins syntaxiques reliant deux entités nommées dans des arbres de dépendance. Nous avons également exploité la dualité de la représentation des relations sémantiques et le résultat de notre expérience comparative a montré que cette approche améliorait les rappels.</resume>
        <mots_cles>Extraction des connaissances, relations entre entités nommées, dualité relationnelle</mots_cles>
        <title/>
        <abstract>We propose in this paper an unsupervised method for relation and pattern extraction. The proposed method is characterized by using parsed corpora, especially by leveraging syntactic paths that connect two named entities in dependency trees. We also use the dual representation of semantic relations and the result of our comparative experiment showed that this approach improves recall.</abstract>
        <keywords>Knowledge extraction, named entity relationships, relational duality</keywords>
      </article>
      <article id="taln-2011-court-034" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Corinna Anderson</nom>
            <email>andersoc@loria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Christophe Cerisara</nom>
            <email>cerisara@loria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Claire Gardent</nom>
            <email>gardent@loria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LORIA-CNRS UMR 7503, Campus Scientifique, Vandoeuvre-les-Nancy</affiliation>
        </affiliations>
        <titre>Vers la détection des dislocations à gauche dans les transcriptions automatiques du Français parlé</titre>
        <type>court</type>
        <pages/>
        <resume>Ce travail prend place dans le cadre plus général du développement d’une plate-forme d’analyse syntaxique du français parlé. Nous décrivons la conception d’un modèle automatique pour résoudre le lien anaphorique présent dans les dislocations à gauche dans un corpus de français parlé radiophonique. La détection de ces structures devrait permettre à terme d’améliorer notre analyseur syntaxique en enrichissant les informations prises en compte dans nos modèles automatiques. La résolution du lien anaphorique est réalisée en deux étapes : un premier niveau à base de règles filtre les configurations candidates, et un second niveau s’appuie sur un modèle appris selon le critère du maximum d’entropie. Une évaluation expérimentale réalisée par validation croisée sur un corpus annoté manuellement donne une F-mesure de l’ordre de 40%.</resume>
        <mots_cles>Détection des dislocations à gauche, Maximum Entropy, français parlé</mots_cles>
        <title>Towards automatic recognition of left dislocation in transcriptions of Spoken French</title>
        <abstract>Left dislocations are an important distinguishing feature of spoken French. In this paper, we present a hybrid approach for detecting the coreferential link that holds between left-dislocated elements and the coreferential pronoun occurring further on in the sentence. The approach combines a symbolic graph rewrite step with a maximum entropy classifier and achieves around 40% F-score. We conjecture that developing such approaches could contribute to the general anaphora resolution task and help improve parsers trained on corpora enriched with left dislocation anaphoric links.</abstract>
        <keywords>Left dislocation detection, Maximum Entropy, spoken French</keywords>
      </article>
      <article id="taln-2011-court-035" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Nabil Hathout</nom>
            <email>nabil.hathout@univ-tlse2.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Fiammetta Namer</nom>
            <email>fiammetta.namer@univ-nancy2.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">UMR CLLE-ERSS, Toulouse</affiliation>
          <affiliation affiliationId="2">UMR ATILF-Université Nancy2, Nancy</affiliation>
        </affiliations>
        <titre>Règles et paradigmes en morphologie informatique lexématique</titre>
        <type>court</type>
        <pages/>
        <resume>Les familles de mots produites par deux analyseurs morphologiques, DériF (basé sur des règles) et Morphonette (basé sur l'analogie), appliqués à un même corpus lexical, sont comparées. Cette comparaison conduit à l'examen de trois sous-ensembles : 
			- un sous-ensemble commun aux deux systèmes dont la taille montre que, malgré leurs différences, les approches expérimentées par chaque système sont valides et décrivent en partie la même réalité morphologique.
			- un sous-ensemble propre à DériF et un autre à Morphonette. Ces ensembles (a) nous renseignent sur les caractéristiques propres à chaque système, et notamment sur ce que l'autre ne peut pas produire, (b) ils mettent en évidence les erreurs d’un système, en ce qu’elles n’apparaissent pas dans l’autre, (c) ils font apparaître certaines limites de la description, notamment celles qui sont liées aux objets et aux notions théoriques comme les familles morphologiques, les bases, l'existence de RCL « transversales » entre les lexèmes qui n'ont pas de relation d'ascendance ou de descendance.</resume>
        <mots_cles>morphologie constructionnelle, analyse automatique, règles, analogie, familles morphologiques, comparaison, synergie</mots_cles>
        <title/>
        <abstract>The word families produced by two morphological analyzers of French, DériF (rule-based) and Morphonette (analogybased), applied on the same lexical corpus have been compared. The comparison led us to examine three classes of relations:
			- one subset of relations that are shared by both systems. It shows that, despite their differences, the approaches implemented in these systems are valid and describe, to some extent, one and the same morphological reality.
			- one subset of relations specific to DériF and another one to Morphonette. These sets (a) give us informations on the characteristics proper to each system, and especially on what the other system is unable to produce; (b) they highlight the errors of one system, in so that they are absent from the results of the other; (c) they reveal some of the limits of the description, especially the ones related to theoretical objects and concepts such as morphological family, base or the existence of transverse LCR (lexeme construction rules) between lexemes that are not ascendant nor descendant of each other.</abstract>
        <keywords>Word formation, automatic analysis, rules, analogy, morphological families, comparison, synergy</keywords>
      </article>
      <article id="taln-2011-court-036" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Andrea Gesmundo</nom>
            <email>andrea.gesmundo@unige.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université de Genève, route de Drize 7, 1227 Genève</affiliation>
        </affiliations>
        <titre/>
        <type>court</type>
        <pages/>
        <resume>Dans cet article nous présentons une série d’adaptations de l’algorithme du "cadre d’apprenstissage guidé" pour résoudre différentes tâches d’étiquetage. La spécificité du système proposé réside dans sa capacité à apprendre l’ordre de l’inférence avec les paramètres du classifieur local au lieu de la forcer dans un ordre pré-défini (de gauche à droite). L’algorithme d’entraînement est basé sur l’algorithme du "perceptron". Nous appliquons le système à différents types de tâches d’étiquetage pour atteindre des résultats au niveau de l’état de l’art en un court temps d’exécution.</resume>
        <mots_cles>Bidirectionnel, Classification de Séquence, Apprentissage Guidé</mots_cles>
        <title>Bidirectional Sequence Classification for Tagging Tasks with Guided Learning</title>
        <abstract>In this paper we present a series of adaptations of the Guided Learning framework to solve different tagging tasks. The specificity of the proposed system lies in its ability to learn the order of inference together with the parameters of the local classifier instead of forcing it into a pre-defined order (left-to-right). The training algorithm is based on the Perceptron Algorithm. We apply the system to different kinds of tagging tasks reaching state of the art results with short execution time.</abstract>
        <keywords>Bidirectional, Sequence Classification, Guided Learning</keywords>
      </article>
      <article id="taln-2011-court-037" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Dominique Legallois</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Peggy Cellier</nom>
            <email/>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Thierry Charnois</nom>
            <email/>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CRISCO Université de Caen Basse-Normandie, Campus 1, 14000 Caen</affiliation>
          <affiliation affiliationId="2">IRISA-INSA de Rennes, Campus Beaulieu 35042 Rennes cedex</affiliation>
          <affiliation affiliationId="3">GREYC Université de Caen Basse-Normandie, Campus 2, 14000 Caen</affiliation>
        </affiliations>
        <titre>Calcul de réseaux phrastiques pour l’analyse et la navigation textuelle</titre>
        <type>court</type>
        <pages/>
        <resume>Le travail présente une méthode de navigation dans les textes, fondée sur la répétition lexicale. La méthode choisie est celle développée par le linguiste Hoey. Son application manuelle à des textes de grandeur conséquente est problématique. Nous proposons dans cet article un processus automatique qui permet d’analyser selon cette méthode des textes de grande taille ; des expériences ont été menées appliquant le processus à différents types de textes (narratif, expositif) et montrant l’intérêt de l’approche.</resume>
        <mots_cles>Réseau phrastique, Appariement de phrases, Analyse textuelle, Navigation textuelle</mots_cles>
        <title/>
        <abstract>In this paper, we present an automatic process based on lexical repetition introduced by Hoey. The application of that kind of approaches on large texts is difficult to do by hand. In the paper, we propose an automatic process to treat large texts. We have conducted some experiments on different kinds of texts (narrative, expositive) to show the benefits of the approach.</abstract>
        <keywords>Sentence network, Bonds between sentences, Textual analysis, Textual navigation</keywords>
      </article>
      <article id="taln-2011-court-038" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Achille Falaise</nom>
            <email>achille.falaise@imag.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Agnès Tutin</nom>
            <email>agnes.tutin@u-grenoble3.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Olivier Kraif</nom>
            <email>olivier.kraif@u-grenoble3.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">GETALP-LIG</affiliation>
          <affiliation affiliationId="2">LIDILEM</affiliation>
        </affiliations>
        <titre>Exploitation d'un corpus arboré pour non spécialistes par des requêtes guidées et des requêtes sémantiques</titre>
        <type>court</type>
        <pages/>
        <resume>L'exploitation de corpus analysés syntaxiquement (ou corpus arborés) pour le public non spécialiste n'est pas un problème trivial. Si la communauté du TAL souhaite mettre à la disposition des chercheurs non-informaticiens des corpus comportant des annotations linguistiques complexes, elle doit impérativement développer des interfaces simples à manipuler mais permettant des recherches fines. Dans cette communication, nous présentons les modes de recherche « grand public » développé(e)s dans le cadre du projet Scientext, qui met à disposition un corpus d'écrits scientifiques interrogeable par partie textuelle, par partie du discours et par fonction syntaxique. Les modes simples sont décrits : un mode libre et guidé, où l'utilisateur sélectionne lui-même les éléments de la requête, et un mode sémantique, qui comporte des grammaires locales préétablies à l'aide des fonctions syntaxiques.</resume>
        <mots_cles>environnement d'étude de corpus, corpus étiquetés et arborés, création de grammaires assistée, visualisation d'information linguistique</mots_cles>
        <title/>
        <abstract>The exploitation of syntactically analysed corpora (or treebanks) by non-specialist is not a trivial problem. If the NLP community wants to make publicly available corpora with complex annotations, it is imperative to develop simple interfaces able to handle advanced queries. In this paper, we present queries methods for the general public developed during the Scientext project, which provides a searchable corpus of scientific texts searchable from textual part, part of speech and syntactic relation. The simple query modes are described: a guided query mode, where the user easily selects the elements of the query, and a semantic mode which includes local pre-established grammars using syntactic functions.</abstract>
        <keywords>corpus study environment, treebanks, assisted grammars creation, visualization of linguistic information</keywords>
      </article>
      <article id="taln-2011-court-039" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Mohammad Daoud</nom>
            <email>Mohammad.Daoud@imag.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Christian Boitet</nom>
            <email>Christian.Boitet@imag.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire LIG — Université Joseph Fourier — 385, rue de la Bibliothèque, 38041 Grenoble, France</affiliation>
        </affiliations>
        <titre>Communautés Internet comme sources de préterminologie</titre>
        <type>court</type>
        <pages/>
        <resume>Cet article décrit deux expériences sur la construction de ressources terminologiques multilingues (preterminologies) préliminaires, mais grandes, grâce à des communautés Internet, et s'appuie sur ces expériences pour cibler des données terminologiques plus raffinées venant de communautés Internet et d'applications Web 2.0. La première expérience est une passerelle de contribution pour le site Web de la Route de la Soie numérique (DSR). Les visiteurs contribuent en effet à un référentiel lexical multilingue dédié, pendant qu'ils visitent et lisent les livres archivés, parce qu'ils sont intéressés par le domaine et ont tendance à être polygottes. Nous avons recueilli 1400 contributions lexicales en 4 mois. La seconde expérience est basée sur le JeuxDeMots arabe, où les joueurs en ligne contribuent à un réseau lexical arabe. L'expérience a entraîné une croissance régulière du nombre de joueurs et de contributions, ces dernières contenant des termes absents et des mots de dialectes oraux.</resume>
        <mots_cles>terminologie, préterminologie, approches collaboratives, réseaux lexicaux, DSR, jeux sérieux</mots_cles>
        <title/>
        <abstract>This paper describes two experiments on building preliminary but large multilingual terminological resources (preterminologies) through Internet communities, and draws on these experiments to target more refined terminological data from Internet communities and Web 2.0 applications. The first experiment is a contribution gateway for the Digital Silk Road (DSR) website. Visitors indeed contribute to a dedicated multilingual lexical repository while they visit and read the archived books, because they are interested in the domain and tend to be multilingual. We collected 1400 lexical contributions in 4 months. The second experiment is based on the Arabic JeuxDeMots, where online players contribute to an Arabic lexical network. The experiment resulted in a steady growth of number of players and contributions, the latter containing absent terms and spoken dialectic words.</abstract>
        <keywords>terminology, preterminology, collaborative approaches, lexical networks, DSR, serious games</keywords>
      </article>
      <article id="taln-2011-court-040" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Wigdan Mekki</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Julien Gosme</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Fathi Debili</nom>
            <email/>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Yves Lepage</nom>
            <email/>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Nadine Lucas</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">GREYC, UMR 6072, CNRS, Université de Caen Basse-Normandie, Caen, France</affiliation>
          <affiliation affiliationId="2">LLACAN, UMR 8135, CNRS, Villejuif, France</affiliation>
          <affiliation affiliationId="3">IPS, Université Waseda, Japon</affiliation>
        </affiliations>
        <titre>Évaluation de G-LexAr pour la traduction automatique statistique</titre>
        <type>court</type>
        <pages/>
        <resume>G-LexAr est un analyseur morphologique de l’arabe qui a récemment reçu des améliorations substantielles. Cet article propose une évaluation de cet analyseur en tant qu’outil de pré-traitement pour la traduction automatique statistique, ce dont il n’a encore jamais fait l’objet. Nous étudions l’impact des différentes formes proposées par son analyse (voyellation, lemmatisation et segmentation) sur un système de traduction arabe-anglais, ainsi que l’impact de la combinaison de ces formes. Nos expériences montrent que l’utilisation séparée de chacune de ces formes n’a que peu d’influence sur la qualité des traductions obtenues, tandis que leur combinaison y contribue de façon très bénéfique.</resume>
        <mots_cles>traduction automatique statistique, analyse morphologique, pré-traitement de l’arabe</mots_cles>
        <title/>
        <abstract>G-LexAr is an Arabic morphological analyzer that has recently been improved for speed. This paper gives an assessment of this analyzer as a preprocessing tool for statistical machine translation. We study the impact of the use of its possible outputs (vocalized, lemmatized and segmented) through an Arabic-English machine translation system, as well as the impact of the combination of these outputs. Our experiments show that using these outputs separately does not influence much translation quality. However, their combination leads to major improvements.</abstract>
        <keywords>statistical machine translation, morphological analysis, arabic preprocessing</keywords>
      </article>
      <article id="taln-2011-court-041" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Marion laignelet</nom>
            <email>marion.laignelet@univ-tlse2.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Mouna Kamel</nom>
            <email>kamel@irit.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Nathalie Aussenac-Gilles</nom>
            <email>aussenac@irit.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CLLE-ERSS, Université de Toulouse 2, 5 allée A. Machado, 31058 Toulouse Cedex 9</affiliation>
          <affiliation affiliationId="2">IRIT, Université Paul Sabatier, 118 Route de Narbonne, 31062 Toulouse Cedex 9</affiliation>
        </affiliations>
        <titre>Enrichir la notion de patron par la prise en compte de la structure textuelle - Application à la construction d’ontologie</titre>
        <type>court</type>
        <pages/>
        <resume>La projection de patrons lexico-syntaxiques sur corpus est une des manières privilégiées pour identifier des relations sémantiques précises entre éléments lexicaux. Dans cet article, nous proposons d’étendre la notion de patron en prenant en compte la sémantique que véhiculent les éléments de structure d’un document (définitions, titres, énumérations) dans l’identification de relations. Nous avons testé cette hypothèse dans le cadre de la construction d’ontologies à partir de textes fortement structurés du domaine de la cartographie.</resume>
        <mots_cles>Construction d’ontologie, patron lexico-syntaxique, structure textuelle</mots_cles>
        <title/>
        <abstract>Matching lexico-syntactic patterns on text corpora is one of the favorite ways to identify precise semantic relations between lexical items. In this paper, we propose to rely on text structure to extend the notion of pattern and to take into account the semantics that the structure (definitions, titles, item lists) may bear when identifying semantic relations between concepts. We have checked this hypothesis by building an ontology via highly structured texts describing spatial, i.e. geographical information.</abstract>
        <keywords>Ontology engineering, lexico-syntactic patterns, textual structure</keywords>
      </article>
      <article id="taln-2011-court-042" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Lorenza Russo</nom>
            <email>Lorenza.Russo@unige.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Éric Wehrli</nom>
            <email>Eric.Wehrli@unige.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire d’Analyse et de Technologie du Langage (LATL), Département de linguistique – Université de Genève, 2, rue de Candolle – CH-1211 Genève 4</affiliation>
        </affiliations>
        <titre>La traduction automatique des séquences clitiques dans un traducteur à base de règles</titre>
        <type>court</type>
        <pages/>
        <resume>Dans cet article, nous discutons la méthodologie utilisée par Its-2, un système de traduction à base de règles, pour la traduction des pronoms clitiques. En particulier, nous nous focalisons sur les séquences clitiques, pour la traduction automatique entre le français et l’anglais. Une évaluation basée sur un corpus de phrases construites montre le potentiel de notre approche pour des traductions de bonne qualité.</resume>
        <mots_cles>Analyseur syntaxique, traduction automatique, pronom clitique, séquences clitiques</mots_cles>
        <title/>
        <abstract>In this paper we discuss the methodology applied by Its-2, a rule-based MT system, in order to translate clitic pronouns. In particular, we focus on French clitic clusters, for automatic translation between French and English. An evaluation based on a corpus of constructed sentences shows the potential of this approach for high-quality translation.</abstract>
        <keywords>Syntactic parser, automatic translation, clitic pronoun, clitic clusters</keywords>
      </article>
      <article id="taln-2011-court-043" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Lorenza Russo</nom>
            <email>lorenza.russo@unige.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Yves Scherrer</nom>
            <email>yves.scherrer@unige.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Jean-Philippe Goldman</nom>
            <email>jean-philippe.goldman@unige.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Sharid Loáiciga</nom>
            <email>sharid.loaiciga@unige.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Luka Nerima</nom>
            <email>luka.nerima@unige.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Éric Wehrli</nom>
            <email>eric.wehrli@unige.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire d’Analyse et de Technologie du Langage, Département de Linguistique – Université de Genève, 2, rue de Candolle – CH-1211 Genève 4</affiliation>
        </affiliations>
        <titre>Étude inter-langues de la distribution et des ambiguïtés syntaxiques des pronoms</titre>
        <type>court</type>
        <pages/>
        <resume>Ce travail décrit la distribution des pronoms selon le style de texte (littéraire ou journalistique) et selon la langue (français, anglais, allemand et italien). Sur la base d’un étiquetage morpho-syntaxique effectué automatiquement puis vérifié manuellement, nous pouvons constater que la proportion des différents types de pronoms varie selon le type de texte et selon la langue. Nous discutons les catégories les plus ambiguës de manière détaillée. Comme nous avons utilisé l’analyseur syntaxique Fips pour l’étiquetage des pronoms, nous l’avons également évalué et obtenu une précision moyenne de plus de 95%.</resume>
        <mots_cles>Pronoms, ambiguïté pronominale, étiquetage morpho-syntaxique</mots_cles>
        <title/>
        <abstract>This paper compares the distribution of pronouns according to the text genre (literary or news) and to the language (French, English, German and Italian). On the basis of manually verified part-of-speech tags, we find that the proportion of different pronoun types depends on the text and on the language. We discuss the most ambiguous cases in detail. As we used the Fips parser for the tagging of pronouns, we have evaluated it and obtained an overall precision of over 95%.</abstract>
        <keywords>Pronouns, pronominal ambiguity, part-of-speech tagging</keywords>
      </article>
      <article id="taln-2011-court-044" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Yves Scherrer</nom>
            <email>yves.scherrer@unige.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Lorenza Russo</nom>
            <email>lorenza.russo@unige.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Jean-Philippe Goldman</nom>
            <email>jean-philippe.goldman@unige.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Sharid Loáiciga</nom>
            <email>sharid.loaiciga@unige.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Luka Nerima</nom>
            <email>luka.nerima@unige.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Éric Wehrli</nom>
            <email>eric.wehrli@unige.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire d’Analyse et de Technologie du Langage, Département de Linguistique – Université de Genève, 2, rue de Candolle – CH-1211 Genève 4</affiliation>
        </affiliations>
        <titre>La traduction automatique des pronoms. Problèmes et perspectives</titre>
        <type>court</type>
        <pages/>
        <resume>Dans cette étude, notre système de traduction automatique, Its-2, a fait l’objet d’une évaluation manuelle de la traduction des pronoms pour cinq paires de langues et sur deux corpus : un corpus littéraire et un corpus de communiqués de presse. Les résultats montrent que les pourcentages d’erreurs peuvent atteindre 60% selon la paire de langues et le corpus. Nous discutons ainsi deux pistes de recherche pour l’amélioration des performances de Its-2 : la résolution des ambiguïtés d’analyse et la résolution des anaphores pronominales.</resume>
        <mots_cles>Pronoms, traduction automatique, analyse syntaxique, anaphores pronominales</mots_cles>
        <title/>
        <abstract>In this work, we present the results of a manual evaluation of our machine translation system, Its-2, on the task of pronoun translation for five language pairs and in two corpora : a litterary corpus and a corpus of press releases. The results show that the error rates reach 60% depending on the language pair and the corpus. Then we discuss two proposals for improving the performances of Its-2 : resolution of source language ambiguities and resolution of pronominal anaphora.</abstract>
        <keywords>Pronouns, machine translation, parsing, pronominal anaphora</keywords>
      </article>
      <article id="taln-2011-court-045" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Daniel Kayser</nom>
            <email>Daniel.Kayser@lipn.univ-paris13.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIPN – UMR 7030 du CNRS, Institut Galilée - Université Paris-Nord, 93430 Villetaneuse</affiliation>
        </affiliations>
        <titre>Ressources lexicales pour une sémantique inférentielle : un exemple, le mot « quitter »</titre>
        <type>court</type>
        <pages/>
        <resume>On étudie environ 500 occurrences du verbe « quitter » en les classant selon les inférences qu’elles suggèrent au lecteur. On obtient ainsi 43 « schémas inférentiels ». Ils ne s’excluent pas l’un l’autre : si plusieurs d’entre eux s’appliquent, les inférences produites se cumulent ; cependant, comme l’auteur sait que le lecteur dispose de tels schémas, s’il veut l’orienter vers une seule interprétation, il fournit des indices permettant d’éliminer les autres. On conjecture que ces schémas présentent des régularités observables sur des familles de mots, que ces régularités proviennent du fonctionnement d’opérations génériques, et qu’il est donc sans gravité de ne pas être exhaustif, dans la mesure où ces opérations permettent d’engendrer les schémas manquants en cas de besoin.</resume>
        <mots_cles>Sémantique lexicale, Inférence, Glissements de sens</mots_cles>
        <title/>
        <abstract>Around 500 occurrences of the French verb “quitter” are scrutinized and sorted according to the inferences they trigger in the reader’s mind. This yields 43 so-called inferential schemata. They are not exclusive from one another: when several of them are applicable, their conclusions add together; however, as the author knows that the reader possesses this kind of schema, if s/he wants to direct the reader towards a given interpretation, s/he provides some clues to block the other ones. The schemata reveal regularities across families of similar words, and these regularities are conjectured to be due to the operation of generic procedures: omitting some schemata is thus harmless, insofar as these procedures have the ability to generate the missing ones in case of need.</abstract>
        <keywords>Lexical Semantics, Inference, Shifts in Meaning</keywords>
      </article>
      <article id="taln-2011-court-046" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Caroline Brun</nom>
            <email>Caroline.Brun@xrce.xerox.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Xerox Research Centre Europe, 6 chemin de Maupertuis, 38240 Meylan, France</affiliation>
        </affiliations>
        <titre>Un système de détection d’opinions fondé sur l’analyse syntaxique profonde</titre>
        <type>court</type>
        <pages/>
        <resume>Dans cet article, nous présentons un système de détection d’opinions construit à partir des sorties d’un analyseur syntaxique robuste produisant des analyses profondes. L’objectif de ce système est l’extraction d’opinions associées à des produits (les concepts principaux) ainsi qu’aux concepts qui leurs sont associés (en anglais «features-based opinion extraction»). Suite à une étude d’un corpus cible, notre analyseur syntaxique est enrichi par l’ajout de polarité aux éléments pertinents du lexique et par le développement de règles génériques et spécialisées permettant l’extraction de relations sémantiques d’opinions, qui visent à alimenter un modèle de représentation des opinions. Une première évaluation montre des résultats très encourageants, mais de nombreuses perspectives restent à explorer.</resume>
        <mots_cles>détection d’opinions, analyse de sentiments, analyse syntaxique robuste, extraction d’information</mots_cles>
        <title/>
        <abstract>In this paper, we present an opinion detection system built on top of a deep robust syntactic parser. The goal of this system is to extract opinions associated to products but also to characteristics of these products, i.e. to perform feature-based opinion extraction. To carry out this task, and following the results of a target corpus study, the robust syntactic analyzer is enriched by the association of polarity to pertinent lexical elements and by the development of generic rules extracting semantic relations of opinions, in order to feed an opinion representation model. A first evaluation gave very encouraging results, but many perspectives remain to be explored.</abstract>
        <keywords>opinion detection, sentiment analysis, robust parsing, information extraction</keywords>
      </article>
      <article id="taln-2011-court-047" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Caroline Hagège</nom>
            <email>Caroline.Hagege@xrce.xerox.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Denys Proux</nom>
            <email>Denys.Proux@xrce.xerox.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Quentin Gicquel</nom>
            <email>Quentin.Gicquel@chu-lyon.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Stefan Darmoni</nom>
            <email>Stefan.Darmoni@cismef.fr</email>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Suzanne Pereira</nom>
            <email>Suzanne.Pereira@vidal.fr</email>
            <affiliationId>4</affiliationId>
          </auteur>
          <auteur>
            <nom>Frédérique Segond</nom>
            <email>Frederique.Segond@xrce.xerox.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Marie-Helène Metzger</nom>
            <email>Marie-Helene.Metzger@chu-lyon.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">XRCE, 6 Chemin de Maupertuis, 38240 Meylan, France</affiliation>
          <affiliation affiliationId="2">UCBL-CNRS, UMR 5558 Lyon, France</affiliation>
          <affiliation affiliationId="3">CISMEF, Rouen, France</affiliation>
          <affiliation affiliationId="4">VIDAL, Issy les Moulineaux, France</affiliation>
        </affiliations>
        <titre>Développement d’un système de détection des infections associées aux soins à partir de l’analyse de comptes-rendus d’hospitalisation</titre>
        <type>court</type>
        <pages/>
        <resume>Cet article décrit la première version et les résultats de l’évaluation d’un système de détection des épisodes d’infections associées aux soins. Cette détection est basée sur l’analyse automatique de comptes-rendus d’hospitalisation provenant de différents hôpitaux et différents services. Ces comptes-rendus sont sous forme de texte libre. Le système de détection a été développé à partir d’un analyseur linguistique que nous avons adapté au domaine médical et extrait à partir des documents des indices pouvant conduire à une suspicion d’infection. Un traitement de la négation et un traitement temporel des textes sont effectués permettant de restreindre et de raffiner l’extraction d’indices. Nous décrivons dans cet article le système que nous avons développé et donnons les résultats d’une évaluation préliminaire.</resume>
        <mots_cles>Extraction d’information médicale, compte-rendus d’hospitalisation, infection nosocomiale, analyse syntaxique</mots_cles>
        <title/>
        <abstract>This paper describes the first version and the results obtained by a system which detects occurrences of healthcare-associated infections. The system automatically analyzes hospital discharge summaries coming from different hospitals and from different care units. The output of the system consists in stating for each document, if there is a case of healthcare-associated infection. The linguistic processor which analyzes hospital discharge summaries is a general purpose tool which has been adapted for the medical domain. It extracts textual elements that may lead to an infection suspicion. Jointly with the extraction of suspicious terms, the system performs a negation and temporal processing of texts in order to refine the extraction. We first describe the system that has been developed and give then the results of a preliminary evaluation.</abstract>
        <keywords>Information extraction in medical domain, hospital discharge summaries, hospital acquired infections, parsing</keywords>
      </article>
      <article id="taln-2011-demo-001" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Richard Beaufort</nom>
            <email>richard.beaufort@uclouvain.be</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Sophie Roekhaut</nom>
            <email>sophie.roekhaut@uclouvain.be</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CENTAL, UCLouvain, Place Blaise Pascal 1, B-1348 Louvain-la-Neuve</affiliation>
        </affiliations>
        <titre>PLATON, Plateforme d’apprentissage et d’enseignement de l’orthographe sur le Net</titre>
        <type>démonstration</type>
        <pages/>
        <resume/>
        <mots_cles/>
        <title/>
        <abstract/>
        <keywords/>
      </article>
      <article id="taln-2011-demo-002" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Annelies Braffort</nom>
            <email>annelies.braffort@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Laurence Bolot</nom>
            <email>laurence.bolot@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIMSI-CNRS, Campus d'Orsay Bt. 508, BP 133, F-91403 Orsay cx, France</affiliation>
        </affiliations>
        <titre>SpatiAnn, un outil pour annoter l’utilisation de l’espace dans les corpus vidéo</titre>
        <type>démonstration</type>
        <pages/>
        <resume/>
        <mots_cles/>
        <title/>
        <abstract/>
        <keywords/>
      </article>
      <article id="taln-2011-demo-003" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>François Brown de Colstoun</nom>
            <email>fbc@lingua-et-machina.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Estelle Delpech</nom>
            <email>ed@lingua-et-machina.com</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Etienne Monneret</nom>
            <email>em@lingua-et-machina.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LINGUA ET MACHINA, Laval Technopole, 6 rue Léonard de Vinci, 53001 Laval Cedex et c/o Inria, Rocquencourt BP 105, 78 153 Le Chesnay Cedex</affiliation>
          <affiliation affiliationId="2">LINA FRE CNRS 2729, 2 rue de la Houssinière BP 92208, 44322 Nantes Cedex 3</affiliation>
        </affiliations>
        <titre>Libellex : une plateforme multiservices pour la gestion des contenus multilingues</titre>
        <type>démonstration</type>
        <pages/>
        <resume/>
        <mots_cles/>
        <title/>
        <abstract/>
        <keywords/>
      </article>
      <article id="taln-2011-demo-004" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Jacques Chauché</nom>
            <email>jacques.chauche@lirmm.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIRMM, 161, rue ADA 34392 Montpellier Cedex 5</affiliation>
        </affiliations>
        <titre>Une application de la grammaire structurelle: L’analyseur syntaxique du français SYGFRAN</titre>
        <type>démonstration</type>
        <pages/>
        <resume>La démonstration présentée produit une analyse syntaxique du français. Elle est écrite en SYGMART, fournie avec les actes, exécutable à l’adresse : http ://www.lirmm.fr/ chauche/ExempleAnl.html et téléchargeable à l’adresse : http ://www.sygtext.fr.</resume>
        <mots_cles>Analyse syntaxique</mots_cles>
        <title/>
        <abstract>The software produces a syntactic analysis of french. It is written in SYGMART, including acts, runable at http ://www.lirmm.fr/ chauche/ExempleAnl.html and downlodable at : http ://www.sygtext.fr.</abstract>
        <keywords>syntactic analysis</keywords>
      </article>
      <article id="taln-2011-demo-005" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>François-Régis Chaumartin</nom>
            <email>frc@proxem.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Proxem, 19 bd de Magenta, 75010 Paris</affiliation>
        </affiliations>
        <titre>Proxem Ubiq : une solution d’e-réputation par analyse de feedbacks clients</titre>
        <type>démonstration</type>
        <pages/>
        <resume/>
        <mots_cles>e-réputation, reconnaissance d’entités nommées, classification, clustering, analyse syntaxique, apprentissage</mots_cles>
        <title/>
        <abstract/>
        <keywords/>
      </article>
      <article id="taln-2011-demo-006" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Béatrice Daille</nom>
            <email>beatrice.daille@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Christine Jacquin</nom>
            <email>christine.jacquin@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Laura Monceaux</nom>
            <email>laura.monceaux@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Emmanuel Morin</nom>
            <email>emmanuel.morin@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Jérome Rocheteau</nom>
            <email>jerome.rocheteau@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université de Nantes - LINA – 2 rue de la Houssinière – BP 92208 – 44322 Nantes cedex 3, France</affiliation>
        </affiliations>
        <titre>TTC TermSuite : une chaîne de traitement pour la fouille terminologique multilingue</titre>
        <type>démonstration</type>
        <pages/>
        <resume/>
        <mots_cles/>
        <title/>
        <abstract/>
        <keywords/>
      </article>
      <article id="taln-2011-demo-007" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Rodolfo Delmonte</nom>
            <email>delmonte@unive.it</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Vincenzo Pallotta</nom>
            <email>Vincenzo.Pallotta@internalytics.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Violeta Seretan</nom>
            <email>violeta.seretan@gmail.com</email>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Lammert Vrieling</nom>
            <email>Lammert.Vrieling@internalytics.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>David Walker</nom>
            <email>David.Walker@internalytics.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Interanalytics, Geneva, Switzerland</affiliation>
          <affiliation affiliationId="2">Department of Language Science, University of Venice, Italy</affiliation>
          <affiliation affiliationId="3">School of Informatics, University of Edinburgh, United Kingdom</affiliation>
        </affiliations>
        <titre/>
        <type>démonstration</type>
        <pages/>
        <resume/>
        <mots_cles/>
        <title>An Interaction Mining Suite Based On Natural Language Understanding</title>
        <abstract/>
        <keywords/>
      </article>
      <article id="taln-2011-demo-008" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>François-Xavier Desmarais</nom>
            <email>francois-xavier.desmarais@polymtl.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Éric Charton</nom>
            <email>eric.charton@polymtl.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">École Polytechnique de Montréal, 2900 boul. Edouard Montpetit, Montréal, Canada H3T 1J4</affiliation>
        </affiliations>
        <titre>Démonstration de l'API de NLGbAse</titre>
        <type>démonstration</type>
        <pages/>
        <resume/>
        <mots_cles/>
        <title/>
        <abstract/>
        <keywords/>
      </article>
      <article id="taln-2011-demo-009" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Michel Généreux</nom>
            <email>genereux@clul.ul.pt</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Centro de Linguística da Universidade de Lisboa, Av. Prof. Gama Pinto, 2, 1649-003 Lisboa - Portugal</affiliation>
        </affiliations>
        <titre>Système d’analyse de la polarité de dépêches financières</titre>
        <type>démonstration</type>
        <pages/>
        <resume>Nous présentons un système pour la classification en continu de dépêches financières selon une polarité positive ou négative. La démonstration permettra ainsi d’observer quelles sont les dépêches les plus à même de faire varier la valeur d’actions cotées en bourse, au moment même de la démonstration. Le système traitera de dépêches écrites en anglais et en français.</resume>
        <mots_cles>Analyse de Sentiments, Linguistique de Corpus, Dépêches Financières</mots_cles>
        <title/>
        <abstract>We present a system for classifying on-line financial news items into a positive or negative polarity. The demonstration will therefore allow us to observe which news are most likely to influence the price of shares traded at the time of the demonstration. The system will cover news items written in English and French.</abstract>
        <keywords>Sentiment Analysis, Corpus Linguistics, Financial News Items</keywords>
      </article>
      <article id="taln-2011-demo-010" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Laurence Longo</nom>
            <email>longo@unistra.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Amalia Todirascu</nom>
            <email>todiras@unistra.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université de Strasbourg, 22 avenue René Descartes, 67084 Strasbourg Cedex, France</affiliation>
        </affiliations>
        <titre>RefGen, outil d’identification automatique des chaînes de référence en français</titre>
        <type>démonstration</type>
        <pages/>
        <resume/>
        <mots_cles/>
        <title/>
        <abstract/>
        <keywords/>
      </article>
      <article id="taln-2011-demo-011" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Clément de Groc</nom>
            <email>cdegroc@syllabs.com</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Javier Couto</nom>
            <email>jcouto@syllabs.com</email>
            <affiliationId>1</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Helena Blancafort</nom>
            <email>blancafort@syllabs.com</email>
            <affiliationId>1</affiliationId>
            <affiliationId>4</affiliationId>
          </auteur>
          <auteur>
            <nom>Claude de Loupy</nom>
            <email>loupy@syllabs.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Syllabs, 15 rue Jean-Baptiste Berlier, 75013 Paris</affiliation>
          <affiliation affiliationId="2">Univ. Paris Sud et LIMSI-CNRS, F-91405 Orsay</affiliation>
          <affiliation affiliationId="3">MoDyCo, UMR 7114, CNRS-Université Paris Ouest Nanterre, La Défense</affiliation>
          <affiliation affiliationId="4">Universitat Pompeu Fabra Roc Boronat,138, 08018 Barcelona, Spain</affiliation>
        </affiliations>
        <titre>Babouk – exploration orientée du web pour la constitution de corpus et de terminologies</titre>
        <type>démonstration</type>
        <pages/>
        <resume/>
        <mots_cles/>
        <title/>
        <abstract/>
        <keywords/>
      </article>
      <article id="taln-2011-demo-012" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Cyril Grouin</nom>
            <email>Cyril.Grouin@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Louise Deléger</nom>
            <email>louise.deleger@cchmc.org</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Anne-Lyse Minard</nom>
            <email>Anne-Lyse.Minard@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Anne-Laure Ligozat</nom>
            <email>Anne-Laure.Ligozat@limsi.fr</email>
            <affiliationId>.</affiliationId>
          </auteur>
          <auteur>
            <nom>Asma Ben Abacha</nom>
            <email>Asma.BenAbacha@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Delphine Bernhard</nom>
            <email>Delphine.Bernhard@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Bruno Cartoni</nom>
            <email>bruno.cartoni@unige.ch</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Brigitte Grau</nom>
            <email>Brigitte.Grau@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Sophie Rosset</nom>
            <email>Sophie.Rosset@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Pierre Zweigenbaum</nom>
            <email>Pierre.Zweigenbaum@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIMSI-CNRS, BP133, 91403 Orsay Cedex, France</affiliation>
          <affiliation affiliationId="2">Département de Linguistique, Université de Genève, Suisse</affiliation>
        </affiliations>
        <titre>Extraction d’informations médicales au LIMSI</titre>
        <type>démonstration</type>
        <pages/>
        <resume/>
        <mots_cles/>
        <title/>
        <abstract/>
        <keywords/>
      </article>
      <article id="taln-2011-demo-013" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Juyeon Kang</nom>
            <email>kjuyeon79@yahoo.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Jean-Pierre Desclés</nom>
            <email>jean-pierre.desclés@paris-sorbonne.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LaLIC, 28, Rue Serpente, 75006 Paris, France</affiliation>
        </affiliations>
        <titre>Système d’analyse catégorielle ACCG : adéquation au traitement de problèmes syntaxiques complexes</titre>
        <type>démonstration</type>
        <pages/>
        <resume/>
        <mots_cles/>
        <title/>
        <abstract/>
        <keywords/>
      </article>
      <article id="taln-2011-demo-014" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Jimmy Ma</nom>
            <email>ma@syllabs.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Mickaël Mounier</nom>
            <email>mounier@syllabs.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Helena Blancafort</nom>
            <email>blancafort@syllabs.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Javier Couto</nom>
            <email>couto@syllabs.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Claude de Loupy</nom>
            <email>loupy@syllabs.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Syllabs, 15 rue Jean-Baptiste Berlier, 75013 Paris, France</affiliation>
        </affiliations>
        <titre>LOL : Langage objet dédié à la programmation linguistique</titre>
        <type>démonstration</type>
        <pages/>
        <resume/>
        <mots_cles/>
        <title/>
        <abstract/>
        <keywords/>
      </article>
      <article id="taln-2011-demo-015" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Yann Mathet</nom>
            <email>Yann.Mathet@unicaen.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Antoine Widlöcher</nom>
            <email>Antoine.Widlocher@unicaen.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">GREYC, UMR CNRS 6072, Université de Caen, 14032 Caen Cedex</affiliation>
        </affiliations>
        <titre>Aligner : un outil d’alignement et de mesure d’accord inter-annotateurs</titre>
        <type>démonstration</type>
        <pages/>
        <resume/>
        <mots_cles/>
        <title/>
        <abstract/>
        <keywords/>
      </article>
      <article id="taln-2011-demo-016" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Yann Mathet</nom>
            <email>Yann.Mathet@unicaen.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Antoine Widlöcher</nom>
            <email>Antoine.Widlocher@unicaen.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">GREYC, UMR CNRS 6072, Université de Caen, 14032 Caen Cedex</affiliation>
        </affiliations>
        <titre>GlozzQL : un langage de requêtes incrémental pour les textes annotés</titre>
        <type>démonstration</type>
        <pages/>
        <resume/>
        <mots_cles/>
        <title/>
        <abstract/>
        <keywords/>
      </article>
      <article id="taln-2011-demo-017" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Frédéric Meunier</nom>
            <email>frederic.meunier@watchsystance.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Laurence Danlos</nom>
            <email>laurence.danlos@linguist.jussieu.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Vanessa Combet</nom>
            <email>vanessa.combet@watchsystance.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Watch System Assistance</affiliation>
          <affiliation affiliationId="2">Université Paris Diderot, ALPAGE</affiliation>
        </affiliations>
        <titre>EASYTEXT : un système opérationnel de génération de textes</titre>
        <type>démonstration</type>
        <pages/>
        <resume/>
        <mots_cles/>
        <title/>
        <abstract/>
        <keywords/>
      </article>
      <article id="taln-2011-demo-018" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Yoann Moreau</nom>
            <email>yoann.moreau@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Eric SanJuan</nom>
            <email>eric.sanjuan@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Patrice Bellot</nom>
            <email>patrice.bellot@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIA, 339, chemin des Meinajaries 84911 AVIGNON Cedex 9</affiliation>
        </affiliations>
        <titre>Restad : un logiciel d’indexation et de stockage relationnel de contenus XML</titre>
        <type>démonstration</type>
        <pages/>
        <resume/>
        <mots_cles/>
        <title/>
        <abstract/>
        <keywords/>
      </article>
      <article id="taln-2011-demo-019" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Gaëlle Recourcé</nom>
            <email>recource@kwaga.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Kwaga SAS, 15 rue J-B. Berlier, 75013 Paris, France</affiliation>
        </affiliations>
        <titre>Une chaîne d’analyse des e-mails pour l’aide à la gestion de sa messagerie</titre>
        <type>démonstration</type>
        <pages/>
        <resume/>
        <mots_cles/>
        <title/>
        <abstract/>
        <keywords/>
      </article>
      <article id="taln-2011-demo-020" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Jean Rohmer</nom>
            <email>jean.rohmer@devinci.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Ecole Supérieure d'Ingénieurs Léonard de Vinci 92916 Paris La Défense Cedex</affiliation>
        </affiliations>
        <titre>Démonstration d’un outil de « Calcul Littéraire »</titre>
        <type>démonstration</type>
        <pages/>
        <resume/>
        <mots_cles/>
        <title/>
        <abstract/>
        <keywords/>
      </article>
    </articles>
  </conference>
  <conference>
    <edition>
      <acronyme>TALN'2012</acronyme>
      <titre>conférence sur le Traitement Automatique des Langues Naturelles</titre>
      <ville>Grenoble</ville>
      <pays>France</pays>
      <dateDebut>2012-06-04</dateDebut>
      <dateFin>2012-06-08</dateFin>
      <presidents>
        <nom>Georges Antoniadis</nom>
        <nom>Hervé Blanchon</nom>
      </presidents>
      <typeArticles>
        <type id="long">Papiers longs</type>
        <type id="court">Papiers courts</type>
        <type id="démonstration">Démonstrations</type>
      </typeArticles>
      <statistiques>
        <acceptations id="long" soumissions="62">24</acceptations>
        <acceptations id="court" soumissions="61">29</acceptations>
      </statistiques>
      <siteWeb>http://www.jeptaln2012.org/</siteWeb>
      <meilleurArticle>
        <articleId>taln-2012-long-005</articleId>
      </meilleurArticle>
    </edition>
    <articles>
      <article id="taln-2012-long-001" session="Extraction d’informations/de relations">
        <auteurs>
          <auteur>
            <nom>Anne-Lyse Minard</nom>
            <email>anne-lyse.minard@limsi.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Anne-Laure Ligozat</nom>
            <email>anne-laure.ligozat@limsi.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Brigitte Grau</nom>
            <email>brigitte.grau@limsi.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIMSI-CNRS, BP 133, 91403 Orsay Cedex</affiliation>
          <affiliation affiliationId="2">Université Paris Sud, 91400 Orsay</affiliation>
          <affiliation affiliationId="3">ENSIIE, square de la résistance, 91000 Évry</affiliation>
        </affiliations>
        <titre>Simplification de phrases pour l'extraction de relations</titre>
        <type>long</type>
        <pages>1-14</pages>
        <resume>L’extraction de relations par apprentissage nécessite un corpus annoté de très grande taille pour couvrir toutes les variations d’expressions des relations. Pour contrer ce problème, nous proposons une méthode de simplification de phrases qui permet de réduire la variabilité syntaxique des relations. Elle nécessite l’annotation d’un petit corpus qui sera par la suite augmenté automatiquement. La première étape est l’annotation des simplifications grâce à un classifieur à base de CRF, puis l’extraction des relations, et ensuite une complétion automatique du corpus d’entraînement des simplifications grâce aux résultats de l’extraction des relations. Les premiers résultats que nous avons obtenus pour la tâche d’extraction de relations d’i2b2 2010 sont très encourageants.</resume>
        <mots_cles>Extraction de relations, simplification de phrases, apprentissage automatique</mots_cles>
        <title>Automatic Information Extraction in the Medical Domain by Cross-Lingual Projection</title>
        <abstract>Machine learning based relation extraction requires large annotated corpora to take into account the variability in the expression of relations. To deal with this problem, we propose a method for simplifying sentences, i.e. for reducing the syntactic variability of the relations. Simplification requires the annotation of a small corpus, which will be automatically augmented. The process starts with the annotation of the simplification thanks to a CRF classifier, then the relation extraction, and lastly the automatic completion of the training corpus for the simplification through the results of the relation extraction. The first results we obtained for the task of relation extraction of the i2b2 2010 challenge are encouraging.</abstract>
        <keywords>Relation extraction, sentence simplification, machine learning</keywords>
      </article>
      <article id="taln-2012-long-002" session="Extraction d’informations/de relations">
        <auteurs>
          <auteur>
            <nom>Asma Ben Abacha</nom>
            <email>abacha@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Pierre Zweigenbaum</nom>
            <email>pz@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Aurélien Max</nom>
            <email>aurelien.max@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIMSI-CNRS, BP 133 91403 Orsay cedex</affiliation>
        </affiliations>
        <titre>Extraction d'information automatique en domaine médical par projection inter-langue : vers un passage à l'échelle</titre>
        <type>long</type>
        <pages>15-28</pages>
        <resume>Cette recherche est issue de notre volonté de tester de nouvelles méthodes automatiques d’annotation ou d’extraction d’information à partir d’une langue L1 en exploitant des ressources et des outils disponibles pour une autre langue L2. Cette approche repose sur le passage par un corpus parallèle (L1-L2) aligné au niveau des phrases et des mots. Pour faire face au manque de corpus médicaux français annotés, nous nous intéressons au couple de langues (françaisanglais) dans le but d’annoter automatiquement des textes médicaux en français. En particulier, nous nous intéressons dans cet article à la reconnaissance des entités médicales. Nous évaluons dans un premier temps notre méthode de reconnaissance d’entités médicales sur le corpus anglais. Dans un second temps, nous évaluons la reconnaissance des entités médicales du corpus français par projection des annotations du corpus anglais. Nous abordons également le problème de l’hétérogénéité des données en exploitant un corpus extrait du Web et nous proposons une méthode statistique pour y pallier.</resume>
        <mots_cles>Extraction d’information, projection d’annotation, reconnaissance des entités médicales, apprentissage</mots_cles>
        <title>Automatic Information Extraction in the Medical Domain by Cross-Lingual Projection</title>
        <abstract>This research stems from our willingness to test new methods for automatic annotation or information extraction from one language L1 by exploiting resources and tools available to another language L2. This approach involves the use of a parallel corpus (L1-L2) aligned at the level of sentences and words. To address the lack of annotated medical French corpus, we focus on the French-English language pair to annotate automatically medical French texts. In particular, we focus in this article on medical entity recognition. We evaluate our medical entity recognition method on the English corpus and the projection of the annotations on the French corpus. We also discuss the problem of scalability since we use a parallel corpus extracted from the Web and propose a statistical method to handle heterogeneous corpora.</abstract>
        <keywords>Automatic Information Extraction, Annotation Projection, Medical Entity Recognition, Machine Learning</keywords>
      </article>
      <article id="taln-2012-long-003" session="Extraction d’informations/de relations">
        <auteurs>
          <auteur>
            <nom>Ludovic Jean-Louis</nom>
            <email>ludovic.jean-louis@cea.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Romaric Besançon</nom>
            <email>romaric.besancon@cea.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Olivier Ferret</nom>
            <email>olivier.ferret@cea.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CEA, LIST, Laboratoire Vision et Ingénierie des Contenus, F-91191 Gif-sur-Yvette, France</affiliation>
        </affiliations>
        <titre>Une méthode d'extraction d'information fondée sur les graphes pour le remplissage de formulaires</titre>
        <type>long</type>
        <pages>29-42</pages>
        <resume>Dans les systèmes d’extraction d’information sur des événements, une tâche importante est le remplissage automatique de formulaires regroupant les informations sur un événement donné à partir d’un texte non structuré. Ce remplissage de formulaire peut s’avérer difficile lorsque l’information est dispersée dans tout le texte et mélangée à des éléments d’information liés à un autre événement similaire. Nous proposons dans cet article une approche en deux étapes pour ce problème : d’abord une segmentation du texte en événements pour sélectionner les phrases relatives au même événement ; puis une méthode de sélection dans les phrases sélectionnées des entités liées à l’événement. Une évaluation de cette approche sur un corpus annoté de dépêches dans le domaine des événements sismiques montre un F-score de 72% pour la tâche de remplissage de formulaires.</resume>
        <mots_cles>Extraction d’information, segmentation de texte, remplissage de formulaires</mots_cles>
        <title>A Graph-Based Method for Template Filling in Information Extraction</title>
        <abstract>In event-based Information Extraction systems, a major task is the automated filling from unstructured texts of a template gathering information related to a particular event. Such template filling may be a hard task when the information is scattered throughout the text and mixed with similar pieces of information relative to a different event. We propose in this paper a two-step approach for template filling : first, an event-based segmentation is performed to select the parts of the text related to the target event ; then, a graph-based method is applied to choose the most relevant entities in these parts for characterizing the event. Using an evaluation of this model based on an annotated corpus for earthquake events, we achieve a 72% F-measure for the template-filling task.</abstract>
        <keywords>Information Extraction, Text Segmentation, Template Filling</keywords>
      </article>
      <article id="taln-2012-long-004" session="Analyse">
        <auteurs>
          <auteur>
            <nom>Anaïs Lefeuvre</nom>
            <email>anais.lefeuvre@labri.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Richard Moot</nom>
            <email>moot@labri.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Christian Retoré</nom>
            <email>retore@labri.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Noémie-Fleur Sandillon-Rezer</nom>
            <email>nfsr@labri.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université de Bordeaux</affiliation>
          <affiliation affiliationId="2">LaBRI-CNRS</affiliation>
          <affiliation affiliationId="3">INRIA</affiliation>
        </affiliations>
        <titre>Traitement automatique sur corpus de récits de voyages pyrénéens : Une analyse syntaxique, sémantique et temporelle</titre>
        <type>long</type>
        <pages>43-56</pages>
        <resume>Cet article présente notre utilisation de la théorie des types dans laquelle nous nous situons pour l’analyse syntaxique, sémantique et pour la construction du lexique. Notre outil, Grail permet de traiter le discours automatiquement à partir du texte brut et nous le testons sur un corpus de récit de voyages pyrénéens, Ititpy. Nous expliquons donc notre usage des grammaires catégorielles et plus particulièrement du calcul de Lambek et la correspondance entre ces catégories et le lambda-calcul simplement typé dans le cadre de la DRT. Une flexibilité du typage doit être autorisée dans certains cas et bloquée dans d’autres. Quelques phénomènes linguistiques participant à une forme de glissement de sens provocant des conflits de types sont présentés. Nous expliquons ensuite nos motivations d’ordre pragmatique à utiliser un système à sortes et types variables en sémantique lexicale puis notre traitement compositionnel du temps des évènements inspiré du Binary Tense de (Verkuyl, 2008).</resume>
        <mots_cles>compositionalité, interface syntaxe-sémantique, interface sémantique-pragmatique, grammaire catégorielle, théorie des types, récit de voyage</mots_cles>
        <title>Processing of a Pyrenees travel novels corpus : a syntactical, semantical and temporal analysis</title>
        <abstract>In this article, we present a type theoretical framework which we apply to the syntactic analysis and the computation of DRS semantics. Our tool, Grail, is used for the automatic treatment of French text and we use a Pyrenées travel novels corpus, Itipy, as a test case. We explain our use of categorial grammars and specifically the Lambek calculus and its connection to the simply typed lambda-calculus in connection with DRT. Flexible typing has to be allowed in some cases and forbidden in others. Some linguistic phenomena presenting some kind of meaning shifts inducing typing conflicts will be introduced. We then present our motivations in the pragmatic field to use a system with sorts and variable types in lexical semantics and then we present how we process events temporality, in the light of Verkuyl’s Binary Tense (Verkuyl, 2008).</abstract>
        <keywords>compositionality, syntax-semantics interface, semantics-pragmatics interface, categorial grammar, type theory, travel novel</keywords>
      </article>
      <article id="taln-2012-long-005" session="Analyse">
        <auteurs>
          <auteur>
            <nom>Matthieu Constant</nom>
            <email>mconstan@univ-mlv.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Anthony Sigogne</nom>
            <email>sigogne@univ-mlv.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Patrick Watrin</nom>
            <email>patrick.watrin@uclouvain.be</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">université Paris-Est, LIGM, CNRS, 5, bd Descartes 774545 Marne-la-Vallée</affiliation>
          <affiliation affiliationId="2">Université de Louvain, CENTAL, Louvain-la-Neuve</affiliation>
        </affiliations>
        <titre>La reconnaissance des mots composés à l'épreuve de l'analyse syntaxique et vice-versa : évaluation de deux stratégies discriminantes</titre>
        <type>long</type>
        <pages>57-70</pages>
        <resume>Nous proposons deux stratégies discriminantes d’intégration des mots composés dans un processus réel d’analyse syntaxique : (i) pré-segmentation lexicale avant analyse, (ii) post-segmentation lexicale après analyse au moyen d’un réordonnanceur. Le segmenteur de l’approche (i) se fonde sur un modèle CRF et permet d’obtenir un reconnaisseur de mots composés état-de-l’art. Le réordonnanceur de l’approche (ii) repose sur un modèle MaxEnt intégrant des traits dédiés aux mots composés. Nous montrons que les deux approches permettent de combler jusqu’à 18% de l’écart entre un analyseur baseline et un analyseur avec segmentation parfaite et jusqu’à 25% pour la reconnaissance des mots composés.</resume>
        <mots_cles>Mots composés, analyse syntaxique, champs markoviens aléatoires, réordonnanceur</mots_cles>
        <title>Recognition of compound words tested against parsing and vice-versa : evaluation of two discriminative approaches</title>
        <abstract>We propose two discriminative strategies to integrate compound word recognition in a real parsing context : (i) state-of-the-art compound pregrouping with Conditional Random Fields before parsing, (ii) reranking parses with features dedicated to compounds after parsing. We show that these two approaches help reduce up to 18% of the gap between a baseline parser and parser with golden segmentation and up to 25% for compound recognition.</abstract>
        <keywords>Multiword expressions, parsing, Conditional random Fields, reranker</keywords>
      </article>
      <article id="taln-2012-long-006" session="Analyse">
        <auteurs>
          <auteur>
            <nom>Ramadan Alfared</nom>
            <email>Ramadan.Alfared@etu.univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Denis Bechet</nom>
            <email>Denis.Bechet@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Alexander Dikovsky</nom>
            <email>Alexandre.Dikovsky@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LINA, Université de Nantes, 2, rue de la Houssinière, 44000 Nantes</affiliation>
        </affiliations>
        <titre>Calcul des cadres de sous catégorisation des noms déverbaux français (le cas du génitif)</titre>
        <type>long</type>
        <pages>71-84</pages>
        <resume>L’analyse syntaxique fine en dépendances nécessite la connaissance des cadres de souscatégorisation des unités lexicales. Le cas des verbes étant bien étudié, nous nous intéressons dans cet article au cas des noms communs dérivés de verbes. Notre intérêt principal est de calculer le cadre de sous-catégorisation des noms déverbaux à partir de celui du verbe d’origine pour le français. Or, pour ce faire il faut disposer d’une liste représentative de noms déverbaux français. Pour calculer cette liste nous utilisons un algorithme simplifié de repérage des noms déverbaux, l’appliquons à un corpus et comparons la liste obtenue avec la liste Verbaction des déverbaux exprimant l’action ou l’activité du verbe. Pour les noms déverbaux ainsi obtenus et attestés ensuite par une expertise linguistique, nous analysons la provenance des groupes prépositionnels subordonnés des déverbaux dans des contextes différents en tenant compte du verbe d’origine. L’analyse est effectuée sur le corpus Paris 7 et est limitée au cas le plus fréquent du génitif, c’est-à-dire des groupes prépositionnels introduits par de, des, etc.</resume>
        <mots_cles>nom déverbal, cadre de sous-catégorisation, groupe prépositionnel, analyse en dépendances</mots_cles>
        <title>On Computing Subcategorization Frames of French Deverbal Nouns (Case of Genitive)</title>
        <abstract>Fine dependency analysis needs exact information on the subcategoriziation frames of lexical units. These frames being well studied for the verbs, we are interested in this paper by the case of the noun deverbals. Our main goal is to calculate the subcategoriziation frame of deverbals in French from that of the source verb. However, this task needs a representative list of French deverbal nouns. To obtain such a list, we use a simplified algorithm detecting deverbal nouns in texts. The obtained list attested by linguists is compared with the existing list Verbaction of deverbals expressing the action/activity of French verbs. For these deverbal nouns, we analyse the origin of their subordinate prepositional phrases in different contexts relative to their source verbs. This analysis is carried out over the corpus Paris 7 and is limited to the most frequent cases of the genitive, i.e. to the prepositional phrases headed by the prepositions de, des, etc.</abstract>
        <keywords>Deverbal Noun, Subcategorization Frame, Prepositional Phrase, Dependency Tree</keywords>
      </article>
      <article id="taln-2012-long-007" session="Session Commune JEP-TALN">
        <auteurs>
          <auteur>
            <nom>Vincent Claveau</nom>
            <email>vincent.claveau@irisa.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">IRISA-CNRS, Campus de Beaulieu, 35042 Rennes, France</affiliation>
        </affiliations>
        <titre>Vectorisation, Okapi et calcul de similarité pour le TAL : pour oublier enfin le TF-IDF</titre>
        <type>long</type>
        <pages>85-98</pages>
        <resume>Dans cette prise de position, nous nous intéressons au calcul de similarité (ou distances) entre textes, problématique présente dans de nombreuses tâches de TAL. Nous nous efforçons de montrer que ce qui n’est souvent qu’un composant dans des systèmes plus complexes est parfois négligé et des solutions sous-optimales sont employées. Ainsi, le calcul de similarité par TF-IDF/cosinus est souvent présenté comme « état-de-l’art », alors que des alternatives souvent plus performantes sont employées couramment dans le domaine de la Recherche d’Information (RI). Au travers de quelques expériences concernant plusieurs tâches, nous montrons combien ce simple calcul de similarité peut influencer les performances d’un système. Nous considérons plus particulièrement deux alternatives. La première est le schéma de pondération Okapi-BM25, bien connu en RI et directement interchangeable avec le TF-IDF. L’autre, la vectorisation, est une technique de calcul de similarité que nous avons développée et qui offrent d’intéressantes propriétés.</resume>
        <mots_cles>Calcul de similarité, modèle vectoriel, TF-IDF, Okapi BM-25, vectorisation</mots_cles>
        <title>Vectorization, Okapi and computing similarity for NLP : say goodbye to TF-IDF</title>
        <abstract>In this position paper, we review a problem very common for many NLP tasks: computing similarity (or distances) between texts. We aim at showing that what is often considered as a small component in a broader complex system is very often overlooked, leading to the use of sub-optimal solutions. Indeed, computing similarity with TF-IDF weighting and cosine is often presented as “state-of-theart”, while more effective alternatives are in the Information Retrieval (IR) community. Through some experiments on several tasks, we show how this simple calculation of similarity can influence system performance. We consider two particular alternatives. The first is the weighting scheme Okapi-BM25, well known in IR and directly interchangeable with TF-IDF. The other, called vectorization, is a technique for calculating text similarities that we have developed which offers some interesting properties.</abstract>
        <keywords>Calculating similarities, vector space model, TF-IDF, Okapi BM-25, vectorization</keywords>
      </article>
      <article id="taln-2012-long-008" session="Session Commune JEP-TALN">
        <auteurs>
          <auteur>
            <nom>Christophe Benzitoun</nom>
            <email>christophe.benzitoun@atilf.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Karën Fort</nom>
            <email>karen.fort@inist.fr</email>
            <affiliationId>2</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Benoît Sagot</nom>
            <email>benoit.sagot@inria.fr</email>
            <affiliationId>4</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">ATILF, Nancy Université &amp; CNRS, 44, avenue de la Libération, BP 30687, 54063 Nancy cedex</affiliation>
          <affiliation affiliationId="2">INIST-CNRS, 2 allée de Brabois, 54500 Vandoeuvre-lès-Nancy</affiliation>
          <affiliation affiliationId="3">LIPN, Université Paris 13 &amp; CNRS, 99 av. J.B. Clément, 93430 Villetaneuse</affiliation>
          <affiliation affiliationId="4">Alpage, INRIA Paris-Rocquencourt &amp; Université Paris 7, Rocquencourt, France</affiliation>
        </affiliations>
        <titre>TCOF-POS : un corpus libre de français parlé annoté en morphosyntaxe</titre>
        <type>long</type>
        <pages>99-112</pages>
        <resume>Nous présentons dans cet article un travail portant sur la création d’un corpus de français parlé spontané annoté en morphosyntaxe. Nous détaillons la méthodologie suivie afin d’assurer le contrôle de la qualité de la ressource finale. Ce corpus est d’ores et déjà librement diffusé pour la recherche et peut servir aussi bien de corpus d’apprentissage pour des logiciels que de base pour des descriptions linguistiques. Nous présentons également les résultats obtenus par deux étiqueteurs morphosyntaxiques entrainés sur ce corpus.</resume>
        <mots_cles>Etiquetage morpho-syntaxique, français parlé, ressources langagières</mots_cles>
        <title>TCOF-POS : A Freely Available POS-Tagged Corpus of Spoken French</title>
        <abstract>This article details the creation of TCOF-POS, the first freely available corpus of spontaneous spoken French. We present here the methodology that was followed in order to obtain the best possible quality in the final resource. This corpus already is freely available and can be used as a training/validation corpus for NLP tools, as well as a study corpus for linguistic research. We also present the results obtained by two POS-taggers trained on the corpus.</abstract>
        <keywords>Etiquetage morpho-syntaxique, français parlé, ressources langagières</keywords>
      </article>
      <article id="taln-2012-long-009" session="Alignement">
        <auteurs>
          <auteur>
            <nom>Adrien Lardilleux</nom>
            <email>adrien.lardilleux@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>François Yvon</nom>
            <email>francois.yvon@limsi.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Yves Lepage</nom>
            <email/>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIMSI-CNRS</affiliation>
          <affiliation affiliationId="2">Université Paris-Sud</affiliation>
          <affiliation affiliationId="3">Université Waseda, Japon</affiliation>
        </affiliations>
        <titre>Alignement sous-phrastique hiérarchique avec Anymalign</titre>
        <type>long</type>
        <pages>113-126</pages>
        <resume>Nous présentons un algorithme d’alignement sous-phrastique permettant d’aligner très facilement un couple de phrases à partir d’une matrice d’alignement pré-remplie. Cet algorithme s’inspire de travaux antérieurs sur l’alignement par segmentation binaire récursive ainsi que de travaux sur le clustering de documents. Nous évaluons les alignements produits sur des tâches de traduction automatique et montrons qu’il est possible d’atteindre des résultats du niveau de l’état de l’art, affichant des gains très conséquents allant jusqu’à plus de 4 points BLEU par rapport à nos travaux antérieurs, à l’aide une méthode très simple, indépendante de la taille du corpus à traiter, et produisant directement des alignements symétriques. En utilisant cette méthode en tant qu’extension à l’outil d’extraction de traductions Anymalign, nos expériences nous permettent de cerner certaines limitations de ce dernier et de définir des pistes pour son amélioration.</resume>
        <mots_cles>corpus parallèle, alignement sous-phrastique, traduction automatique statistique</mots_cles>
        <title>Hierarchical sub-sentential alignment with Anymalign</title>
        <abstract>We present a sub-sentential alignment algorithm that aligns sentence pairs from an existing alignment matrix in a very easy way. This algorithm is inspired by previous work on alignment by recursive binary segmentation and on document clustering. We evaluate the alignments produced on machine translation tasks and show that we can obtain state-of-the-art results, with gains up to more than 4 BLEU points compared to our previous work, with a method that is very simple, independent of the size of the corpus to be aligned, and can directly produce symmetric alignments. When using this method as an extension of the translation extraction tool Anymalign, our experiments allow us to determine some of its limitations and to define possible leads for further improvements.</abstract>
        <keywords>parallel corpus, sub-sentential alignment, statistical machine translation</keywords>
      </article>
      <article id="taln-2012-long-010" session="Alignement">
        <auteurs>
          <auteur>
            <nom>Saadane Houda</nom>
            <email>houda.saadane@e.u-grenoble3.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Semmar Nasredine</nom>
            <email>nasredine.semmar@cea.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIDILEM, Université de Grenoble, 38400 Grenoble Cedex 9</affiliation>
          <affiliation affiliationId="2">CEA, LIST, Laboratoire Vision et Ingénierie des Contenus, 91191 Gif-sur-Yvette Cedex</affiliation>
        </affiliations>
        <titre>Utilisation de la translittération arabe pour l’amélioration de l’alignement de mots à partir de corpus parallèles français-arabe</titre>
        <type>long</type>
        <pages>127-140</pages>
        <resume>Dans cet article, nous nous intéressons à l’utilisation de la translittération arabe pour l’amélioration des résultats d’une approche linguistique d’alignement de mots simples et composés à partir de corpus de textes parallèles français-arabe. Cette approche utilise, d’une part, un lexique bilingue et les caractéristiques linguistiques des entités nommées et des cognats pour l’alignement de mots simples, et d’autre part, les relations de dépendance syntaxique pour aligner les mots composés. Nous avons évalué l’aligneur de mots simples et composés intégrant la translittération arabe en utilisant deux procédés : une évaluation de la qualité d’alignement à l’aide d’un alignement de référence construit manuellement et une évaluation de l’impact de cet alignement sur la qualité de la traduction en faisant appel au système de traduction automatique statistique Moses. Les résultats obtenus montrent que la translittération améliore aussi bien la qualité de l’alignement que celle de la traduction.</resume>
        <mots_cles>Translittération, alignement de mots, construction de dictionnaires multilingues, traduction automatique, recherche d’information interlingue</mots_cles>
        <title>Using Arabic transliteration to improve word alignment from French-Arabic parallel corpora</title>
        <abstract>In this paper, we focus on the use of Arabic transliteration to improve the results of a linguistic word alignment approach from parallel text corpora. This approach uses, on the one hand, a bilingual lexicon, named entity and cognates linguistic properties to align single words, and on the other hand, syntactic dependency relations to align compound words. We have evaluated the word aligner integrating Arabic transliteration using two methods: A manual evaluation of the alignment quality and an evaluation of the impact of this alignment on the translation quality by using the statistical machine translation system Moses. The obtained results show that Arabic transliteration improves the quality of both alignment and translation.</abstract>
        <keywords>Transliteration, word alignment, multilingual lexicons construction, machine translation, cross-language information retrieval</keywords>
      </article>
      <article id="taln-2012-long-011" session="Alignement">
        <auteurs>
          <auteur>
            <nom>Emmanuel Morin</nom>
            <email>emmanuel.morin@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Béatrice Daille</nom>
            <email>beatrie.daille@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université de Nantes, LINA UMR CNRS 6241, 2, rue de la Houssinière, BP 92208, F-44322 Nantes cedex 03</affiliation>
        </affiliations>
        <titre>Compositionnalité et contextes issus de corpus comparables pour la traduction terminologique</titre>
        <type>long</type>
        <pages>141-154</pages>
        <resume>Dans cet article, nous cherchons à mettre en correspondance de traduction des termes extraits de chaque partie monolingue d’un corpus comparable. Notre objectif concerne l’identification et la traduction de termes spécialisés. Pour ce faire, nous mettons en oeuvre une approche compositionnelle dopée avec des informations contextuelles issues du corpus comparable. Notre évaluation montre que cette approche améliore significativement l’approche compositionnelle de base pour la traduction de termes complexes extraits de corpus comparables.</resume>
        <mots_cles>Corpus comparable, compositionnalité, information contextuelle, lexique bilingue</mots_cles>
        <title>Compositionality and Context for Bilingual Lexicon Extraction from Comparable Corpora</title>
        <abstract>In this article, we study the possibilities of improving the alignment of equivalent terms monolingually acquired from bilingual comparable corpora. Our overall objective is to identify and to translate highly specialised terminology. We applied a compositional approach enhanced with pre-processed context information. Our evaluation demonstrates that our alignment method outperforms the compositional approach for translationally equivalent term discovery from comparable corpora.</abstract>
        <keywords>Comparable Corpora, compositionality, context information, bilingual lexicon</keywords>
      </article>
      <article id="taln-2012-long-012" session="Lexique">
        <auteurs>
          <auteur>
            <nom>Paul Bédaride</nom>
            <email>paul.bedaride@gmail.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université de Stuttgart</affiliation>
        </affiliations>
        <titre>Raffinement du Lexique des Verbes Français</titre>
        <type>long</type>
        <pages>155-168</pages>
        <resume>Nous présentons dans cet article les améliorations apportées à la ressource « Les Verbes Français » afin de la rendre plus formelle et utilisable pour le traitement automatique des langues naturelles. Les informations syntaxiques et sémantiques ont été corrigées, restructurées, unifiées puis intégrées à la version XML de cette ressource, afin de pouvoir être utilisée par un système d’étiquetage de rôles sémantiques.</resume>
        <mots_cles>ressource, lexique, verbes, raffinement, étiquetage de rôles sémantiques</mots_cles>
        <title>Resource Refining : « Les Verbes Français »</title>
        <abstract>This paper introduce the impovements we made to the resource « Les Verbes Français » in order to make it more usable in the field of natural language processing. Syntactic and semantic information is corrected, restructured, unified and then integrated to the XML version of this resource, in order to be used by a semantic role labelling system.</abstract>
        <keywords>resource, lexicon, verbs, refinement, semantic roles labeling</keywords>
      </article>
      <article id="taln-2012-long-013" session="Lexique">
        <auteurs>
          <auteur>
            <nom>François Morlane-Hondère</nom>
            <email>francois.morlane@univ-tlse2.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Cécile Fabre</nom>
            <email>cecile.fabre@univ-tlse2.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CLLE-ERSS, Université de Toulouse - Le Mirail, 5, allées Antonio Machado - Toulouse Cedex 9</affiliation>
        </affiliations>
        <titre>Étude des manifestations de la relation de méronymie dans une ressource distributionnelle</titre>
        <type>long</type>
        <pages>169-182</pages>
        <resume>Cette étude vise à étudier les manifestations de la relation de méronymie dans une ressource lexicale générée automatiquement à partir d’un corpus de langue générale. La démarche que nous adoptons consiste à recueillir un jeu de couples de méronymes issus d’une ressource externe que nous croisons avec une base distributionnelle calculée à partir d’un corpus de textes encyclopédiques. Une annotation sémantique des mots qui entrent dans ces couples de méronymes montre que la prise en compte de la nature sémantique des mots composant les couples de méronymes permet de mettre au jour des inégalités au niveau du repérage de la relation par la méthode d’analyse distributionnelle.</resume>
        <mots_cles>analyse distributionnelle, sémantique lexicale, méronymie, évaluation</mots_cles>
        <title>Study of meronymy in a distribution-based lexical resource</title>
        <abstract>In this paper, we study the way meronymy behaves in a distribution-based lexical resource. We address the question of the evaluation of such resources through a semantic-based approach. Our method consists in collecting meronyms from a resource which we cross with a distributionbased lexical resource made from an encyclopedic corpus. Meronyms are then sub-categorized manually : firstly following the sub-relation they bear (STUFF/OBJECT, MEMBER/COLLECTION, etc.), then following the semantic class of their members. Results show that distributional analysis identifies meronymic relations in different proportions according to the semantic classes of the words involved in the meronymic pairs.</abstract>
        <keywords>distributional analysis, lexical semantics, meronymy, evaluation</keywords>
      </article>
      <article id="taln-2012-long-014" session="Lexique">
        <auteurs>
          <auteur>
            <nom>Clément de Groc</nom>
            <email>cdegroc@limsi.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Xavier Tannier</nom>
            <email>xtannier@limsi.fr</email>
            <affiliationId>2</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Claude de Loupy</nom>
            <email>loupy@syllabs.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Syllabs, 15 rue Jean-Baptiste Berlier, 75013 Paris</affiliation>
          <affiliation affiliationId="2">Univ. Paris-Sud, 91403 Orsay Cedex</affiliation>
          <affiliation affiliationId="3">LIMSI-CNRS, B.P. 133, 91403 Orsay Cedex</affiliation>
        </affiliations>
        <titre>Un critère de cohésion thématique fondé sur un graphe de cooccurrences</titre>
        <type>long</type>
        <pages>183-195</pages>
        <resume>Dans cet article, nous définissons un nouveau critère de cohésion thématique permettant de pondérer les termes d’un lexique thématique en fonction de leur pertinence. Le critère s’inspire des approches Web as corpus pour accumuler des connaissances exogènes sur un lexique. Ces connaissances sont ensuite modélisées sous forme de graphe et un algorithme de marche aléatoire est appliqué pour attribuer un score à chaque terme. Après avoir étudié les performances et la stabilité du critère proposé, nous l’évaluons sur une tâche d’aide à la création de lexiques bilingues.</resume>
        <mots_cles>Cohésion thématique, graphe de cooccurrences, marche aléatoire</mots_cles>
        <title>Topical Cohesion using Graph Random Walks</title>
        <abstract>In this article, we propose a novel metric to weight specialized lexicons terms according to their relevance to the underlying thematic. Our method is inspired by Web as corpus approaches and accumulates exogenous knowledge about a specialized lexicon from the web. Terms cooccurrences are modelled as a graph and a random walk algorithm is applied to compute terms relevance. Finally, we study the performance and stability of the metric and evaluate it in a bilingual lexicon creation context.</abstract>
        <keywords>Thematic relevance, cooccurrence graph, random walk</keywords>
      </article>
      <article id="taln-2012-long-015" session="Réécriture">
        <auteurs>
          <auteur>
            <nom>Houda Bouamor</nom>
            <email>houda.bouamor@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Aurélien Max</nom>
            <email>aurelien.max@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Gabriel Illouz</nom>
            <email>gabriel.illouz@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Anne Vilnat</nom>
            <email>anne.vilnat@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIMSI-CNRS, Univ. Paris Sud 11, Orsay, France</affiliation>
        </affiliations>
        <titre>Validation sur le Web de reformulations locales: application à la Wikipédia</titre>
        <type>long</type>
        <pages>197-210</pages>
        <resume>Ce travail présente des expériences initiales en validation de paraphrases en contexte. Les révisions de Wikipédia nous servent de domaine d’évaluation : pour un énoncé ayant connu une courte révision dans l’encyclopédie, nous disposons d’un ensemble de réécritures possibles, parmi lesquelles nous cherchons à identifier celles qui correspondent à des paraphrases valides. Nous abordons ce problème comme une tâche de classification fondée sur des informations issues du Web, et parvenons à améliorer la performance de plusieurs techniques simples de référence.</resume>
        <mots_cles>paraphrase, Wikipédia, aide à la rédaction</mots_cles>
        <title>Assisted rephrasing for Wikipedia contributors through Web-based validation</title>
        <abstract>This works describes initial experiments on the validation of paraphrases in context. Wikipedia’s revisions are used : we assume that a set of possible rewritings are available for a given phrase that has been rewritten in the encyclopedia’s revision history, and we attempt to find the subset of those rewritings that can be considered as valid paraphrases. We tackle this problem as a classication task which we provide with features obtained from Web data. Our experiments show that our system improves performance over a set of simple baselines.</abstract>
        <keywords>paraphrasing, Wikipedia, authoring aids</keywords>
      </article>
      <article id="taln-2012-long-016" session="Réécriture">
        <auteurs>
          <auteur>
            <nom>Laetitia Brouwers</nom>
            <email/>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Delphine Bernhard</nom>
            <email/>
            <affiliationId>1</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Anne-Laure Ligozat</nom>
            <email/>
            <affiliationId>1</affiliationId>
            <affiliationId>4</affiliationId>
          </auteur>
          <auteur>
            <nom>Thomas François</nom>
            <email/>
            <affiliationId>2</affiliationId>
            <affiliationId>5</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIMSI-CNRS, 91403 Orsay, France</affiliation>
          <affiliation affiliationId="2">Université catholique de Louvain, Belgique</affiliation>
          <affiliation affiliationId="3">LiLPa, Université de Strasbourg, France</affiliation>
          <affiliation affiliationId="4">ENSIIE, Evry, France</affiliation>
          <affiliation affiliationId="5">University of Pennsylvania, USA</affiliation>
        </affiliations>
        <titre>Simplification syntaxique de phrases pour le français</titre>
        <type>long</type>
        <pages>211-224</pages>
        <resume>Cet article présente une méthode de simplification syntaxique de textes français. La simplification syntaxique a pour but de rendre des textes plus abordables en simplifiant les éléments qui posent problème à la lecture. La méthode mise en place à cette fin s’appuie tout d’abord sur une étude de corpus visant à étudier les phénomènes linguistiques impliqués dans la simplification de textes en français. Nous avons ainsi constitué un corpus parallèle à partir d’articles de Wikipédia et Vikidia, ce qui a permis d’établir une typologie de simplifications. Dans un second temps, nous avons implémenté un système qui opère des simplifications syntaxiques à partir de ces observations. Des règles de simplification ont été décrites afin de générer des phrases simplifiées. Un module sélectionne ensuite le meilleur ensemble de phrases. Enfin, nous avons mené une évaluation de notre système montrant qu’environ 80% des phrases générées sont correctes.</resume>
        <mots_cles>simplification automatique, lisibilité, analyse syntaxique</mots_cles>
        <title>Syntactic Simplification for French Sentences</title>
        <abstract>This paper presents a method for the syntactic simplification of French texts. Syntactic simplification aims at making texts easier to understand by simplifying the elements that hinder reading. It is based on a corpus study that aimed at investigating the linguistic phenomena involved in the manual simplification of French texts. We have first gathered a parallel corpus of articles from Wikipedia and Vikidia, that we used to establish a typology of simplifications. In a second step, we implemented a system that carries out syntactic simplifications based on these corpus observations. We described simplification rules in order to generate simplified sentences. A module subsequently selects the best subset of sentences. The evaluation of our system shows that about 80% of the sentences produced by our system are accurate.</abstract>
        <keywords>automatic simplification, readability, syntactic analysis</keywords>
      </article>
      <article id="taln-2012-long-017" session="Réécriture">
        <auteurs>
          <auteur>
            <nom>Iskandar Keskes</nom>
            <email>Keskes@irit.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Mohamed Mahdi Boudabous</nom>
            <email>mehdiboudabous@gmail.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Mohamed Hédi Maâloul</nom>
            <email>mohamed.maaloul@lpl-aix.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Lamia Hadrich Belguith</nom>
            <email>l.belguith@fsegs.rnu.tn</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">ANLP Research Group, Laboratoire MIRACL, Route de Tunis Km 10, BP 242, Sfax, Tunisie</affiliation>
          <affiliation affiliationId="2">Laboratoire IRIT, 118 Route de Narbonne, F-31062 Toulouse Cedex 9, France</affiliation>
          <affiliation affiliationId="3">Laboratoire LPL, 5 avenue Pasteur, BP 80975, 13604 Aix-en-Provence, France</affiliation>
        </affiliations>
        <titre>Étude comparative entre trois approches de résumé automatique de documents arabes</titre>
        <type>long</type>
        <pages>225-238</pages>
        <resume>Dans cet article, nous proposons une étude comparative entre trois approches pour le résumé automatique de documents arabes. Ainsi, nous avons proposé trois méthodes pour l’extraction des phrases les plus représentatives d'un document. La première méthode se base sur une approche symbolique, la deuxième repose sur une approche numérique et la troisième se base sur une approche hybride. Ces méthodes sont implémentées respectivement par le système ARSTResume, le système R.I.A et le système HybridResume. Nous présentons, par la suite, les résultats obtenus par les trois systèmes et nous procédons à une étude comparative entre les résultats obtenus afin de souligner les avantages et les limites de chaque méthode. Les résultats de l’évaluation ont montré que l‘approche numérique est plus performante que l’approche symbolique au niveau des textes longs. Mais, l’intégration de ces deux approches en une approche hybride aboutit aux résultats les plus performants dans notre corpus de textes.</resume>
        <mots_cles>Résumé automatique, approche symbolique, approche numérique, approche hybride, document arabe</mots_cles>
        <title>Comparative study of three approaches to automatic summarization of Arabic documents</title>
        <abstract>In this paper, we propose a comparative study between three approaches for automatic summarization of Arabic documents. Thus, we proposed three methods for extracting most representative sentences of a document. The first method is based on a symbolic approach, the second is relied on a numerical approach and the third is based on a hybrid approach. These methods are implemented respectively by the ARSTResume, R.I.A and HybridResume systems. Then, we present the results obtained by the three systems and we conduct a comparative study between the obtained results in order to highlight the advantages and limitations of each method. The evaluation results showed that the numerical approach has better performances than the symbolic approach. But, combining into a hybrid approach achieved the best results for our text corpus.</abstract>
        <keywords>Automatic summarization, symbolic approach, numerical approach, hybrid approach, Arabic document</keywords>
      </article>
      <article id="taln-2012-long-018" session="Exploitation de corpus">
        <auteurs>
          <auteur>
            <nom>Ann Bertels</nom>
            <email>ann.bertels@ilt.kuleuven.be</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Dirk De Hertog</nom>
            <email>dirk.dehertog@arts.kuleuven.be</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Kris Heylen</nom>
            <email>kris.heylen@arts.kuleuven.be</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">ILT, KU Leuven, Dekenstraat 6, B-3000 Leuven (Belgique)</affiliation>
          <affiliation affiliationId="2">QLVL, KU Leuven, Faculty of Arts, Blijde-Inkomststraat 21, B-3000 Leuven (Belgique)</affiliation>
        </affiliations>
        <titre>Etude sémantique des mots-clés et des marqueurs lexicaux stables dans un corpus technique</titre>
        <type>long</type>
        <pages>239-252</pages>
        <resume>Cet article présente les résultats d’une analyse sémantique quantitative des unités lexicales spécifiques dans un corpus technique, relevant du domaine des machines-outils pour l’usinage des métaux. L’étude vise à vérifier si et dans quelle mesure les mots-clés du corpus technique sont monosémiques. A cet effet, nous procédons à une analyse statistique de régression simple, qui permet d’étudier la corrélation entre le rang de spécificité des mots-clés et leur rang de monosémie, mais qui soulève des problèmes statistiques et méthodologiques, notamment un biais de fréquence. Pour y remédier, nous adoptons une approche alternative pour le repérage des unités lexicales spécifiques, à savoir l’analyse des marqueurs lexicaux stables ou Stable Lexical Marker Analysis (SLMA). Nous discutons les résultats quantitatifs et statistiques de cette approche dans la perspective de la corrélation entre le rang de spécificité et le rang de monosémie.</resume>
        <mots_cles>unités lexicales spécifiques, analyse des mots-clés, analyse des marqueurs lexicaux stables, sémantique quantitative, analyse de régression</mots_cles>
        <title>Semantic analysis of keywords and stable lexical markers in a technical corpus</title>
        <abstract>This article presents the results of a quantitative semantic analysis of typical lexical units in a specialised technical corpus of metalworking machinery in French. The study aims to find out whether and to what extent the keywords of the technical corpus are monosemous. A simple regression analysis, used to examine the correlation between typicality rank and monosemy rank of the keywords, points out some statistical and methodological problems, notably a frequency bias. In order to overcome these problems, we adopt an alternative approach for the identification of typical lexical units, called Stable Lexical Marker Analysis (SLMA). We discuss the quantitative and statistical results of this approach with respect to the correlation between typicality rank and monosemy rank.</abstract>
        <keywords>typical lexical units, Keyword Analysis, Stable Lexical Marker Analysis (SLMA), quantitative semantics, regression analysis</keywords>
      </article>
      <article id="taln-2012-long-019" session="Exploitation de corpus">
        <auteurs>
          <auteur>
            <nom>Solen Quiniou</nom>
            <email>solen.quiniou@unicaen.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Peggy Cellier</nom>
            <email>peggy.cellier@irisa.fr</email>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Thierry Charnois</nom>
            <email>thierry.charnois@unicaen.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Dominique Legallois</nom>
            <email>dominique.legallois@unicaen.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">GREYC Université de Caen Basse-Normandie, Campus 2, 14000 Caen</affiliation>
          <affiliation affiliationId="2">CRISCO Université de Caen Basse-Normandie, Campus 1, 14000 Caen</affiliation>
          <affiliation affiliationId="3">IRISA-INSA de Rennes, Campus de Beaulieu, 35042 Rennes Cedex</affiliation>
        </affiliations>
        <titre>Fouille de graphes sous contraintes linguistiques pour l'exploration de grands textes</titre>
        <type>long</type>
        <pages>253-266</pages>
        <resume>Dans cet article, nous proposons une approche pour explorer des textes de taille importante en mettant en évidence des sous-parties cohérentes. Cette méthode d’exploration s’appuie sur une représentation en graphe du texte, en utilisant le modèle linguistique de Hoey pour sélectionner et apparier les phrases dans le graphe. Notre contribution porte sur l’utilisation de techniques de fouille de graphes sous contraintes pour extraire des sous-parties pertinentes du texte (c’est-à-dire des collections de sous-réseaux phrastiques homogènes). Nous avons réalisé des expérimentations sur deux textes anglais de taille conséquente pour montrer l’intérêt de l’approche que nous proposons.</resume>
        <mots_cles>Fouille de graphes, réseaux phrastiques, analyse textuelle, navigation textuelle</mots_cles>
        <title>Graph Mining Under Linguistic Constraints to Explore Large Texts</title>
        <abstract>In this paper, we propose an approach to explore large texts by highlighting coherent sub-parts. The exploration method relies on a graph representation of the text according to the Hoey linguistic model which allows the selection and the binding of sentences in the graph. Our contribution relates to using graph mining techniques under constraints to extract relevant subparts of the text (i.e., collections of homogeneous sentence sub-networks). We have conducted some experiments on two large English texts to show the interest of the proposed approach.</abstract>
        <keywords>Graph Mining, sentence networks, textual analysis, textual navigation</keywords>
      </article>
      <article id="taln-2012-long-020" session="Exploitation de corpus">
        <auteurs>
          <auteur>
            <nom>Houda Bouamor</nom>
            <email>houda.bouamor@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Aurélien Max</nom>
            <email>aurelien.max@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Anne Vilnat</nom>
            <email>anne.vilnat@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIMSI-CNRS, Univ. Paris-Sud, Orsay, France</affiliation>
        </affiliations>
        <titre>Une étude en 3D de la paraphrase: types de corpus, langues et techniques</titre>
        <type>long</type>
        <pages>267-280</pages>
        <resume>Cet article présente une étude détaillée de l’impact du type du corpus sur la tâche d’acquisition de paraphrases sous-phrastiques. Nos expériences sont menées sur deux langues et quatre types de corpus, et incluent une combinaison efficace de quatre systèmes d’acquisition de paraphrases. Nous obtenons une amélioration relative de plus de 27% en F-mesure par rapport au meilleur système, en anglais et en français, ainsi qu’une amélioration relative à notre combinaison de systèmes de 22% pour l’anglais et de 5% pour le français quand tous les types de corpus sont utilisés pour l’acquisition depuis le type de corpus le plus couramment disponible.</resume>
        <mots_cles>acquisition de paraphrases, constitution de corpus</mots_cles>
        <title>A study of paraphrase along 3 dimensions : corpus types, languages and techniques</title>
        <abstract>In this paper, we report a detailed study of the impact of corpus type on the task of sub-sentential paraphrase acquisition. Our experiments are for 2 languages and 4 corpus types, and involve an efficient machine learning-based combination of 4 paraphrase acquisition systems. We obtain relative improvements of more than 27% in F-measure over the best individual system on English and French, and obtain a relative improvement over the combination system of 22% for English and 5% for French when using all other corpus types as additional training data for our most readily available corpus type.</abstract>
        <keywords>paraphrase acquisition, corpus collection</keywords>
      </article>
      <article id="taln-2012-long-021" session="Banques d’arbres">
        <auteurs>
          <auteur>
            <nom>Florian Boudin</nom>
            <email>florian.boudin@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Nicolas Hernandez</nom>
            <email>nicolas.hernandez@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université de Nantes</affiliation>
        </affiliations>
        <titre>Détection et correction automatique d'erreurs d'annotation morpho-syntaxique du French TreeBank</titre>
        <type>long</type>
        <pages>281-291</pages>
        <resume>La qualité de l’annotation morpho-syntaxique d’un corpus est déterminante pour l’entraînement et l’évaluation de méthodes d’étiquetage. Cet article présente une série d’expériences que nous avons menée sur la détection et la correction automatique des erreurs du French Treebank. Deux méthodes sont utilisées. La première consiste à identifier les mots sans étiquette et leur attribuer celle d’une forme correspondante observée dans le corpus. La seconde méthode utilise les variations de n-gramme pour détecter et corriger les anomalies d’annotation. L’évaluation des corrections apportées au corpus est réalisée de manière extrinsèque en comparant les scores de performance de différentes méthodes d’étiquetage morpho-syntaxique en fonction du niveau de correction. Les résultats montrent une amélioration significative de la précision et indiquent que la qualité du corpus peut être sensiblement améliorée par l’application de méthodes de correction automatique des erreurs d’annotation.</resume>
        <mots_cles>Étiquetage morpho-syntaxique, correction automatique, qualité d’annotation</mots_cles>
        <title>Detecting and correcting POS annotation in the French TreeBank</title>
        <abstract>The quality of the Part-Of-Speech (POS) annotation in a corpus has a large impact on training and evaluating POS taggers. In this paper, we present a series of experiments that we have conducted on automatically detecting and correcting annotation errors in the French TreeBank. Two methods are used. The first simply relies on identifying tokens with missing tags and correct them by assigning the tag the same token observed in the corpus. The second method uses n-gram variations to detect and correct conflicting annotations. The evaluation of the automatic correction is performed extrinsically by comparing the performance of different POS taggers in relation to the level of correction. Results show a statistically significant improvement in precision and indicate that the POS annotation quality can be noticeably enhanced by using automatic correction methods.</abstract>
        <keywords>Part-Of-Speech tagging, automatic correction, annotation quality</keywords>
      </article>
      <article id="taln-2012-long-022" session="Banques d’arbres">
        <auteurs>
          <auteur>
            <nom>Bruno Guillaume</nom>
            <email>bruno.guillaume@loria.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Guy Perrier</nom>
            <email>guy.perrier@loria.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LORIA - Campus Scientifique - BP 239 - 54506 Vandoeuvre-lès-Nancy cedex</affiliation>
          <affiliation affiliationId="2">INRIA Grand Est - 615, rue du Jardin Botanique - 54600 Villers-lès-Nancy</affiliation>
          <affiliation affiliationId="3">Université de Lorraine - 34, cours Léopold - CS 25233 - 54502 Nancy cedex</affiliation>
        </affiliations>
        <titre>Annotation sémantique du French Treebank à l’aide de la réécriture modulaire de graphes</titre>
        <type>long</type>
        <pages>293-306</pages>
        <resume>Nous proposons d’annoter le French Treebank à l’aide de dépendances sémantiques dans le cadre de la DMRS en partant d’une annotation en dépendances syntaxiques de surface et en utilisant la réécriture modulaire de graphes. L’article présente un certain nombre d’avancées concernant le calcul de réécriture utilisé : l’utilisation de règles pour faire le lien avec des lexiques, en particulier le lexique des verbes de Dicovalence, et l’introduction de filtres pour écarter à certaines étapes les annotations incohérentes. Il présente aussi des avancées dans le système de réécriture lui-même, qui a une plus large couverture (constructions causatives, verbes à montée, . . .) et dont l’ordre des modules a été étudié de façon plus systématique. Ce système a été expérimenté sur l’ensemble du French Treebank à l’aide du prototype GREW, qui implémente le calcul de réécriture utilisé.</resume>
        <mots_cles>réécriture de graphes, interface syntaxe-sémantique, dépendances, DMRS</mots_cles>
        <title>Semantic Annotation of the French Treebank using Modular Graph Rewriting</title>
        <abstract>We propose to annotate the French Treebank with semantic dependencies in the framework of DMRS starting from an annotation with surface syntactic dependencies and using modular graph rewriting. The article presents some new results related to the rewriting calculus: the use of rules to make a link with lexicons, especially with the lexicon of verbs Dicovalence, and the introduction of filters to discard inconsistent annotations at some computation steps. It also presents new results related to the rewriting system itself: the system has a larger coverage (causative constructions, rising verbs, . . .) and the order between modules has been studied in a more systematic way. This system has been experimented on the whole French Treebank with the prototype GREW, which implements the used rewriting calculus.</abstract>
        <keywords>graph rewriting, syntax-semantics interface, dependencies, DMRS</keywords>
      </article>
      <article id="taln-2012-long-023" session="Banques d’arbres">
        <auteurs>
          <auteur>
            <nom>Philippe Blache</nom>
            <email>blache@lpl-aix.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Stéphane Rauzy</nom>
            <email>rauzy@lpl-aix.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LPL, 5 Avenue Pasteur, 13100 Aix-en-Provence</affiliation>
        </affiliations>
        <titre>Enrichissement du FTB : un treebank hybride constituants/propriétés</titre>
        <type>long</type>
        <pages>307-320</pages>
        <resume>Cet article présente les mécanismes de création d’un treebank hybride enrichissant le FTB à l’aide d’annotations dans le formalisme des Grammaires de Propriétés. Ce processus consiste à acquérir une grammaire GP à partir du treebank source et générer automatiquement les structures syntaxiques dans le formalisme cible en s’appuyant sur la spécification d’un schéma d’encodage adapté. Le résultat produit, en partant d’une version du FTB corrigée et modifiée en fonction de nos besoins, constitue une ressource ouvrant de nouvelles perspectives pour le traitement et la description du français.</resume>
        <mots_cles>Treebank hybride, French Treebank, Grammaires de Propriétés</mots_cles>
        <title>Enriching the French Treebank with Properties</title>
        <abstract>We present in this paper the hybridation of the French Treebank with Property Grammars annotations. This process consists in acquiring a PG grammar from the source treebank and generating the new syntactic encoding on top of the original one. The result is a new resource for French, opening the way to new tools and descriptions.</abstract>
        <keywords>Hybrid treebank, French Treebank, Property Grammars</keywords>
      </article>
      <article id="taln-2012-long-024" session="Banques d'arbres">
        <auteurs>
          <auteur>
            <nom>Marie Candito</nom>
            <email>marie.candito@linguist.jussieu.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Djamé Seddah</nom>
            <email>djame.seddah@paris-sorbonne.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Alpage (Univ. Paris Diderot &amp; INRIA), 175 rue du Chevaleret, 75013 Paris, France</affiliation>
          <affiliation affiliationId="2">Univ. Paris Sorbonne, 28, rue Serpente, 75006 Paris, France</affiliation>
        </affiliations>
        <titre>Le corpus Sequoia : annotation syntaxique et exploitation pour l'adaptation d'analyseur par pont lexical</titre>
        <type>long</type>
        <pages>321-334</pages>
        <resume>Nous présentons dans cet article la méthodologie de constitution et les caractéristiques du corpus Sequoia, un corpus en français, syntaxiquement annoté d’après un schéma d’annotation très proche de celui du French Treebank (Abeillé et Barrier, 2004), et librement disponible, en constituants et en dépendances. Le corpus comporte des phrases de quatre origines : Europarl français, le journal l’Est Républicain, Wikipédia Fr et des documents de l’Agence Européenne du Médicament, pour un total de 3204 phrases et 69246 tokens. En outre, nous présentons une application de ce corpus : l’évaluation d’une technique d’adaptation d’analyseurs syntaxiques probabilistes à des domaines et/ou genres autres que ceux du corpus sur lequel ces analyseurs sont entraînés. Cette technique utilise des clusters de mots obtenus d’abord par regroupement morphologique à l’aide d’un lexique, puis par regroupement non supervisé, et permet une nette amélioration de l’analyse des domaines cibles (le corpus Sequoia), tout en préservant le même niveau de performance sur le domaine source (le FTB), ce qui fournit un analyseur multi-domaines, à la différence d’autres techniques d’adaptation comme le self-training.</resume>
        <mots_cles>Corpus arboré, analyse syntaxique statistique, adaptation de domaine</mots_cles>
        <title>The Sequoia corpus : syntactic annotation and use for a parser lexical domain adaptation method</title>
        <abstract>We present the building methodology and the properties of the Sequoia treebank, a freely available French corpus annotated following the French Treebank guidelines (Abeillé et Barrier, 2004). The Sequoia treebank comprises 3204 sentences (69246 tokens), from the French Europarl, the regional newspaper L’Est Républicain, the French Wikipedia and documents from the European Medicines Agency. We then provide a method for parser domain adaptation, that makes use of unsupervised word clusters. The method improves parsing performance on target domains (the domains of the Sequoia corpus), without degrading performance on source domain (the French treenbank test set), contrary to other domain adaptation techniques such as self-training.</abstract>
        <keywords>Treebank, statistical parsing, parser domain adaptation</keywords>
      </article>
      <article id="taln-2012-court-001" session="Poster 1">
        <auteurs>
          <auteur>
            <nom>Francis Brunet-Manquat</nom>
            <email>Francis.Brunet-Manquat@imag.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Jérôme Goulian</nom>
            <email>Jerome.Goulian@imag.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIG-GETALP, Université Pierre Mendès France Grenoble 2</affiliation>
        </affiliations>
        <titre>ACOLAD Plateforme pour l’édition collaborative dépendancielle</titre>
        <type>court</type>
        <pages>335-342</pages>
        <resume>Cet article présente une plateforme open-source pour l’édition collaborative de corpus de dépendances. Cette plateforme, nommée ACOLAD (Annotation de COrpus Linguistique pour l’Analyse de Dépendances), propose des services manuels de segmentation et d’annotation multi-niveaux (segmentation en mots et en syntagmes minimaux (chunks), annotation morphosyntaxique des mots, annotation syntaxique des chunks et annotation syntaxique des dépendances entre mots ou entre chunks). Dans cet article, nous présentons la plateforme ACOLAD, puis nous détaillons la représentation pivot utilisée pour gérer les annotations concurrentes, enfin décrivons le mécanisme d’importation de ressources linguistiques externes.</resume>
        <mots_cles>annotation collaborative de corpus, annotations concurrentes, dépendances</mots_cles>
        <title>ACOLAD: platform for collaborative dependency annotation</title>
        <abstract>This paper presents an open-source platform for collaborative editing dependency corpora. ACOLAD platform (Annotation of corpus linguistics for the analysis of dependencies) offers manual annotation services such as segmentation and multi-level annotation (segmentation into words and phrases minimum (chunks), morphosyntactic annotation of words, syntactic annotation chunks and annotating syntactic dependencies between words or chunks). In this paper, we present ACOLAD platform, then we detail the representation used to manage concurrent annotations, then we describe the mechanism for importing external linguistic resources.</abstract>
        <keywords>corpus collaborative annotation, concurrent annotations, dependencies</keywords>
      </article>
      <article id="taln-2012-court-002" session="Poster 1">
        <auteurs>
          <auteur>
            <nom>Anaïs Cadilhac</nom>
            <email>adilha@irit.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Farah Benamara</nom>
            <email>benamara@irit.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Vladimir Popescu</nom>
            <email>popescu@irit.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Nicholas Asher</nom>
            <email>asher@irit.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Mohamadou Seck</nom>
            <email>seck@irit.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">IRIT, 118, Route de Narbonne, 31062 Toulouse Cedex 9</affiliation>
        </affiliations>
        <titre>Extraction de préférences à partir de dialogues de négociation</titre>
        <type>court</type>
        <pages>343-350</pages>
        <resume>Cet article présente une approche linguistique pour l’extraction d’expressions de préférence à partir de dialogues de négociation. Nous proposons un nouveau schéma d’annotation pour encoder les préférences et les dépendances exprimées linguistiquement dans deux genres de corpus différents. Ensuite, nous proposons une méthode d’apprentissage qui extrait les expressions de préférence en utilisant une combinaison de traits locaux et discursifs. Finalement, nous évaluons la fiabilité de notre approche sur chaque genre de corpus.</resume>
        <mots_cles>Préférence, dialogue, apprentissage automatique</mots_cles>
        <title>Towards Preference Extraction From Negotiation Dialogues</title>
        <abstract>This paper presents an NLP based approach for preference expression extraction from negotiation dialogues. We propose a new annotation schema for preferences and dependencies among them and illustrate on two different corpus genres. We then suggest a learning approach that efficiently extracts preference expressions using a combination of local and discursive features and assess the reliability of our approach on each corpus genre.</abstract>
        <keywords>Preference, dialogue, machine learning</keywords>
      </article>
      <article id="taln-2012-court-003" session="Poster 1">
        <auteurs>
          <auteur>
            <nom>Alexandre Denis</nom>
            <email>denis@loria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Matthieu Quignard</nom>
            <email>matthieu.quignard@univ-lyon2.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Dominique Freard</nom>
            <email>dominique.freard@telecom-paristech.fr</email>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Francoise Detienne</nom>
            <email>francois.detienne@telecom-paristech.fr</email>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Michael Baker</nom>
            <email>michael.baker@telecom-paristech.fr</email>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Flore Barcellini</nom>
            <email>flore.barcellini@cnam.fr</email>
            <affiliationId>4</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">UMR 7503 LORIA, CNRS Campus scientifique 54 506 Vandoeuvre-lès-Nancy</affiliation>
          <affiliation affiliationId="2">UMR 5191 ICAR, CNRS 5 parvis René Descartes 69342 Lyon Cedex 07</affiliation>
          <affiliation affiliationId="3">UMR 5141 LTCI, CNRS 46 rue Barrault 75 634 Paris Cedex 13</affiliation>
          <affiliation affiliationId="4">CNAM-CRTD, 41 rue Gay-Lussac 75 005 Paris</affiliation>
        </affiliations>
        <titre>Détection de conflits dans les communautés épistémiques en ligne</titre>
        <type>court</type>
        <pages>351-358</pages>
        <resume>La présence de conflits dans les communautés épistémiques en ligne peut s’avérer bloquante pour l’activité de conception. Nous présentons une étude sur la détection automatique de conflit dans les discussions entre contributeurs Wikipedia qui s’appuie sur des traits de surface tels que la subjectivité ou la connotation des énoncés et évaluons deux règles de décision : l’une découle d’un modèle dialectique en exploitant localement la structure linéaire de la discussion, la subjectivité et la connotation ; l’autre, plus globale, ne s’appuie que sur la taille des fils et les marques de subjectivité au détriment des marques de connotation. Nous montrons que ces deux règles produisent des résultats similaires mais que la simplicité de la règle globale en fait une approche préférée dans la détection des conflits.</resume>
        <mots_cles>wikipedia, conflit, syntaxe, sémantique, interaction</mots_cles>
        <title>Conflicts detection in online epistemic communities</title>
        <abstract>Conflicts in online epistemic communities can be a blocking factor when producing knowledge. We present a way to automatically detect conflict in Wikipedia discussions, based on subjectivity and connotation marks. Two rules are evaluated : a local rule that uses the structure of the discussion threads, connotation and subjectivity marks and a global rule that takes the whole thread into account and only subjectivity. We show that the two rules produce similar results but that the simplicity of the global rule makes it a preferred approach to detect conflicts.</abstract>
        <keywords>wikipedia, conflict, syntax, semantics, interaction</keywords>
      </article>
      <article id="taln-2012-court-004" session="Poster 1">
        <auteurs>
          <auteur>
            <nom>Camille Dutrey</nom>
            <email>camille.dutrey@edf.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Chloé Clavel</nom>
            <email>chloe.clavel@edf.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Sophie Rosset</nom>
            <email>sophie.rosset@limsi.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Ioana Vasilescu</nom>
            <email>vasilescu@limsi.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Martine Adda-Decker</nom>
            <email>madda@univ-paris3.fr</email>
            <affiliationId>2</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">EDF R&amp;D, 1 avenue du Général de Gaulle 92141 Clamart</affiliation>
          <affiliation affiliationId="2">LIMSI-CNRS, rue John von Neumann 91403 Orsay</affiliation>
          <affiliation affiliationId="3">LPP, 19 rue des Bernardins 75005 Paris</affiliation>
        </affiliations>
        <titre>Quel est l'apport de la détection d'entités nommées pour l'extraction d'information en domaine restreint ?</titre>
        <type>court</type>
        <pages>359-366</pages>
        <resume>Les travaux liés à la définition et à la reconnaissance des entités nommées sont généralement envisagés en domaine ouvert, à travers la conception de catégories génériques (noms de personnes, de lieux, etc.) et leur application à des données textuelles issues de la presse (orale comme écrite). Par ailleurs, la fouille des données issues de centres d’appel est stratégique pour une entreprise comme EDF, compte tenu du rôle crucial joué par l’opinion pour les applications marketing, ce qui passe par la définition d’entités d’intérêt propres au domaine. Nous comparons les deux types de modèles d’entités - génériques et spécifiques à un domaine précis - afin d’observer leurs points de recouvrement, via l’annotation manuelle d’un corpus de conversations en centres d’appel. Nous souhaitons ainsi étudier l’apport d’une détection en entités nommées génériques pour l’extraction d’information métier en domaine restreint.</resume>
        <mots_cles>entités nommées, concepts métier, extraction d’information, données conversationnelles, annotation</mots_cles>
        <title>What is the contribution of named entities detection for information extraction in restricted domain ?</title>
        <abstract>In the framework of general domain dialog corpora a particular focus is dedicated to Named Entities definition and recognition, which are mostly very generic (personal names, locations, etc.). Moreover, call-centre data mining is strategic for a company like EDF, the public opinion analysis playing a significant role in EDF services quality evaluation and for marketing applications. In this purpose a domain dependant definition of entities of interest is essential. In this primary work we compare two types of entities models (generic and specific to the domain) in order to observe their respective coverage. We annotated manually a sub-corpus extracted from a large corpus of oral dialogs recorded in an EDF call-centre. The respective proportion of generic vs domain-specific Named Entities is then estimated. Impact for future work on building EDF domain-specific entities models is discussed.</abstract>
        <keywords>named entities, business concept, information extraction, conversational data, annotation</keywords>
      </article>
      <article id="taln-2012-court-005" session="Poster 1">
        <auteurs>
          <auteur>
            <nom>Egle Eensoo</nom>
            <email>egle.eensoo@inalco.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Mathieu Valette</nom>
            <email>mathieu.valette@inalco.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">INALCO, ERTIM, 2 rue de Lille, 75343 Paris Cedex 07</affiliation>
        </affiliations>
        <titre>Sur l'application de méthodes textométriques à la construction de critères de classification en analyse des sentiments</titre>
        <type>court</type>
        <pages>367-374</pages>
        <resume>Depuis une dizaine d'années, le TAL s'intéresse à la subjectivité, notamment dans la perspective d'applications telles que la fouille d'opinion et l'analyse des sentiments. Or, la linguistique de corpus outillée par des méthodes textométriques a souvent abordé la question de la subjectivité dans les textes. Notre objectif est de montrer d'une part, ce que pourrait apporter à l'analyse des sentiments l'analyse textométrique et d'autre part, comment mutualiser les avantages d'une association entre celle-ci et une méthode de classification automatique basée sur l'apprentissage supervisé. En nous appuyant sur un corpus de témoignages issus de forums de discussion, nous montrerons que la prise en compte de critères sélectionnés suivant une analyse textométrique permet d'obtenir des résultats de classification satisfaisants par rapport à une vision purement lexicale.</resume>
        <mots_cles>linguistique de corpus, textométrie, analyse de sentiments, classification automatique supervisée</mots_cles>
        <title>About the application of textometric methods for developing classification criteria in Sentiment analysis</title>
        <abstract>Over the last ten years, NLP has contributed to applied research on subjectivity, especially in applications such as Opinion mining and Sentiment analysis. However, corpus linguistics and textometry have often addressed the issue of subjectivity in text. Our purpose is to show, !rst, what textometric analysis could bring to sentiment analysis, and second, the bene!ts of pooling linguistic/textometric analysis and automatic classification methods based on supervised learning. By processing a corpus of posts from fora, we will show that the building of criteria from a textometric analysis could improve classification results, compared to a purely lexical approach.</abstract>
        <keywords>corpus linguistics, textometry, sentiment analysis, supervised learning</keywords>
      </article>
      <article id="taln-2012-court-006" session="Poster 1">
        <auteurs>
          <auteur>
            <nom>Michael Filhol</nom>
            <email>michael.filhol@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Annelies Braffort</nom>
            <email>annelies.braffort@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIMSI-CNRS, Campus d'Orsay bat 508, BP133, 91403 Orsay cx</affiliation>
        </affiliations>
        <titre>Méthodologie d'exploration de corpus et de formalisation de règles grammaticales pour les langues des signes</titre>
        <type>court</type>
        <pages>375-382</pages>
        <resume>Cet article présente une méthodologie visant, à partir d'une observation de corpus vidéo de langue des signes, à repérer puis formaliser les régularités de structure dans les constructions linguistiques. Cette méthodologie est applicable à tous les niveaux du langage, du sub-lexical à l'énoncé complet. En s'appuyant sur deux exemples, il présente une application de cette méthodologie ainsi que le modèle AZee qui, intégrant la souplesse nécessaire en termes de synchronisation des articulateurs, permet une formalisation des règles repérées.</resume>
        <mots_cles>Langue des signes, analyse de corpus, modèle grammatical, synchronisation</mots_cles>
        <title>Methodology for corpus exploration and grammatical rule building in Sign Language</title>
        <abstract>This paper presents a methodology for Sign Language video observation to extract and then formalise observed linguistic structure. This methodology is relevant to all linguistic layers from sub-lexical to discourse as a whole. Relying on two examples, we apply this methodology and describe the AZee model, which integrates the required flexibility for synchronising articulators, hence enables a specification of any new systematic rule observed.</abstract>
        <keywords>Sign Language, corpus analysis, grammatical models, synchronisation</keywords>
      </article>
      <article id="taln-2012-court-007" session="Poster 1">
        <auteurs>
          <auteur>
            <nom>Karën Fort</nom>
            <email>karen.fort@inist.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Vincent Claveau</nom>
            <email>vincent.claveau@irisa.fr</email>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">INIST-CNRS, 2 allée de Brabois, 54500 Vandoeuvre-lès-Nancy</affiliation>
          <affiliation affiliationId="2">LIPN, Université Paris 13 &amp; CNRS, 99 av. J.B. Clément, 93430 Villetaneuse</affiliation>
          <affiliation affiliationId="3">IRISA - CNRS, Campus de Beaulieu, 35200 Rennes</affiliation>
        </affiliations>
        <titre>Annotation manuelle de matchs de foot : Oh la la la ! l'accord inter-annotateurs ! et c'est le but !</titre>
        <appartenance>
          <affiliation>INIST-CNRS</affiliation>
          <affiliation>LIPN, Université Paris 13</affiliation>
          <affiliation>IRISA-CNRS</affiliation>
        </appartenance>
        <type>court</type>
        <pages>383-390</pages>
        <resume>Cet article présente une campagne d’annotation de commentaires de matchs de football en français. L’annotation a été réalisée à partir d’un corpus très hétérogène, contenant à la fois des comptes-rendus minute par minute et des transcriptions des commentaires vidéo. Nous montrons ici comment les accords intra- et inter-annotateurs peuvent être utilisés efficacement, en en proposant une définition adaptée à notre type de tâche et en mettant en exergue l’importance de certaines bonnes pratiques concernant leur utilisation. Nous montrons également comment certains indices collectés à l’aide d’outils statistiques simples peuvent être utilisés pour indiquer des pistes de corrections des annotations. Ces différentes propositions nous permettent par ailleurs d’évaluer l’impact des modalités sources de nos textes (oral ou écrit) sur le coût et la qualité des annotations.</resume>
        <mots_cles>annotation manuelle, accords inter-annotateurs</mots_cles>
        <title>Manual Annotation of Football Matches : Inter-annotator Agreement ! Gooooal !</title>
        <abstract>We present here an annotation campaign of commentaries of football matches in French. The annotation was done from a very heterogeneous text corpus of both match minutes and video commentary transcripts. We show how the intra- and inter-annotator agreement can be used efficiently during the whole campaign by proposing a definition of the markables suited to our type of task, as well as emphasizing the importance of using it appropriately. We also show how some clues, collected through statistical analyses, could be used to help correcting the annotations. These statistical analyses are then used to assess the impact of the source modality (written or spoken) on the cost and quality of the annotation process.</abstract>
        <keywords>manual annotation, inter-annotator agreement</keywords>
      </article>
      <article id="taln-2012-court-008" session="Poster 1">
        <auteurs>
          <auteur>
            <nom>Anne Garcia-Fernandez</nom>
            <email>Anne.Garcia-Fernandez@cea.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Olivier Ferret</nom>
            <email>Olivier.Ferret@cea.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CEA, LIST, Laboratoire Vision et Ingénierie des Contenus, Gif-sur-Yvette, F-91191 France</affiliation>
        </affiliations>
        <titre>Etude de différentes stratégies d'adaptation à un nouveau domaine en fouille d'opinion</titre>
        <type>court</type>
        <pages>391-398</pages>
        <resume>Le travail présenté dans cet article se situe dans le contexte de la fouille d’opinion et se focalise sur la détermination de la polarité d’un texte en adoptant une approche par apprentissage. Dans ce cadre, son objet est d’étudier différentes stratégies d’adaptation à un nouveau domaine dans le cas de figure fréquent où des données d’entraînement n’existent que pour un ou plusieurs domaines différents du domaine cible. Cette étude montre en particulier que l’utilisation d’une forme d’auto-apprentissage par laquelle un classifieur annote un corpus du domaine cible et modifie son corpus d’entraînement en y incorporant les textes classés avec la plus grande confiance se révèle comme la stratégie la plus performante et la plus stable pour les différents domaines testés. Cette stratégie s’avère même supérieure dans un nombre significatif de cas à la méthode proposée par (Blitzer et al., 2007) sur les mêmes jeux de test tout en étant plus simple.</resume>
        <mots_cles>fouille d’opinion, adaptation à un nouveau domaine, auto-apprentissage</mots_cles>
        <title>Study of various strategies for adapting an opinion classifier to a new domain</title>
        <abstract>The work presented in this article takes place in the field of opinion mining and aims more particularly at finding the polarity of a text by relying on machine learning methods. In this context, it focuses on studying various strategies for adapting a statistical classifier to a new domain when training data only exist for one or several other domains. This study shows more precisely that a self-training procedure consisting in enlarging the initial training corpus with texts from the target domain that were reliably classified by the classifier is the most successful and stable strategy for the tested domains. Moreover, this strategy gets better results in most cases than (Blitzer et al., 2007)’s method on the same evaluation corpus while it is more simple.</abstract>
        <keywords>opinion mining, domain adaptation, self-training</keywords>
      </article>
      <article id="taln-2012-court-009" session="Poster 1">
        <auteurs>
          <auteur>
            <nom>Olivier Kraif</nom>
            <email>olivier.kraif@u-grenoble3.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Sascha Diwersy</nom>
            <email>sascha.diwersy@uni-koeln.de</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIDILEM, Université Stendhal Grenoble 3, BP 25, 38040 Grenoble Cedex</affiliation>
          <affiliation affiliationId="2">Université de Cologne</affiliation>
        </affiliations>
        <titre>Le Lexicoscope : un outil pour l'étude de profils combinatoires et l'extraction de constructions lexico-syntaxiques</titre>
        <type>court</type>
        <pages>399-406</pages>
        <resume>Dans le cadre du projet franco-allemand Emolex, dédié à l'étude contrastive de la combinatoire du lexique des émotions en 5 langues, nous avons développé des outils et des méthodes permettant l'extraction, la visualisation et la comparaison de profls combinatoires pour des expressions simples et complexes. Nous présentons ici l'architecture d'ensemble de la plate-forme, conçue pour efectuer des extractions sur des corpus de grandes dimensions (de l'ordre de la centaine de millions de mots) avec des temps de réponse réduits (le corpus étant interrogeable en ligne1). Nous décrivons comment nous avons introduit la notion de pivots complexes, afn de permettre aux utilisateurs de rafner progressivement leurs requêtes pour caractériser des constructions lexico-syntaxiques élaborées. Enfn, nous donnons les premiers résultats d'un module d'extraction automatique d'expressions polylexicales récurrentes.</resume>
        <mots_cles>collocations, cooccurrences, profl combinatoire, expressions polylexicales, lexique des émotions</mots_cles>
        <title>The Lexicoscope : an integrated tool for combinatoric profles observation and lexico-syntactic constructs extraction</title>
        <abstract>The German-French research project Emolex whose aim is the contrastive study of the combinatorial behaviour of emotion lexemes in 5 languages has led to the development of methods and tools to extract, display and compare the combinatorial profles of simple and complex expressions. In this paper, we present the overall architecture of the query platform which has been conceived to ensure efcient processing of huge annotated text corpora (consisting of several hundred millions of word tokens) accessible through a web-based interface. We put forward the concept of “complex query nodes” introduced to enable users to carry out progressively elaborated extractions of lexical-syntactic patterns. We fnally give primary results of an automated method for the retrieval of recurrent multi-word expressions, which takes advantage of the complex query nodes implementation.</abstract>
        <keywords>collocations, combinatorial profles, multi-word expressions</keywords>
      </article>
      <article id="taln-2012-court-010" session="Poster 1">
        <auteurs>
          <auteur>
            <nom>Audrey Laroche</nom>
            <email>audrey.laroche@umontreal.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">RALI-DIRO, Université de Montréal, C.P. 6128, Succ. Centre-Ville Montréal (Québec) H3C 3J7, Canada</affiliation>
        </affiliations>
        <titre>Analyse des contextes et des candidats dans l'identification des équivalents terminologiques en corpus comparables</titre>
        <type>court</type>
        <pages>407-414</pages>
        <resume>L’approche standard d’identification d’équivalents terminologiques à partir de corpus comparables repose sur la comparaison de mots contextuels en langues source et cible et sur l’utilisation d’un lexique bilingue. Nous analysons manuellement, selon des critères linguistiques (parties du discours, spécificité et relations sémantiques), les propriétés des mots contextuels et des erreurs commises par l’approche standard appliquée à la terminologie médicale pour suggérer des améliorations basées sur la sélection de mots contextuels.</resume>
        <mots_cles>équivalents terminologiques, vecteurs contextuels, corpus comparables, terminologie médicale, étude qualitative</mots_cles>
        <title>Analysis of contexts and candidates in term-translation spotting in comparable corpora</title>
        <abstract>The standard approach for identifying terminological equivalents from comparable corpora is based on the comparison of source and target language context words using a bilingual lexicon. We cary a manual analysis of the linguistic properties (parts of speech, specificity and semantic relations) of the context words and the inacurrate equivalents given by the standard approach applied to medical terminology, in order to suggest improvements based on the selection of context words.</abstract>
        <keywords>terminological equivalents, contextual vectors, comparable corpora, medical terminology, qualitative study.</keywords>
      </article>
      <article id="taln-2012-court-011" session="Poster 1">
        <auteurs>
          <auteur>
            <nom>Emmanuel Planas</nom>
            <email>emmanuel.planas@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">UNAM, LINA, 2 rue de la Houssinière, BP 92208, 44322 Nantes</affiliation>
          <affiliation affiliationId="2">UNAM, UCO, ST, 3, place André Leroy, 49008 Angers</affiliation>
        </affiliations>
        <titre>BiTermEx Un prototype d'extraction de mots composés à partir de documents comparables via la méthode compositionnelle</titre>
        <type>court</type>
        <pages>415-422</pages>
        <resume>Nous décrivons BiTermEx, un prototype d'expérimentation de l'extraction de terminologie bilingue de mots composés, à partir de documents comparables, via la méthode compositionnelle. Nous expliquons la variation morphologique et la combinaison des constituants lexicaux des termes composés. Cette permet une précision TOP1 de 92% et 97,5% en français anglais, et de 94% en français japonais pour l'alignement de termes composés (textes scientifiques et de vulgarisation scientifique).</resume>
        <mots_cles>extraction terminologique, prototype, terminologie bilingue, documents comparables, méthode compositionnelle, mots composés, corpus</mots_cles>
        <title>BiTermEx , A prototype for the extraction of multiword terms from comparable documents through the compositional approach</title>
        <abstract>We describe BiTermEx, a prototype for extracting multiword terms from comparable corpora, using the compositional method. We focus on morphology-based variations of multiword constituents and their recombinaison. We experimented our approach on scientific and popular science corpora. We record TOP1 precisions of 92% and 97,5% on French to English alignments and 94% on French to Japanese.</abstract>
        <keywords>term extraction, prototype, bilingual terminology, comparable documents, compositional method, multiword terms, corpus</keywords>
      </article>
      <article id="taln-2012-court-012" session="Poster 1">
        <auteurs>
          <auteur>
            <nom>Laurie Serrano</nom>
            <email>Laurie.Serrano@unicaen.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Thierry Charnois</nom>
            <email>Thierry.Charnois@unicaen.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Stephan Brunessau</nom>
            <email>Stephan.Brunessau@cassidian.com</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Bruno Grilheres</nom>
            <email>Bruno.Grilheres@cassidian.com</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Maroua Bouzid</nom>
            <email>Maroua.Bouzid@unicaen.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire GREYC, Université de Caen Basse-Normandie, Campus Côte de Nacre, Boulevard du Maréchal Juin, BP 5186 - 14032 Caen</affiliation>
          <affiliation affiliationId="2">Département IPCC, Cassidian, Parc d’Affaires des Portes - 27600 Val de Reuil</affiliation>
        </affiliations>
        <titre>Combinaison d'approches pour l'extraction automatique d'événements</titre>
        <type>court</type>
        <pages>423-430</pages>
        <resume>Dans cet article, nous présentons un système d’extraction automatique d’événements fondé sur deux approches actuelles en extraction d’information : la première s’appuie sur des règles linguistiques construites manuellement et la seconde se fonde sur un apprentissage automatique de patrons linguistiques. Les expérimentations réalisées montrent que combiner ces deux méthodes d’extraction permet d’améliorer significativement la qualité des événements extraits (amélioration de près de 10 points de F-mesure).</resume>
        <mots_cles>Extraction d’information, événements, approche symbolique, apprentissage de patrons linguistiques</mots_cles>
        <title>Automatic events extraction by combining multiple approaches</title>
        <abstract>In this paper, we present an automatic system for extracting events based on the combination of two existing information extraction approaches : the first one is made of hand-crafted linguistic rules and the second one is based on an automatic learning of linguistic patterns. We have shown that this mixed approach leads to a significant improvement of extraction performances.</abstract>
        <keywords>Text mining, events, symbolic extraction, linguistic pattern learning</keywords>
      </article>
      <article id="taln-2012-court-013" session="Poster 1">
        <auteurs>
          <auteur>
            <nom>Isabelle Tellier</nom>
            <email>isabelle.tellier@univ-paris3.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Denys Duchier</nom>
            <email>denys.duchier@univ-orleans.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Iris Eshkol</nom>
            <email>iris.eshkol@univ-orleans.fr</email>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Arnaud Courmet</nom>
            <email>arnaud.coumet@gmail.com</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Mathieu Martinet</nom>
            <email>mathieu_martinet@hotmail.fr</email>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LaTTiCe, université Paris 3 - Sorbonne Nouvelle</affiliation>
          <affiliation affiliationId="2">LIFO, université d’Orléans</affiliation>
          <affiliation affiliationId="3">LLL, université d’Orléans</affiliation>
        </affiliations>
        <titre>Apprentissage automatique d'un chunker pour le français</titre>
        <type>court</type>
        <pages>431-438</pages>
        <resume>Nous décrivons dans cet article comment nous avons procédé pour apprendre automatiquement un chunker à partir du French Tree Bank, en utilisant les CRF (Conditional Random Fields). Nous avons réalisé diverses expériences, pour reconnaître soit l’ensemble de tous les chunks possibles, soit les seuls groupes nominaux simples. Nous évaluons le chunker obtenu aussi bien de manière interne (sur le French Tree Bank lui-même) qu’externe (sur un corpus distinct transcrit de l’oral), afin de mesurer sa robustesse.</resume>
        <mots_cles>chunking, apprentissage automatique, French Tree Bank, CRF</mots_cles>
        <title>Machine Learning of a chunker for French</title>
        <abstract>We describe in this paper how to automatically learn a chunker for French, from the French Tree Bank and CRFs (Conditional Random Fields). We did several experiments, either to recognize every possible kind of chunks, or to focus on simple nominal phrases only. We evaluate the obtained chunker on internal data (i.e. also extracted from the French Tree Bank) as well as on external (i.e from a distinct corpus) ones, to measure its robustness.</abstract>
        <keywords>chunking, Machine Learning, French Tree Bank, CRF</keywords>
      </article>
      <article id="taln-2012-court-014" session="Poster 1">
        <auteurs>
          <auteur>
            <nom>Nikola Tulechki</nom>
            <email>tanguy@univ-tlse2.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Ludovic Tanguy</nom>
            <email>tulechki@univ-tlse2.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CLLE-ERSS : CNRS et Université de Toulouse 2, 5 allées Antonio Machado, 31058 Toulouse CEDEX 9</affiliation>
          <affiliation affiliationId="2">Conseil en Facteurs Humains, 4 impasse Montcabrier. 31500 Toulouse</affiliation>
        </affiliations>
        <titre>Effacement de dimensions de similarité textuelle pour l’exploration de collections de rapports d’incidents aéronautiques</titre>
        <type>court</type>
        <pages>439-446</pages>
        <resume>Cet article étudie le lien entre la similarité textuelle et une classification extrinsèque dans des collections de rapports d’incidents aéronautiques. Nous cherchons à compléter les stratégies d’analyse de ces collections en établissant automatiquement des liens de similarité entre les documents de façon à ce qu’ils ne reflètent pas l’organisation des schémas de codification utilisés pour leur classement. Afin de mettre en évidence les dimensions de variation transversales à la classification, nous calculons un score de dépendance entre les termes et les classes et excluons du calcul de similarité les termes les plus corrélés à une classe donnée. Nous montrons par une application sur 500 documents que cette méthode permet effectivement de dégager des thématiques qui seraient passées inaperçues au vu de la trop grande saillance des similarités de haut niveau.</resume>
        <mots_cles>similarité textuelle, classification de documents, corpus spécialisé</mots_cles>
        <title>Deletion of dimensions of textual similarity for the exploration of collections of accident reports in aviation</title>
        <abstract>In this paper we study the relationship between external classification and textual similarity in collections of incident reports. Our goal is to complement the existing classification-based analysis strategies by automatically establishing similarity links between documents in such a way that they do not reflect the dominant organisation of the classification schemas. In order to discover such transversal dimensions of similarity, we compute association scores between terms and classes and exlude the most correlated terms from the similarity calculation. We demonstrate on a 500 document corpus that by using this method, we can isolate topics that would otherwise have been masked by the dominant dimensions of similarity in the collection.</abstract>
        <keywords>textual simliarity, document classification, specialised corpora</keywords>
      </article>
      <article id="taln-2012-court-015" session="Poster 2">
        <auteurs>
          <auteur>
            <nom>Haithem Afli</nom>
            <email>Haithem.Afli@lium.univ-lemans.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Loïc Barrault</nom>
            <email>Loic.Barrault@lium.univ-lemans.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Holger Schwenk</nom>
            <email>Holger.Schwenk@lium.univ-lemans.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire d’Informatique de l’Université du Maine</affiliation>
        </affiliations>
        <titre>Traduction automatique à partir de corpus comparables: extraction de phrases parallèles à partir de données comparables multimodales</titre>
        <type>court</type>
        <pages>447-454</pages>
        <resume>Les performances des systèmes de traduction automatique statistique dépendent de la disponibilité de textes parallèles bilingues, appelés aussi bitextes. Cependant, les corpus parallèles sont des ressources limitées et parfois indisponibles pour certains couples de langues ou domaines. Nous présentons une technique pour l’extraction de phrases parallèles à partir d’un corpus comparable multimodal (audio et texte). Ces enregistrements sont transcrits avec un système de reconnaissance automatique de la parole et traduits avec un système de traduction automatique. Ces traductions sont ensuite utilisées comme requêtes d’un système de recherche d’information pour sélectionner des phrases parallèles sans erreur et générer un bitexte. Plusieurs expériences ont été menées sur les données de la campagne IWSLT’11 (TED) qui montrent la faisabilité de notre approche.</resume>
        <mots_cles>Reconnaissance de la parole, traduction automatique statistique, corpus comparables multimodaux, extraction de phrases parallèles</mots_cles>
        <title>Automatic Translation from Comparable corpora : extracting parallel sentences from multimodal comparable corpora</title>
        <abstract>Statistical Machine Translation (SMT) systems depend on the availability of bilingual parallel text, also called bitext. However parallel corpora are a limited resource and are often not available for some domains or language pairs. We present an alternative method for extracting parallel sentences from multimodal comparable corpora. This work extends the use of comparable corpora, in using audio instead of text on the source side. The audio is transcribed by an automatic speech recognition system and translated with a base-line SMT system. We then use information retrieval in a large text corpus of the target language to extract parallel sentences. We have performed a series of experiments on data of the IWSLT’11 speech translation task (TED) that shows the feasibility of our approach.</abstract>
        <keywords>Automatic speech recognition, statistical machine translation, multimodal comparable corpora, extraction of parallel sentences</keywords>
      </article>
      <article id="taln-2012-court-016" session="Poster 2">
        <auteurs>
          <auteur>
            <nom>Yacine Ben Yahia</nom>
            <email>benyacine.sint@gmail.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Souha Mezghani Hammami</nom>
            <email>souha.mezghani@fsegs.rnu.tn</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Lamia Hadrich Belguith</nom>
            <email>l.belguith@fsegs.rnu.tn</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">ANLP Research Group - Laboratoire MIRACL/ FSEGS Sfax, Tunisie</affiliation>
        </affiliations>
        <titre>La reconnaissance automatique de la fonction des pronoms démonstratifs en langue arabe</titre>
        <type>court</type>
        <pages>455-462</pages>
        <resume>La résolution d'anaphores est l'une des tâches les plus difficiles du Traitement Automatique du Langage Naturel (TALN). La capacité de classifier les pronoms avant de tenter une tâche de résolution d'anaphores serait importante, puisque pour traiter un pronom cataphorique le système doit chercher l’antécédent dans le segment qui suit le pronom. Alors que, pour le pronom anaphorique, le système doit chercher l’antécédent dans le segment qui précède le pronom. En outre, le nombre des pronoms a été jugée non-trivial dans la langue arabe. C’est dans ce cadre que se situe notre travail qui consiste à proposer une méthode pour la classification automatique des pronoms démonstratifs arabes, basée sur l’apprentissage. Nous avons évalué notre approche sur un corpus composé de 365585 mots contenant 14318 pronoms démonstratifs et nous avons obtenu des résultats encourageants : 99.3% comme F-Mesure.</resume>
        <mots_cles>Pronoms démonstratifs, résolution des anaphores, traitement de la langue arabe</mots_cles>
        <title>Automatic recognition of demonstrative pronouns function in Arabic</title>
        <abstract>Anaphora resolution is one of the most difficult tasks in NLP. Classifying pronouns before attempting a task of anaphora resolution is important because to handle the cataphoric pronoun, the system should determine the antecedent into the segment following the pronoun. Although, for the anaphoric pronoun, the system should look for the antecedent into the segment before the pronoun. In addition, the number of demonstrative pronouns is very important in Arabic. In this paper, we describe a machine learning method for classifying demonstrative pronouns in Arabic. We have evaluated our approach on a corpus of 365585 words which contain 14318 demonstrative pronouns and we have obtained encouraging results: 99.3% as F-Measure.</abstract>
        <keywords>Demonstrative pronouns, anaphora resolution, ANLP</keywords>
      </article>
      <article id="taln-2012-court-017" session="Poster 2">
        <auteurs>
          <auteur>
            <nom>Andre Bittar</nom>
            <email>Andre.Bittar@xrce.xerox.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Caroline Hagège</nom>
            <email>Caroline.Hagege@xrce.xerox.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">XRCE, 6 Chemin de Maupertuis, 38240 Meylan, FRANCE</affiliation>
        </affiliations>
        <titre>Un annotateur automatique d'expressions temporelles du français et son évaluation sur le TimeBank du français</titre>
        <type>court</type>
        <pages>463-470</pages>
        <resume>Dans cet article, nous présentons un outil d’extraction et de normalisation d’un sous-ensemble d’expressions temporelles développé pour le français. Cet outil est mis au point et utilisé dans le cadre du projet ANR Chronolines1 et il est appliqué sur un corpus fourni par l’AFP. Notre but final dans le cadre du projet est de construire semi-automatiquement des chronologies événementielles à partir de la base de depêches de l’AFP. L’une des étapes du traitement est l’analyse de l’information temporelle véhiculée dans les textes. Nous avons donc développé un annotateur d’expressions temporelles pour le français que nous décrivons dans cet article. Nous présenterons également les résultats de son évaluation.</resume>
        <mots_cles>Analyse temporelle, évaluation</mots_cles>
        <title>An Automatic Temporal Expression Annotator and its Evaluation on the French TimeBank</title>
        <abstract>In this article, we present a tool that extracts and normalises a subset of temporal expressions in French. This tool is being developed and used in the ANR (French National Research Agency) project Chronolines, applied to a corpus of provided by the Agence France Presse. The aim of the project is to semi-automatically construct event chronologies from this corpus. To do this, a detailed analysis of the temporal information conveyed by texts, is required. The system we present here is the first version of a temporal annotator that we have developed for French. We describe it in this article and present the results of an evaluation.</abstract>
        <keywords>Temporal processing, evaluation</keywords>
      </article>
      <article id="taln-2012-court-018" session="Poster 2">
        <auteurs>
          <auteur>
            <nom>Laurence Danlos</nom>
            <email>Laurence.Danlos@linguist.jussieu.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Diégo Antolinos-Basso</nom>
            <email>Diego.Antolinos-Basso@linguist.jussieu.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Chloé Braud</nom>
            <email>Chloe.Braud@linguist.jussieu.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Charlotte Roze</nom>
            <email>Charlotte.Roze@linguist.jussieu.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">ALPAGE, Université Paris Diderot, 175 rue du Chevaleret, 75013 Paris</affiliation>
        </affiliations>
        <titre>Vers le FDTB : French Discourse Tree Bank</titre>
        <type>court</type>
        <pages>471-478</pages>
        <resume>Nous présentons les premiers pas vers la création d’un corpus annoté en discours pour le français : le French Discourse TreeBank enrichissant le FTB. La méthodologie adoptée s’inspire du Penn Discourse TreeBank (PDTB) mais elle s’en distingue sur au moins deux points à caractère théorique. D’abord, notre objectif est de fournir une couverture totale d’un texte du corpus, tandis que le PDTB ne fournit qu’une couverture partielle, qui ne peut donc pas être qualifiée d’analyse discursive comme celle faite en RST ou SDRT, deux théories majeures sur le discours. Ensuite, nous avons été amenés à définir une nouvelle hiérarchie des relations de discours qui s’inspire de RST, de SDRT et du PDTB.</resume>
        <mots_cles>Discours, corpus annoté manuellement, analyse discursive, PDTB, RST, SDRT</mots_cles>
        <title>Towards the FDTB : French Discourse Tree Bank</title>
        <abstract>We present the first steps towards creating an annotated corpus for discourse in French : the French Discourse Treebank enriching the FTB. Our methodology is based on the Penn Discourse Treebank (PDTB), but it differs in at least two points of a theoretical nature. First, our goal is to provide full coverage of a text, while the PDTB provides only partial coverage, which can not be described as discourse analysis such as the one made in RST or SDRT, two major theories on discourse. Second, we were led to define a new hierarchy of discourse relations which is based on RST, SDRT and PDTB.</abstract>
        <keywords>Discourse, manually annotated corpus, discourse analysis, PDTB, RST, SDRT</keywords>
      </article>
      <article id="taln-2012-court-019" session="Poster 2">
        <auteurs>
          <auteur>
            <nom>Romain Deveaud</nom>
            <email>romain.deveaud@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Patrice Bellot</nom>
            <email>patrice.bellot@lsis.org</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIA - Université d’Avignon</affiliation>
          <affiliation affiliationId="2">LSIS - Université Aix-Marseille</affiliation>
        </affiliations>
        <titre>Combinaison de ressources générales pour une contextualisation implicite de requêtes</titre>
        <type>court</type>
        <pages>479-486</pages>
        <resume>L’utilisation de sources externes d’informations pour la recherche documentaire a été considérablement étudiée dans le passé. Des améliorations de performances ont été mises en lumière avec des corpus larges ou structurés. Néanmoins, dans ces études les ressources sont souvent utilisées séparément mais rarement combinées. Nous présentons une évaluation de la combinaison de quatre différentes ressources générales, standards et accessibles. Nous utilisons une mesure de distance informative pour extraire les caractéristiques contextuelles des différentes ressources et améliorer la représentation de la requête. Cette évaluation est menée sur une tâche de recherche d’information sur le Web en utilisant le corpus ClueWeb09 et les topics de la piste Web de TREC. Les meilleurs résultats sont obtenus en combinant les quatre ressources, et sont statistiquement significativement supérieurs aux autres approches.</resume>
        <mots_cles>Combinaison de ressources, RI contextuelle, recherche web</mots_cles>
        <title>Query Contextualization and Reformulation by Combining External Corpora</title>
        <abstract>Improving document retrieval using external sources of information has been extensively studied throughout the past. Improvements with either structured or large corpora have been reported. However, in these studies resources are often used separately and rarely combined together. We present an evaluation of the combination of four different scalable corpora over a web search task. An informative divergence measure is used to extract contextual features from the corpora and improve query representation. We use the ClueWeb09 collection along with TREC’s Web Track topics for the purpose of our evaluation. Best results are achieved when combining all four corpora, and are significantly better than the results of other approaches.</abstract>
        <keywords>Resources combination, contextual IR, web search.</keywords>
      </article>
      <article id="taln-2012-court-020" session="Poster 2">
        <auteurs>
          <auteur>
            <nom>Souhir Gahbiche-Braham</nom>
            <email>souhir.gahbiche@limsi.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Hélène Bonneau-Maynard</nom>
            <email>helene.maynard@limsi.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Thomas Lavergne</nom>
            <email>thomas.lavergne@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>François Yvon</nom>
            <email>francois.yvon@limsi.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIMSI-CNRS</affiliation>
          <affiliation affiliationId="2">Université Paris-Sud</affiliation>
        </affiliations>
        <titre>Repérage des entités nommées pour l'arabe : adaptation non-supervisée et combinaison de systèmes</titre>
        <type>court</type>
        <pages>487-494</pages>
        <resume>La détection des Entités Nommées (EN) en langue arabe est un prétraitement potentiellement utile pour de nombreuses applications du traitement des langues, en particulier pour la traduction automatique. Cette tâche représente toutefois un sérieux défi, compte tenu des spécificités de l’arabe. Dans cet article, nous présentons un compte-rendu de nos efforts pour développer un système de repérage des EN s’appuyant sur des méthodes statistiques, en détaillant les aspects liés à la sélection des caractéristiques les plus utiles pour la tâche ; puis diverses tentatives pour adapter ce système d’une manière entièrement non supervisée.</resume>
        <mots_cles>Adaptation non supervisée, Repérage des entités nommées</mots_cles>
        <title>Named Entity Recognition for Arabic : Unsupervised adaptation and Systems combination</title>
        <abstract>The recognition of Arabic Named Entities (NE) is a potentially useful preprocessing step for many Natural Language Processing Applications, such as Machine Translation. This task is however made very complex by some peculiarities of the Arabic language. In this paper, we present a summary of our recent efforts aimed at developing a statistical NE recognition system, with a specific focus on feature engineering aspects. We also report several approaches for adapting this system in an entirely unsupervised manner to a new domain.</abstract>
        <keywords>Unsupervised domain adaptation, named entity recognition</keywords>
      </article>
      <article id="taln-2012-court-021" session="Poster 2">
        <auteurs>
          <auteur>
            <nom>Nuria Gala</nom>
            <email>nuria.gala@lif.univ-mrs.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Caroline Brun</nom>
            <email>caroline.brun@xrce.xerox.com</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIF-CNRS UMR 7279, 163 av. de Luminy case 901, 13288 Marseille Cedex 9, France</affiliation>
          <affiliation affiliationId="2">Xerox Research Centre Europe, 6 chemin de Maupertuis 38240 Meylan, France</affiliation>
        </affiliations>
        <titre>Propagation de polarités dans des familles de mots : impact de la morphologie dans la construction d'un lexique pour l'analyse de sentiments</titre>
        <type>court</type>
        <pages>495-502</pages>
        <resume>Les ressources lexicales sont cruciales pour de nombreuses applications de traitement automatique de la langue (par exemple, l'extraction d'opinions à partir de corpus). Cependant, leur construction pose des problèmes à différents niveaux (coût, couverture, etc.). Dans cet article, nous avons voulu vérifier si les informations morphologiques liées à la dérivation pouvaient être exploitées pour l'annotation automatique d'informations sémantiques. En partant d'une ressource regroupant les mots en familles morphologiques en français, nous avons construit un lexique de polarités pour 4 065 mots, à partir d'une liste initiale d'adjectifs annotés manuellement. Les résultats obtenus montrent que la propagation des polarités est correcte pour 78,89% des familles avec un seul adjectif. Le lexique ainsi obtenu améliore aussi les résultats du système d'extraction d'opinions.</resume>
        <mots_cles>ressources lexicales, morphologie dérivationnelle, analyse de sentiments</mots_cles>
        <title>Spreading Polarities among Word Families: Impact of Morphology on Building a Lexicon for Sentiment Analysis</title>
        <abstract>Lexical resources are essential for many natural language applications (for example, opinion mining from corpora). However, building them entails different problems (cost, coverage, etc.). In this paper, we wanted to verify whether morphological information about derivation could be used to automatically annotate semantic information. Starting from a resource that groups words into morphological families in French, we have built a lexicon with polarities for 4 065 words from an initial seed set of manual annotated adjectives. The results obtained show that spreading polarities is accurate for 78.89% of the families with a unique adjective. The lexicon obtained also improves the results of the opinion mining system on different corpora.</abstract>
        <keywords>lexical resources, derivational morphology, opinion mining</keywords>
      </article>
      <article id="taln-2012-court-022" session="Poster 2">
        <auteurs>
          <auteur>
            <nom>Alexandre Labadié</nom>
            <email>Alexandre.Labadie@imag.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Patrice Enjalbert</nom>
            <email>Patrice.Enjalbert@unicaen.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Stephane Ferrari</nom>
            <email>Stephane.Ferrari@unicaen.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">GETALP, LIG, BP 53, 38041 Grenoble Cedex 9, France</affiliation>
          <affiliation affiliationId="2">Laboratoire GREYC, Université de Caen &amp; CNRS, Bd Maréchal Juin, BP 5186 F-14032 Caen Cedex, France</affiliation>
        </affiliations>
        <titre>Transitions thématiques : Annotation d'un corpus journalistique et premières analyses</titre>
        <type>court</type>
        <pages>503-510</pages>
        <resume>Le travail présenté dans cet article est centré sur la constitution d’un corpus de textes journalistiques annotés au niveau discursif d’un point de vue thématique. Le modèle d’annotation est une segmentation classique, à laquelle nous ajoutons un repérage de zones de transition entre unités thématiques. Nous faisons l’hypothèse que dans un texte bien construit, le scripteur fournit des indications aidant le lecteur à passer d’un sujet à un autre, l’identification de ces indices étant susceptible d’améliorer les procédures de segmentation automatique. Les annotations produites ont fait l’objet d’analyses quantitatives mettant en évidence un ensemble de propriétés des transitions entre thèmes.</resume>
        <mots_cles>Structure du discours, segments thématiques, transitions thématiques, annotation</mots_cles>
        <title>Manual thematic annotation of a journalistic corpus : first observations and evaluation</title>
        <abstract>The work presented in this paper focuses on the creation of a corpus of journalistic texts annotated at dicourse level, more precisely on a topic level. The annotation model is a classic segmentation one, to which we add transition zones between topical units. We assume that in a well-structured text, the author provides information helping the reader to move from one topic to another, where an identification of these clues is likely to improve automatic segmentation. The produced annotations have been subject of several quantitative analyses showing a set of linguistic properties of topical transitions.</abstract>
        <keywords>Discourse structure, topical segments, topical transitions, annotation</keywords>
      </article>
      <article id="taln-2012-court-023" session="Poster 2">
        <auteurs>
          <auteur>
            <nom>Blandine Plaisantin Alecu</nom>
            <email>blandine.alecu@prolipsia.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Izabella Thomas</nom>
            <email>izabella.thomas@univ-fcomte.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Julie Renahy</nom>
            <email>julie.renahy@prolipsia.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Prolipsia SAS, TEMIS Innovation, 18 r Alain Savary, 25000 Besançon</affiliation>
          <affiliation affiliationId="2">Centre Tesnière, Université de Franche-Comté, UFR SLHS, 30 r Mégevand, 25030 Besançon</affiliation>
        </affiliations>
        <titre>La "multi-extraction" comme stratégie d'acquisition optimisée de ressources terminologiques et non terminologiques</titre>
        <type>court</type>
        <pages>511-518</pages>
        <resume>A partir de l'évaluation d'extracteurs de termes menée initialement pour détecter le meilleur outil d'acquisition du lexique d'une langue contrôlée, nous proposons dans cet article une stratégie d'optimisation du processus d'extraction terminologique. Nos travaux, menés dans le cadre du projet ANR Sensunique, prouvent que la « multiextraction », c'est-à-dire la coopération de plusieurs extracteurs de termes, donne des résultats significativement meilleurs que l’extraction via un seul outil. Elle permet à la fois de réduire le silence et de filtrer automatiquement le bruit grâce à la variation d'un indice relatif au potentiel terminologique.</resume>
        <mots_cles>terminologie, extraction, langue contrôlée, potentiel terminologique, filtrage de termes</mots_cles>
        <title>Multi-extraction as a strategy of optimized extraction of terminological and lexical resources</title>
        <abstract>Based on the evaluation of terminological extractors, initially to find the best tool for building a controlled language lexicon, we propose a strategy of optimized extraction of terminological resources. Our work highlights that the cooperation of several extraction tools gives better results than the use of a single one. It both reduces silence and automatically filters noise thanks to a variable related to termhood.</abstract>
        <keywords>terminology, extraction, controlled language, termhood, term filtering</keywords>
      </article>
      <article id="taln-2012-court-024" session="Poster 2">
        <auteurs>
          <auteur>
            <nom>Arnaud Renard</nom>
            <email>arnaud.renard@insa-lyon.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Sylvie Calabretto</nom>
            <email>sylvie.calabretto@insa-lyon.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Béatrice Rumpler</nom>
            <email>beatrice.rumpler@insa-lyon.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université de Lyon, CNRS</affiliation>
          <affiliation affiliationId="2">INSA-Lyon, LIRIS, UMR 5205, F-69621 Villeurbanne Cedex</affiliation>
        </affiliations>
        <titre>Une Approche de Recherche d’Information Structurée fondée sur la Correction d’Erreurs à l’Indexation des Documents</titre>
        <type>court</type>
        <pages>519-526</pages>
        <resume>Dans cet article, nous nous sommes intéressés à la prise en compte des erreurs dans les contenus textuels des documents XML. Nous proposons une approche visant à diminuer l’impact de ces erreurs sur les systèmes de Recherche d’Information (RI). En effet, ces systèmes produisent des index associant chaque document aux termes qu’il contient. Les erreurs affectent donc la qualité des index ce qui conduit par exemple à considérer à tort des documents mal indexés comme non pertinents (resp. pertinents) vis-à-vis de certaines requêtes. Afin de faire face à ce problème, nous proposons d’inclure un mécanisme de correction d’erreurs lors de la phase d’indexation des documents. Nous avons implémenté cette approche au sein d’un prototype que nous avons évalué dans le cadre de la campagne d’évaluation INEX.</resume>
        <mots_cles>Recherche d’information, dysorthographie, correction d’erreurs, xml</mots_cles>
        <title>Structured Information Retrieval Approach based on Indexing Time Error Correction</title>
        <abstract>In this paper, we focused on errors in the textual content of XML documents. We propose an approach to reduce the impact of these errors on Information Retrieval (IR) systems. Indeed, these systems rely on indexes associating each document to corresponding terms. Indexes quality is negatively affected by those misspellings. These errors makes it difficult to later retrieve documents (or parts of them) in an effective way during the querying phase. In order to deal with this problem we propose to include an error correction mechanism during the indexing phase of documents. We achieved an implementation of this spelling aware information retrieval system which is currently evaluated over INEX evaluation campaign documents collection.</abstract>
        <keywords>Information retrieval, misspellings, error correction, xml</keywords>
      </article>
      <article id="taln-2012-court-025" session="Poster 2">
        <auteurs>
          <auteur>
            <nom>Raphaël Rubino</nom>
            <email>Raphael.Rubino@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Stéphane Huet</nom>
            <email>Stephane.Huet@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Fabrice Lefèvre</nom>
            <email>Fabrice.Lefevre@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Georges Linarès</nom>
            <email>Georges.Linares@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIA-CERI, Université d’Avignon et des Pays de Vaucluse, Avignon, France</affiliation>
        </affiliations>
        <titre>Post-édition statistique pour l'adaptation aux domaines de spécialité en traduction automatique</titre>
        <type>court</type>
        <pages>527-534</pages>
        <resume>Cet article présente une approche de post-édition statistique pour adapter aux domaines de spécialité des systèmes de traduction automatique génériques. En utilisant les traductions produites par ces systèmes, alignées avec leur traduction de référence, un modèle de post-édition basé sur un alignement sous-phrastique est construit. Les expériences menées entre le français et l’anglais pour le domaine médical montrent qu’une telle adaptation a posteriori est possible. Deux systèmes de traduction statistiques sont étudiés : une implémentation locale état-de-l’art et un outil libre en ligne. Nous proposons aussi une méthode de sélection de phrases à post-éditer permettant d’emblée d’accroître la qualité des traductions et pour laquelle les scores oracles indiquent des gains encore possibles.</resume>
        <mots_cles>Traduction automatique statistique, post-édition, adaptation aux domaines de spécialité</mots_cles>
        <title>Statistical Post-Editing of Machine Translation for Domain Adaptation</title>
        <abstract>This paper presents a statistical approach to adapt generic machine translation systems to the medical domain through an unsupervised post-edition step. A statistical post-edition model is built on statistical machine translation outputs aligned with their translation references. Evaluations carried out to translate medical texts from French to English show that a generic machine translation system can be adapted a posteriori to a specific domain. Two systems are studied : a state-of-the-art phrase-based implementation and an online publicly available software. Our experiments also indicate that selecting sentences for post-edition leads to significant improvements of translation quality and that more gains are still possible with respect to an oracle measure.</abstract>
        <keywords>Statistical Machine Translation, Post-editing, Domain Adaptation</keywords>
      </article>
      <article id="taln-2012-court-026" session="Poster 2">
        <auteurs>
          <auteur>
            <nom>Benoît Sagot</nom>
            <email>benoit.sagot@inria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Marion Richard</nom>
            <email>ma.rih.on75@gmail.com</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Rosa Stern</nom>
            <email>rosa.stern@afp.com</email>
            <affiliationId>1</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Alpage, INRIA Paris-Rocquencourt &amp; Université Paris Diderot, 175 rue du Chevaleret, 75013 Paris</affiliation>
          <affiliation affiliationId="2">ISHA, Université Paris Sorbonne, 7 rue Victor Cousin, 75006 Paris</affiliation>
          <affiliation affiliationId="3">AFP MediaLab, 2 place de la Bourse, 75002 Paris</affiliation>
        </affiliations>
        <titre>Annotation référentielle du Corpus Arboré de Paris 7 en entités nommées</titre>
        <type>court</type>
        <pages>535-542</pages>
        <resume>Le Corpus Arboré de Paris 7 (ou French TreeBank) est le corpus de référence pour le français aux niveaux morphosyntaxique et syntaxique. Toutefois, il ne contient pas d’annotations explicites en entités nommées. Ces dernières sont pourtant parmi les informations les plus utiles pour de nombreuses tâches en traitement automatique des langues et de nombreuses applications. De plus, aucun corpus du français annoté en entités nommées et de taille importante ne contient d’annotation référentielle, qui complète les informations de typage et d’empan sur chaque mention par l’indication de l’entité à laquelle elle réfère. Nous avons annoté manuellement avec ce type d’informations, après pré-annotation automatique, le Corpus Arboré de Paris 7. Nous décrivons les grandes lignes du guide d’annotation sous-jacent et nous donnons quelques informations quantitatives sur les annotations obtenues.</resume>
        <mots_cles>Résolution d’entités nommées, Corpus annoté, Corpus arboré de Paris 7</mots_cles>
        <title>Referential named entity annotation of the Paris 7 French TreeBank</title>
        <abstract>The French TreeBank developed at the University Paris 7 is the main source of morphosyntactic and syntactic annotations for French. However, it does not include explicit information related to named entities, which are among the most useful information for several natural language processing tasks and applications. Moreover, no large-scale French corpus with named entity annotations contain referential information, which complement the type and the span of each mention with an indication of the entity it refers to. We have manually annotated the French TreeBank with such information, after an automatic pre-annotation step. We sketch the underlying annotation guidelines and we provide a few figures about the resulting annotations.</abstract>
        <keywords>Named entity resolution, Annotated corpus, French TreeBank</keywords>
      </article>
      <article id="taln-2012-court-027" session="Poster 2">
        <auteurs>
          <auteur>
            <nom>Christophe Servan</nom>
            <email>christophe.servan@lium.univ-lemans.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Simon Petitrenaud</nom>
            <email>simon.petit-renaud@lium.univ-lemans.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIUM, Le Mans</affiliation>
        </affiliations>
        <titre>Utilisation des fonctions de croyance pour l'estimation de paramètres en traduction automatique</titre>
        <type>court</type>
        <pages>543-550</pages>
        <resume>Cet article concerne des travaux effectués dans le cadre du 7ème atelier de traduction automatique statistique et du projet ANR COSMAT. Ces travaux se focalisent sur l’estimation de paramètres contenus dans une table de traduction. L’approche classique consiste à estimer ces paramètres à partir de fréquences relatives d’éléments de traduction. Dans notre approche, nous proposons d’utiliser le concept de masses de croyance afin d’estimer ces paramètres. La théorie des fonctions de croyances est une théorie très adaptée à la gestion des incertitudes dans de nombreux domaines. Les expériences basées sur notre approche s’appliquent sur la traduction de la paire de langue français-anglais dans les deux sens de traduction.</resume>
        <mots_cles>Traduction automatique statistique, fonctions de croyance, apprentissage automatique, estimation de paramètres</mots_cles>
        <title>Feature calculation for Statistical Machine Translation by using belief functions</title>
        <abstract>In this paper, we consider the translation of texts within the framework of the 7th Workshop of Machine Translation evaluation task and the COSMAT corpus using a statistical machine translation approach. This work is focused on the translation features calculation of the phrase contained in a phrase table. The classical way to estimate these features are based on the direct computation counts or frequencies. In our approach, we propose to use the concept of belief masses to estimate the phrase probabilities. The Belief Function theory has proven to be suitable and adapted for the management of uncertainties in many domains. The experiments based on our approach are focused on the language pair English-French.</abstract>
        <keywords>Statistical machine Translation, belief function, machine learning, feature estimation</keywords>
      </article>
      <article id="taln-2012-court-028" session="Poster 2">
        <auteurs>
          <auteur>
            <nom>Philippe Suignard</nom>
            <email>Philippe.Suignard@edf.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Frederik Cailliau</nom>
            <email>cailliau@sinequa.com</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Ariane Cavet</nom>
            <email>cavet@sinequa.com</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">EDF R&amp;D, 1, avenue du Général de Gaulle, 92141, Clamart</affiliation>
          <affiliation affiliationId="2">Sinequa, 12 rue d’Athènes, 75009 Paris</affiliation>
        </affiliations>
        <titre>La longueur des tours de parole comme critère de sélection de conversations dans un centre d’appels</titre>
        <type>court</type>
        <pages>551-558</pages>
        <resume>Cet article s’intéresse aux conversations téléphoniques d’un Centre d’Appels EDF, automatiquement découpées en « tours de parole » et automatiquement transcrites. Il fait apparaître une relation entre la longueur des tours de parole et leur contenu, en ce qui concerne le vocabulaire qui les compose et les sentiments qui y sont véhiculés. Après avoir montré qu’il y a un intérêt à étudier ces longs tours, l’article analyse leur contenu et liste quelques exemples autour des notions d’argumentation et de réclamation. Il montre ainsi que la longueur des tours de parole peut être un critère utile de sélection de conversations.</resume>
        <mots_cles>Centre d’appels, Conversation, Tour de parole, Reconnaissance de Parole</mots_cles>
        <title>Turn-taking length as criterion to select call center conversations</title>
        <abstract>This article focuses on telephone conversations collected in an EDF Call Center, automatically segmented in “turn-taking” and automatically transcribed. It shows a relationship between the length of the turns and their content regarding the vocabulary and the feelings that are conveyed. After showing that there is an interest in studying these long turns, the article analyzes their content and lists some examples around the notions of argumentation and claim. It shows that the length of turns can be a useful criterion for selecting conversations.</abstract>
        <keywords>Call Center, Conversation, Turn Taking, Automatic Speech Recognition</keywords>
      </article>
      <article id="taln-2012-court-029" session="Poster 2">
        <auteurs>
          <auteur>
            <nom>Tristan Vanrullen</nom>
            <email>tristan.vanrullen@gmail.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Leïla Boutora</nom>
            <email>leila.boutora@lpl-aix.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Jean Dagron</nom>
            <email>jean.dagron@ap-hm.fr</email>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">TVSI, 13009 Marseille</affiliation>
          <affiliation affiliationId="2">LPL, UMR 7309 CNRS/Univ. d’Aix-Marseille, 13100 Aix-en-Provence</affiliation>
          <affiliation affiliationId="3">Assistance publique - Hôpitaux de Marseille, 13005 Marseille</affiliation>
        </affiliations>
        <titre>Enjeux méthodologiques, linguistiques et informatiques pour le traitement du français écrit des sourds</titre>
        <type>court</type>
        <pages>559-566</pages>
        <resume>L’ouverture du Centre National de Réception des Appels d’Urgence (CNRAU) accessible aux sourds et malentendants fait émerger des questions linguistiques qui portent sur le français écrit des sourds, et des questions informatiques dans le domaine du traitement automatique du langage naturel. Le français écrit des sourds, pratiqué par une population hétérogène, comporte des spécificités morpho-syntaxiques et morpholexicales qui peuvent rendre problématique la communication écrite entre les personnes sourdes appelantes et les agents du CNRAU. Un premier corpus de français écrit sourd élicité avec mise en situation d’urgence (FAX-ESSU) a été recueilli dans la perspective de proposer des solutions TAL et linguistiques aux agents du CNRAU dans le cadre de ces échanges écrits. Nous présentons une première étude lexicale, morphosyntaxique et syntaxique de ce corpus reposant en partie sur une chaîne de traitement automatique, afin de valider les phénomènes linguistiques décrits dans la littérature et d'enrichir la connaissance du français écrit des sourds.</resume>
        <mots_cles>Français écrit des sourds, TAL, Français Langue Etrangère, linguistique de corpus, lexique, syntaxe, méthodologie</mots_cles>
        <title>Methodological, linguistic and computational challenges for processing written French of deaf people</title>
        <abstract>With the setup of a national emergency call-center for deaf people in France (CNRAU), some questions arise in linguistics and natural language processing about the written expression of deaf people. It is practiced by an heterogeneous population and shows morpho-syntactic, lexical and syntactic specificities which increase the difficulty, over the emergency situation, to successfully communicate between the deaf callers and the call-center operators. A first corpus (FAX-ESSU) of written French of deaf people was built with emergency conditions in order to provide linguistic and NLP solutions to the call center operators. On this corpus, we present a first study realized with the help of a natural language processing toolbox, in order to validation linguistic phenomenons described in the scientific literature and to enrich the knowledge of written French of deaf people.</abstract>
        <keywords>Written French of deaf people, NLP, French as a foreign language, corpus linguistics, lexicon, syntax, methodology</keywords>
      </article>
      <article id="taln-2012-demo-001" session="Démonstration">
        <auteurs>
          <auteur>
            <nom>Bruno Guillaume</nom>
            <email/>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Guillame Bonfante</nom>
            <email/>
            <affiliationId>1</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Paul Masson</nom>
            <email/>
            <affiliationId/>
          </auteur>
          <auteur>
            <nom>Mathieu Morey</nom>
            <email/>
            <affiliationId>4</affiliationId>
            <affiliationId>5</affiliationId>
          </auteur>
          <auteur>
            <nom>Guy Perrier</nom>
            <email/>
            <affiliationId>1</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LORIA - Campus Scientifique - BP 239 - 54506 Vandoeuvre-lès-Nancy cedex</affiliation>
          <affiliation affiliationId="2">INRIA Grand Est - 615, rue du Jardin Botanique - 54600 Villers-lès-Nancy</affiliation>
          <affiliation affiliationId="3">Université de Lorraine - 34, cours Léopold - CS 25233 - 54502 Nancy cedex</affiliation>
          <affiliation affiliationId="4">Laboratoire Parole et Langage, Aix-Marseille Université</affiliation>
          <affiliation affiliationId="5">Linguistics and Multilingual Studies, Nanyang Technological University</affiliation>
        </affiliations>
        <titre>Grew : un outil de réécriture de graphes pour le TAL</titre>
        <type>démonstration</type>
        <pages>1-2</pages>
        <resume>Nous présentons un outil de réécriture de graphes qui a été conçu spécifiquement pour des applications au TAL. Il permet de décrire des graphes dont les noeuds contiennent des structures de traits et dont les arcs décrivent des relations entre ces noeuds. Nous présentons ici la réécriture de graphes que l’on considère, l’implantation existante et quelques expérimentations.</resume>
        <mots_cles>réécriture de graphes, interface syntaxe-sémantique</mots_cles>
        <title>Grew: a Graph Rewriting Tool for NLP</title>
        <abstract>We present a Graph Rewriting Tool dedicated to NLP applications. Graph nodes contain feature structures and edges describe relations between nodes. We explain the Graph Rewriting framework we use, the implemented system and some experiments.</abstract>
        <keywords>graph rewriting, syntax-semantics interface</keywords>
      </article>
      <article id="taln-2012-demo-002" session="Démonstration">
        <auteurs>
          <auteur>
            <nom>Géraldine Damnati</nom>
            <email>geraldine.damnati@orange.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">France Telecom, Orange Labs, Lannion</affiliation>
        </affiliations>
        <titre>Interfaces de navigation dans des contenus audio et vidéo</titre>
        <type>démonstration</type>
        <pages>3-4</pages>
        <resume>Deux types de démonstrateurs sont présentés. Une première interface à visée didactique permet d'observer des traitements automatiques sur des documents vidéo. Plusieurs niveaux de représentation peuvent être montrés simultanément, ce qui facilite l'analyse d'approches multi-vues. La seconde interface est une interface opérationnelle de "consommation" de documents audio. Elle offre une expérience de navigation enrichie dans des documents audio grâce à une visualisation de métadonnées extraites automatiquement.</resume>
        <mots_cles>Traitements multi-vues, navigation enrichie</mots_cles>
        <title>Navigation interfaces through audio and video contents</title>
        <abstract>Two types of demonstrators are shown. A first interface, with didactic purposes, allows automatic processing of video documents to be observed. Several representation levels can be viewed simultaneously, which is particularly helpful to analyse the behaviour of multi-view approaches. The second interface is an operational audio document "consumption" interface. It offers an enriched navigation experience through the visualisation of automatically extracted metadata.</abstract>
        <keywords>Multi-view processing, enriched navigation</keywords>
      </article>
      <article id="taln-2012-demo-003" session="Démonstration">
        <auteurs>
          <auteur>
            <nom>Lionel Clément</nom>
            <email>lionel.clement@labri.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université Bordeaux 1 - LaBRI (UMR 5800) - CLEE-ERSS (UMR 5263)</affiliation>
        </affiliations>
        <titre>Synthèse de texte avec le logiciel Syntox</titre>
        <type>démonstration</type>
        <pages>5-6</pages>
        <resume>Le logiciel Syntox, dont une interface utilisateur en ligne ce trouve à cette URL : http://www.syntox.net, est une mise en application d’un modèle basé sur les grammaires attribuées, dans le cadre de la synthèse de texte. L’outil est une plateforme d’expérimentation dont l’ergonomie est simple. Syntox est capable de traiter des lexiques et des grammaires volumineux sur des textes ambigus à partir de la description explicite de phénomènes linguistiques.</resume>
        <mots_cles>Synthèse de texte, Grammaire attribuée, Syntaxe</mots_cles>
        <title>Automated generation of text with Syntox</title>
        <abstract>Syntox, which includes an online user interface at URL http://www.syntox.net, is an implementation of a model based on attribute grammars, in the context of automated generation of text. The sofware is intended as a platform for experimentation with an ergonomic interface. Syntox is usable with large vocabularies and grammars to produce ambiguous texts from an explicit description of linguistic phenomena.</abstract>
        <keywords>Text generation, Attribute Grammars, Syntax</keywords>
      </article>
      <article id="taln-2012-demo-004" session="Démonstration">
        <auteurs>
          <auteur>
            <nom>Isabelle Tellier</nom>
            <email>isabelle.tellier@univ-paris3.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Yoann Dupont</nom>
            <email>yoann.dupont@etu.univ-orleans.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Arnaud Courmet</nom>
            <email>arnaud.coumet@gmail.com</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LaTTiCe, université Paris 3 - Sorbonne Nouvelle</affiliation>
          <affiliation affiliationId="2">LIFO, université d’Orléans</affiliation>
        </affiliations>
        <titre>Un segmenteur-étiqueteur et un chunker pour le français</titre>
        <type>démonstration</type>
        <pages>7-8</pages>
        <resume>Nous proposons une démonstration de deux programmes : un segmenteur-étiqueteur POS pour le français et un programme de parenthésage en “chunks” de textes préalablement traités par le programme précédent. Tous deux ont été appris à partir du French Tree Bank.</resume>
        <mots_cles>étiquetage POS, chunking, apprentissage automatique, French Tree Bank, CRF</mots_cles>
        <title>A Segmenter-POS Labeller and a Chunker for French</title>
        <abstract>We propose a demo of two softwares : a Segmenter-POS Labeller for French and a Chunker for texts treated by the first program. Both have been learned from the French Tree Bank.</abstract>
        <keywords>POS tagging, chunking, Machine Learning, French Tree Bank, CRF</keywords>
      </article>
      <article id="taln-2012-demo-005" session="Démonstration">
        <auteurs>
          <auteur>
            <nom>Brigitte Bigi</nom>
            <email>brigitte.bigi@lpl-aix.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire Parole et Langage, CNRS &amp; Aix-Marseille Université, 5 avenue Pasteur, BP80975, 13604 Aix-en-Provence France</affiliation>
        </affiliations>
        <titre>SPPAS : segmentation, phonétisation, alignement, syllabation</titre>
        <type>démonstration</type>
        <pages>9-10</pages>
        <resume>SPPAS est le nouvel outil du LPL pour l’alignement texte/son. La segmentation s’opère en 4 étapes successives dans un processus entièrement automatique ou semi-automatique, à partir d’un fichier audio et d’une transcription. Le résultat comprend la segmentation en unités inter-pausales, en mots, en syllabes et en phonèmes. La version actuelle propose un ensemble de ressources qui permettent le traitement du français, de l’anglais, de l’italien et du chinois. L’ajout de nouvelles langues est facilitée par la simplicité de l’architecture de l’outil et le respect des formats de fichiers les plus usuels. L’outil bénéficie en outre d’une documentation en ligne et d’une interface graphique afin d’en faciliter l’accessibilité aux non-informaticiens. Enfin, SPPAS n’utilise et ne contient que des ressources et programmes sous licence libre GPL.</resume>
        <mots_cles>segmentation, phonétisation, alignement, syllabation</mots_cles>
        <title>SPPAS : a tool to perform text/speech alignment</title>
        <abstract>SPPAS is a new tool dedicated to phonetic alignments, from the LPL laboratory. SPPAS produces automatically or semi-automatically annotations which include utterance, word, syllabic and phonemic segmentations from a recorded speech sound and its transcription. SPPAS is currently implemented for French, English, Italian and Chinese There is a very simple procedure to add other languages in SPPAS : it is just needed to add related resources in the appropriate directories. SPPAS can be used by a large community of users : accessibility and portability are importants aspects in its development. The tools and resources will all be distributed with a GPL license.</abstract>
        <keywords>segmentation, phonetization, alignement, syllabification</keywords>
      </article>
      <article id="taln-2012-demo-006" session="Démonstration">
        <auteurs>
          <auteur>
            <nom>François-Régis Chaumartin</nom>
            <email>frc@proxem.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Proxem, 19 bd de Magenta, 75010 Paris</affiliation>
        </affiliations>
        <titre>Solution Proxem d’analyse sémantique verticale : adaptation au domaine des Ressources Humaines</titre>
        <type>démonstration</type>
        <pages>11-12</pages>
        <resume>Proxem développe depuis 2007 une plate-forme de traitement du langage, Antelope, qui permet de construire rapidement des applications sémantiques verticales (par exemple, pour l’e-réputation, la veille économique ou l’analyse d’avis de consommateurs). Antelope a servi à créer une solution pour les Ressources Humaines, utilisée notamment par l’APEC, permettant (1) d’extraire de l’information à partir d’offres et de CVs et (2) de trouver les offres d’emploi correspondant le mieux à un CV (ou réciproquement). Nous présentons ici l’adaptation d’Antelope à un domaine particulier, en l’occurrence les RH.</resume>
        <mots_cles>entités nommées, extraction de relations, création d’ontologies, similarité</mots_cles>
        <title>How to adapt the Proxem semantic analysis engine to the Human Resources field</title>
        <abstract>Proxem develops since 2007 the NLP platform, Antelope, with which one can quickly build vertical semantic applications (for e-reputation, business intelligence or consumer reviews analysis, for instance). Antelope was used to create a Human Resources solution, notably used by APEC, making it possible (1) to extract information from resumes and offers and (2) to find the most relevant jobs matching a given resume (or vice versa). We present here how to adapt Antelope to a particular area, namely HR.</abstract>
        <keywords>named entities, information extraction, ontologies development, matching</keywords>
      </article>
      <article id="taln-2012-demo-007" session="Démonstration">
        <auteurs>
          <auteur>
            <nom>Estelle Delpech</nom>
            <email>Estelle@nomao.com</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Laurent Candillier</nom>
            <email>Laurent@nomao.com</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">NOMAO, 1 avenue Jean Rieux 31500 Toulouse</affiliation>
          <affiliation affiliationId="2">EBUZZING GROUP, 97 rue du cherche-midi 75006 Paris</affiliation>
        </affiliations>
        <titre>Nomao : un moteur de recherche géolocalisé spécialisé dans la recommandation de lieux et l’e-réputation</titre>
        <type>démonstration</type>
        <pages>13-14</pages>
        <resume>Cette démonstration présente NOMAO, un moteur de recherche géolocalisé qui permet à ses utilisateurs de trouver des lieux (bars, magasins...) qui correspondent à leurs goûts, à ceux de leurs amis et aux recommandations des internautes.</resume>
        <mots_cles>recherche d’information, analyse d’opinion, génération de texte, fouille du web</mots_cles>
        <title>Nomao : a geolocalized search engine dedicated to place recommendation and ereputation</title>
        <abstract>This demonstration showcases NOMAO, a geolocalized search engine which recommends places (bars, shops...) based on the user’s and its friend’s tastes and on the web surfers’ recommendations</abstract>
        <keywords>information retrieval, opinion mining, text generation, web mining</keywords>
      </article>
      <article id="taln-2012-demo-008" session="Démonstration">
        <auteurs>
          <auteur>
            <nom>Samira MOUKRIM</nom>
            <email>samiramoukrim@yahoo.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université Sidi Mohamed Ben Abdellah</affiliation>
        </affiliations>
        <titre>Le DictAm : Dictionnaire électronique des verbes amazighs-français</titre>
        <type>démonstration</type>
        <pages>15-16</pages>
        <resume>Le DictAm est un dictionnaire électronique des verbes amazighs-français. Il vise à rendre compte de l’ensemble des verbes dans le domaine berbère : conjugaison, diathèse et sens. Le DictAm comporte actuellement près de 3000 verbes dans une soixantaine de parlers berbères. C’est un travail qui est en cours de réalisation et qui a pour ambition de répertorier tous les verbes berbères ainsi que leurs équivalents en français.</resume>
        <mots_cles>Dictionnaire électronique, dimension bilingue, diversité linguistique, verbes</mots_cles>
        <title>The DictAm : An electronic dictionary of Amazigh-French verbs</title>
        <abstract>In the frame of promoting the linguistic diversity among knowledge society, we suggest to elaborate an electronic dictionary of Amazigh-French verbs (DictAm). This dictionary aims at accounting for the verbs in the Amazigh sphere as a whole: conjugation, diathesis and meaning. The DictAm also adopts a comparative perspective in the sense that it collects the lexical materials of different dialectal varieties and makes them reachable. Now, the DictAm comprises around 3000 verbs deriving from about sixty Amazigh speeches. This work is currently in progress as well as it aspires to set up a repertoire of all Amazigh verbs and their French equivalents.</abstract>
        <keywords>Electronic dictionary, bilingual dimension, linguistic diversity, verbs</keywords>
      </article>
      <article id="taln-2012-demo-009" session="Démonstration">
        <auteurs>
          <auteur>
            <nom>Thomas Hueber</nom>
            <email>Thomas.Hueber@gipsa-lab.grenoble-inp.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Atef Ben-Youssef</nom>
            <email>Atef.Ben-Youssef@gipsa-lab.grenoble-inp.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Pierre Badin</nom>
            <email>Pierre.Badin@gipsa-lab.grenoble-inp.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Gérard Bailly</nom>
            <email>Gerard.Bailly@gipsa-lab.grenoble-inp.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Frédéric Eliséi</nom>
            <email>Frederic.Elisei@gipsa-lab.grenoble-inp.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">GIPSA-lab, UMR 5216/CNRS/INP/UJF/U.Stendhal, Grenoble, France</affiliation>
        </affiliations>
        <titre>Vizart3D : Retour Articulatoire Visuel pour l’Aide à la Prononciation</titre>
        <type>démonstration</type>
        <pages>17-18</pages>
        <resume>L’objectif du système Vizart3D est de fournir à un locuteur, en temps réel, et de façon automatique, un retour visuel sur ses propres mouvements articulatoires. Les applications principales de ce système sont l’aide à l’apprentissage des langues étrangères et la rééducation orthophonique (correction phonétique). Le système Vizart3D est basé sur la tête parlante 3D développée au GIPSA-lab, qui laisse apparaître, en plus des lèvres, les articulateurs de la parole normalement cachés (comme la langue). Cette tête parlante est animée automatiquement à partir du signal audio de parole, à l’aide de techniques de conversion de voix et de régression acoustico-articulatoire par GMM.</resume>
        <mots_cles>retour visuel, aide à la prononciation, GMM, temps réel, tête parlante</mots_cles>
        <title>Vizart3D: Visual Articulatory Feedack for Computer-Assisted Pronunciation Training</title>
        <abstract>We describe a system of visual articulatory feedback, which aims to provide any speaker with a real feedback on his/her own articulation. Application areas are computerassisted pronunciation training (phonetic correction) for second-language learning and speech rehabilitation. This system, named Vizartd3D, is based on the 3D augmented talking head developed at GIPSA-lab, which is able to display all speech articulators including usually hidden ones like the tongue. In our approach, the talking head is animated automatically from the audio speech signal, using GMM-based voice conversion and acoustic-to-articulatory regression.</abstract>
        <keywords>visual feedback, pronunciation training, GMM, real-time, talking head</keywords>
      </article>
      <article id="taln-2012-demo-010" session="Démonstration">
        <auteurs>
          <auteur>
            <nom>Emmanuel Ferragne</nom>
            <email>rocme@ish-lyon.cnrs.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Sébastien Flavier</nom>
            <email>rocme@ish-lyon.cnrs.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Christian Fressard</nom>
            <email>rocme@ish-lyon.cnrs.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CLILLAC-ARP, Université Paris 7</affiliation>
          <affiliation affiliationId="2">Dynamique Du Langage, UMR 5596, CNRS-Université de Lyon</affiliation>
        </affiliations>
        <titre>ROCme! : logiciel pour l’enregistrement et la gestion de corpus oraux</titre>
        <type>démonstration</type>
        <pages>19-20</pages>
        <resume>ROCme! permet une gestion rationalisée, autonome et dématérialisée de l’enregistrement de corpus oraux. Il dispose notamment d’une interface pour le recueil de métadonnées sur les locuteurs totalement paramétrable via des balises XML. Les locuteurs peuvent gérer les réponses au questionnaire, l’enregistrement audio, la lecture, la sauvegarde et le défilement des phrases (ou autres types de corpus) en toute autonomie. ROCme! affiche du texte, avec ou sans mise en forme HTML, des images, du son et des vidéos.</resume>
        <mots_cles>corpus, oral, linguistique, logiciel</mots_cles>
        <title>ROCme!: software for the recording and management of oral corpora</title>
        <abstract>management of speech recordings. Users can create interfaces for metadata collection thanks to XML tags. Speakers autonomously fill in questionnaires, record, play, and save audio; and browse sentences (or other types of corpora). ROCme! can display text, optionally with HTML formatting, images, sounds, and video.</abstract>
        <keywords>corpus, oral, linguistics, software</keywords>
      </article>
    </articles>
  </conference>
  <conference>
    <edition>
      <acronyme>TALN'2013</acronyme>
      <titre>conférence sur le Traitement Automatique des Langues Naturelles</titre>
      <ville>Les Sables d'Olonne</ville>
      <pays>France</pays>
      <dateDebut>2013-06-17</dateDebut>
      <dateFin>2012-06-21</dateFin>
      <presidents>
        <nom>Emmanuel Morin</nom>
        <nom>Yannick Estève</nom>
      </presidents>
      <typeArticles>
        <type id="invite">Conférenciers invités</type>
        <type id="charte">Charte Ethique et Big Data</type>
        <type id="long">Articles longs</type>
        <type id="court">Articles courts</type>
        <type id="démonstration">Démonstrations</type>
      </typeArticles>
      <statistiques>
        <acceptations id="long" soumissions="70">36</acceptations>
        <acceptations id="court" soumissions="57">35</acceptations>
        <acceptations id="démonstration" soumissions="13">13</acceptations>
      </statistiques>
      <siteWeb>http://www.taln2013.org/</siteWeb>
      <meilleurArticle>
        <articleId/>
      </meilleurArticle>
    </edition>
    <articles>
      <article id="taln-2013-invite-001" session="Conférence invité">
        <auteurs>
          <auteur>
            <nom>Alexander Fraser</nom>
            <email>fraser@cis.uni-muenchen.de</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Center for Information and Language Processing, LMU, Munich</affiliation>
        </affiliations>
        <titre>Améliorer la traduction des langages morphologiquement riches</titre>
        <type>invite</type>
        <pages>1-1</pages>
        <resume>Si les techniques statistiques pour la traduction automatique ont fait des progrès significatifs au cours des 20 dernières années, les résultats pour la traduction de langues morphologiquement riches sont toujours mitigés par rapport aux précédentes générations de systèmes à base de règles. Les recherches actuelles en traduction statistique de langues morphologiquement riches varient grandement en fonction de la quantité de connaissances linguistiques utilisées et de la nature de ces connaissances. Cette variation est plus importante en langue cible (par exemple, les ressources utilisées en traduction automatique statistique respectueuse de linguistique en arabe, en français et en allemand sont très différentes). La conférence portera sur les techniques état de l’art dédiées à la tâche de traduction statistique pour une langue cible qui est morphologiquement plus riche que la langue source.</resume>
        <mots_cles>traduction statistique, langages morphologiquement riches, connaissances linguistiques</mots_cles>
        <title>Improving Translation to Morphologically Rich Languages</title>
        <abstract>While statistical techniques for machine translation have made significant progress in the last 20 years, results for translating to morphologically rich languages are still mixed versus previous generation rule-based systems. Current research in statistical techniques for translating to morphologically rich languages varies greatly in the amount of linguistic knowledge used and the form of this linguistic knowledge. This varies most strongly by target language (e.g., the resources used for linguistically-aware statistical machine translation to Arabic, French, German are very different). The talk will discuss state-of-the-art techniques for statistical translation tasks involving translating to a target language which is morphologically richer than the source language.</abstract>
        <keywords>statistical translation, morphologically rich languages, linguistic knowledge</keywords>
      </article>
      <article id="taln-2013-invite-002" session="Conférence invité">
        <auteurs>
          <auteur>
            <nom>Josiane Mothe</nom>
            <email>Josiane.mothe@irit.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">IRIT, UMR 5505, Université de Toulouse, 118 Route de Narbonne, 31062 Toulouse Cedex</affiliation>
          <affiliation affiliationId="2">IUFM, Ecole interne, Université de Toulouse, 56 av. de l’URSS, 31079 Toulouse</affiliation>
        </affiliations>
        <titre>Recherche d’Information et Traitement Automatique des Langues Naturelles</titre>
        <type>invite</type>
        <pages>2-2</pages>
        <resume>La recherche d’information s’intéresse à l’accès aux documents et une majorité de travaux dans le domaine s’appuie sur les éléments textuels de ces documents écrits en langage naturel. Les requêtes soumisses par les utilisateurs de moteurs de recherche sont également textuelles, même si elles sont très pauvres d’un point de vue linguistique. Il parait donc naturel que les travaux en recherche d’information cherchent à s’alimenter par les avancées et les résultats en traitement automatique des langues naturelles. Malgré les espoirs déçus des années 80, l’engouement pour l’utilisation du traitement du langage naturel en recherche d’information reste intact, poussé par les nouvelles perspectives offertes. Dans cette conférence, nous balayerons les aspects de la recherche d’information qui se sont le plus appuyés sur des éléments du traitement automatique des langues naturelles. Nous présenterons en particulier quelques résultats relatifs à la reformulation automatique de requêtes, à la prédiction de la difficulté des requêtes, au résumé automatique et à la contextualisation de textes courts ainsi que les perspectives actuelles offertes en particulier par les travaux en linguistique computationnelle.</resume>
        <mots_cles>Recherche d’information, traitement automatique des langues, reformulation de requêtes, difficulté des requêtes, résumé automatique</mots_cles>
        <title>Information Retrieval and Natural Language Processing</title>
        <abstract>Information retrieval aims at providing means to access documents. Most of current work in the domain relies on the textual elements of these documents which are written in natural language. Users’ queries are also generally textual, even if the queries are very poor from a linguistic point of view. As a results information retrieval field aimed at feeding on advances and results from natural language processing field. In spite of the disappointed hopes of the 80s, the enthusiasm for using natural language processing in information retrieval remains high, pushed by the new perspectives. In this talk, we will mention the various aspects of information retrieval which rely, at various levels, on natural language processing components. We will present in particular some results regardless automatic query reformulation, query difficulty prediction, automatic summarization and short text contextualization as well as some perspectives offered in particular considering computational linguistics.</abstract>
        <keywords>Information retrieval, natural language processing, query reformulation, query difficulty, automatic summarization</keywords>
      </article>
      <article id="taln-2013-charte-001" session="Charte Ethique &amp; Big Data">
        <auteurs>
          <auteur>
            <nom>Karën Fort</nom>
            <email>karen.fort@loria.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Alain Couillault</nom>
            <email>alain.couillault@univ-lr.fr</email>
            <affiliationId>4</affiliationId>
            <affiliationId>5</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université de Lorraine</affiliation>
          <affiliation affiliationId="2">LORIA 54500 Vandoeuvre-lès-Nancy</affiliation>
          <affiliation affiliationId="3">ATALA</affiliation>
          <affiliation affiliationId="4">Université de La Rochelle</affiliation>
          <affiliation affiliationId="5">APROGED</affiliation>
        </affiliations>
        <titre>La Charte Éthique et Big Data : pour des ressources pour le TAL (enfin !) traçables et pérennes</titre>
        <type>charte</type>
        <pages>3-4</pages>
        <resume>La charte Ethique &amp; Big Data a été conçue à l’initiative de l’ATALA, de l’AFCP, de l’APROGED et de CAP DIGITAL, au sein d’un groupe de travail mixte réunissant d’autres partenaires académiques et industriels (tels que le CERSA-CNRS, Digital Ethics, Eptica-Lingway, le cabinet Itéanu ou ELRA/ELDA). Elle se donne comme objectif de fournir des garanties concernant la traçabilité des données (notamment des ressources langagières), leur qualité et leur impact sur l’emploi. Cette charte a été adoptée par Cap Digital (co-rédacteur). Nous avons également proposé à la DGLFLF et à l’ANR de l’utiliser. Elle est aujourd’hui disponible sous forme de wiki, de fichier pdf et il en existe une version en anglais. La charte est décrite en détails dans (Couillault et Fort, 2013).</resume>
        <mots_cles>éthique, big data, ressources langagières</mots_cles>
        <title>The Ethics &amp; Big Data Charter : for tractable and lasting NLP resources</title>
        <abstract>The Ethics &amp; Big Data Charter was designed by ATALA, AFCP, APROGED and CAP DIGITAL, in a working group including other academic and industrial partners (such as CERSA-CNRS, Digital Ethics, Eptica-Lingway, Itéanu office or ELRA/ELDA). Its aims at ensuring the traceability and quality of the data (including language resources), how they are produced and their impact on working conditions. This charter has been adopted by Cap Digital (co-writer). We also proposed it to DGLFLF and ANR. As of today, it is available as a wiki, a pdf file and an English version 6. The charter is detailled in (Couillault et Fort, 2013).</abstract>
        <keywords>ethics, big data, language resources</keywords>
      </article>
      <article id="taln-2013-long-001" session="Morphologie et Segmentation">
        <auteurs>
          <auteur>
            <nom>Fatima Zahra Nejme</nom>
            <email>fatimazahra.nejme@gmail.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Siham Boulaknadel</nom>
            <email>Boulaknadel@ircam.ma</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Driss Aboutajdine</nom>
            <email>aboutaj@fsr.ac.ma</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LRIT, Unité Associée au CNRST (URAC 29), Faculté des Sciences, Mohammed V-Agdal, Rabat, Maroc.</affiliation>
          <affiliation affiliationId="2">IRCAM, Avenue Allal El Fassi, Madinat Al Irfane, Rabat-Instituts, Maroc.</affiliation>
        </affiliations>
        <titre>Analyse Automatique de la Morphologie Nominale Amazighe</titre>
        <type>long</type>
        <pages>5-18</pages>
        <resume>Dans le but de préserver le patrimoine amazighe et éviter qu’il soit menacé de disparition, il semble opportun de doter cette langue de moyens nécessaires pour faire face aux enjeux de l'accès au domaine de l'Information et de la Communication (TIC). Dans ce contexte, et dans la perspective de construire des outils et des ressources linguistiques pour le traitement automatique de cette langue, nous avons entrepris de construire un système d’analyse morphologique pour l’amazighe standard du Maroc. Ce système profite des apports des modèles { états finis au sein de l’environnement linguistique de développement NooJ en faisant appel à des règles grammaticales à large couverture.</resume>
        <mots_cles>La langue amazighe, TALN, NooJ, analyse morphologique, morphologie flexionnelle, morphologie dérivationnelle</mots_cles>
        <title>Morphological analysis of the standard Amazigh language using NooJ platform</title>
        <abstract>In the aim of safeguarding the Amazigh heritage from being threathned of disappearance, it seems opportune to equip this language of necessary means to confront the stakes of access to the domain of New Information and Communication Technologies (ICT). In this context, and in the perspective to build tools and linguistic resources for the automatic processing of Amazigh language, we have undertaken to develop a system of a morphological description for standard Amazigh of Morocco. This system uses finite state technology, within the linguistic developmental environment NooJ by using a large-coverage of morphological grammars covering all grammatical rules.</abstract>
        <keywords>Amazigh language, NLP, NooJ, morphological analysis, inflectional morphology, derivational morphology</keywords>
      </article>
      <article id="taln-2013-long-002" session="Apprentissage">
        <auteurs>
          <auteur>
            <nom>Isabelle Tellier</nom>
            <email>isabelle.tellier@univ-paris3.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Yoann Dupont</nom>
            <email>yoa.dupont@gmail.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire Lattice, 1 rue Maurice Arnoux, 92320 Montrouge</affiliation>
        </affiliations>
        <titre>Apprentissage symbolique et statistique pour le chunking:comparaison et combinaisons</titre>
        <type>long</type>
        <pages>19-32</pages>
        <resume>Nous décrivons dans cet article l’utilisation d’algorithmes d’inférence grammaticale pour la tâche de chunking, pour ensuite les comparer et les combiner avec des CRF (Conditional Random Fields), à l’efficacité éprouvée pour cette tâche. Notre corpus est extrait du French TreeBank. Nous proposons et évaluons deux manières différentes de combiner modèle symbolique et modèle statistique appris par un CRF et montrons qu’ils bénéficient dans les deux cas l’un de l’autre.</resume>
        <mots_cles>apprentissage automatique, chunking, CRF, inférence grammaticale, k-RI, FrenchTreeBank</mots_cles>
        <title>Symbolic and statistical learning for chunking : comparison and combinations</title>
        <abstract>We describe in this paper how to use grammatical inference algorithms for chunking, then compare and combine them to CRFs (Conditional Random Fields) which are known efficient for this task. Our corpus is extracted from the FrenchTreebank. We propose and evaluate two ways of combining a symbolic model and a statistical model learnt by a CRF, and show that in both cases they benefit from one another.</abstract>
        <keywords>machine learning, chunking, CRF, grammatical inference, k-RI, French TreeBank</keywords>
      </article>
      <article id="taln-2013-long-003" session="">
        <auteurs>
          <auteur>
            <nom>Yllias Chali</nom>
            <email>yllias.chali@uleth.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Sadid A.Hasan</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Mustapha Mojahid</nom>
            <email>mustapha.mojahid@irit.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">University of Lethbridge, AB, Canada</affiliation>
          <affiliation affiliationId="2">Université Paul Sabatier – IRIT, 118 Rte de Narbonne 31062 Toulouse Cedex</affiliation>
        </affiliations>
        <titre>L’utilisation des POMDP pour les résumés multi-documents orientés par une thématique</titre>
        <type>long</type>
        <pages>33-47</pages>
        <resume>L’objectif principal du résumé multi-documents orienté par une thématique est de générer un résumé à partir de documents sources en réponse à une requête formulée par l’utilisateur. Cette tâche est difficile car il n’existe pas de méthode efficace pour mesurer la satisfaction de l’utilisateur. Cela introduit ainsi une incertitude dans le processus de génération de résumé. Dans cet article, nous proposons une modélisation de l’incertitude en formulant notre système de résumé comme un processus de décision markovien partiellement observables (POMDP) car dans de nombreux domaines on a montré que les POMDP permettent de gérer efficacement les incertitudes. Des expériences approfondies sur les jeux de données du banc d’essai DUC ont démontré l’efficacité de notre approche.</resume>
        <mots_cles>Résumé multi-document, résumé orienté requête, POMDP</mots_cles>
        <title>Using POMDPs for Topic-Focused Multi‐Document Summarization</title>
        <abstract>The main goal of topic-focused multidocument summarization is to generate a summary from the source documents in response to a given query or particular information requested by the user. This task is difficult in large part because there is no significant way of measuring whether the user is satisfied with the information provided. This introduces uncertainty in the current state of the summary generation procedure. In this paper, we model the uncertainty explicitly by formulating our summarization system as a Partially Observable Markov Decision Process (POMDP) since researchers in many areas have shown that POMDPs can deal with uncertainty successfully. Extensive experiments on the DUC benchmark datasets demonstrate the effectiveness of our approach.</abstract>
        <keywords>Topic-focused multi-document summarization, POMDP</keywords>
      </article>
      <article id="taln-2013-long-004" session="Plénière">
        <auteurs>
          <auteur>
            <nom>Olivier Ferret</nom>
            <email>olivier.ferret@cea.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CEA, LIST, Laboratoire Vision et Ingénierie des Contenus, Gif-sur-Yvette, F-91191 France.</affiliation>
        </affiliations>
        <titre>Sélection non supervisée de relations sémantiques pour améliorer un thésaurus distributionnel</titre>
        <type>long</type>
        <pages>48-61</pages>
        <resume>Les travaux se focalisant sur la construction de thésaurus distributionnels ont montré que les relations sémantiques qu’ils recèlent sont principalement fiables pour les mots de forte fréquence. Dans cet article, nous proposons une méthode pour rééquilibrer de tels thésaurus en faveur des mots de fréquence faible sur la base d’un mécanisme d’amorçage : un ensemble d’exemples et de contre-exemples de mots sémantiquement similaires sont sélectionnés de façon non supervisée et utilisés pour entraîner un classifieur supervisé. Celui-ci est ensuite appliqué pour réordonner les voisins sémantiques du thésaurus utilisé pour sélectionner les exemples et contre-exemples. Nous montrons comment les relations entre les constituants de noms composés similaires peuvent être utilisées pour réaliser une telle sélection et comment conjuguer ce critère à un critère déjà expérimenté sur la symétrie des relations sémantiques. Nous évaluons l’intérêt de cette procédure sur un large ensemble de noms en anglais couvrant un vaste spectre de fréquence.</resume>
        <mots_cles>Sémantique lexicale, similarité sémantique, thésaurus</mots_cles>
        <title>Unsupervised selection of semantic relations for improving a distributional thesaurus</title>
        <abstract>Work about distributional thesauri has shown that the relations in these thesauri are mainly reliable for high frequency words. In this article, we propose a method for improving such a thesaurus through its re-balancing in favor of low frequency words. This method is based on a bootstrapping mechanism : a set of positive and negative examples of semantically similar words are selected in an unsupervised way and used for training a supervised classifier. This classifier is then applied for reranking the semantic neighbors of the thesaurus used for example selection. We show how the relations between the mono-terms of similar nominal compounds can be used for performing this selection and how to associate this criterion with an already tested criterion based on the symmetry of semantic relations. We evaluate the interest of the global procedure for a large set of English nouns with various frequencies.</abstract>
        <keywords>Lexical semantics, semantic similarity, distributional thesauri</keywords>
      </article>
      <article id="taln-2013-long-005" session="Plénière">
        <auteurs>
          <auteur>
            <nom>Marie Dupuch</nom>
            <email>mdupuch@objetdirect.com</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Thierry Hamon</nom>
            <email>thierry.hamon@univ-paris13.fr</email>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom> Natalia Grabar</nom>
            <email>natalia.grabar@univ-lille3.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CNRS UMR 8163 STL, Université Lille 3, 59653 Villeneuve d’Ascq, France</affiliation>
          <affiliation affiliationId="2">Viseo-Objet Direct, 4, avenue Doyen Louis Weil, 38000 Grenoble</affiliation>
          <affiliation affiliationId="3">LIM&amp;BIO (EA3969), Université Paris 13, Sorbonne Paris Cité, 74, rue Marcel Cachin, 93017 Bobigny Cedex France</affiliation>
        </affiliations>
        <titre>Groupement de termes basé sur des régularités linguistiques et sémantiques dans un contexte cross-langue</titre>
        <type>long</type>
        <pages>62-75</pages>
        <resume>Nous proposons d’exploiter des méthodes du Traitement Automatique de Langues dédiées à la structuration de terminologie indépendamment dans deux langues (anglais et français) et de fusionner ensuite les résultats obtenus dans chaque langue. Les termes sont groupés en clusters grâce aux relations générées. L’évaluation de ces relations est effectuée au travers de la comparaison des clusters avec des données de référence et la baseline, tandis que la complémentarité des relations est analysée au travers de leur implication dans la création de clusters de termes. Les résultats obtenus indiquent que : chaque langue contribue de manière équilibrée aux résultats, le nombre de relations hiérarchiques communes est plus grand que le nombre de relations synonymiques communes. Globalement, les résultats montrent que, dans un contexte cross-langue, chaque langue permet de détecter des régularités linguistiques et sémantiques complémentaires. L’union des résultats obtenus dans les deux langues améliore la qualité globale des clusters.</resume>
        <mots_cles>Relations sémantiques, termes, domaine de spécialité, médecine, contexte crosslangue</mots_cles>
        <title>Grouping of terms based on linguistic and semantic regularities in a cross-lingual context</title>
        <abstract>We propose to exploit the Natural Language Processing methods dedicated to terminology structuring independently in two languages (English and French) and then to merge the results obtained in each language. The terms are grouped into clusters thanks to the generated relations. The evaluation of the relations is done via the comparison of the clusters with the reference data and the baseline, while the complementarity of the relations is analyzed through their involvement in the clusters of terms. Our results indicate that : each language contributes almost equally to the generated results ; the number of common hierarchical relations is greater than the number of common synonym relations. On the whole, the obtained results point out that in a cross-language context, each language brings additional linguistic and semantic regularities. The union of the results obtained in each language improves the overall quality of the clusters.</abstract>
        <keywords>Semantic relations, terms, specialized areas, medicine, cross-lingual context</keywords>
      </article>
      <article id="taln-2013-long-006" session="Traduction et Alignement">
        <auteurs>
          <auteur>
            <nom>Quentin Pradet</nom>
            <email>quentin.pradet@cea.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Jeanne Baguenier-Desormeaux</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Gaël de Chalendar</nom>
            <email>gael.de-chalendar@cea.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Laurence Danlos</nom>
            <email>laurence.danlos@linguist.univ-paris-diderot.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CEA, LIST, Laboratoire Vision et Ingénierie des Contenus,Gif-sur-Yvette, F-91191, France.</affiliation>
          <affiliation affiliationId="2">Univ Paris Diderot, Sorbonne Paris Cité, ALPAGE, UMR-I 001 INRIA</affiliation>
        </affiliations>
        <titre>WoNeF : amélioration, extension et évaluation d’une traduction française automatique de WordNet</titre>
        <type>long</type>
        <pages>76-89</pages>
        <resume>Identifier les sens possibles des mots du vocabulaire est un problème difficile demandant un travail manuel très conséquent. Ce travail a été entrepris pour l’anglais : le résultat est la base de données lexicale WordNet, pour laquelle il n’existe encore que peu d’équivalents dans d’autres langues. Néanmoins, des traductions automatiques de WordNet vers de nombreuses langues cibles existent, notamment pour le français. JAWS est une telle traduction automatique utilisant des dictionnaires et un modèle de langage syntaxique. Nous améliorons cette traduction, la complétons avec les verbes et adjectifs de WordNet, et démontrons la validité de notre approche via une nouvelle évaluation manuelle. En plus de la version principale nommée WoNeF, nous produisons deux versions supplémentaires : une version à haute précision (93% de précision, jusqu’à 97% pour les noms), et une version à haute couverture contenant 109 447 paires (littéral, synset).</resume>
        <mots_cles>WordNet, désambiguïsation lexicale, traduction, ressource</mots_cles>
        <title>WoNeF, an improved, extended and evaluated automatic French translation of WordNet</title>
        <abstract>Identifying the various possible meanings of each word of the vocabulary is a difficult problem that requires a lot of manual work. It has been tackled by the WordNet lexical semantics database in English, but there are still few resources available for other languages. Automatic translations of WordNet have been tried to many target languages such as French. JAWS is such an automatic translation of WordNet nouns to French using bilingual dictionaries and a syntactic langage model. We improve the existing translation precision and coverage, complete it with translations of verbs and adjectives and enhance its evaluation method, demonstrating the validity of the approach. In addition to the main result called WoNeF, we produce two additional versions : a high-precision version with 93% precision (up to 97% on nouns) and a high-coverage version with 109,447 (literal, synset) pairs.</abstract>
        <keywords>WordNet, Word Sense Disambiguation, translation, resource</keywords>
      </article>
      <article id="taln-2013-long-007" session="Sémantique">
        <auteurs>
          <auteur>
            <nom>Bassam Jabaian</nom>
            <email>{bassam.jabaian@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Fabrice Lefèvre</nom>
            <email>fabrice.lefevre@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Laurent Besacier</nom>
            <email>laurent.besacier@imag.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIA, Université d’Avignon et des Pays de Vaucluse, Avignon, France</affiliation>
          <affiliation affiliationId="2">LIG, Université Joseph Fourrier, Grenoble, France</affiliation>
        </affiliations>
        <titre>Approches statistiques discriminantes pour l’interprétation sémantique multilingue de la parole</titre>
        <type>long</type>
        <pages>90-103</pages>
        <resume>Les approches statistiques sont maintenant très répandues dans les différentes applications du traitement automatique de la langue et le choix d’une approche particulière dépend généralement de la tâche visée. Dans le cadre de l’interprétation sémantique multilingue, cet article présente une comparaison entre les méthodes utilisées pour la traduction automatique et celles utilisées pour la compréhension de la parole. Cette comparaison permet de proposer une approche unifiée afin de réaliser un décodage conjoint qui à la fois traduit une phrase et lui attribue ses étiquettes sémantiques. Ce décodage est obtenu par une approche à base de transducteurs à états finis qui permet de composer un graphe de traduction avec un graphe de compréhension. Cette représentation peut être généralisée pour permettre des transmissions d’informations riches entre les composants d’un système d’interaction vocale homme-machine.</resume>
        <mots_cles>compréhension multilingue, système de dialogue, CRF, graphes d’hypothèses</mots_cles>
        <title>Discriminative statistical approaches for multilingual speech understanding</title>
        <abstract>Statistical approaches are now widespread in the various applications of natural language processing and the elicitation of an approach usually depends on the targeted task. This paper presents a comparison between the methods used for machine translation and speech understanding. This comparison allows to propose a unified approach to perform a joint decoding which translates a sentence and assign semantic tags to the translation at the same time. This decoding is achieved through a finite-state transducer approach which allows to compose a translation graph with an understanding graph. This representation can be generalized to allow the rich transmission of information between the components of a human-machine vocal interface.</abstract>
        <keywords>multilingual understanding, dialogue system, CRF, hypothesis graphs</keywords>
      </article>
      <article id="taln-2013-long-008" session="Connaissances et Discours">
        <auteurs>
          <auteur>
            <nom>Chloé Braud</nom>
            <email>chloe.braud@inria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Pascal Denis</nom>
            <email>pascal.denis@inria.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">ALPAGE, INRIA Paris-Rocquencourt &amp; Université Paris Diderot</affiliation>
          <affiliation affiliationId="2">MAGNET, INRIA Lille Nord-Europe</affiliation>
        </affiliations>
        <titre>Identification automatique des relations discursives « implicites » à partir de données annotées et de corpus bruts</titre>
        <type>long</type>
        <pages>104-117</pages>
        <resume>Cet article présente un système d’identification des relations discursives dites « implicites » (à savoir, non explicitement marquées par un connecteur) pour le français. Etant donné le faible volume de données annotées disponibles, notre système s’appuie sur des données étiquetées automatiquement en supprimant les connecteurs non ambigus pris comme annotation d’une relation, une méthode introduite par (Marcu et Echihabi, 2002). Comme l’ont montré (Sporleder et Lascarides, 2008) pour l’anglais, cette approche ne généralise pas très bien aux exemples de relations implicites tels qu’annotés par des humains. Nous arrivons au même constat pour le français et, partant du principe que le problème vient d’une différence de distribution entre les deux types de données, nous proposons une série de méthodes assez simples, inspirées par l’adaptation de domaine, qui visent à combiner efficacement données annotées et données artificielles. Nous évaluons empiriquement les différentes approches sur le corpus ANNODIS : nos meilleurs résultats sont de l’ordre de 45.6% d’exactitude, avec un gain significatif de 5.9% par rapport à un système n’utilisant que les données annotées manuellement.</resume>
        <mots_cles>analyse du discours, relations implicites, apprentissage automatique</mots_cles>
        <title>Automatically identifying implicit discourse relations using annotated data and raw corpora</title>
        <abstract>This paper presents a system for identifying « implicit » discourse relations (that is, relations that are not marked by a discourse connective). Given the little amount of available annotated data for this task, our system also resorts to additional automatically labeled data wherein unambiguous connectives have been suppressed and used as relation labels, a method introduced by (Marcu et Echihabi, 2002). As shown by (Sporleder et Lascarides, 2008) for English, this approach doesn’t generalize well to implicit relations as annotated by humans. We show that the same conclusion applies to French due to important distribution differences between the two types of data. In consequence, we propose various simple methods, all inspired from work on domain adaptation, with the aim of better combining annotated data and artificial data. We evaluate these methods through various experiments carried out on the ANNODIS corpus : our best system reaches a labeling accuracy of 45.6%, corresponding to a 5.9% significant gain over a system solely trained on manually labeled data.</abstract>
        <keywords>discourse analysis, implicit relations, machine learning</keywords>
      </article>
      <article id="taln-2013-long-009" session="Apprentissage">
        <auteurs>
          <auteur>
            <nom>Emmanuel Lassalle</nom>
            <email>emmanuel.lassalle@ens-lyon.org</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Pascal Denis</nom>
            <email>pascal.denis@inria.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Alpage : INRIA - Université Paris Diderot, Sorbonne Paris Cité</affiliation>
          <affiliation affiliationId="2">Magnet : INRIA Nord Lille Europe - Université de Lille LIFL</affiliation>
        </affiliations>
        <titre>Apprentissage d’une hiérarchie de modèles à paires spécialisés pour la résolution de la coréférence</titre>
        <type>long</type>
        <pages>118-131</pages>
        <resume>Nous proposons une nouvelle méthode pour améliorer significativement la performance des modèles à paires de mentions pour la résolution de la coréférence. Étant donné un ensemble d’indicateurs, notre méthode apprend à séparer au mieux des types de paires de mentions en classes d’équivalence, chacune de celles-ci donnant lieu à un modèle de classification spécifique. La procédure algorithmique proposée trouve le meilleur espace de traits (créé à partir de combinaisons de traits élémentaires et d’indicateurs) pour discriminer les paires de mentions coréférentielles. Bien que notre approche explore un très vaste ensemble d’espaces de trait, elle reste efficace en exploitant la structure des hiérarchies construites à partir des indicateurs. Nos expériences sur les données anglaises de la CoNLL-2012 Shared Task indiquent que notre méthode donne des gains de performance par rapport au modèle initial utilisant seulement les traits élémentaires, et ce, quelque soit la méthode de formation des chaînes ou la métrique d’évaluation choisie. Notre meilleur système obtient une moyenne de 67.2 en F1-mesure MUC, B3 et CEAF ce qui, malgré sa simplicité, le situe parmi les meilleurs systèmes testés sur ces données.</resume>
        <mots_cles>résolution de la coréférence, apprentissage automatique</mots_cles>
        <title>Learning a hierarchy of specialized pairwise models for coreference resolution</title>
        <abstract>This paper proposes a new method for significantly improving the performance of pairwise coreference models. Given a set of indicators, our method learns how to best separate types of mention pairs into equivalence classes for which we construct distinct classification models. In effect, our approach finds the best feature space (derived from a base feature set and indicator set) for discriminating coreferential mention pairs. Although our approach explores a very large space of possible features spaces, it remains tractable by exploiting the structure of the hierarchies built from the indicators. Our experiments on the CoNLL-2012 shared task English datasets indicate that our method is robust to different clustering strategies and evaluation metrics, showing large and consistent improvements over a single pairwise model using the same base features. Our best system obtains 67.2 of average F1 over MUC, B3, and CEAF which, despite its simplicity, places it among the best performing systems on these datasets.</abstract>
        <keywords>coreference resolution, machine learning</keywords>
      </article>
      <article id="taln-2013-long-010" session="Connaissances &amp; Discours">
        <auteurs>
          <auteur>
            <nom>Jean-Philippe Fauconnier</nom>
            <email>Jean-Philippe.Fauconnier@irit.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Mouna Kamel</nom>
            <email>Mouna.Kamel@irit.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Bernard Rothenburger</nom>
            <email>Bernard.Rothenburger@irit.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Nathalie Aussenac-Gilles</nom>
            <email>Nathalie.Aussenac-Gilles@irit.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">IRIT, 118 route de Narbonne 31060 Toulouse Cedex 5</affiliation>
        </affiliations>
        <titre>Apprentissage supervisé pour l’identification de relations sémantiques au sein de structures énumératives parallèles</titre>
        <type>long</type>
        <pages>132-145</pages>
        <resume>Ce travail s’inscrit dans le cadre de la construction et l’enrichissement d’ontologies à partir de textes de type encyclopédique ou scientifique. L’originalité de notre travail réside dans l’extraction de relations sémantiques exprimées au-delà de la linéarité du texte. Pour cela, nous nous appuyons sur la sémantique véhiculée par les caractères typo-dispositionels qui ont pour fonction de suppléer des formulations strictement linguistiques qui seraient plus difficilement exploitables. L’étude que nous proposons concerne les relations sémantiques portées par les structures énumératives parallèles qui, bien qu’affichant des discontinuités entre ses différents composants, présentent un tout sur le plan sémantique. Ce sont des structures textuelles qui sont propices aux relations hiérarchiques. Après avoir défini une typologie des relations portées par ce type de structure, nous proposons une approche par apprentissage visant à leur identification. Sur la base de traits incorporant informations lexico-syntaxiques et typo-dispositionnelles, les premiers résultats aboutissent à une exactitude de 61,1%.</resume>
        <mots_cles>extraction de relations, structures énumératives parallèles, mise en forme matérielle, apprentissage supervisé, construction d’ontologies</mots_cles>
        <title>A Supervised learning for the identification of semantic relations in parallel enumerative structures</title>
        <abstract>This work falls within the framework of ontology engineering and learning from encyclopedic or scientific texts. Our original contribution lies within the extraction of semantic relations expressed beyond the text linearity. To this end, we relied on the semantics behind the typo-dispositional characters whose function is to supplement the strictly linguistic formulations that could be more difficult to exploit. The work reported here is dealing with the semantic relations carried by the parallel enumerative structures. Although they display discontinuities between their various components, these enumerative structures form a whole at the semantic level. They are textual structures that are prone to hierarchic relations. After defining a typology of the relationships carried by this type of structure, we are proposing a learning approach aimed at their identification. Based on features including lexico-syntactic and typo-dispositional informations, the first results led an accuracy of 61.1%.</abstract>
        <keywords>relationship extraction, parallel enumerative structures, material shaping, supervised learning, ontology learning</keywords>
      </article>
      <article id="taln-2013-long-011" session="Lexique et Corpus">
        <auteurs>
          <auteur>
            <nom>Marie-Paule Jacques</nom>
            <email>marie-paule.jacques@ujf-grenoble.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Laura Hartwell</nom>
            <email>laura.hartwell@ujf-grenoble.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Achille Falaise</nom>
            <email>achille.falaise@imag.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Univ. Grenoble Alpes, UJF &amp; LIDILEM, F-38040 Grenoble</affiliation>
          <affiliation affiliationId="2">Univ. Grenoble Alpes, UPMF &amp; LIG-GETALP, F-38040 Grenoble</affiliation>
        </affiliations>
        <titre>Techniques de TAL et corpus pour faciliter les formulations en anglais scientifique écrit</titre>
        <type>long</type>
        <pages>146-159</pages>
        <resume>Nous présentons l'adaptation de la base d'écrits scientifiques en ligne Scientext pour un « nouveau » public : chercheurs et autres auteurs français d'écrits scientifiques, ayant besoin de rédiger en anglais. Cette adaptation a consisté à ajouter dans la base des requêtes précodées qui permettent d'afficher les contextes dans lesquels les auteurs d'articles scientifiques en anglais expriment leur objectif de recherche et à enrichir l'interface ScienQuest de nouvelles fonctionnalités pour mémoriser et réafficher les contextes pertinents, pour faciliter la consultation par un public plus large. Les nombreuses descriptions linguistiques de la rhétorique des articles scientifiques insistent sur l'importance de la création et de l'occupation d'une « niche » de recherche. Chercheurs et doctorants ont ici un moyen d'en visualiser des exemples sans connaître sa formulation a priori, via nos requêtes. Notre évaluation sur le corpus de test en donne une précision globale de 86,5 %.</resume>
        <mots_cles>anglais, patrons lexico-syntaxiques, ScienQuest, Scientext</mots_cles>
        <title>NLP and corpus techniques for finding formulations that facilitate scientific writing in English</title>
        <abstract>This paper presents adaptations of the query options integrated into the online corpus Scientext so as to better serve a new audience: French scientists writing in English. We added pre-coded queries that display the contexts in which authors of scientific articles in English state their research objective. Furthermore, new functional options enrich the ScienQuest interface allowing results to be filtered for noise and then saved for consultation by a larger public. Previous studies on the scientific discourse and rhetoric of scientific articles have highlighted the importance of establishing and occupying a research niche. Here, francophone researchers and doctoral students without prior discursive knowledge, can access authentic and multiple ways of formulating a research objective. Our evaluation of a test corpus showed an overall accuracy of 86.5 %.</abstract>
        <keywords>ESP, lexico-syntactic patterns, ScienQuest, Scientext</keywords>
      </article>
      <article id="taln-2013-long-012" session="Lexique et Corpus">
        <auteurs>
          <auteur>
            <nom>Nicolas Hernandez</nom>
            <email>nicolas.hernandez@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Florian Boudin</nom>
            <email>florian.boudin@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université de Nantes</affiliation>
        </affiliations>
        <titre>Construction d’un large corpus écrit libre annoté morpho-syntaxiquement en français</titre>
        <type>long</type>
        <pages>160-173</pages>
        <resume>Cet article étudie la possibilité de créer un nouveau corpus écrit en français annoté morphosyntaxiquement à partir d’un corpus annoté existant. Nos objectifs sont de se libérer de la licence d’exploitation contraignante du corpus d’origine et d’obtenir une modernisation perpétuelle des textes. Nous montrons qu’un corpus pré-annoté automatiquement peut permettre d’entraîner un étiqueteur produisant des performances état-de-l’art, si ce corpus est suffisamment grand.</resume>
        <mots_cles>corpus arboré, construction de corpus, étiquetage morpho-syntaxique</mots_cles>
        <title>Construction of a Free Large Part-of-Speech Annotated Corpus in French</title>
        <abstract>This paper studies the possibility of creating a new part-of-speech annotated corpus in French from an existing one. The objectives are to propose an exit from the restrictive licence of the source corpus and to obtain a perpetual modernisation of texts. Results show that it is possible to train a state-of-the-art POS-tagger from an automatically tagged corpus if this one is large enough.</abstract>
        <keywords>French treebank, Building a corpus, Part-of-Speech Tagging</keywords>
      </article>
      <article id="taln-2013-long-013" session="Lexique et Corpus">
        <auteurs>
          <auteur>
            <nom>Anne Abeillé</nom>
            <email>abeille@univ-paris-diderot.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Benoit Crabbé</nom>
            <email>bcrabbe@univ-paris-diderot.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LLF, CNRS-Université Paris Diderot, 75013 Paris</affiliation>
          <affiliation affiliationId="2">IUF Alpage, INRIA, Université Paris Diderot, 75013 Paris</affiliation>
          <affiliation affiliationId="3">PRES Sorbonne Paris Cité</affiliation>
        </affiliations>
        <titre>Vers un treebank du français parlé</titre>
        <type>long</type>
        <pages>174-187</pages>
        <resume>Nous présentons les premiers résultats d’un corpus arboré pour le français parlé. Il a été réalisé dans le cadre du projet ANR Etape (resp. G. Gravier) en 2011 et 2012. Contrairement à d’autres langues comme l’anglais (voir le Switchboard treebank de (Meteer, 1995)), il n’existe pas de grand corpus oral du francais annoté et validé pour les constituants et les fonctions syntaxiques. Nous souhaitons construire une ressource comparable, qui serait une extension naturelle du Corpus arboré de Paris 7 (FTB : (Abeillé et al., 2003))) basé sur des textes du journal Le Monde. Nous serons ainsi en mesure de comparer, avec des annotations comparables, l’écrit et l’oral. Les premiers résultats, qui consistent à réutiliser l’analyseur de (Petrov et al., 2006) entraîné sur l’écrit, avec une phase de correction manuelle, sont encourageants.</resume>
        <mots_cles>Corpus arboré, français parlé, corpus oral, analyse syntaxique automatique</mots_cles>
        <title>Towards a treebank of spoken French</title>
        <abstract>We present the first results of an attempt to build a spoken treebank for French. It has been conducted as part of the ANR project Etape (resp. G. Gravier). Contrary to other languages such as English (see the Switchboard treebank (Meteer, 1995)), there is no sizable spoken corpus for French annotated for syntactic constituents and grammatical functions. Our project is to build such a resource which will be a natural extension of the Paris 7 treebank (FTB : (Abeillé et al., 2003))) for written French, in order to be able to compare with similar annotations written and spoken French. We have reused and adapted the parser (Petrov et al., 2006) which has been trained on the written treebank, with manual correction and validation. The first results are promising.</abstract>
        <keywords>Treebank, spoken French, spoken corpus, parsing</keywords>
      </article>
      <article id="taln-2013-long-014" session="Syntaxe">
        <auteurs>
          <auteur>
            <nom>Assaf Urieli</nom>
            <email>assaf.urieli@univ-tlse2.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Ludovic Tanguy</nom>
            <email>ludovic.tanguy@univ-tlse2.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CLLE-ERSS : CNRS &amp; Université de Toulouse 2</affiliation>
        </affiliations>
        <titre>L’apport du faisceau dans l’analyse syntaxique en dépendances par transitions : études de cas avec l’analyseur Talismane</titre>
        <type>long</type>
        <pages>188-201</pages>
        <resume>L’analyse syntaxique (ou parsing) en dépendances par transitions se fait souvent de façon déterministe, où chaque étape du parsing propose une seule solution comme entrée de l’étape suivante. Il en va de même pour la chaîne complète d’analyse qui transforme un texte brut en graphe de dépendances, généralement décomposé en quatre modules (segmentation en phrases, en mots, étiquetage et parsing) : chaque module ne fournit qu’une seule solution au module suivant. On sait cependant que certaines ambiguïtés ne peuvent pas être levées sans prendre en considération le niveau supérieur. Dans cet article, nous présentons l’analyseur Talismane, outil libre et complet d’analyse syntaxique probabiliste du français, et nous étudions plus précisément l’apport d’une recherche par faisceau (beam search) à l’analyse syntaxique. Les résultats nous permettent à la fois de dégager la taille de faisceau la plus adaptée (qui permet d’atteindre un score de 88,5 % d’exactitude, légèrement supérieur aux outils comparables), ainsi que les meilleures stratégies concernant sa propagation.</resume>
        <mots_cles>Analyse syntaxique en dépendances, ambiguïtés, évaluation, beam search</mots_cles>
        <title>APPLYING A BEAM SEARCH TO TRANSITION-BASED DEPENDENCY PARSING: A CASE STUDY FOR FRENCH WITH THE TALISMANE SUITE</title>
        <abstract>Transition-based dependency parsing often uses deterministic techniques, where each parse step provides a single solution as the input to the next step. The same is true for the entire analysis chain which transforms raw text into a dependency graph, generally composed of four modules (sentence detection, tokenising, pos-tagging and parsing): each module provides only a single solution to the following module. However, some ambiguities cannot be resolved without taking the next level into consideration. In this article, we present Talismane, an open-source suite of tools providing a complete statistical parser of French. More specifically, we study the contribution of a beam search to syntax parsing. Our analysis allows us to conclude on the most appropriate beam width (enabling us to attain an accuracy of 88.5%, slightly higher than comparable tools), and on the best strategies concerning beam propagation from one level of analysis to the next.</abstract>
        <keywords>Dependency parsing, ambiguities, evaluation, beam search</keywords>
      </article>
      <article id="taln-2013-long-015" session="Morphologie et Segmentaion">
        <auteurs>
          <auteur>
            <nom>Anca Simon</nom>
            <email>anca-roxana.simon@irisa.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Guillaume Gravier</nom>
            <email>guillaume.gravier@irisa.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Pascale Sébillot</nom>
            <email>pascale.sebillot@irisa.fr</email>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université de Rennes 1</affiliation>
          <affiliation affiliationId="2">CNRS</affiliation>
          <affiliation affiliationId="3">INSA de Rennes IRISA &amp; INRIA Rennes</affiliation>
        </affiliations>
        <titre>Un modèle segmental probabiliste combinant cohésion lexicale et rupture lexicale pour la segmentation thématique</titre>
        <type>long</type>
        <pages>202-214</pages>
        <resume>L’identification d’une structure thématique dans des données textuelles quelconques est une tâche difficile. La plupart des techniques existantes reposent soit sur la maximisation d’une mesure de cohésion lexicale au sein d’un segment, soit sur la détection de ruptures lexicales. Nous proposons une nouvelle technique combinant ces deux critères de manière à obtenir le meilleur compromis entre cohésion et rupture. Nous définissons un nouveau modèle probabiliste, fondé sur l’approche proposée par Utiyama et Isahara (2001), en préservant les propriétés d’indépendance au domaine et de faible a priori de cette dernière. Des évaluations sont menées sur des textes écrits et sur des transcriptions automatiques de la parole à la télévision, transcriptions qui ne respectent pas les normes des textes écrits, ce qui accroît la difficulté. Les résultats expérimentaux obtenus démontrent la pertinence de la combinaison des critères de cohésion et de rupture.</resume>
        <mots_cles>segmentation thématique, cohésion lexicale, rupture de cohésion, journaux télévisés</mots_cles>
        <title>A probabilistic segment model combining lexical cohesion and disruption for topic segmentation</title>
        <abstract>Identifying topical structure in any text-like data is a challenging task. Most existing techniques rely either on maximizing a measure of the lexical cohesion or on detecting lexical disruptions. A novel method combining the two criteria so as to obtain the best trade-off between cohesion and disruption is proposed in this paper. A new statistical model is defined, based on the work of Isahara and Utiyama (2001), maintaining the properties of domain independence and limited a priori of the latter. Evaluations are performed both on written texts and on automatic transcripts of TV shows, the latter not respecting the norms of written texts, thus increasing the difficulty of the task. Experimental results demonstrate the relevance of combining lexical cohesion and disrupture.</abstract>
        <keywords>topic segmentation, lexical cohesion, lexical disrupture, TV broadcast news</keywords>
      </article>
      <article id="taln-2013-long-016" session="Syntaxe">
        <auteurs>
          <auteur>
            <nom>Pierre Bourreau</nom>
            <email>bourreau@hhu.de</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">SFB 991 Institut für Sprache und Information Université Heinrich-Heine, 40225 Düsseldorf</affiliation>
        </affiliations>
        <titre>Traitements d’ellipses : deux approches par les grammaires catégorielles abstraites</titre>
        <type>long</type>
        <pages>215-228</pages>
        <resume>L’étude de phénomènes d’ellipses dans les modèles de l’interface syntaxe-sémantique pose certains problèmes du fait que le matériel linguistique effacé au niveau phonologique est néanmoins présent au niveau sémantique. Tel est le cas d’une ellipse verbale ou d’une élision du sujet, par exemple, phénomènes qui interviennent lorsque deux phrases reliées par une conjonction partagent le même verbe, ou le même sujet. Nous proposons un traitement de ces phénomènes dans le formalisme des grammaires catégorielles abstraites selon un patron que nous intitulons extraction/instanciation et que nous implémentons de deux manières différentes dans les ACGs.</resume>
        <mots_cles>ellipse, coordination, interface syntaxe-sémantique, grammaires catégorielles abstraites, grammaires d’arbres adjoints, grammaires IO d’arbres</mots_cles>
        <title>Treating ellipsis : two abstract categorial grammar perspectives</title>
        <abstract>The treatment of ellipsis in models of the syntax-semantics interface is troublesome as the linguistic material removed in the phonologic interpretation is still necessary in the semantics. Examples are particular cases of coordination, especially the ones involving verbal phrase ellipsis or subject elision. We show a way to use abstract categorial grammars so as to implement a pattern we call extraction/instantiation in order to deal with some of these phenomena ; we exhibit two different constructions of this principle into ACGs.</abstract>
        <keywords>ellipsis, coordination, syntax-semantics interface, abstract categorial grammars, tree-adjoining grammars, IO tree-grammars</keywords>
      </article>
      <article id="taln-2013-long-017" session="Plénière">
        <auteurs>
          <auteur>
            <nom>Philippe Blache</nom>
            <email>blache@lpl-aix.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Aix-Marseille Université, CNRS, LPL 5 Avenue Pasteur, 13100 Aix-en-Provence</affiliation>
        </affiliations>
        <titre>Chunks et activation : un modèle de facilitation du traitement linguistique</titre>
        <type>long</type>
        <pages>229-242</pages>
        <resume>Nous proposons dans cet article d’intégrer la notion de chunk au sein d’une architecture globale de traitement de la phrase. Les chunks jouent un rôle important dans les théories cognitives comme ACT-R (Anderson et al., 2004) : il s’agit d’unités de traitement globales auxquelles il est possible d’accéder directement via des buffers en mémoire à court ou long terme. Ces chunks sont construits par une fonction d’activation (processus cognitif pouvant être quantifié) s’appuyant sur l’évaluation de leur relation au contexte. Nous proposons une interprétation de cette théorie appliquée à l’analyse syntaxique. Un mécanisme de construction des chunks est proposé. Nous développons pour cela une fonction d’activation tirant parti de la représentation de l’information linguistique sous forme de contraintes. Cette fonction permet de montrer en quoi les chunks sont faciles à construire et comment leur existence facilite le traitement de la phrase. Plusieurs exemples sont proposés, illustrant cette hypothèse de facilitation.</resume>
        <mots_cles>Chunks, ACT-R, activation, mémoire, parsing, traitement de la phrase, expérimentation</mots_cles>
        <title>Chunks and the notion of activation : a facilitation model for sentence processing</title>
        <abstract>We propose in this paper to integrate the notion of chunk within a global architecture for sentence processing. Chunks play an important role in cognitive theories such as ACT-R cite Anderson04 : they constitute global processing units which can be accessed directly via short or long term memory buffers. Chunks are built on the basis of an activation function evaluating their relationship to the context. We propose an interpretation of this theory applied to parsing. A construction mechanism is proposed, based on an adapted version of the activation function which takes advantage of the representation of linguistic information in terms of constraints. This feature allows to show how chunks are easy to build and how they can facilitate treatment. Several examples are given, illustrating this hypothesis of facilitation.</abstract>
        <keywords>Chunks, ACT-R, activation, memory, parsing, sentence processing, experimentation</keywords>
      </article>
      <article id="taln-2013-long-018" session="Traduction et Alignement">
        <auteurs>
          <auteur>
            <nom>Amir Hazem</nom>
            <email>amir.hazem@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Emmanuel Morin</nom>
            <email>emmanuel.morin@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LINA - UMR CNRS 6241, 2 rue de la houssinière, BP 92208, 44322 Nantes Cedex 03</affiliation>
        </affiliations>
        <titre>Extraction de lexiques bilingues à partir de corpus comparables par combinaison de représentations contextuelles</titre>
        <type>long</type>
        <pages>243-256</pages>
        <resume>La caractérisation du contexte des mots constitue le coeur de la plupart des méthodes d’extraction de lexiques bilingues à partir de corpus comparables. Dans cet article, nous revisitons dans un premier temps les deux principales stratégies de représentation contextuelle, à savoir celle par fenêtre ou sac de mots et celle par relations de dépendances syntaxiques. Dans un second temps, nous proposons deux nouvelles approches qui exploitent ces deux représentations de manière conjointe. Nos expériences montrent une amélioration significative des résultats sur deux corpus de langue de spécialité.</resume>
        <mots_cles>Multilingualisme, corpus comparables, lexique bilingue, vecteurs de contexte, dépendances syntaxiques</mots_cles>
        <title>Bilingual Lexicon Extraction from Comparable Corpora by Combining Contextual Representations</title>
        <abstract>Words context characterisation constitute the heart of most methods of bilingual lexicon extraction from comparable corpora. In this article, we first revisit the two main strategies of context representation, that is : the window-based and the syntactic based context representation. Secondly, we propose two new methods that exploit jointly these different representations . Our experiments show a significant improvement of the results obtained on two different domain specific comparable corpora.</abstract>
        <keywords>Multilingualism, comparable corpora, bilingual lexicon, context vectors, syntactic dependencies</keywords>
      </article>
      <article id="taln-2013-long-019" session="Entités Nommées">
        <auteurs>
          <auteur>
            <nom>Vincent Claveau</nom>
            <email>vincent.claveau@irisa.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Abir Ncibi</nom>
            <email>abir.ncibi@inria.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">IRISA-CNRS Campus de Beaulieu, 35042 Rennes, France</affiliation>
          <affiliation affiliationId="2">INRIA-IRISA Campus de Beaulieu, 35042 Rennes, France</affiliation>
        </affiliations>
        <titre>Découverte de connaissances dans les séquences par CRF non-supervisés</titre>
        <type>long</type>
        <pages>257-270</pages>
        <resume>Les tâches de découverte de connaissances ont pour but de faire émerger des groupes d’entités cohérents. Ils reposent le plus souvent sur du clustering, tout l’enjeu étant de définir une notion de similarité pertinentes entre ces entités. Dans cet article, nous proposons de détourner les champs aléatoires conditionnels (CRF), qui ont montré leur intérêt pour des tâches d’étiquetage supervisées, pour calculer indirectement ces similarités sur des séquences de textes. Pour cela, nous générons des problèmes d’étiquetage factices sur les données à traiter pour faire apparaître des régularités dans les étiquetages des entités. Nous décrivons comment ce cadre peut être mis en oeuvre et l’expérimentons sur deux tâches d’extraction d’informations. Les résultats obtenus démontrent l’intérêt de cette approche non-supervisée, qui ouvre de nombreuses pistes pour le calcul de similarités dans des espaces de représentations complexes de séquences.</resume>
        <mots_cles>Découverte de connaissances, CRF, clustering, apprentissage non-supervisé, extraction d’informations</mots_cles>
        <title>Unsupervised CRF for knowledge discovery</title>
        <abstract>Knowledge discovery aims at bringing out coherent groups of entities. They are usually based on clustering ; the challenge is then to define a notion of similarity between the relevant entities. In this paper, we propose to divert Conditional Random Fields (CRF), which have shown their interest in supervised labeling tasks, in order tocalculate indirectly the similarities among text sequences. Our approach consists in generate artificial labeling problems on the data to be processed to reveal regularities in the labeling of the entities. We describe how this framework can be implemented and experiment it on two information retrieval tasks. The results demonstrate the usefulness of this unsupervised approach, which opens many avenues for defining similarities for complex representations of sequential data.</abstract>
        <keywords>Knowledge discovery, CRF, clustering, unsupervised machine learning, information extraction</keywords>
      </article>
      <article id="taln-2013-long-020" session="Lexique et Corpus">
        <auteurs>
          <auteur>
            <nom>Thomas Gaillat</nom>
            <email>thomas.gaillat@univ-rennes1.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université ParisDiderot – CLILLACARP (3967) &amp; Université de Rennes 1</affiliation>
        </affiliations>
        <titre>Annotation automatique d'un corpus d'apprenants d'anglais avec un jeu d'étiquettes modifié du Penn Treebank</titre>
        <type>long</type>
        <pages>271-284</pages>
        <resume>Cet article aborde la problématique de l'annotation automatique d'un corpus d'apprenants d'anglais. L'objectif est de montrer qu'il est possible d'utiliser un étiqueteur PoS pour annoter un corpus d'apprenants afin d'analyser les erreurs faites par les apprenants. Cependant, pour permettre une analyse suffisamment fine, des étiquettes fonctionnelles spécifiques aux phénomènes linguistiques à étudier sont insérées parmi celles de l'étiqueteur. Celuici est entraîné avec ce jeu d'étiquettes étendu sur un corpus de natifs avant d'être appliqué sur le corpus d'apprenants. Dans cette expérience, on s'intéresse aux usages erronés de this et that par les apprenants. On montre comment l'ajout d'une couche fonctionnelle sous forme de nouvelles étiquettes pour ces deux formes, permet de discriminer des usages variables chez les natifs et nonnatifs et, partant, d’identifier des schémas incorrects d'utilisation. Les étiquettes fonctionnelles éclairent sur le fonctionnement discursif.</resume>
        <mots_cles>Apprentissage L2, corpus d'apprenants, analyse linguistique d'erreurs, étiquetage automatique, this, that</mots_cles>
        <title>Automatic tagging of a learner corpus of English with a modified version of the Penn Treebank tagset</title>
        <abstract>This article covers the issue of automatic annotation of a learner corpus of English. The objective is to show that it is possible to PoStag the corpus with a tagger to prepare the ground for learner error analysis. However, in order to have a finegrain analysis, some functional tags for the study of specific linguistic points are inserted within the tagger's tagset. This tagger is trained on a native-English corpus with an extended tagset and the tagging is done on the learner corpus. This experiment focuses on the incorrect use of this and that by learners. We show how the insertion of a functional layer by way of new tags for the forms allows us to discriminate varying uses among natives and nonnatives. This opens the path to the identification of incorrect patterns of use. The functional tags cast a light on the way the discourse functions.</abstract>
        <keywords>Second Language Acquisition, learner corpus, linguistic error analysis, automated tagging, this, that</keywords>
      </article>
      <article id="taln-2013-long-021" session="Lexique et Corpus">
        <auteurs>
          <auteur>
            <nom>Franck Sajous</nom>
            <email>franck.sajous@univ-tlse2.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Nabil Hathout</nom>
            <email>nabil.hathout@univ-tlse2.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Basilio Calderone</nom>
            <email>basilio.calderone@univ-tlse2.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CLLE-ERSS – CNRS et Université de Toulouse 2 Le Mirail</affiliation>
        </affiliations>
        <titre>GLÀFF, un Gros Lexique À tout Faire du Français</titre>
        <type>long</type>
        <pages>285-298</pages>
        <resume>Cet article présente GLÀFF, un lexique du français à large couverture extrait du Wiktionnaire, le dictionnaire collaboratif en ligne. GLÀFF contient pour chaque entrée une description morphosyntaxique et une transcription phonémique. Il se distingue des autres lexiques existants principalement par sa taille, sa licence libre et la possibilité de le faire évoluer de façon constante. Nous décrivons ici comment nous l’avons construit, puis caractérisé en le comparant à différentes ressources connues. Cette comparaison montre que sa taille et sa qualité font de GLÀFF un candidat sérieux comme nouvelle ressource standard pour le TAL, la linguistique et la psycholinguistique.</resume>
        <mots_cles>Lexique morpho-phonologique, ressources lexicales libres, Wiktionnaire</mots_cles>
        <title>GLÀFF, a Large Versatile French Lexicon</title>
        <abstract>This paper introduces GLÀFF, a large-scale versatile French lexicon extracted from Wiktionary, the collaborative online dictionary. GLÀFF contains, for each entry, a morphosyntactic description and a phonetic transcription. It distinguishes itself from the other available lexicons mainly by its size, its potential for constant updating and its copylefted license that makes it available for use, modification and redistribution. We explain how we have built GLÀFF and compare it to other known resources. We show that its size and quality are strong assets that could allow GLÀFF to become a reference lexicon for NLP, linguistics and psycholinguistics.</abstract>
        <keywords>Morpho-phonological lexicon, free lexical resources, French Wiktionary</keywords>
      </article>
      <article id="taln-2013-long-022" session="Lexique et Corpus">
        <auteurs>
          <auteur>
            <nom>Authoul Abdul Hay</nom>
            <email>authoul@voila.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Olivier Kraif</nom>
            <email>olivier.Kraif@u-grenoble3.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Alzaytoonah University of Jordan - 11733 Jordan</affiliation>
          <affiliation affiliationId="2">Univ. Grenoble Alpes, LIDILEM, F-38040 Grenoble</affiliation>
        </affiliations>
        <titre>Constitution d’une ressource sémantique arabe à partir de corpus multilingue aligné</titre>
        <type>long</type>
        <pages>299-312</pages>
        <resume>Cet article porte sur la mise en oeuvre etsur l'étudede techniques d'extraction de relations sémantiques à partir d'un corpus multilingue aligné, en vue de construire une ressource lexicale pour l’arabe. Ces relations sontextraites par transitivité de l'équivalence traductionnelle, deux lexèmes qui possèdent les mêmes équivalents dans une langue cible étant susceptibles de partager un même sens. A partir d’équivalences extraites d’un corpus multilingue aligné, nous tâchons d'extraire des "cliques", ou sous-graphes maximaux complets connexes, dont toutes les unités sont en interrelation, du fait d'une probable intersection sémantique. Ces cliques présentent l'intérêt de renseigner à la fois sur la synonymie et la polysémie des unités, et d'apporter une forme de désambiguïsation sémantique. Ensuite nous tâchons de relier ces cliques avec un lexique sémantique (de type Wordnet) afin d'évaluer la possibilité de récupérer pour les unités arabes des relations sémantiques définies pour des unités en d’autres langues (français, anglais ou espagnol). Les résultats sont encourageants, et montrent qu’avec des corpus adaptés ces relations pourraient permettrede construire automatiquement un réseau utile pour certaines applications de traitement de la langue arabe.</resume>
        <mots_cles>Corpus multilingues alignés, désambigüisation sémantique, cliques, lexiques multilingues, réseaux sémantiques, traitement de l’arabe</mots_cles>
        <title>The constitution of an Arabic semantic resource from a multilingual aligned corpus</title>
        <abstract>This paper aims at the implementation and evaluation of techniques for extracting semantic relations from a multilingual alignedcorpus, in order to build a lexical resource for Arabic language. We first extract translational equivalents froma multilingual aligned corpus. From these equivalences, we try to extract "cliques", which are maximum complete related sub-graphs, where all units are interrelated because of a probable semantic intersection. These cliques have the advantage of giving information on both the synonymy and polysemy of units, providing a kindof semantic disambiguation. Secondly, we attempt to link these cliques with a semantic lexicon (like WordNet) in order to assess the possibility of recovering, for the Arabicunits, a semantic relationships already defined for English, French or Spanish units. These relations would automatically build a semantic resource which would be useful for different applications of NLP, such as Question Answering systems, Machine Translation, alignment systems, Information Retrieval…etc.</abstract>
        <keywords>Multilingual aligned corpus, semantic disambiguation, cliques, multilingual lexicons, word net, Arabic Language Processing</keywords>
      </article>
      <article id="taln-2013-long-023" session="Traduction et Alignement">
        <auteurs>
          <auteur>
            <nom>Rima Harastani</nom>
            <email>rima.harastani@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Béatrice Daille</nom>
            <email>beatrice.daille@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Emmanuel Morin</nom>
            <email>emmanuel.morin@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LINA UMR CNRS 6241, 2 Chemin de la Houssinière 44300 Nantes</affiliation>
        </affiliations>
        <titre>Identification, alignement, et traductions des adjectifs relationnels en corpus comparables</titre>
        <type>long</type>
        <pages>313-326</pages>
        <resume>Dans cet article, nous extrayons des adjectifs relationnels français et nous les alignons automatiquement avec les noms dont ils sont dérivés en utilisant un corpus monolingue. Les alignements adjectif-nom seront ensuite utilisés dans la traduction compositionelle des termes complexes de la forme [N AdjR] à partir d’un corpus comparable français-anglais. Un nouveau terme [N N0] (ex. cancer du poumon) sera obtenu en remplaçant l’adjectif relationnel Ad jR (ex. pulmonaire) dans [N AdjR] (ex. cancer pulmonaire) par le nom N0 (ex. poumon) avec lequel il est aligné. Si aucune traduction n’est proposée pour [N AdjR], nous considérons que ses traduction(s) sont équivalentes à celle(s) de sa paraphrase [N N0]. Nous expérimentons avec un corpus comparable dans le domaine de cancer du sein, et nous obtenons des alignements adjectif-nom qui aident à traduire des termes complexes de la forme [N AdjR] vers l’anglais avec une précision de 86 %.</resume>
        <mots_cles>Adjectifs relationnels, Corpus comparables, Méthode compositionnelle, Termes complexes</mots_cles>
        <title>Identification, Alignment, and Tranlsation of Relational Adjectives from Comparable Corpora</title>
        <abstract>In this paper, we extract French relational adjectives and automatically align them with the nouns they are derived from by using a monolingual corpus. The obtained adjective-noun alignments are then used in the compositional translation of compound nouns of the form [N ADJR] with a French-English comparable corpora. A new term [N N0] (eg, cancer du poumon) is obtained by replacing the relational adjective Ad jR (eg, pulmonaire) in [N AdjR] (eg, cancer pulmonaire) by its corresponding N0 (eg, poumon). If no translation(s) are obtained for [N AdjR], we consider the one(s) obtained for its paraphrase [N N0]. We experiment with a comparable corpora in the field of breast cancer, and we get adjective-noun alignments that help in translating French compound nouns of the form [N AdjR] to English with a precision of 86%.</abstract>
        <keywords>Relational adjectives, Comparable corpora, Compositional method, Complex terms</keywords>
      </article>
      <article id="taln-2013-long-024" session="Traduction et Alignement">
        <auteurs>
          <auteur>
            <nom>Dhouha Bouamor</nom>
            <email>dhouha.bouamor@cea.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Nasredine Semmar</nom>
            <email>nasredine.semmar@cea.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Pierre Zweigenbaum</nom>
            <email>pz@limsi.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CEA-LIST, LVIC, F91191 Gif sur Yvette Cedex, France</affiliation>
          <affiliation affiliationId="2">LIMSI-CNRS, F-91403 Orsay, France</affiliation>
          <affiliation affiliationId="3">Univ. Paris Sud, Orsay, France</affiliation>
        </affiliations>
        <titre>Utilisation de la similarité sémantique pour l’extraction de lexiques bilingues à partir de corpus comparables</titre>
        <type>long</type>
        <pages>327-338</pages>
        <resume>Cet article présente une nouvelle méthode visant à améliorer les résultats de l’approche standard utilisée pour l’extraction de lexiques bilingues à partir de corpus comparables spécialisés. Nous tentons de résoudre le problème de la polysémie des mots dans les vecteurs de contexte par l’introduction d’un processus de désambiguïsation sémantique basé sur WordNet. Pour traduire les vecteurs de contexte, au lieu de considérer toutes les traductions proposées par le dictionnaire bilingue, nous n’utilisons que les mots caractérisant au mieux les contextes en langue cible. Les expériences menées sur deux corpus comparables spécialisés français-anglais (financier et médical) montrent que notre méthode améliore les résultats de l’approche standard plus particulièrement lorsque plusieurs mots du contexte sont ambigus.</resume>
        <mots_cles>lexique bilingue, corpus comparable spécialisé, désambiguïsation sémantique, WordNet</mots_cles>
        <title/>
        <abstract>This paper presents a new method that aims to improve the results of the standard approach used for bilingual lexicon extraction from specialized comparable corpora. We attempt to solve the problem of context vector word polysemy. Instead of using all the entries of the dictionary to translate a context vector, we only use the words of the lexicon that are more likely to give the best characterization of context vectors in the target language. On two specialised French-English comparable corpora, empirical experimental results show that our method improves the results obtained by the standard approach especially when many words are ambiguous.</abstract>
        <keywords>bilingual lexicon, specialized comparable corpora, semantic disambiguation, Word-Net</keywords>
      </article>
      <article id="taln-2013-long-025" session="Sémantique">
        <auteurs>
          <auteur>
            <nom>Manel Zarrouk</nom>
            <email>manel.zarrouk@lirmm.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Mathieu Lafourcade</nom>
            <email>mathieu.lafourcade@lirmm.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Alain Joubert</nom>
            <email>Alain.Joubert@lirmm.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIRMM, 161, rue ADA 34095 Montpellier Cedex 5</affiliation>
        </affiliations>
        <titre>Inférences déductives et réconciliation dans un réseau lexico-sémantique</titre>
        <type>long</type>
        <pages>339-352</pages>
        <resume>La construction et la validation des réseaux lexico-sémantiques est un enjeu majeur en TAL. Indépendamment des stratégies de construction utilisées, inférer automatiquement de nouvelles relations à partir de celles déjà existantes est une approche possible pour améliorer la couverture et la qualité globale de la ressource. Dans ce contexte, le moteur d’inférences a pour but de formuler de nouvelles conclusions (c’est-à-dire des relations entre les termes) à partir de prémisses (des relations préexistantes). L’approche que nous proposons est basée sur une méthode de triangulation impliquant la transitivité sémantique avec un mécanisme de blocage pour éviter de proposer des relations douteuses. Les relations inférées sont proposées aux contributeurs pour être validées. Dans le cas d’invalidation, une stratégie de réconciliation est engagée pour identifier la cause de l’inférence erronée : une exception, une erreur dans les prémisses, ou une confusion d’usage causée par la polysémie.</resume>
        <mots_cles>inférence de relations, réconciliation, enrichissement, réseau lexical, peuplonomie</mots_cles>
        <title>Inductive and deductive inferences in a Crowdsourced Lexical-Semantic Network</title>
        <abstract>In Computational Linguistics, validated lexical-semantic networks are crucial resources. Regardless the construction strategies used, automatically inferring new relations from already existing ones may improve coverage and global quality of the resource. In this context, an inference engine aims at producing new conclusions (i.e. potential relations) from premises (pre-existing relations). The approach we propose is based on a triangulation method involving the semantic transitivity with a blocking mechanism to avoid proposing dubious relations. Inferred relations are then proposed to contributors to be validated or rejected. In cas of invalidation, a reconciliation strategy is implemented to identify the cause of the erroneous inference : an exception, an error in the premises, or a confusion caused by polysemy.</abstract>
        <keywords>relation inferences, reconcialiation, enrichment, lexical network, crowdsourcing</keywords>
      </article>
      <article id="taln-2013-long-026" session="Fouille de textes et applications">
        <auteurs>
          <auteur>
            <nom>Wei Wang</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Romaric Besançon</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Olivier Ferret</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Brigitte Grau</nom>
            <email/>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CEA, LIST, Laboratoire Vision et Ingénierie des Contenus, Gif-sur-Yvette, F-91191 France.</affiliation>
          <affiliation affiliationId="2">LIMSI, UPR-3251 CNRS-DR4, Bât. 508, BP 133, 91403 Orsay Cedex.</affiliation>
        </affiliations>
        <titre>Regroupement sémantique de relations pour l’extraction d’information non supervisée</titre>
        <type>long</type>
        <pages>353-366</pages>
        <resume>Beaucoup des recherches menées en extraction d’information non supervisée se concentrent sur l’extraction des relations et peu de travaux proposent des méthodes pour organiser les relations extraites. Nous présentons dans cet article une méthode de clustering en deux étapes pour regrouper des relations sémantiquement équivalentes : la première étape regroupe des relations proches par leur expression tandis que la seconde fusionne les premiers clusters obtenus sur la base d’une mesure de similarité sémantique. Nos expériences montrent en particulier que les mesures distributionnelles permettent d’obtenir pour cette tâche de meilleurs résultats que les mesures utilisant WordNet. Nous montrons également qu’un clustering à deux niveaux permet non seulement de limiter le nombre de similarités sémantiques à calculer mais aussi d’améliorer la qualité des résultats du clustering.</resume>
        <mots_cles>Extraction d’Information Non Supervisée, Similarité Sémantique, Clustering</mots_cles>
        <title>Semantic relation clustering for unsupervised information extraction</title>
        <abstract>Most studies in unsupervised information extraction concentrate on the relation extraction and few work has been proposed on the organization of the extracted relations. We present in this paper a two-step clustering procedure to group semantically equivalent relations : a first step clusters relations with similar expressions while a second step groups these first clusters into larger semantic clusters, using different semantic similarities. Our experiments show the stability of distributional similarities over WordNet-based similarities for semantic clustering. We also demonstrate that the use of a multi-level clustering not only reduces the calculations from all relation pairs to basic clusters pairs, but it also improves the clustering results.</abstract>
        <keywords>Unsupervised Information Extraction, Semantic Similarity, Relation Clustering</keywords>
      </article>
      <article id="taln-2013-long-027" session="Sémantique">
        <auteurs>
          <auteur>
            <nom>Christian Retoré</nom>
            <email>Christian.Retore@irit.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">IRIT (CNRS, Toulouse) &amp; Université de Bordeaux (LaBRI)</affiliation>
        </affiliations>
        <titre>Sémantique des déterminants dans un cadre richement typé</titre>
        <type>long</type>
        <pages>367-380</pages>
        <resume>La variation du sens des mots en contexte nous a conduit à enrichir le système de types utilisés dans notre analyse syntaxico-sémantique du français basé sur les grammaires catégorielles et la sémantique de Montague (ou la lambda-DRT). L’avantage majeur d’une telle sémantique profonde est de représenter le sens par des formules logiques aisément exploitables, par exemple par un moteur d’inférence. Déterminants et quantificateurs jouent un rôle fondamental dans la construction de ces formules, et il nous a fallu leur trouver des termes sémantiques adaptés à ce nouveau cadre. Nous proposons une solution inspirée des opérateurs epsilon et tau de Hilbert, éléments génériques qui s’apparentent à des fonctions de choix. Cette modélisation unifie le traitement des différents types de déterminants et de quantificateurs et autorise le liage dynamique des pronoms. Surtout, cette description calculable des déterminants s’intègre parfaitement à l’analyseur à large échelle du français Grail, tant en théorie qu’en pratique.</resume>
        <mots_cles>Analyse sémantique automatique, Sémantique formelle, Compositionnalité</mots_cles>
        <title>On the semantics of determiners in a rich type-theoretical framework</title>
        <abstract>The variation of word meaning according to the context led us to enrich the type system of our syntactical and semantic analyser of French based on categorial grammars and Montague semantics (or lambda-DRT). The main advantage of a deep semantic analyse is too represent meaning by logical formulae that can be easily used e.g. for inferences. Determiners and quantifiers play a fundamental role in the construction of those formulae and we needed to provide them with semantic terms adapted to this new framework. We propose a solution inspired by the tau and epsilon operators of Hilbert, generic elements that resemble choice functions. This approach unifies the treatment of the different determiners and quantifiers and allows a dynamic binding of pronouns. Above all, this fully computational view of determiners fits in well within the wide coverage parser Grail, both from a theoretical and a practical viewpoint.</abstract>
        <keywords>Automated semantic analysis, Formal Semantics, Compositional Semantics</keywords>
      </article>
      <article id="taln-2013-long-028" session="Traduction et Alignement">
        <auteurs>
          <auteur>
            <nom>Charlotte Lecluze</nom>
            <email>Charlotte.Lecluze@unicaen.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Romain Brixtel</nom>
            <email>Romain.Brixtel@unicaen.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Loïs Rigouste</nom>
            <email>Loïs.Rigouste</email>
            <affiliationId/>
          </auteur>
          <auteur>
            <nom>Emmanuel Giguet</nom>
            <email>Emmanuel.Giguet@unicaen.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Régis Clouard</nom>
            <email>Régis.Clouard@unicaen.fr</email>
            <email>Régis.Clouard@ensicaen.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Gaël Lejeune</nom>
            <email>Gaël.Lejeune@unicaen.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Patrick Constant</nom>
            <email>Patrick.Constant@pertimm.com</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">GREYC - CNRS UMR 6072 - Université de Caen Basse-Normandie, Caen, France</affiliation>
          <affiliation affiliationId="2">Pertimm, Asnières-sur-Seine, France</affiliation>
          <affiliation affiliationId="3">EnsiCaen, Ecole Nationale Supérieure d’Ingénieurs de Caen, France</affiliation>
        </affiliations>
        <titre>Détection de zones parallèles à l’intérieur de multi-documents pour l’alignement multilingue</titre>
        <type>long</type>
        <pages>381-394</pages>
        <resume>Cet article aborde une question centrale de l’alignement automatique, celle du diagnostic de parallélisme des documents à aligner. Les recherches en la matière se sont jusqu’alors concentrées sur l’analyse de documents parallèles par nature : corpus de textes réglementaires, documents techniques ou phrases isolées. Les phénomènes d’inversions et de suppressions/ajouts pouvant exister entre les différentes versions d’un document sont ainsi souvent ignorées. Nous proposons donc une méthode pour diagnostiquer en contexte des zones parallèles à l’intérieur des documents. Cette méthode permet la détection d’inversions ou de suppressions entre les documents à aligner. Elle repose sur l’affranchissement de la notion de mot et de phrase, ainsi que sur la prise en compte de la Mise en Forme Matérielle du texte (MFM). Sa mise en oeuvre est basée sur des similitudes de répartition de chaînes de caractères répétées dans les différents documents. Ces répartitions sont représentées sous forme de matrices et l’identification des zones parallèles est effectuée à l’aide de méthodes de traitement d’image.</resume>
        <mots_cles>détection et alignement de zones, appariement de N-grammes de caractères, corpus de multidocuments</mots_cles>
        <title>Parallel areas detection in multi-documents for multilingual alignment</title>
        <abstract>This article broaches a central issue of the automatic alignment : diagnosing the parallelism of documents. Previous research was concentrated on the analysis of documents which are parallel by nature such as corpus of regulations, technical documents or simple sentences. Inversions and deletions/additions phenomena that may exist between different versions of a document has often been overlooked. To the contrary, we propose a method to diagnose in context the parallel areas allowing the detection of deletions or inversions between documents to align. This original method is based on the freeing from word and sentence as well as the consideration of the text formatting. The implementation is based on the detection of repeated character strings and the identification of parallel segments by image processing.</abstract>
        <keywords>area detection and alignment, character N-grams matching, multidocuments corpora</keywords>
      </article>
      <article id="taln-2013-long-029" session="Traduction et Alignement">
        <auteurs>
          <auteur>
            <nom>Ahmed Hamdi</nom>
            <email>ahmed.hamdi@lif.univ-mrs.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Rahma Boujelbane</nom>
            <email>rahma.boujelbane@lif.univ-mrs.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Nizar Habash</nom>
            <email>habash@ccls.columbia.edu</email>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Alexis Nasr</nom>
            <email>alexis.nasr@lif.univ-mrs.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire d’Informatique Fondamentale de Marseille- CNRS - UMR 7279 Université Aix-Marseille</affiliation>
          <affiliation affiliationId="2">Multimedia, InfoRmation Systems and Advanced Computing Laboratory, Sfax 3021, TUNISIE.</affiliation>
          <affiliation affiliationId="3">Center for Computational Learning Systems Columbia University New York, NY 10115, USA</affiliation>
        </affiliations>
        <titre>Un système de traduction de verbes entre arabe standard et arabe dialectal par analyse morphologique profonde</titre>
        <type>long</type>
        <pages>395-406</pages>
        <resume>Le développement d’outils de TAL pour les dialectes de l’arabe se heurte à l’absence de ressources pour ces derniers. Comme conséquence d’une situation de diglossie, il existe une variante de l’arabe, l’arabe moderne standard, pour laquelle de nombreuses ressources ont été développées et ont permis de construire des outils de traitement automatique de la langue. Etant donné la proximité des dialectes de l’arabe, le tunisien dans notre cas, avec l’arabe moderne standard, une voie consiste à réaliser une traduction surfacique du dialecte vers l’arabe moderne standard afin de pouvoir utiliser les outils existants pour l’arabe standard. Nous décrivons dans cet article une architecture pour une telle traduction et nous l’évaluons sur les verbes.</resume>
        <mots_cles>dialectes, langues peu dotées, analyse morphologique, traitement automatique de l’arabe</mots_cles>
        <title>Translating verbs between MSA and arabic dialects through deep morphological analysis</title>
        <abstract>The developpment of NLP tools for dialects faces the severe problem of lack of resources for such dialects. In the case of diglossia, as in arabic, a variant of arabic, Modern Standard Arabic, exists, for which many resources have been developped which can be used to build NLP tools. Taking advantage of the closeness of MSA and dialects, one way to solve the problem consist in performing a surfacic translation of the dialect into MSA in order to use the tools developped for MSA. We describe in this paper an achitecture for such a translation and we evaluate it on arabic verbs.</abstract>
        <keywords>dialects, Arabic NLP, morphological analysis</keywords>
      </article>
      <article id="taln-2013-long-030" session="Morphologie et Segmentation">
        <auteurs>
          <auteur>
            <nom>Benoît Sagot</nom>
            <email>Benoît.Sagot@inria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Damien Nouvel</nom>
            <email>Damien.Nouvel@inria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Virginie Mouilleron</nom>
            <email>Virginie.Mouilleron@inria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Marion Baranes</nom>
            <email>Marion.Baranes@inria.fr</email>
            <affiliationId>2</affiliationId>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Alpage, INRIA &amp; Université Paris-Diderot, 75013 Paris</affiliation>
          <affiliation affiliationId="2">viavoo, 92100 Boulogne Billancourt</affiliation>
        </affiliations>
        <titre>Extension dynamique de lexiques morphologiques pour le français à partir d’un flux textuel</titre>
        <type>long</type>
        <pages>407-420</pages>
        <resume>L’incomplétude lexicale est un problème récurrent lorsque l’on cherche à traiter le langage naturel dans sa variabilité. Effectivement, il semble aujourd’hui nécessaire de vérifier et compléter régulièrement les lexiques utilisés par les applications qui analysent d’importants volumes de textes. Ceci est plus particulièrement vrai pour les flux textuels en temps réel. Dans ce contexte, notre article présente des solutions dédiées au traitement des mots inconnus d’un lexique. Nous faisons une étude des néologismes (linguistique et sur corpus) et détaillons la mise en oeuvre de modules d’analyse dédiés à leur détection et à l’inférence d’informations (forme de citation, catégorie et classe flexionnelle) à leur sujet. Nous y montrons que nous sommes en mesure, grâce notamment à des modules d’analyse des dérivés et des composés, de proposer en temps réel des entrées pour ajout aux lexiques avec une bonne précision.</resume>
        <mots_cles>Néologismes, analyse morphologique, lexiques dynamiques</mots_cles>
        <title>Dynamic extension of a French morphological lexicon based a text stream</title>
        <abstract>Lexical incompleteness is a recurring problem when dealing with natural language and its variability. It seems indeed necessary today to regularly validate and extend lexica used by tools processing large amounts of textual data. This is even more true when processing real-time text flows. In this context, our paper introduces techniques aimed at addressing words unknown to a lexicon. We first study neology (from a theoretic and corpus-based point of view) and describe the modules we have developed for detecting them and inferring information about them (lemma, category, inflectional class). We show that we are able, using among others modules for analyzing derived and compound neologisms, to generate lexical entries candidates in real-time and with a good precision.</abstract>
        <keywords>Neologisms, Morphological Analysis, Dynamic Lexica</keywords>
      </article>
      <article id="taln-2013-long-031" session="Entités Nommées">
        <auteurs>
          <auteur>
            <nom>Damien Nouvel</nom>
            <email>Damien.Nouvel@univ-tours.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Jean-Yves Antoine</nom>
            <email>Jean-Yves.Antoine@univ-tours.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Nathalie Friburger</nom>
            <email>Nathalie.Friburger@univ-tours.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Arnaud Soulet</nom>
            <email>Arnaud.Soulet@univ-tours.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LI, 3 place Jean Jaurès, 41000 Blois</affiliation>
          <affiliation affiliationId="2">Alpage, INRIA &amp; Université Paris-Diderot, 75013 Paris</affiliation>
        </affiliations>
        <titre>Fouille de règles d’annotation partielles pour la reconnaissance des entités nommées</titre>
        <type>long</type>
        <pages>421-434</pages>
        <resume>Ces dernières décennies, l’accroissement des volumes de données a rendu disponible une diversité toujours plus importante de types de contenus échangés (texte, image, audio, vidéo, SMS, tweet, données statistiques, spatiales, etc.). En conséquence, de nouvelles problématiques ont vu le jour, dont la recherche d’information au sein de données potentiellement bruitées. Dans cet article, nous nous penchons sur la reconnaissance d’entités nommées au sein de transcriptions (manuelles ou automatiques) d’émissions radiodiffusées et télévisuelles. À cet effet, nous mettons en oeuvre une approche originale par fouille de données afin d’extraire des motifs, que nous nommons règles d’annotation. Au sein d’un modèle, ces règles réalisent l’annotation automatique de transcriptions. Dans le cadre de la campagne d’évaluation Etape, nous mettons à l’épreuve le système implémenté, mXS, étudions les règles extraites et rapportons les performances du système. Il obtient de bonnes performances, en particulier lorsque les transcriptions sont bruitées.</resume>
        <mots_cles>Entités nommées, Fouille de données, Règles d’annotation</mots_cles>
        <title>Mining Partial Annotation Rules for Named Entity Recognition</title>
        <abstract>During the last decades, the unremitting increase of numeric data available has led to a more and more urgent need for efficient solution of information retrieval (IR). This paper concerns a problematic of first importance for the IR on linguistic data : the recognition of named entities (NE) on speech transcripts issued from radio or TV broadcasts.We present an original approach for named entity recognition which is based on data mining techniques. More precisely, we propose to adapt hierarchical sequence mining techniques to extract automatically from annotated corpora intelligible rules of NE detection. This research was carried out in the framework of the Etape NER evaluation campaign, where mXS, our text-mining based system has shown good performances challenging the best symbolic or data-driven systems</abstract>
        <keywords>Named Entities, Data Mining, Annotation Rules</keywords>
      </article>
      <article id="taln-2013-long-032" session="Morphologie et Segmentation">
        <auteurs>
          <auteur>
            <nom>Iskandar Keskes</nom>
            <email>keskes@irit.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Farah Beanamara</nom>
            <email>benamara@irit.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Lamia Hadrich Belguith</nom>
            <email>l.belguith@fsegs.rnu.tn</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">ANLP Research Group, MIRACL, Route de Tunis km 10, 3021, Sfax, Tunisie</affiliation>
          <affiliation affiliationId="2">IRIT, 118, route de Narbonne F-31062 Toulouse Cedex 9</affiliation>
        </affiliations>
        <titre>Segmentation de textes arabes en unités discursives minimales</titre>
        <type>long</type>
        <pages>435-449</pages>
        <resume>La segmentation d’un texte en Unités Discursives Minimales (UDM) a pour but de découper le texte en segments qui ne se chevauchent pas. Ces segments sont ensuite reliés entre eux afin de construire la structure discursive d’un texte. La plupart des approches existantes utilisent une analyse syntaxique extensive. Malheureusement, certaines langues ne disposent pas d'analyseur syntaxique robuste. Dans cet article, nous étudions la faisabilité de la segmentation discursive de textes arabes en nous basant sur une approche d'apprentissage supervisée qui prédit les UDM et les UDM imbriqués. La performance de notre segmentation a été évaluée sur deux genres de corpus : des textes de livres de l’enseignement secondaire et des textes du corpus Arabic Treebank. Nous montrons que la combinaison de traits typographiques, morphologiques et lexicaux permet une bonne reconnaissance des bornes de segments. De plus, nous montrons que l'ajout de traits syntaxiques n’améliore pas les performances de notre segmentation.</resume>
        <mots_cles>Segmentation discursive, unité discursive minimale, langue arabe</mots_cles>
        <title>Segmenting Arabic Texts into Elementary Discourse Units</title>
        <abstract>Discourse segmentation aims at splitting texts into Elementary Discourse Units (EDUs) which are non-overlapping units that serve to build a discourse structure of a document. Current state of the art approaches in discourse segmentation make an extensive use of syntactic information. Unfortunately, some languages do not have any robust parser. In this paper, we investigate the feasibility of Arabic discourse segmentation using a supervised learning approach that predicts nested EDUs. The performance of our segmenter was assessed on two genres of corpora: elementary school textbooks that we build ourselves and documents extracted from the Arabic Treebank. We show that a combination of typographical, morphological and lexical features is sufficient to achieve good results in segment boundaries detection. In addition, we show that adding low-level syntactic features that are manually encoded in ATB does not enhance the performance of our segmenter.</abstract>
        <keywords>Discourse segmentation, Elementary discourse units, Arabic language</keywords>
      </article>
      <article id="taln-2013-long-033" session="Traduction et Alignement">
        <auteurs>
          <auteur>
            <nom>Thomas Lavergne</nom>
            <email>lavergne@limsi.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Alexandre Allauzen</nom>
            <email>allauzen@limsi.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>François Yvon</nom>
            <email>yvon@limsi.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université Paris Sud 91 405 Orsay</affiliation>
          <affiliation affiliationId="2">LIMSI/CNRS rue John von Neuman 91 405 Orsay</affiliation>
        </affiliations>
        <titre>Un cadre d’apprentissage intégralement discriminant pour la traduction statistique</titre>
        <type>long</type>
        <pages>450-463</pages>
        <resume>Une faiblesse des systèmes de traduction statistiques est le caractère ad hoc du processus d’apprentissage, qui repose sur un empilement d’heuristiques et conduit à apprendre des paramètres dont la valeur est sous-optimale. Dans ce travail, nous reformulons la traduction automatique sous la forme familière de l’apprentissage d’un modèle probabiliste structuré utilisant une paramétrisation log-linéaire. Cette entreprise est rendue possible par le développement d’une implantation efficace qui permet en particulier de prendre en compte la présence de variables latentes dans le modèle. Notre approche est comparée, avec succès, avec une approche de l’état de l’art sur la tâche de traduction de données du BTEC pour le couple Français-Anglais.</resume>
        <mots_cles>Traduction Automatique, Apprentissage Discriminant</mots_cles>
        <title>A fully discriminative training framework for Statistical Machine Translation</title>
        <abstract>A major pitfall of existing statistical machine translation systems is their lack of a proper training procedure. In fact, the phrase extraction and scoring processes that underlie the construction of the translation model typically rely on a chain of crude heuristics, a situation deemed problematic by many. In this paper, we recast machine translation in the familiar terms of a probabilistic structure learning problem, using a standard log-linear parameterization. The tractability of this enterprise is achieved through an efficient implementation that can take into account all the aspects of the underlying translation process through latent variables. We also address the reference reachability issue by using oracle decoding techniques. This approach is experimentally contrasted with a state-of-the-art system on the French-English BTEC translation task.</abstract>
        <keywords>Machine Translation, Discriminative Learning</keywords>
      </article>
      <article id="taln-2013-long-034" session="Sémantique">
        <auteurs>
          <auteur>
            <nom>Yue Ma</nom>
            <email>mayue@tcs.inf.tu-dresden.de</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>François Lévy</nom>
            <email>francois.levy@lipn.univ-paris13.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Adeline Nazarenko</nom>
            <email>adeline.nazarenko@lipn.univ-paris13.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">TU-Dresden, Germany</affiliation>
          <affiliation affiliationId="2">LIPN, Université Paris 13-CNRS, France</affiliation>
        </affiliations>
        <titre>Annotation sémantique pour des domaines spécialisés et des ontologies riches</titre>
        <type>long</type>
        <pages>464-478</pages>
        <resume>Explorer et maintenir une documentation technique est une tâche difficile pour laquelle on pourrait bénéficier d’un outillage efficace, à condition que les documents soient annotés sémantiquement. Les annotations doivent être riches, cohérentes, suffisamment spécialisées et s’appuyer sur un modèle sémantique explicite – habituellement une ontologie – qui modélise la sémantique du domaine cible. Il s’avère que les approches d’annotation traditionnelles donnent pour cette tâche des résultats limités. Nous proposons donc une nouvelle approche, l’annotation sémantique statistique basée sur les syntagmes, qui prédit les annotations sémantiques à partir d’un ensemble d’apprentissage réduit. Cette modélisation facilite l’annotation sémantique spécialisée au regard de modèles sémantiques de domaine arbitrairement riches. Nous l’évaluons à l’aide de plusieurs métriques et sur deux textes décrivant des réglementations métier. Notre approche obtient de bons résultats. En particulier, la F-mesure est de l’ordre de 91, 9% et 97, 6% pour la prédiction de l’étiquette et de la position avec différents paramètres. Cela suggère que les annotateurs humains peuvent être fortement aidés pour l’annotation sémantique dans des domaines spécifiques.</resume>
        <mots_cles>Annotation sémantique, Ontologie de domaine, Annotation automatique, Analyse sémantique des textes, Méthodes statistiques</mots_cles>
        <title>Semantic Annotation in Specific Domains with rich Ontologies</title>
        <abstract>Technical documentations are generally difficult to explore and maintain. Powerful tools can help, but they require that the documents have been semantically annotated. The annotations must be sufficiently specialized, rich and consistent. They must rely on some explicit semantic model – usually an ontology – that represents the semantics of the target domain. We observed that traditional approaches have limited success on this task and we propose a novel approach, phrase-based statistical semantic annotation, for predicting semantic annotations from a limited training data set. Such a modeling makes the challenging problem, domain specific semantic annotation regarding arbitrarily rich semantic models, easily handled. Our approach achieved a good performance, with several evaluation metrics and on two different business regulatory texts. In particular, it obtained 91.9% and 97.65% F-measure in the label and position predictions with different settings. This suggests that human annotators can be highly supported in domain specific semantic annotation tasks.</abstract>
        <keywords>Semantic Annotation, Domain Ontology, Automatic annotation, Semantic Text Analysis, Statistical methods</keywords>
      </article>
      <article id="taln-2013-long-035" session="Fouille de textes et applications">
        <auteurs>
          <auteur>
            <nom>Nicolas Foucault</nom>
            <email>Nicolas.Foucault@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Sophie Rosset</nom>
            <email>Sophie.Rosset@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Gilles Adda</nom>
            <email>Gilles.Adda@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIMSI-CNRS - 508 rue John von Neumann - Plateau du Moulon Université de Paris-Sud - B.P. 133 - 91403 Orsay Cedex - France</affiliation>
        </affiliations>
        <titre>Pré-segmentation de pages web et sélection de documents pertinents en Questions-Réponses</titre>
        <type>long</type>
        <pages>479-492</pages>
        <resume>Dans cet article, nous présentons une méthode de segmentation de pages web en blocs de texte pour la sélection de documents pertinents en questions-réponses. La segmentation des documents se fait préalablement à leur indexation en plus du découpage des segments obtenus en passages au moment de l’extraction des réponses. L’extraction du contenu textuel des pages est faite à l’aide d’un extracteur maison. Nous avons testé deux méthodes de segmentation. L’une segmente les textes extraits des pages web uniformément en blocs de taille fixe, l’autre les segmente par TextTiling (Hearst, 1997) en blocs thématiques de taille variable. Les expériences menées sur un corpus de 500K pages web et un jeu de 309 questions factuelles en français, issus du projet Quaero (Quintard et al., 2010), montrent que la méthode employée tend à améliorer la précision globale (top-10) du système RITEL–QR (Rosset et al., 2008) dans sa tâche.</resume>
        <mots_cles>pages web, TextTiling, sélection de documents, questions-réponses, Quaero, Ritel, segmentation textuelle, segmentation thématique</mots_cles>
        <title>Web pages segmentation for document selection in Question Answering</title>
        <abstract>In this paper, we study two different kinds of web pages segmentation for document selection in question answering. The segmentation is applied prior to indexation in addition to the traditionnal passage retrieval step in question answering. In both cases, the segmentation is textual and processed once the web pages textual content has been extracted using our own extraction system. In the first case, a document is tilled homogeneously in text blocs of fixed size while in the second case the segmentation is based on the TextTiling algorithm (Hearst, 1997). Evaluation on 309 factoid questions and a collection of 500K French web pages, coming from the Quaero project (Quintard et al., 2010), showed that such approaches tend to support properly the RITEL–QR system (Rosset et al., 2008) in this task.</abstract>
        <keywords>web pages, TextTiling, document selection, question answering, Quaero, Ritel, textual segmentation, topic segmentation</keywords>
      </article>
      <article id="taln-2013-long-036" session="Fouille de textes et applications">
        <auteurs>
          <auteur>
            <nom>Anne-Laure Ligozat</nom>
            <email/>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Cyril Grouin</nom>
            <email/>
            <affiliationId>1</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Anne Garcia-Fernandez</nom>
            <email/>
            <affiliationId>4</affiliationId>
          </auteur>
          <auteur>
            <nom>Delphine Bernhard</nom>
            <email/>
            <affiliationId>5</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIMSI–CNRS, Orsay</affiliation>
          <affiliation affiliationId="2">ENSIIE, Évry</affiliation>
          <affiliation affiliationId="3">INSERM U872 Eq 20 &amp; UPMC, Paris</affiliation>
          <affiliation affiliationId="4">LAS, CNRS/EHESS/Collège de France, Paris</affiliation>
          <affiliation affiliationId="5">LiLPa, Université de Strasbourg, Strasbourg</affiliation>
        </affiliations>
        <titre>Approches à base de fréquences pour la simplification lexicale</titre>
        <type>long</type>
        <pages>493-506</pages>
        <resume>La simplification lexicale consiste à remplacer des mots ou des phrases par leur équivalent plus simple. Dans cet article, nous présentons trois modèles de simplification lexicale, fondés sur différents critères qui font qu’un mot est plus simple à lire et à comprendre qu’un autre. Nous avons testé différentes tailles de contextes autour du mot étudié : absence de contexte avec un modèle fondé sur des fréquences de termes dans un corpus d’anglais simplifié ; quelques mots de contexte au moyen de probabilités à base de n-grammes issus de données du web ; et le contexte étendu avec un modèle fondé sur les fréquences de cooccurrences.</resume>
        <mots_cles>simplification lexicale, fréquence lexicale, modèle de langue</mots_cles>
        <title>Studying frequency-based approaches to process lexical simplification</title>
        <abstract>Lexical simplification aims at replacing words or phrases by simpler equivalents. In this paper, we present three models for lexical simplification, focusing on the criteria that make one word simpler to read and understand than another. We tested different contexts of the considered word : no context, with a model based on word frequencies in a simplified English corpus ; a few words context, with n-grams probabilites on Web data, and an extended context, with a model based on co-occurrence frequencies.</abstract>
        <keywords>lexical simplification, lexical frequency, language model</keywords>
      </article>
      <article id="taln-2013-court-001" session="Poster">
        <auteurs>
          <auteur>
            <nom>Florian Boudin</nom>
            <email>florian.boudin@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LINA - UMR CNRS 6241, Université de Nantes, France</affiliation>
        </affiliations>
        <titre>TALN Archives : une archive numérique francophone des articles de recherche en Traitement Automatique de la Langue</titre>
        <type>court</type>
        <pages>507-514</pages>
        <resume>La recherche scientifique est un processus incrémental. La première étape à effectuer avant de débuter des travaux consiste à réaliser un état de l’art des méthodes existantes. La communauté francophone du Traitement Automatique de la Langue (TAL) produit de nombreuses publications scientifiques qui sont malheureusement dispersées sur différents sites et pour lesquelles aucune méta-donnée n’est disponible. Cet article présente la construction de TALN Archives, une archive numérique francophone des articles de recherche en TAL dont le but est d’offrir un accès simplifié aux différents travaux effectués dans notre domaine. Nous présentons également une analyse du réseau de collaboration construit à partir des méta-données que nous avons extraites et dévoilons l’identité du Kevin Bacon de TALN Archives, i.e. l’auteur le plus central dans le réseau de collaboration.</resume>
        <mots_cles>TALN Archives, archive numérique, articles scientifiques</mots_cles>
        <title>TALN Archives : a digital archive of French research articles in Natural Language Processing</title>
        <abstract>Scientific research is an incremental process. Reviewing the literature is the first step to do before starting a new research project. The French Natural Language Processing (NLP) community produces numerous scientific publications which are scattered across different sources and for which no metadata is available. This paper presents the construction of TALN Archives, a digital archive of French research articles whose aim is to provide efficient access to articles in the NLP field. We also present an analysis of the collaboration network constructed from the metadata and disclose the identity of the Kevin Bacon of the TALN Archives, i.e. the most central author in the collaboration network.</abstract>
        <keywords>TALN Archives, digital archive, scientific articles</keywords>
      </article>
      <article id="taln-2013-court-002" session="Poster">
        <auteurs>
          <auteur>
            <nom>Pierre-Francois Marteau</nom>
            <email>pierre-francois.marteau@univ-ubs.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Gildas Ménier</nom>
            <email>gildas.menier@univ-ubs.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">IRISA, UMR 6074</affiliation>
          <affiliation affiliationId="2">Université de Bretagne Sud, 56000 Vannes</affiliation>
        </affiliations>
        <titre>Similarités induites par mesure de comparabilité : signification et utilité pour le clustering et l’alignement de textes comparables</titre>
        <type>court</type>
        <pages>515-522</pages>
        <resume>En présence de corpus comparables bilingues, nous sommes confrontés à des données qu’il est naturel de plonger dans deux espaces de représentation linguistique distincts, chacun éventuellement muni d’une mesure quantifiable de similarité (ou d’une distance). Dès lors que ces données bilingues sont comparables au sens d’une mesure de comparabilité également calculable (Li et Gaussier, 2010), nous pouvons établir une connexion entre ces deux espaces de représentation linguistique en exploitant une carte d’association pondérée ("mapping") appréhendée sous la forme d’un graphe bi-directionnel dit de comparabilité. Nous abordons dans cet article les conséquences conceptuelles et pratique d’une telle connexion similarité-comparabilité en développant un algorithme (Hit-ComSim) basé sur sur le principe de similarité induite par la topologie du graphe de comparabilité. Nous essayons de qualifier qualitativement l’intérêt de cet algorithme en considérant quelques expériences préliminaires de clustering de documents comparables bilingues (Français/Anglais) collectés sur des flux RSS.</resume>
        <mots_cles>Graphe de comparabilité, Similarités induites, Documents comparables, Clustering</mots_cles>
        <title>Similarities induced by a comparability mapping : meaning and utility in the context of the clustering of comparable texts</title>
        <abstract>In the presence of bilingual comparable corpora it is natural to embed the data in two distinct linguistic representation spaces in which a "computational" notion of similarity is potentially defined. As far as these bilingual data are comparable in the sense of a measure of comparability also computable (Li et Gaussier, 2010), we can establish a connection between these two areas of linguistic representation by exploiting a weighted mapping that can be represented in the form of a weighted bidirectional graph of comparability. We study in this paper the conceptual and practical consequences of such a similarity-comparability connection, while developing an algorithm (Hit-ComSim) based on the concept of similarities induced by the topology of the graph of comparability. We try to evaluate the benefit of this algorithm considering some preliminary categorization or clustering tasks of bilingual (English/French) documents collected from RSS feeds.</abstract>
        <keywords>Comparability graph, Induced similarities, Comparable documents, Clustering</keywords>
      </article>
      <article id="taln-2013-court-003" session="Poster">
        <auteurs>
          <auteur>
            <nom>Denis Maurel</nom>
            <email>denis.maurel@univ-tours.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Béatrice Bouchou Markhoff</nom>
            <email>beatrice.bouchou@univ-tours.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université François Rabelais Tours</affiliation>
        </affiliations>
        <titre>ProLMF version 1.2. Une ressource libre de noms propres avec des expansions contextuelles</titre>
        <type>court</type>
        <pages>523-530</pages>
        <resume>ProLMF est la version LMF de la base lexicale multilingue de noms propres Prolexbase. Disponible librement sur le site du CNRTL, la version 1.2 a été largement améliorée et augmentée par de nouvelles entrées en français, complétées par des expansions contextuelles, et par de petits lexiques en une huitaine de langues.</resume>
        <mots_cles>ressource libre, base lexicale multilingue, noms propres, expansions contextuelles, schémas de contextualisation, relations sémantiques, alias, point de vue, Prolexbase</mots_cles>
        <title>ProLMF 1.2, Proper Names with their Expansions</title>
        <abstract>ProLMF is the LMF version of Prolexbase, a multilingual lexical database of Proper Names. It can be freely downloaded on the CNRTL Website. Version 1.2 had been widely improved and increased, with new French entries whose description includes contextual expansions, and eight small lexica for other languages.</abstract>
        <keywords>free resource, multilingual lexical database, Proper Names, context, semantic relations, alias, point of view, Prolexbase</keywords>
      </article>
      <article id="taln-2013-court-004" session="Poster">
        <auteurs>
          <auteur>
            <nom>Benjamin Lecouteux</nom>
            <email>benjamin.lecouteux@imag.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Laurent Besacier</nom>
            <email>laurent.besacier@imag.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire d’Informatique de Grenoble (LIG), Université de Grenoble</affiliation>
        </affiliations>
        <titre>Vers un décodage guidé pour la traduction automatique</titre>
        <type>court</type>
        <pages>531-538</pages>
        <resume>Récemment, le paradigme du décodage guidé a montré un fort potentiel dans le cadre de la reconnaissance automatique de la parole. Le principe est de guider le processus de décodage via l’utilisation de transcriptions auxiliaires. Ce paradigme appliqué à la traduction automatique permet d’envisager de nombreuses applications telles que la combinaison de systèmes, la traduction multi-sources etc. Cet article présente une approche préliminaire de l’application de ce paradigme à la traduction automatique (TA). Nous proposons d’enrichir le modèle log-linéaire d’un système primaire de TA avec des mesures de distance relatives à des systèmes de TA auxiliaires. Les premiers résultats obtenus sur la tâche de traduction Français/Anglais issue de la campagne d’évaluation WMT 2011 montrent le potentiel du décodage guidé.</resume>
        <mots_cles>Décodage guidé, traduction automatique, combinaison de systèmes</mots_cles>
        <title>Driven Decoding for machine translation</title>
        <abstract>Recently, the concept of driven decoding (DD), has been sucessfully applied to the automatic speech recognition (speech-to-text) task : an auxiliary transcription guide the decoding process. There is a strong interest in applying this concept to statistical machine translation (SMT). This paper presents our approach on this topic. Our first attempt in driven decoding consists in adding several feature functions corresponding to the distance between the current hypothesis decoded and the auxiliary translations available. Experimental results done for a french-to-english machine translation task, in the framework of the WMT 2011 evaluation, show the potential of the DD approach proposed.</abstract>
        <keywords>Driven Decoding, machine translation, system combination</keywords>
      </article>
      <article id="taln-2013-court-005" session="Poster">
        <auteurs>
          <auteur>
            <nom>Johanna Gerlach</nom>
            <email>Johanna.Gerlach@unige.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Victoria Porro</nom>
            <email>Victoria.Porro@unige.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Pierrette Bouillon</nom>
            <email>Pierrette.Bouillon@unige.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Sabine Lehmann</nom>
            <email>Sabine.Lehmann@acrolinx.com</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université de Genève fti/tim - 40, bvd Du Pont-d’Arve, CH-1211 Genève 4, Suisse</affiliation>
          <affiliation affiliationId="2">Acrolinx GmbH, Friedrichstr. 100, 10117 Berlin, Allemagne</affiliation>
        </affiliations>
        <titre>La La préédition avec des règles peu coûteuses, utile pour la TA statistique des forums ?</titre>
        <type>court</type>
        <pages>539-546</pages>
        <resume>Cet article s’intéresse à la traduction automatique statistique des forums, dans le cadre du projet européen ACCEPT (« Automated Community Content Editing Portal »). Nous montrons qu’il est possible d’écrire des règles de préédition peu coûteuses sur le plan des ressources linguistiques et applicables sans trop d’effort avec un impact très significatif sur la traduction automatique (TA) statistique, sans avoir à modifier le système de TA. Nous décrivons la méthodologie proposée pour écrire les règles de préédition et les évaluer, ainsi que les résultats obtenus par type de règles.</resume>
        <mots_cles>préédition, langage contrôlé, traduction statistique, forums</mots_cles>
        <title>Can lightweight pre-editing rules improve statistical MT of forum content?</title>
        <abstract>This paper focuses on the statistical machine translation (SMT) of forums within the context of the European Framework ACCEPT («Automated Community Content Editing Portal») project. We demonstrate that it is possible to write lightweight pre-editing rules that require few linguistic resources, are relatively easy to apply and have significant impact on SMT without any changes to the machine translation system. We describe methodologies for rule development and evaluation, and provide results obtained for different rule types.</abstract>
        <keywords>pre-edition, controlled language, statistical machine translation, forums</keywords>
      </article>
      <article id="taln-2013-court-006" session="Poster">
        <auteurs>
          <auteur>
            <nom>Ludovic Hamon</nom>
            <email>ludovic.hamon@univ-ubs.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Sylvie Gibet</nom>
            <email>Sylvie.Gibet@univ-ubs.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Sabah Boustila</nom>
            <email>boustila@unistra.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">IRISA, Université Bretagne Sud, Campus de Tohannic, Rue Yves Mainguy, 56000 Vannes</affiliation>
          <affiliation affiliationId="2">ICUBE, Université de Strasbourg, 300 bd Sébastien Brant, BP 10413, 67412 IIIkirch Cedex, France</affiliation>
        </affiliations>
        <titre>Édition interactive d’énoncés en langue des signes française dédiée aux avatars signeurs</titre>
        <type>court</type>
        <pages>547-554</pages>
        <resume>Les avatars signeurs en Langue des Signes Française (LSF) sont de plus en plus utilisés en tant qu’interface de communication à destination de la communauté sourde. L’un des critères d’acceptation de ces avatars est l’aspect naturel et réaliste des gestes produits. Par conséquent, des méthodes de synthèse de gestes ont été élaborées à l’aide de corpus de mouvements capturés et annotés provenant d’un signeur réel. Néanmoins, l’enrichissement d’un tel corpus, en faisant fi des séances de captures supplémentaires, demeure une problématique certaine. De plus, l’application automatique d’opérations sur ces mouvements (e.g. concaténation, mélange, etc.) ne garantit pas la consistance sémantique du geste résultant. Une alternative est d’insérer l’opérateur humain dans la boucle de construction des énoncés en LSF. Dans cette optique, cet article propose un premier système interactif d’édition de gestes en LSF, basé "données capturées" et dédié aux avatars signeurs.</resume>
        <mots_cles>Langue des Signes Française, édition, geste, base de données sémantiques, signeur virtuel, interaction</mots_cles>
        <title>Interactive editing of utterances in French sign language dedicated to signing avatars</title>
        <abstract>Signing avatars dedicated to French Sign Language (LSF) are more and more used as a communication interface for the deaf community. One of the acceptation criteria of these avatars is the natural and realistic aspect of the constructed gestures. Consequently, gestures synthesis methods have been designed thanks to some corpus of captured and annotated motions, performed by a real signer. However, the enlarging of such a corpus, without requiring of some additional capture sessions, is a major issue. Furthermore, the automatic application of motion transformations (e.g. concatenation, blending, etc.) does not guarantee the semantic consistency of the resulting gesture. Another option is to insert the human operator in the utterance building loop. In this context, this paper provides a first interactive editing system of FSL gestures, based on captured motions and dedicated to signing avatars.</abstract>
        <keywords>French sign language, editing, gesture, semantic data base, virtual signer, interaction</keywords>
      </article>
      <article id="taln-2013-court-007" session="Poster">
        <auteurs>
          <auteur>
            <nom>Judith Muzerelle</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Anaïs Lefeuvre</nom>
            <email/>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Jean-Yves Antoine</nom>
            <email>Jean-Yves.Antoine@univ-tour.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Emmanuel Schang</nom>
            <email>Emmanuel.Schang@univ-orleans.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Denis Maurel</nom>
            <email/>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Jeanne Villaneau</nom>
            <email>Jeanne.Villaneau@univ-ubs.fr</email>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Iris Eshkol</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LLL Orléans, Université d’Orléans</affiliation>
          <affiliation affiliationId="2">Université François Rabelais Tours, LI, 3 place Jean Jaurès, 41000 Blois</affiliation>
          <affiliation affiliationId="3">IRISA, Université Européenne de Bretagne, 56100 Lorient</affiliation>
        </affiliations>
        <titre>ANCOR, premier corpus de français parlé d’envergure annoté en coréférence et distribué librement</titre>
        <type>court</type>
        <pages>555-563</pages>
        <resume>Cet article présente la réalisation d’ANCOR, qui constitue par son envergure (453 000 mots) le premier corpus francophone annoté en anaphores et coréférences permettant le développement d’approches centrées sur les données pour la résolution des anaphores et autres traitements de la coréférence. L’annotation a été réalisée sur trois corpus de parole conversationnelle (Accueil_UBS, OTG et ESLO) qui le destinent plus particulièrement au traitement du langage parlé. En l’absence d’équivalent pour le langage écrit, il est toutefois susceptible d’intéresser l’ensemble de la communauté TAL. Par ailleurs, le schéma d’annotation retenu est suffisamment riche pour permettre des études en linguistique de corpus. Le corpus sera diffusé librement à la mi-2013 sous licence Creative Commons BY-NC-SA. Cet article se concentre sur sa mise en oeuvre et décrit brièvement quelques résultats obtenus sur la partie déjà annotée de la ressource.</resume>
        <mots_cles>Corpus, annotation, coréférence, anaphore, parole conversationnelle</mots_cles>
        <title>ANCOR, the first large French speaking corpus of conversational speech annotated in coreference to be freely available</title>
        <abstract>This paper presents the first French spoken corpus annotated in coreference whose size (453,000 words) is sufficient to investigate the achievement of data oriented systems of coreference resolution. The annotation was conducted on three different corpora of conversational speech (Accueil_UBS, OTG, ESLO) but this resource can also be interesting for NLP researchers working on written language, considering the lack of a large written French corpus annotated in coreference. We followed a rich annotation scheme which enables also research motivated by linguistic considerations. This corpus will be freely available (Creative Commons BY-NC-SA) around mid-2013. The paper details the achievement of the resource as well as preliminary experiments conducted on the part of the corpus already annotated.</abstract>
        <keywords>Corpus, annotation, coreference, anaphora, conversational speech</keywords>
      </article>
      <article id="taln-2013-court-008" session="Poster">
        <auteurs>
          <auteur>
            <nom>Elizaveta Loginova-Clouet</nom>
            <email>elizaveta.loginova@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Béatrice Daille</nom>
            <email>beatrice.daille@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LINA, 2, rue de la Houssinière 44322 Nantes Cedex 03</affiliation>
        </affiliations>
        <titre>Segmentation Multilingue des Mots Composés</titre>
        <type>court</type>
        <pages>564-571</pages>
        <resume>La composition est un phénomène fréquent dans plusieurs langues, surtout dans des langues ayant une morphologie riche. Le traitement des mots composés est un défi pour les systèmes de TAL car pour la plupart, ils ne sont pas présents dans les lexiques. Dans cet article, nous présentons une méthode de segmentation des composés qui combine des caractéristiques indépendantes de la langue (mesure de similarité, données du corpus) avec des règles de transformation sur les frontières des composants spécifiques à une langue. Nos expériences de segmentation de termes composés allemands et russes montrent une exactitude jusqu’à 95 % pour l’allemand et jusqu’à 91 % pour le russe. Nous constatons que l’utilisation de corpus spécialisés relevant du même domaine que les composés améliore la qualité de segmentation.</resume>
        <mots_cles>segmentation des mots composés, outil multilingue, mesure de similarité, règles de transformation des composants, corpus spécialisés</mots_cles>
        <title>Multilingual Compound Splitting</title>
        <abstract>Compounding is a common phenomenon for many languages, especially those with a rich morphology. Dealing with compounds is a challenge for natural language processing systems since all compounds can not be included in lexicons. In this paper, we present a compound splitting method combining language independent features (similarity measure, corpus data) and language dependent features (component transformation rules). We report on our experiments in splitting of German and Russian compound terms giving accuracy up to 95% for German and up to 91% for Russian language. We observe that the usage of a corpus of the same domain as compounds improves splitting quality.</abstract>
        <keywords>compound splitting, multilingual tool, similarity measure, component transformation rules, specialized corpora</keywords>
      </article>
      <article id="taln-2013-court-009" session="Poster">
        <auteurs>
          <auteur>
            <nom>Ying Zhang</nom>
            <email>ying.zhang@imag.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Mathieu Mangeot</nom>
            <email>mathieu.mangeot@imag.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">GETALP-LIG, 41, rue des Mathématiques BP53 38041 Grenoble Cedex 9</affiliation>
        </affiliations>
        <titre>Gestion des terminologies riches : l'exemple des acronymes</titre>
        <type>court</type>
        <pages>572-579</pages>
        <resume>La gestion des terminologies pose encore des problèmes, en particulier pour des constructions complexes comme les acronymes. Dans cet article, nous proposons une solution en reliant plusieurs termes différents à un seul référent via les notions de pivot et de prolexème. Ces notions permettent par exemple de faire le lien entre plusieurs termes qui désignent un même et unique référent : Nations Unies, ONU, Organisation des Nations Unies et onusien. Il existe Jibiki, une plate-forme générique de gestion de bases lexicales permettant de gérer n'importe quel type de structure (macro et microstructure). Nous avons implémenté une nouvelle macrostructure de ProAxie dans la plate-forme Jibiki pour réaliser la gestion des acronymes.</resume>
        <mots_cles>base lexicale multilingue, macrostructure, Jibiki, Common Dictionary Markup, Proaxie, Prolèxeme</mots_cles>
        <title>Complex terminologies management - the case of acronyms</title>
        <abstract>Terminology management is still problematic, especially for complex constructions such as acronyms. In this paper, we propose a solution to connect several different terms with a single referent through using the concepts of pivot and prolexeme. These concepts allow for example to link several terms for the same referent: Nations Unies, ONU, Organisation des Nations Unies and onusien. Jibiki is a generic platform for lexical database management, allowing the representation of any type of structure (macro and microstructure). We have implemented a new macrostructure ProAxie in the Jibiki platform to achieve acronym management.</abstract>
        <keywords>multilingual lexical database, macrostructure, Jibiki, Common Dictionary Markup, Proaxie, Prolexeme</keywords>
      </article>
      <article id="taln-2013-court-010" session="Poster">
        <auteurs>
          <auteur>
            <nom>Marcos Zampieri</nom>
            <email>mzampier@uni-koeln.de</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Binyam Gebrekidan Gebre</nom>
            <email>bingeb@mpi.nl</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Sascha Diwersy</nom>
            <email>sascha.diwersy@uni-koeln.de</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">University of Cologne, Germany</affiliation>
          <affiliation affiliationId="2">Max Planck Institute for Psycholinguistics, Nijmegen, Holland</affiliation>
        </affiliations>
        <titre>Ngrammes et Traits Morphosyntaxiques pour la Identification de Variétés de l’Espagnol</titre>
        <type>court</type>
        <pages>580-587</pages>
        <resume>Notre article présente expérimentations portant sur la classification supervisée de variétés nationales de l’espagnol. Outre les approches classiques, basées sur l’utilisation de ngrammes de caractères ou de mots, nous avons testé des modèles calculés selon des traits morphosyntaxiques, l’objectif étant de vérifier dans quelle mesure il est possible de parvenir à une classification automatique des variétés d’une langue en s’appuyant uniquement sur des descripteurs grammaticaux. Les calculs ont été effectués sur la base d’un corpus de textes journalistiques de quatre pays hispanophones (Espagne, Argentine, Mexique et Pérou).</resume>
        <mots_cles>classification automatique, ngrammes, espagnol, variétés nationales</mots_cles>
        <title>N-gram Language Models and POS Distribution for the Identification of Spanish Varieties</title>
        <abstract>This article presents supervised computational methods for the identification of Spanish varieties. The features used for this task were the classical character and word n-gram language models as well as POS and morphological information. The use of these features is to our knowledge new and we aim to explore the extent to which it is possible to identify language varieties solely based on grammatical differences. Four journalistic corpora from different countries were used in these experiments : Spain, Argentina, Mexico and Peru.</abstract>
        <keywords>automatic classification, n-grams, Spanish, language varieties</keywords>
      </article>
      <article id="taln-2013-court-011" session="Poster">
        <auteurs>
          <auteur>
            <nom>Amel Fraisse</nom>
            <email>fraisse@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Patrick Paroubek</nom>
            <email>pap@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Gil Francopoulo</nom>
            <email>gil.francopoulo@tagmatica.com</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIMSI-CNRS, Bât. 508 Université Paris-Sud, 91403 Orsay Cedex, France</affiliation>
          <affiliation affiliationId="2">TAGMATICA, 126 rue de Picpus, 75012 Paris France</affiliation>
        </affiliations>
        <titre>L’apport des Entités Nommées pour la classification des opinions minoritaires</titre>
        <type>court</type>
        <pages>588-595</pages>
        <resume>La majeure partie des travaux en fouille d’opinion et en analyse de sentiment concerne le classement des opinions majoritaires. Les méthodes d’apprentissage supervisé à base de ngrammes sont souvent employées. Elles ont l’inconvénient d’avoir un biais en faveur des opinions majoritaires si on les utilise de manière classique. En fait la présence d’un terme particulier, fortement associé à la cible de l’opinion dans un document peut parfois suffire à faire basculer le classement de ce document dans la classe de ceux qui expriment une opinion majoritaire sur la cible. C’est un phénomène positif pour l’exactitude globale du classifieur, mais les documents exprimant des opinions minoritaires sont souvent mal classés. Ce point est un problème dans le cas où l’on s’intéresse à la détection des signaux faibles (détection de rumeur) ou pour l’anticipation de renversement de tendance. Nous proposons dans cet article d’améliorer la classification des opinions minoritaires en prenant en compte les Entités Nommées dans le calcul de pondération destiné à corriger le biais en faveur des opinions majoritaires.</resume>
        <mots_cles>Fouille d’opinions, Opinion minoritaires, Entités Nommées, Apprentissage, N-grammes, Pondération</mots_cles>
        <title>Improving Minor Opinion Polarity Classification with Named Entity Analysis</title>
        <abstract>The main part of the work on opinion mining and sentiment analysis concerns polarity classification of majority opinions. Supervised machine learning with n-gram features is a common approach to polarity classification, which is often biased towards the majority of opinions about a given opinion target, when using this kind of approach with traditional settings. The presence of a specific term, strongly associated to the opinion target in a document, is often enough to tip the classifier decision toward the majority opinion class. This is actually a good thing for overall accuracy. Howeverm documents about the opinion taget, but expressing a polarity different from the majority one, get misclassified. It is a problem if we want to detect weak signals (rumor detection) or for anticipating opinion reversal trends. We propose in this paper to improve minor reviews polarity classification by taking into account Named Entity information in the computation of specific weighting scheme used for correcting the bias toward majority opinions.</abstract>
        <keywords>Opinion Mining, Minor Opinion, Named Entities, Machine Learning, N-grams, Weighting Scheme</keywords>
      </article>
      <article id="taln-2013-court-012" session="Poster">
        <auteurs>
          <auteur>
            <nom>Gemma Bel-Enguix</nom>
            <email>gemma.belenguix@gmail.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Michael Zock</nom>
            <email>michael.zock@lif.univ-mrs.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CNRS-LIF, UMR 7279, Aix Marseille Université, Marseille</affiliation>
        </affiliations>
        <titre>Trouver les mots dans un simple réseau de co-occurrences</titre>
        <type>court</type>
        <pages>596-603</pages>
        <resume>Au cours des deux dernières décennies des psychologues et des linguistes informaticiens ont essayé de modéliser l'accès lexical en construisant des simulations ou des ressources. Cependant, parmi ces chercheurs, pratiquement personne n'a vraiment cherché à améliorer la navigation dans des 'dictionnaires électroniques destinés aux producteurs de langue'. Pourtant, beaucoup de travaux ont été consacrés à l'étude du phénomène du mot sur le bout de la langue et à la construction de réseaux lexicaux. Par ailleurs, vu les progrès réalisés en neurosciences et dans le domaine des réseaux complexes, on pourrait être tenté de construire un simulacre du dictionnaire mental, ou, à défaut une ressource destinée aux producteurs de langue (écrivains, conférenciers). Nous sommes restreints en construisant un réseau de co-occurrences à partir des résumés de Wikipedia, le but étant de vérifier jusqu'où l'on pouvait pousser une telle ressource pour trouver un mot, sachant que la ressource ne contient pas de liens sémantiques, car le réseau est construit de manière automatique et à partir de textes non-annotés.</resume>
        <mots_cles>accès lexical, anomie, mot sur le bout de la langue, réseaux lexicaux</mots_cles>
        <title>Lexical access via a simple co-occurrence network</title>
        <abstract>During the last two decades psychologists and computational linguists have attempted to tackle the problem of word access via computational resources, yet hardly none of them has seriously tried to support 'interactive' word finding. Yet, a lot of work has been done to understand the causes of the tip-of-the-tongue problem (TOT). Given the progress made in neuroscience, corpus linguistics, and graph theory (complex graphs), one may be tempted to emulate the mental lexicon, or to build a resource likely to help authors (speakers, writers) to overcome word-finding problems. Our goal here is much more limited. We try to identify good hints for finding a target word. To this end we have built a co-occurrence network on the basis of Wikipedia abstracts. Since the network is built automatically and from raw data, i.e. non-annotated text, it does not reveal the kind of relationship holding between the nodes. Despite this shortcoming we tried to see whether we can find a given word, or, to identify what is a good clue word.</abstract>
        <keywords>lexical access, anomia, tip of the tongue (TOT), lexical networks</keywords>
      </article>
      <article id="taln-2013-court-013" session="Poster">
        <auteurs>
          <auteur>
            <nom>Guy Perrier</nom>
            <email>guy.perrier@loria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LORIA, Université de Lorraine, équipe Sémagramme, bât. C, Campus Scientifique BP 239 54506 Vandøeuvre-lès-Nancy, cedex, France</affiliation>
        </affiliations>
        <titre>Analyse statique des interactions entre structures élémentaires d’une grammaire</titre>
        <type>court</type>
        <pages>604-611</pages>
        <resume>Nous nous intéressons ici à la construction semi-automatique de grammaires computationnelles et à leur utilisation pour l’analyse syntaxique. Nous considérons des grammaires lexicalisées dont les structures élémentaires sont des arbres, sous-spécifiés ou pas. Nous présentons un algorithme qui vise à prévoir l’ensemble des arbres élémentaires attachés aux mots qui peuvent s’intercaler entre deux mots donnés d’une phrase, dont on sait que les arbres élémentaires associées sont des compagnons, c’est-à-dire qu’ils interagiront nécessairement dans la composition syntaxique de la phrase.</resume>
        <mots_cles>grammaire lexicalisée, grammaire d’interaction, construction de grammaires</mots_cles>
        <title>Static Analysis of Interactions between Elementary Structures of a Grammar</title>
        <abstract>We are interested in the semi-automatic construction of computational grammars and in their use for parsing. We consider lexicalized grammars with elementary structures which are trees, underspecified or not. We present an algorithm that aims at foreseeing all elementary trees attached at words which can come between two given words of a sentence, whose associated elementary trees are companions, that is, they will necessarily interact in the syntactic composition of the sentence.</abstract>
        <keywords>Lexicalized Grammar, Interaction Grammar, Grammar Construction</keywords>
      </article>
      <article id="taln-2013-court-014" session="Poster">
        <auteurs>
          <auteur>
            <nom>Eric Charton</nom>
            <email>eric.charton@polymtl.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Michel Gagnon</nom>
            <email>michel.gagnon@polymtl.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Ludovic Jean-Louis</nom>
            <email>ludovic.jean-louis@polymtl.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">École Polytechnique de Montréal, Montréal, QC, Canada</affiliation>
        </affiliations>
        <titre>Influence des annotations sémantiques sur un système de détection de coréférence à base de perceptron multi-couches</titre>
        <type>court</type>
        <pages>612-619</pages>
        <resume>La série de campagnes d’évaluation CoNLL-2011/2012 a permis de comparer diverses propositions d’architectures de systèmes de détection de co-références. Cet article décrit le système de résolution de coréférence Poly-co développé dans le cadre de la campagne d’évaluation CoNLL-2011 et évalue son potentiel d’amélioration en introduisant des propriétés sémantiques dans son modèle de détection. Notre système s’appuie sur un classifieur perceptron multi-couches. Nous décrivons les heuristiques utilisées pour la sélection des paires de mentions candidates, ainsi que l’approche de sélection des traits caractéristiques que nous avons utilisée lors de la campagne CoNLL-2011. Nous introduisons ensuite un trait sémantique complémentaire et évaluons son influence sur les performances du système.</resume>
        <mots_cles>Coréférence, Perceptron multi-couches</mots_cles>
        <title>Semantic annotation influence on coreference detection using perceptron approach</title>
        <abstract>The ConLL-2011/2012 evaluation campaign was dedicated to coreference detection systems. This paper presents the coreference resolution system Poly-co submitted to the closed track of the CoNLL-2011 Shared Task and evaluate is potential of evolution when it includes a semantic feature. Our system integrates a multilayer perceptron classifier in a pipeline approach. We describe the heuristic used to select the candidate coreference pairs that are fed to the network for training, and our feature selection method. We introduce a complementary semantic feature and evaluate the performances improvement.</abstract>
        <keywords>Coreference, Multilayer perceptron</keywords>
      </article>
      <article id="taln-2013-court-015" session="Poster">
        <auteurs>
          <auteur>
            <nom>Fatiha Sadat</nom>
            <email>Sadat.fatiha@uqam.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Emad Mohamed</nom>
            <email>emohamed@umail.iu.edu</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université du Québec à Montréal, 201 Président Kennedy, Montréal, H2X 3Y7, QC, Canada</affiliation>
        </affiliations>
        <titre>Traduction automatique statistique pour l’arabe-français améliorée par le prétraitement et l’analyse de la langue</titre>
        <type>court</type>
        <pages>620-627</pages>
        <resume>Dans cet article, nous nous intéressons au prétraitement de la langue arabe comme langue source à des fins de traduction automatique statistique. Nous présentons une étude sur la traduction automatique statistique basée sur les syntagmes, pour la paire de langues arabe-français utilisant le décodeur Moses ainsi que d’autres outils de base. Les propriétés morphologiques et syntaxiques de la langue arabe sont complexes, ce qui rend cette langue difficile à maîtriser dans le domaine du TALN. Aussi, les performances d’un système de traduction statistique dépendent considérablement de la quantité et de la qualité des corpus d’apprentissage. Dans cette étude, nous montrons qu’un prétraitement basé sur les mots de la langue source (arabe) et l’introduction de quelques règles linguistiques par rapport à la syntaxe de la langue cible (français), permet d’obtenir des améliorations du score BLEU. Cette amélioration est réalisée sans augmenter la quantité des corpus d’apprentissage.</resume>
        <mots_cles>Traduction automatique statistique, traduction arabe-français, pré-traitement de corpus, morphologie de l’Arabe</mots_cles>
        <title>Pre-processing and Language Analysis for Arabic to French Statistical Machine Translation</title>
        <abstract>Arabic is a morphologically rich and complex language, which presents significant challenges for natural language processing and machine translation. In this paper, we describe an ongoing effort to build a competitive Arabic-French phrase–based machine translation system using the Moses decoder and other tools. The results show an increase in terms of BLEU score after introducing some pre-processing schemes for Arabic and applying additional language analysis rules in relation to the target language. The proposed approach is completed using pre-processing and language analysis rules without increasing the amount of training data.</abstract>
        <keywords>Statistical machine translation, Arabic-French translation, Corpus pre-processing, Arabic morphology</keywords>
      </article>
      <article id="taln-2013-court-016" session="Poster">
        <auteurs>
          <auteur>
            <nom>Bruno Guillaume</nom>
            <email>bruno.guillaume@loria.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Karën Fort</nom>
            <email>karen.fort@loria.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LORIA 54500 Vandoeuvre-lès-Nancy</affiliation>
          <affiliation affiliationId="2">Inria Nancy Grand-Est</affiliation>
          <affiliation affiliationId="3">Université de Lorraine</affiliation>
        </affiliations>
        <titre>Expériences de formalisation d’un guide d’annotation : vers l’annotation agile assistée</titre>
        <type>court</type>
        <pages>628-635</pages>
        <resume>Nous proposons dans cet article une méthodologie, qui s’inspire du développement agile et qui permettrait d’assister la préparation d’une campagne d’annotation. Le principe consiste à formaliser au maximum les instructions contenues dans le guide d’annotation afin de vérifier automatiquement si le corpus en construction est cohérent avec le guide en cours d’écriture. Pour exprimer la partie formelle du guide, nous utilisons la réécriture de graphes, qui permet de décrire par des motifs les constructions définies. Cette formalisation permet de repérer les constructions prévues par le guide et, par contraste, celles qui ne sont pas cohérentes avec le guide. En cas d’incohérence, un expert peut soit corriger l’annotation, soit mettre à jour le guide et relancer le processus.</resume>
        <mots_cles>annotation, guide d’annotation, annotation agile, réécriture de graphes</mots_cles>
        <title>Formalizing an annotation guide : some experiments towards assisted agile annotation</title>
        <abstract>This article presents a methodology, inspired from the agile development paradigm, that helps preparing an annotation campaign. The idea behind the methodology is to formalize as much as possible the instructions given in the guidelines, in order to automatically check the consistency of the corpus being annotated with the guidelines, as they are being written. To formalize the guidelines, we use a graph rewriting tool, that allows us to use a rich language to describe the instructions. This formalization allows us to spot the rightfully annotated constructions and, by contrast, those that are not consistent with the guidelines. In case of inconsistency, an expert can either correct the annotation or update the guidelines and rerun the process.</abstract>
        <keywords>annotation, annotation guide, agile annotation, graph rewriting</keywords>
      </article>
      <article id="taln-2013-court-017" session="Poster">
        <auteurs>
          <auteur>
            <nom>Catherine Dominguès</nom>
            <email>catherine.domingues@ign.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Iris Eshkol-Taravella</nom>
            <email>iris.eshkol@univ-orleans.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">IGN, laboratoire COGIT 73 avenue de Paris 94160 Saint-Mandé</affiliation>
          <affiliation affiliationId="2">LLL, UMR 7270, 10 Rue de Tours, BP 46527, 45065 ORLEANS cedex 2</affiliation>
        </affiliations>
        <titre>Repérer des toponymes dans des titres de cartes topographiques</titre>
        <type>court</type>
        <pages>636-642</pages>
        <resume>Les titres de cartes topographiques personnalisées composent un corpus spécifique caractérisé par des variations orthographiques et un nombre élevé de désignations de lieux. L'article présente le repérage des toponymes dans ces titres. Ce repérage est fondé sur l'utilisation de BDNyme, la base de données de toponymes géoréférencés de l'IGN, et sur une analyse de surface à l'aide de patrons. La méthode proposée élargit la définition du toponyme pour tenir compte de la nature du corpus et des données qu'il contient. Elle se décompose en trois étapes successives qui tirent parti du contexte extralinguistique de géoréférencement des toponymes et du contexte linguistique. Une quatrième étape qui ne retient pas le géoréférencement est aussi étudiée. Le balisage et le typage des toponymes permettent de mettre en avant d'une part la diversité des désignations de lieux et d'autre part leurs variations d'écriture. La méthode est évaluée (rappel, précision, F-mesure) et les erreurs analysées.</resume>
        <mots_cles>toponyme, information spatiale, écriture des toponymes, BDNyme, ressource lexicale</mots_cles>
        <title>Localizing toponyms in topographic map titles</title>
        <abstract>The titles of customized topographic maps constitute a specific corpus which is characterized by spelling variations and a very significant number of place names. This paper is about identifying toponyms in these titles. The toponym tracking is based on IGN's toponym data base as well as light parsing according to patterns. The method used broadens the definition of the toponym to include the nature of the corpus and the data in it. It consists of three successive stages where both the extralinguistic context - in this case georeferencing toponyms - and the linguistic context are taken into account. The fourth stage which is without georeferencing is examined too. Toponym tagging and typing allow to highlight toponym naming and spelling variations. The method has been assessed (recall, precision, F-measure) and the results analysed.</abstract>
        <keywords>toponyme, spatial information, toponyme writing, BDNyme, lexical resource</keywords>
      </article>
      <article id="taln-2013-court-018" session="Poster">
        <auteurs>
          <auteur>
            <nom>Pierre Zweigenbaum</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Xavier TANNIER</nom>
            <email/>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIMSI–CNRS, Orsay</affiliation>
          <affiliation affiliationId="2">Univ. Paris-Sud, Orsay</affiliation>
        </affiliations>
        <titre>Extraction des relations temporelles entre événements médicaux dans des comptes rendus hospitaliers</titre>
        <type>court</type>
        <pages>643-650</pages>
        <resume>Le défi i2b2/VA 2012 était dédié à la détection de relations temporelles entre événements et expressions temporelles dans des comptes rendus hospitaliers en anglais. Les situations considérées étaient beaucoup plus variées que dans les défis TempEval. Nous avons donc axé notre travail sur un examen systématique de 57 situations différentes et de leur importance dans le corpus d’apprentissage en utilisant un oracle, et avons déterminé empiriquement le classifieur qui se comportait le mieux dans chaque situation, atteignant ainsi une F-mesure globale de 0,623.</resume>
        <mots_cles>extraction d’information, événements médicaux, relations temporelles, médecine</mots_cles>
        <title>Extraction of temporal relations between clinical events in clinical documents</title>
        <abstract>The 2012 i2b2/VA challenge focused on the detection of temporal relations between events and temporal expressions in English clinical texts. The addressed situations were much more diverse than in the TempEval challenges. We thus focused on the systematic study of 57 distinct situations and their importance in the training corpus by using an oracle, and empirically determined the best performing classifier for each situation, thereby achieving a 0.623 F-measure.</abstract>
        <keywords>Information Extraction, Clinical Events, Temporal Relations, Medicine</keywords>
      </article>
      <article id="taln-2013-court-019" session="Poster">
        <auteurs>
          <auteur>
            <nom>Nikola Tulechki</nom>
            <email>tulechki@univ-tlse2.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Ludovic Tanguy</nom>
            <email>tanguy@univ-tlse2.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CLLE-ERSS : CNRS et Université de Toulouse 2, 5 allées Antonio Machado, 31058 Toulouse CEDEX 9</affiliation>
          <affiliation affiliationId="2">Conseil en Facteurs Humains, 4 impasse Montcabrier, 31500 Toulouse</affiliation>
        </affiliations>
        <titre>Similarité de second ordre pour l’exploration de bases textuelles multilingues</titre>
        <type>court</type>
        <pages>651-658</pages>
        <resume>Cet article décrit l’utilisation de la technique de similarité de second ordre pour l’identification de textes semblables au sein d’une base de rapports d’incidents aéronautiques mélangeant les langues française et anglaise. L’objectif du système est, pour un document donné, de retrouver des documents au contenu similaire quelle que soit leur langue. Nous utilisons un corpus bilingue aligné de rapports d’accidents aéronautiques pour construire des paires de pivots et indexons les documents avec des vecteurs de similarités, tels que chaque coordonnée correspond au score de similarité entre un document dans une langue donnée et la partie du pivot de la même langue. Nous évaluons les performances du système sur un volumineux corpus de rapports d’incidents aéronautiques pour lesquels nous disposons de traductions. Les résultats sont prometteurs et valident la technique.</resume>
        <mots_cles>similarité de second ordre, multilingue, ESA</mots_cles>
        <title>Second order similarity for exploring multilingual textual databases</title>
        <abstract>This paper describes the use of second order similarities for identifying similar texts inside a corpus of aviation incident reports written in both French and English. We use a second bilingual corpus to construct pairs of reference documents and map each target document to a vector so each coordinate represents a similarity score between this document and the part of the reference corpus written in the same language. We evaluate the system using a large corpus of translated incident reports. The results are promising and validate the approach.</abstract>
        <keywords>second order similarity, multilingual, ESA</keywords>
      </article>
      <article id="taln-2013-court-020" session="Poster">
        <auteurs>
          <auteur>
            <nom>François-Régis Chaumartin</nom>
            <email>frc@proxem.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Proxem, 19 boulevard de Magenta, 75010 Paris</affiliation>
        </affiliations>
        <titre>Apprentissage d’une classification thématique générique et cross-langue à partir des catégories de la Wikipédia</titre>
        <type>court</type>
        <pages>659-666</pages>
        <resume>La catégorisation de textes nécessite généralement un investissement important en amont, avec une adaptation de domaine. L’approche que nous proposons ici permet d’associer finement à un texte tout-venant écrit dans une langue donnée, un graphe de catégories de la Wikipédia dans cette langue. L’utilisation de l’index inter-langues de l’encyclopédie en ligne permet de plus d’obtenir un sous-ensemble de ce graphe dans la plupart des autres langues.</resume>
        <mots_cles>catégorisation, apprentissage, recherche d’information, Wikipédia, graphes</mots_cles>
        <title>Cross-lingual and generic text categorization</title>
        <abstract>Text categorization usually requires a significant investment, which must often be associated to a field adaptation. The approach we propose here allows to finely associate a graph of Wikipedia categories to any text written in a given language. Moreover, the inter-lingual index of the online encyclopedia allows to get a subset of this graph in most other languages.</abstract>
        <keywords>categorization, machine learning, information retrieval, Wikipedia, graphs</keywords>
      </article>
      <article id="taln-2013-court-021" session="Poster">
        <auteurs>
          <auteur>
            <nom>Nadia Okinina</nom>
            <email>Nadia.Okinina@univ-tours.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Damien Nouvel</nom>
            <email>Damien.Nouvel@univ-tours.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Nathalie Friburger</nom>
            <email>Nathalie.Friburger@univ-tours.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Jean-Yves Antoine</nom>
            <email>Jean-Yves.Antoine@univ-tours.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université François Rabelais Tours, LI, 3 place Jean Jaurès, 41 000 Blois</affiliation>
          <affiliation affiliationId="2">ALPAGE, INRIA Roquencourt</affiliation>
        </affiliations>
        <titre>Apprentissage supervisé sur ressources encyclopédiques pour l’enrichissement d’un lexique de noms propres destiné à la reconnaissance des entités nommées</titre>
        <type>court</type>
        <pages>667-674</pages>
        <resume>Cet article présente une méthode hybride d’enrichissement d’un lexique de noms propres à partir de la base encyclopédique en ligne Wikipedia. Une des particularités de cette recherche est de viser l’enrichissement d’une ressource existante (Prolexbase) très contrôlée décrivant finement les noms propres. A la différence d’autres travaux destinés à la reconnaissance des entités nommées, notre objectif est donc de réaliser un enrichissement automatique de qualité. Notre approche repose sur l’utilisation en pipe-line de règles déterministes basées sur certaines informations DBpedia et d’une catégorisation supervisée à base de classifieur SVM. Nos résultats montrent qu’il est ainsi possible d’enrichir un lexique de noms propres avec une très bonne précision.</resume>
        <mots_cles>reconnaissance des entités nommées, lexique de nom propre, enrichissement automatique de lexique, Wikipedia, règles, classification supervisée, machine à vecteurs de support, SVM</mots_cles>
        <title>Supervised learning on encyclopaedic resources for the extension of a lexicon of proper names dedicated to the recognition of named entities</title>
        <abstract>This paper concerns the automatic extension of a lexicon of proper names by means of a hybrid mining of Wikipedia. The specificity of this research is to focus on the quality of the added lexical entries, since the mining process is supposed to extend a controlled existing resource (Prolexbase). Our approach consists in the successive application of deterministic rules based on some specific information of the DBpedia and of a supervised classification with a SVM classifier. Our experiments show that it is possible to extend automatically such a lexicon without adding a perceptible noise to the resource.</abstract>
        <keywords>named entities recognition, proper names lexicon, automatic extension of lexicon, Wikipedia, rules, supervised classification, support vector machines, SVM</keywords>
      </article>
      <article id="taln-2013-court-022" session="Poster">
        <auteurs>
          <auteur>
            <nom>Patrick Paroubek</nom>
            <email>pap@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Munshi Asadullah</nom>
            <email>munshi@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Anne Vilnat</nom>
            <email>anne@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIMSI-CNRS Bât. 508 Université Paris-Sud, 91403 Orsay Cedex France</affiliation>
        </affiliations>
        <titre>Convertir des analyses syntaxiques en dépendances vers les relations fonctionnelles PASSAGE</titre>
        <type>court</type>
        <pages>675-682</pages>
        <resume>Nous présentons ici les premiers travaux concernant l’établissement d’une passerelle bidirectionnelle entre d’une, part les schémas d’annotation syntaxique en dépendances qui ont été définis pour convertir les annotations du French Treebank en arbres de dépendances de surface pour l’analyseur syntaxique Bonsai, et d’autre part le formalisme d’annotation PASSAGE développé initialement pour servir de support à des campagnes d’évaluation ouvertes en mode objectif quantitatif boîte-noire pour l’analyse syntaxique du français.</resume>
        <mots_cles>Analyse Syntaxique, Corpus arboré, Dependances, DepFTB, ConLL, PASSAGE</mots_cles>
        <title>Converting dependencies for syntactic analysis of French into PASSAGE functional relations</title>
        <abstract>We present here a first attempt at building a bidrictionnal converter between, on the one hand the dependency based syntaxtic formalism which has been defined to map the French Treebank annotation onto surface dependency trees used by the Bonsai parser, on the other hand the PASSAGE formalism developped intially for French parsing quantitative black-box objective open evaluation campaigns.</abstract>
        <keywords>Parsing, Treebank, Dependencies, DepFTB, ConLL, PASSAGE</keywords>
      </article>
      <article id="taln-2013-court-023" session="Poster">
        <auteurs>
          <auteur>
            <nom>Sharid Loáiciga</nom>
            <email>sharid.loaiciga@unige.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire d’Analyse et de Technologie du Langage, CUI - Université de Genève, Battelle - bâtiment A, 7 route de Drize, CH-1227 Carouge</affiliation>
        </affiliations>
        <titre>Résolution d’anaphores et traitement des pronoms en traduction automatique à base de règles</titre>
        <type>court</type>
        <pages>683-690</pages>
        <resume>La traduction des pronoms est l’un des problèmes actuels majeurs en traduction automatique. Étant donné que les pronoms ne transmettent pas assez de contenu sémantique en euxmêmes, leur traitement automatique implique la résolution des anaphores. La recherche en résolution des anaphores s’intéresse à établir le lien entre les entités sans contenu lexical (potentiellement des syntagmes nominaux et pronoms) et leurs référents dans le texte. Dans cet article, nous mettons en oeuvre un premier prototype d’une méthode inspirée de la théorie du liage chomskyenne pour l’interprétation des pronoms dans le but d’améliorer la traduction des pronoms personnels entre l’espagnol et le français.</resume>
        <mots_cles>Résolution d’anaphores, traduction automatique à base de règles, sujets nuls</mots_cles>
        <title>Anaphora Resolution for Machine Translation</title>
        <abstract>Pronoun translation is one of the current problems within Machine Translation. Since pronouns do not convey enough semantic content by themselves, pronoun processing requires anaphora resolution. Research in anaphora resolution is interested in establishing the link between entities (NPs and pronouns) and their antecedents in the text. In this article, we implement a prototype of a linguistic anaphora resolution method inspired from the Chomskyan Binding Theory in order to improve the translation of personal pronouns between Spanish and French.</abstract>
        <keywords>Anaphora Resolution, Rule-based Machine Translation, nul subjects</keywords>
      </article>
      <article id="taln-2013-court-024" session="Poster">
        <auteurs>
          <auteur>
            <nom>Frederik Cailliau</nom>
            <email>cailliau@sinequa.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Ariane Cavet</nom>
            <email>cavet@sinequa.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Clément de Groc</nom>
            <email>cdegroc@syllabs.com</email>
            <affiliationId>2</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Claude de Loupy</nom>
            <email>loupy@syllabs.com</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Sinequa, 12 rue d’Athènes, 75009 Paris</affiliation>
          <affiliation affiliationId="2">Syllabs, 53 bis rue Sedaine, 75011 Paris</affiliation>
          <affiliation affiliationId="3">LIMSI-CNRS, BP 133, 91403 Orsay CEDEX</affiliation>
        </affiliations>
        <titre>Lexiques de corpus comparables et recherche d’information multilingue</titre>
        <type>court</type>
        <pages>691-698</pages>
        <resume>Nous évaluons l’utilité de trois lexiques bilingues dans un cadre de recherche interlingue français vers anglais sur le corpus CLEF. Le premier correspond à un dictionnaire qui couvre le corpus, alors que les deux autres ont été construits automatiquement à partir des sous-ensembles français et anglais de CLEF, en les considérant comme des corpus comparables. L’un contient des mots simples, alors que le deuxième ne contient que des termes complexes. Les lexiques sont intégrés dans des interfaces différentes dont les performances de recherche interlingue sont évaluées par 5 utilisateurs sur 15 thèmes de recherche CLEF. Les meilleurs résultats sont obtenus en intégrant le lexique de mots simples généré à partir des corpus comparables dans une interface proposant les cinq « meilleures » traductions pour chaque mot de la requête.</resume>
        <mots_cles>recherche d’information multilingue, corpus comparables, lexiques multilingues</mots_cles>
        <title>Lexicons from Comparable Corpora for Multilingual Information Retrieval</title>
        <abstract>We evaluate the utility of three bilingual lexicons for English-to-French crosslingual search on the CLEF corpus. The first one is a kind of dictionary whose content covers the corpus. The other two have been automatically built on the French and English subparts of the CLEF corpus, by considering them as comparable corpora. One is made of simple words, the other one of complex words. The lexicons are integrated in different interfaces whose crosslingual search performances are evaluated by 5 users on 15 topics of CLEF. The best results are given with the interface having the simple-words lexicon generated on comparable corpora and proposing 5 translations for each query term.</abstract>
        <keywords>multilingual information retrieval, comparable corpora, multilingual lexicons</keywords>
      </article>
      <article id="taln-2013-court-025" session="Poster">
        <auteurs>
          <auteur>
            <nom>Philippe Suignard</nom>
            <email>philippe.suignard@edf.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Sofiane Kerroua</nom>
            <email>skerroua@aid.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Electricité de France R&amp;D, 1 avenue du Général de Gaulle, 92141 Clamart</affiliation>
          <affiliation affiliationId="2">A.I.D., 4 rue Henri Le Sidaner, 78000 Versailles</affiliation>
        </affiliations>
        <titre>Utilisation de contextes pour la correction automatique ou semi-automatique de réclamations clients</titre>
        <type>court</type>
        <pages>699-706</pages>
        <resume>Cet article présente deux méthodes permettant de corriger des réclamations contenant des erreurs rédactionnelles, en s’appuyant sur le graphe des voisins orthographiques et contextuels. Ce graphe est constitué des formes ou mots trouvés dans un corpus d’apprentissage. Un lien entre deux formes traduit le fait que les deux formes se « ressemblent » et partagent des contextes similaires. La première méthode est semi-automatique et consiste à produire un dictionnaire de substitution à partir de ce graphe. La seconde méthode, plus ambitieuse, est entièrement automatisée. Elle s’appuie sur les contextes pour déterminer à quel mot correspond telle forme abrégée ou erronée. Les résultats ainsi obtenus permettent d’améliorer le processus déjà existant de constitution d’un dictionnaire de substitution mis en place au sein d’EDF.</resume>
        <mots_cles>Correction automatique, analyse distributionnelle, graphe, contexte</mots_cles>
        <title>Using contexts for automatic or semi-automatic correction of customer complaints</title>
        <abstract>This article presents two methods allowing correcting complaints containing spelling errors, by using the spelling and contextual neighbors' graph. This graph is made of forms or words found in a learning corpus. A link between two forms conveys the fact that the two forms ''look alike'' and share similar contexts. The first method is semi-automatic and consists in producing a substitutional dictionary from this graph. The second method, more ambitious, is fully automatic. It is based on contexts to determine to which word corresponds such abbreviated or erroneous form. The results thus obtained allow us to improve the existing process regarding the creation of a substitutional dictionary at EDF.</abstract>
        <keywords>Spelling correction, distributional analysis, graph, context</keywords>
      </article>
      <article id="taln-2013-court-026" session="Poster">
        <auteurs>
          <auteur>
            <nom>Luis Adrián Cabrera-Diego</nom>
            <email>adrian.cabrera@flejay.com</email>
            <affiliationId>1</affiliationId>
            <affiliationId>4</affiliationId>
          </auteur>
          <auteur>
            <nom>Juan-Manuel Torres-Moreno</nom>
            <email>juan-manuel.torres@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Marc El-Bèze</nom>
            <email>marc.elbeze@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIA, Université d’Avignon et des Pays de Vaucluse, France</affiliation>
          <affiliation affiliationId="2">École Polytechnique de Montréal, Canada</affiliation>
          <affiliation affiliationId="3">SFR Agorantic UAPV, France</affiliation>
          <affiliation affiliationId="4">Flejay Group, France</affiliation>
        </affiliations>
        <titre>SegCV : traitement efficace de CV avec analyse et correction d’erreurs</titre>
        <type>court</type>
        <pages>707-714</pages>
        <resume>Le marché d’offres d’emploi et des candidatures sur Internet a connu, ces derniers temps, une croissance exponentielle. Ceci implique des volumes d’information (majoritairement sous la forme de textes libres) intraitables manuellement. Les CV sont dans des formats très divers : .pdf, .doc, .dvi, .ps, etc., ce qui peut provoquer des erreurs lors de la conversion en texte plein. Nous proposons SegCV, un système qui a pour but l’analyse automatique des CV des candidats. Dans cet article, nous présentons des algorithmes reposant sur une analyse de surface, afin de segmenter les CV de manière précise. Nous avons évalué la segmentation automatique selon des corpus de référence que nous avons constitués. Les expériences préliminaires réalisées sur une grande collection de CV en français avec correction du bruit montrent de bons résultats en précision, rappel et F-Score.</resume>
        <mots_cles>RI, Ressources humaines, traitement de CV, Modèle à base de règles</mots_cles>
        <title>SegCV : Eficient parsing of résumés with analysis and correction of errors</title>
        <abstract>Over the last years, the online market of jobs and candidatures offers has reached an exponential growth. This has implied great amounts of information (mainly in a text free style) which cannot be processed manually. The résumés are in several formats : .pdf, .doc, .dvi, .ps, etc., that can provoque errors or noise during the conversion to plain text. We propose SegCV, a system that has as goal the automatic parsing of candidates’ résumés. In this article we present the algoritms, which are based over a surface analysis, to segment the résumés in an accurate way. We evaluated the automatic segmentation using a reference corpus that we have created. The preliminary experiments, done over a large collection of résumés in French with noise correction, show good results in precision, recall and F-score.</abstract>
        <keywords>Information Retrieval, Human Resources, CV Parsing, Rules Model</keywords>
      </article>
      <article id="taln-2013-court-027" session="Poster">
        <auteurs>
          <auteur>
            <nom>Jean-Valère Cossu</nom>
            <email>jean-valere.cossu@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Juan-Manuel Torres-Moreno</nom>
            <email>juan-manuel.torres@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
            <affiliationId>3</affiliationId>
            <affiliationId>4</affiliationId>
          </auteur>
          <auteur>
            <nom>Marc El-Bèze</nom>
            <email>marc.elbeze@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
            <affiliationId>4</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire Informatique d'Avignon - Université d'Avignon et des Pays de Vaucluse 339 chemin des Meinajaries, BP91228 84911 Avignon Cedex 9, France</affiliation>
          <affiliation affiliationId="2">SFR Agorantic Université d’Avignon et des Pays de Vaucluse, 84000 Avignon Cedex</affiliation>
          <affiliation affiliationId="3">École Polytechnique de Montréal, 2900 Bd Edouard-Montpetit Montréal, QC H3T1J4</affiliation>
          <affiliation affiliationId="4">Brain &amp; Language Research Institute, 5 avenue Pasteur, 13604 Aix-en-Provence Cedex 1</affiliation>
        </affiliations>
        <titre>Recherche et utilisation d'entités nommées conceptuelles dans une tâche de catégorisation</titre>
        <type>court</type>
        <pages>715-722</pages>
        <resume>Les recherches présentées sont directement liées aux travaux menés pour résoudre les problèmes de catégorisation automatique de texte. Les mots porteurs d’opinions jouent un rôle important pour déterminer l’orientation du message. Mais il est essentiel de pouvoir identifier les cibles auxquelles ils se rapportent pour en contextualiser la portée. L’analyse peut également être menée dans l’autre sens, on cherchant dans le contexte d’une cible détectée les termes polarisés. Une première étape d’apprentissage depuis des données permet d'obtenir automatiquement les marqueurs de polarité les plus importants. A partir de cette base, nous cherchons les cibles qui apparaissent le plus fréquemment à proximité de ces marqueurs d'opinions. Ensuite, nous construisons un ensemble de couples (marqueur de polarité, cible) pour montrer qu’en s’appuyant sur ces couples, on arrive à expliquer plus finement les prises de positions tout en maintenant (voire améliorant) le niveau de performance du classifieur.</resume>
        <mots_cles>Fouille d’opinion, Marqueurs de polarité, Reconnaissance d’entités nommées</mots_cles>
        <title>Search and usage of named conceptual entities in a categorisazion task</title>
        <abstract>The researchs presented are part of a text automatic categorization task. Words bearing opinions play an important role in determining the overall direction of the message. But it is essential to identify the elements (targets) which they are intended to relativize the scope. The analysis can also be conducted in the reverse direction. When a target is detected we need to search polarized terms in the context. A first step in an automatic learning from data will allow us to obtain the most important polarity markers. From this basis, we look for targets that appear most frequently in the vicinity of these opinions markers. Then, we construct a set of pairs (polarity marker, target) to show that relying on these couples we can maintain (or improve) the performance of the classifier.</abstract>
        <keywords>Opinion Mining, Named Entity Recognition</keywords>
      </article>
      <article id="taln-2013-court-028" session="Poster">
        <auteurs>
          <auteur>
            <nom>Guillaume Wisniewski</nom>
            <email>wisniews@limsi.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Anil Kumar Singh</nom>
            <email>anil@limsi.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Natalia Segal</nom>
            <email>nsegal@softissimo.com</email>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>François Yvon</nom>
            <email>yvon@limsi.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université Paris Sud 91 403 ORSAY CEDEX</affiliation>
          <affiliation affiliationId="2">LIMSI–CNRS 91 403 ORSAY CEDEX</affiliation>
          <affiliation affiliationId="3">Reverso–Softissimo, 5 rue Soyer, 92 500 NEUILLY</affiliation>
        </affiliations>
        <titre>Un corpus d’erreurs de traduction</titre>
        <type>court</type>
        <pages>723-730</pages>
        <resume>Avec le développement de la post-édition, de plus en plus de corpus contenant des corrections de traductions sont disponibles. Ce travail présente un corpus de corrections d’erreurs de traduction collecté dans le cadre du projet ANR/TRACE et illustre les différents types d’analyses auxquels il peut servir. Nous nous intéresserons notamment à la détection des erreurs fréquentes et à l’analyse de la variabilité des post-éditions.</resume>
        <mots_cles>Traduction automatique, Analyse d’erreur, Post-édiition</mots_cles>
        <title>A corpus of post-edited translations</title>
        <abstract>More and more datasets of post-edited translations are being collected. These corpora have many applications, such as failure analysis of SMT systems and the development of quality estimation systems for SMT. This work presents a large corpus of post-edited translations that has been gathered during the ANR/TRACE project. Applications to the detection of frequent errors and to the analysis of the inter-rater agreement of hTER are also reported.</abstract>
        <keywords>Machine Translation, Failure Analysis, Post-edition</keywords>
      </article>
      <article id="taln-2013-court-029" session="Poster">
        <auteurs>
          <auteur>
            <nom>Samira Walha Ellouze</nom>
            <email>ellouze.samira@gmail.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Maher Jaoua</nom>
            <email>Maher.Jaoua@fsegs.rnu.tn</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Lamia Hadrich Belguith</nom>
            <email>l.belguith@fsegs.rnu.tn</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">ANLP Research Group, Laboratoire MIRACL, Route de l’aéroport Km 4, 3018, Sfax, Tunisie</affiliation>
        </affiliations>
        <titre>Une méthode d’évaluation des résumés basée sur la combinaison de métriques automatiques et de complexité textuelle</titre>
        <type>court</type>
        <pages>731-738</pages>
        <resume>Cet article présente une méthode automatique d’évaluation du contenu des résumés automatiques. La méthode proposée est basée sur une combinaison de caractéristiques englobant des scores de contenu et d’autres de complexité textuelle et ce en s’appuyant sur une technique d’apprentissage, à savoir la régression linéaire. L’objectif de cette combinaison consiste à prédire le score manuel PYRAMID à partir des caractéristiques utilisées. Afin d’évaluer la méthode présentée, nous nous sommes intéressés à deux niveaux de granularité d’évaluation : la première est qualifiée de Micro-évaluation et propose l’évaluation de chaque résumé, alors que la deuxième est une Macro-évaluation et s’applique au niveau de chaque système.</resume>
        <mots_cles>Evaluation intrinsèque, évaluation du contenu, résumé automatique, complexité textuelle, régression linéaire</mots_cles>
        <title>An evaluation summary method based on combination of automatic and textual complexity metrics</title>
        <abstract>This article presents an automatic method for evaluating content summaries. The proposed method is based on a combination of features encompassing scores of content and others of textual complexity. This method relies on a learning technique namely the linear regression. The objective of this combination is to predict the PYRAMID score from used features. In order to evaluate the presented method, we are interested in two levels of granularity evaluation: the first is named Micro-evaluation and proposes an evaluation of each summary, while the second is called Macro-evaluation and it applies at the level of each system.</abstract>
        <keywords>Intrinsic evaluation, content evaluation, automatic summary, textual complexity, linear regression</keywords>
      </article>
      <article id="taln-2013-court-030" session="Poster">
        <auteurs>
          <auteur>
            <nom>Abdessalam Bouchekif</nom>
            <email>abdessalam.bouchekif@orange.com</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Géraldine Damnati</nom>
            <email>geraldine.damnati@orange.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Delphine Charlet</nom>
            <email>delphine.charlet@orange.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Orange Labs , 2, Avenue Pierre Marzin 22307 Lannion Cedex</affiliation>
          <affiliation affiliationId="2">Laboratoire d’Informatique de l’Universite du Maine, LIUM - France</affiliation>
        </affiliations>
        <titre>Segmentation thématique : processus itératif de pondération intra-contenu</titre>
        <type>court</type>
        <pages>739-746</pages>
        <resume>Dans cet article, nous nous intéressons à la segmentation thématique d’émissions télévisées exploitant la cohésion lexicale. Le but est d’étudier une approche générique, reposant uniquement sur la transcription automatique sans aucune information externe ni aucune information structurelle sur le contenu traité. L’étude porte plus particulièrement sur le mécanisme de pondération des mots utilisés lors du calcul de la cohésion lexicale. Les poids TF-IDF sont estimés à partir du contenu lui-même, qui est considéré comme une collection de documents mono-thème. Nous proposons une approche itérative, intégrée à un algorithme de segmentation, visant à raffiner la partition du contenu en documents pour l’estimation de la pondération. La segmentation obtenue à une itération donnée fournit un ensemble de documents à partir desquels les poids TF-IDF sont ré-estimés pour la prochaine itération. Des expériences menées sur un corpus couvrant différents formats des journaux télévisés issus de 8 chaînes françaises montrent une amélioration du processus global de segmentation.</resume>
        <mots_cles>Segmentation thématique, pondération TF-IDF, cohésion lexicale, TextTiling</mots_cles>
        <title>An iterative topic segmentation algorithm with intra-content term weighting</title>
        <abstract>This paper deals with topic segmentation of TV Broadcasts using lexical cohesion. The aim is to propose a generic approach, only relying on the automatic speech transcription with no external nor a priori information on the TV content. The study focuses on a new weighting scheme for lexical cohesion computation. TF-IDF weights are estimated from the content itself which is considered as a collection of mono-thematic documents. We propose an iterative process, integrated to a segmentation algorithm, aiming to refine the partition of a content into documents in order to estimate the weights. Topic segmentation obtained at a given iteration provides a set of documents from which TF-IDF weights are re-estimated for the next iteration. An experiment on a rich corpus covering various formats of Broadcast News shows from 8 French TV channels improves the overall topic segmentation process.</abstract>
        <keywords>Topic segmentation, TF-IDF weighting, lexical cohesion, TextTiling</keywords>
      </article>
      <article id="taln-2013-court-031" session="Poster">
        <auteurs>
          <auteur>
            <nom>Alexander Panchenko</nom>
            <email>Alexander.Panchenko@uclouvain.be</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Hubert Naets</nom>
            <email>Hubert.Naets@uclouvain.be</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Laetitia Brouwers</nom>
            <email>Laetitia.Brouwers@uclouvain.be</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Pavel Romanov</nom>
            <email>aromanov@it-claim.ru</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Cédrick Fairon</nom>
            <email>Cedrick.Fairon@uclouvain.be</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CENTAL, Université catholique de Louvain, Belgique</affiliation>
          <affiliation affiliationId="2">Bauman Moscow State Technical University, Russie</affiliation>
        </affiliations>
        <titre>Recherche et visualisation de mots sémantiquement liés</titre>
        <type>court</type>
        <pages>747-754</pages>
        <resume>Nous présentons PatternSim, une nouvelle mesure de similarité sémantique qui repose d’une part sur des patrons lexico-syntaxiques appliqués à de très vastes corpus et d’autre part sur une formule de réordonnancement des candidats extraits. Le système, initialement développé pour l’anglais, a été adapté au français. Nous rendons compte de cette adaptation, nous en proposons une évaluation et décrivons l’usage de ce nouveau modèle dans la plateforme de consultation en ligne Serelex.</resume>
        <mots_cles>Mesure de similarité sémantique, relations sémantiques</mots_cles>
        <title>Search and Visualization of Semantically Related Words</title>
        <abstract>We present PatternSim, a new semantic similarity measure that relies on morpho-syntactic patterns applied to very large corpora and on a re-ranking formula that reorder extracted candidates. The system, originally developed for English, was adapted to French. We explain this adaptation, propose a first evaluation of it and we describe how this new model was used to build the Serelex online search platform.</abstract>
        <keywords>Semantic similarity measure, semantic relations</keywords>
      </article>
      <article id="taln-2013-court-032" session="Poster">
        <auteurs>
          <auteur>
            <nom>Jean-Philippe Guilbaud</nom>
            <email>Jean-Philippe.Guilbaud@imag.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Christian Boitet</nom>
            <email>Christian.Boitet@imag.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Vincent Berment</nom>
            <email>Vincent.Berment@imag.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CNRS, LIG-campus, 38041 Grenoble Cedex 09</affiliation>
          <affiliation affiliationId="2">UJF, Université de Grenoble, LIG-campus, 38041 Grenoble Cedex 09</affiliation>
        </affiliations>
        <titre>Un analyseur morphologique étendu de l'allemand traitant les formes verbales à particule séparée</titre>
        <type>court</type>
        <pages>755-763</pages>
        <resume>Nous décrivons l’organisation et l'état courant de l’analyseur morphologique de l’allemand AMALD de grande taille couvrant (près de 103000 lemmes et 500000 formes fléchies simples, en croissance) développé dans le cadre du projet ANR-Émergence Traouiero. C’est le premier lemmatiseur de l’allemand capable de traiter non seulement les mots simples et les mots composés, mais aussi les verbes à particules séparables quand elles sont séparées, même par un grand nombre de mots (ex : Hier schlagen wir eine neue Methode für die morphologische Analyse vor).</resume>
        <mots_cles>analyse morphologique, lemmatisation, allemand, verbes à particule séparable</mots_cles>
        <title>An extended morphological analyzer of German handling verbal forms with separated separable particles</title>
        <abstract>We describe the organisation and the current state of the large-scale (nearly 103000 lemmas and 500000 simple inflected forms, growing) morphological analyzer AMALD developed in the framework of the ANR-Émergence Traouiero project. It is the first lemmatizer of German able to handle not only simple and compound words, but also verbs with separable particles when they are separated, even by many words (e.g. Hier schlagen wir eine neue Methode für die morphologische Analyse vor.).</abstract>
        <keywords>morphological analysis, lemmatization, German, verbs with separable particles</keywords>
      </article>
      <article id="taln-2013-court-033" session="Poster">
        <auteurs>
          <auteur>
            <nom>Marc Vincent</nom>
            <email>marc.r.vincent@gmail.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Grégoire Winterstein</nom>
            <email>gregoire.winterstein@linguist.jussieu.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">UMR S-775, Université Paris Descartes</affiliation>
          <affiliation affiliationId="2">LLF, UMR 7110, Université Sorbonne Nouvelle</affiliation>
        </affiliations>
        <titre>Construction et exploitation d’un corpus français pour l’analyse de sentiment</titre>
        <type>court</type>
        <pages>764-771</pages>
        <resume>Ce travail présente un corpus en français dédié à l’analyse de sentiment. Nous y décrivons la construction et l’organisation du corpus. Nous présentons ensuite les résultats de l’application de techniques d’apprentissage automatique pour la tâche de classification d’opinion (positive ou négative) véhiculée par un texte. Deux techniques sont utilisées : la régression logistique et la classification basée sur des Support Vector Machines (SVM). Nous mentionnons également l’intérêt d’appliquer une sélection de variables avant la classification (par régularisation par elastic net).</resume>
        <mots_cles>Analyse de sentiments, Corpus, Classification, Apprentissage automatique, Sélection de variable</mots_cles>
        <title>Building and exploiting a French corpus for sentiment analysis</title>
        <abstract>This work introduces a French corpus for sentiment analysis. We describe the construction and organization of the corpus. We then apply machine learning techniques to automatically predict whether a text is positive or negative (the opinion classification task). Two techniques are used : logistic regression and classification based on Support Vector Machines (SVM). Finally, we briefly evaluate the merits of applying feature selection algorithms to our models (via elastic net regularization).</abstract>
        <keywords>Sentiment Analysis, Corpus, Opinion Mining, Classification, Machine Learning, Variable Selection</keywords>
      </article>
      <article id="taln-2013-court-034" session="Poster">
        <auteurs>
          <auteur>
            <nom>Luka Nerima</nom>
            <email>luka.nerima@unige.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Eric Wehrli</nom>
            <email>eric.wehrli@unige.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LATL, Université de Genève, 2 rue de Candolle, 1211 Genève 4</affiliation>
        </affiliations>
        <titre>Résolution d'anaphores appliquée aux collocations: une évaluation préliminaire</titre>
        <type>court</type>
        <pages>772-778</pages>
        <resume>Le traitement des collocations en analyse et en traduction est depuis de nombreuses années au centre de nos intérêts de recherche. L’analyseur Fips a été récemment enrichi d’un module de résolution d’anaphores. Dans cet article nous décrivons comment la résolution d’anaphores a été appliquée à l’identification des collocations et comment cela permet à l’analyseur de repérer une collocation même si un de ses termes a été pronominalisé. Nous décrivons aussi la méthodologie de l’évaluation, notamment la préparation des données pour le calcul du rappel. Dans la tâche d’identification des collocations pronominalisées, Fips montre des résultats très encourageants : la précision mesurée est de 98% alors que le rappel est proche de 50%. Dans cette évaluation nous nous intéressons aux collocations de type verbe-objet direct en conjonction avec les pronoms anaphoriques à la 3e personne. Le corpus utilisé est un corpus anglais d’environ dix millions de mots.</resume>
        <mots_cles>Analyse, résolution d’anaphores, pronoms personnels, collocations, corpus</mots_cles>
        <title>Anaphora Resolution Applied to Collocation Identification: A Preliminary Evaluation</title>
        <abstract>Collocation identification and collocation translation have been at the center of our research interests for several years. Recently, the Fips parser has been enriched by an anaphora resolution mechanism. This article discusses how anaphora resolution has been applied to the collocation identification task, and how it enables the parser to identify a collocation when one of its terms is pronominalized. We also describe the evaluation methodology, in particular the preparation of data for the calculation of the recall. In the task of pronominalized collocation identification, Fips shows encouraging results: the measured precision is 98% while recall approaches 50%. In this paper we focus on collocations of the type verb-direct object and on a widespread type of anaphora: the third personal pronouns. The corpus used is a corpus of approximately ten million English words.</abstract>
        <keywords>Parsing, anaphora resolution, personal pronoun, collocations, corpus</keywords>
      </article>
      <article id="taln-2013-court-035" session="Poster">
        <auteurs>
          <auteur>
            <nom>Thibault Mondary</nom>
            <email>Thibault.Mondary@lipn.univ-paris13.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Adeline Nazarenko</nom>
            <email>Adeline.Nazarenko@lipn.univ-paris13.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Haïfa Zargayouna</nom>
            <email>Haifa.Zargayouna@lipn.univ-paris13.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Sabine Barreaux</nom>
            <email>Sabine.Barreaux@inist.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université Paris 13, Sorbonne Paris Cité, LIPN (UMR 7030), F-93430 Villetaneuse, France</affiliation>
          <affiliation affiliationId="2">INIST-CNRS, Vandoeuvre-lès-Nancy, France</affiliation>
        </affiliations>
        <titre>Aide à l’enrichissement d’un référentiel terminologique : propositions et expérimentations</titre>
        <type>court</type>
        <pages>779-786</pages>
        <resume>En s’appuyant sur une expérience d’enrichissement terminologique, cet article montre comment assister le travail d’acquisition terminologique et surmonter concrètement les deux difficultés qu’il présente : la masse de candidats-termes à considérer et la subjectivité des jugements terminologiques qui varient notamment en fonction du type de terminologie à produire. Nous proposons des stratégies simples pour filtrer a priori une partie du bruit des résultats des extracteurs et rendre ainsi la validation praticable pour des terminologues et nous démontrons leur efficacité sur un échantillon de candidats-termes proposés à la validation de deux spécialistes du domaine. Nous montrons également qu’en appliquant à une campagne de validation terminologique les mêmes principes méthodologiques que pour une campagne d’annotation, on peut contrôler la qualité des jugements de validation posés et de la terminologie qui en résulte.</resume>
        <mots_cles>Acquisition terminologique, validation de candidats-termes, filtrage de termes, distance terminologique, vote, accord inter-juges</mots_cles>
        <title>Help enrich a terminological repository : proposals and experiments</title>
        <abstract>Based on an experience of terminological enrichment, this paper shows how to support the work of terminological acquisition and overcome practical difficulties it presents, i.e. the mass of candidate terms to consider and the subjectivity of terminological judgments which depends on the type of terminology to produce. We propose simple strategies to filter a priori part of the noise from the results of term extractors so as to make the validation practicable for terminologists. We demonstrate their effectiveness on a sample of candidate terms proposed for the validation of two experts. We also show that by applying to term validation campaigns the methodological principles that have been proposed for corpus annotation campaigns, we can control the quality of validation judgments and of the resulting terminologies.</abstract>
        <keywords>Terminology acquisition, term candidate validation, term filtering, terminological distance, vote, inter-judge agreement</keywords>
      </article>
      <article id="taln-2013-demo-001" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Gaël Lejeune</nom>
            <email>Gael.Lejeune@unicaen.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Romain Brixtel</nom>
            <email>Romain.Brixtel@unicaen.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Charlotte Lecluze</nom>
            <email>Charlotte.Lecluze@unicaen.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Antoine Doucet</nom>
            <email>Antoine.Doucet@unicaen.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Nadine Lucas</nom>
            <email>Nadine.Lucas@unicaen.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Normandie Université; UNICAEN, GREYC, CNRS UMR 6072, F-14032 Caen</affiliation>
        </affiliations>
        <titre>DAnIEL : Veille épidémiologique multilingue parcimonieuse</titre>
        <type>démonstration</type>
        <pages>787-788</pages>
        <resume>DAnIEL est un système multilingue de veille épidémiologique. DAnIEL permet de traiter un grand nombre de langues à faible coût grâce à une approche parcimonieuse en ressources.</resume>
        <mots_cles>extraction d’information, recherche d’information, veille, multilinguisme, genre journalistique, grain caractère</mots_cles>
        <title>DAnIEL, parsimonious yet high-coverage multilingual epidemic surveillance</title>
        <abstract>DAnIEL is a multilingual epidemic surveillance system. DAnIEL relies on a parsimonious scheme making it possible to process new languages at small cost.</abstract>
        <keywords>information extraction, information retrieval, news surveillance, multilingualism, news genre, character-level analysis</keywords>
      </article>
      <article id="taln-2013-demo-002" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Elena Kozlova</nom>
            <email>Helen_Koz@abbyy.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Maria Gontcharova</nom>
            <email>maria_go@abbyy.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Tatiana Popova</nom>
            <email>Tatiana_P@abbyy.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">ABBYY, 2B rue Otradnaya, Moscou, Russie</affiliation>
        </affiliations>
        <titre>Lexique multilingue dans le cadre du modèle Compreno développé ABBYY</titre>
        <type>démonstration</type>
        <pages>789-790</pages>
        <resume>Le lexique multilingue basé sur une hiérarchie sémantique universelle fait partie du modèle linguistique Compreno destiné à plusieurs applications du TALN, y compris la traduction automatique et l’analyse sémantique et syntaxique. La ressource est propriétaire et n’est pas librement disponible.</resume>
        <mots_cles>Lexique multilingue, hiérarchie sémantique universelle, traduction automatique</mots_cles>
        <title>Multilingual lexical database in the framework of COMPRENO linguistic model developed by ABBYY</title>
        <abstract>The multilingual lexical database based on the universal semantic hierarchy is part of Compreno linguistic model. This model is meant for various NLP applications dealing with machine translation, semantic and syntactic analysis. The resource is private and is not freely available.</abstract>
        <keywords>Multilingual lexical database, universal semantic hierarchy, machine translation</keywords>
      </article>
      <article id="taln-2013-demo-003" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Manon Quintana</nom>
            <email>mquintana@inbenta.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">INBENTA FR 164 route de Revel 31400 Toulouse</affiliation>
        </affiliations>
        <titre>Inbenta Semantic Search Engine : un moteur de recherche sémantique inspiré de la Théorie Sens-Texte</titre>
        <type>démonstration</type>
        <pages>791-792</pages>
        <resume>Avec la digitalisation massive de documents apparaît la nécessité de disposer de systèmes de recherche capables de s’adapter aux habitudes de recherche des utilisateurs et de leur permettre d’accéder à l’information rapidement et efficacement. INBENTA a ainsi créé un moteur de recherche intelligent appellé Inbenta Semantic Search Engine (ISSE). Les deux tâches principales de l’ISSE sont d’analyser les questions des utilisateurs et de trouver la réponse appropriée à la requête en effectuant une recherche dans une base de connaissances. Pour cela, la solution logicielle d’INBENTA se base sur la Théorie Sens-Texte qui se concentre sur le lexique et la sémantique.</resume>
        <mots_cles>Moteur de Recherche Sémantique, Théorie Sens-Texte, fonction lexicale</mots_cles>
        <title>Inbenta Semantic Search Engine: a semantic search engine inspired by the Meaning-Text Theory</title>
        <abstract>The need to have search systems able to adapt themselves to the particular way users pose their questions so that they can get a quick and efficient access to information is increasingly relevant due to the huge digitalization of documents. To cope with this reality, INBENTA has developed an intelligent search engine, called Inbenta Semantic Search Engine (ISSE). ISSE's main two tasks are analysing users' queries and finding the most appropriate answer to those questions in a knowledge-base. To carry out these tasks, INBENTA's software solution relies upon the Meaning-Text Theory, which focusses on the lexicon and semantics.</abstract>
        <keywords>Semantic Search Engine, Meaning-Text Theory, lexical function</keywords>
      </article>
      <article id="taln-2013-demo-004" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Jean-Leon Bouraoui</nom>
            <email>jl.bouraoui@prometil.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Marc Canitrot</nom>
            <email>m.canitrot@prometil.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Prometil, 42 Avenue du Général de Croutte, 31100 Toulouse</affiliation>
        </affiliations>
        <titre>FMO : un outil d’analyse automatique de l’opinion</titre>
        <type>démonstration</type>
        <pages>793-794</pages>
        <resume>Nous décrivons notre prototype d’analyse automatique d’opinion. Celui-ci est basé sur un moteur d’analyse linguistique. Il permet de détecter finement les segments de texte porteurs d’opinions, de les extraire, et de leur attribuer une note selon la polarité qu’ils expriment. Nous présentons enfin les différentes perspectives que nous envisageons pour ce prototype.</resume>
        <mots_cles>Analyse d’opinion, e-reputation, extraction d’information</mots_cles>
        <title>FMO: a tool for automated opinion mining</title>
        <abstract>We describe our prototype of automatic opinion mining. It is based on a linguistic analysis engine. It allows to subtly identifying the text phrases which bear some opinion, to extract them, and to give them a note according to the polarity that they express. Finally, we present the perspectives that we plan to carry out.</abstract>
        <keywords>Opinion mining, e-reputation, information extraction</keywords>
      </article>
      <article id="taln-2013-demo-005" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Patrick Séguéla</nom>
            <email>patrick.seguela@synapse-f.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Dominique Laurent</nom>
            <email>dlaurent@synapse-fr.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Synapse Développement, 33 rue Maynard, 31000 Toulouse</affiliation>
        </affiliations>
        <titre>Corriger, analyser et représenter le texte Synapse Développement</titre>
        <type>démonstration</type>
        <pages>795-796</pages>
        <resume>Synapse Développement souhaite échanger avec les conférenciers autour des technologies qu'elle commercialise : correction de textes et analyse sémantique. Plusieurs produits et démonstrateurs seront présentés, notre but étant d'instaurer un dialogue et de confronter notre approche du TAL, à base de méthodes symboliques et statistiques influencées par des contraintes de production, et celles utilisées par les chercheurs, industriels ou passionnés qui viendront à notre rencontre.</resume>
        <mots_cles>Correction grammaticale, analyse syntaxique, analyse sémantique, analyse d'opinions</mots_cles>
        <title>Checking, analysing and representing texts</title>
        <abstract>Synapse Développement would like to demonstrate its grammar checker and semantic analysis technologies to open exciting discussions with natural language specialists. We are particularly interested in discussing the scientific issues we have to face and solve according to our industrial needs.</abstract>
        <keywords>Grammar checker, POS tagging, semantic analysis, opinion mining</keywords>
      </article>
      <article id="taln-2013-demo-006" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Xavier Tannier</nom>
            <email>Xavier.Tannier@limsi.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Véronique Moriceau</nom>
            <email>Veronique.Moriceau@limsi.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Erwan Le Flem</nom>
            <email>Erwan.LeFlem@limsi.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIMSI-CNRS</affiliation>
          <affiliation affiliationId="2">Université Paris-Sud, 91403 Orsay, France</affiliation>
          <affiliation affiliationId="3">IUT de Vannes</affiliation>
        </affiliations>
        <titre>Une interface pour la validation et l’évaluation de chronologies thématiques</titre>
        <type>démonstration</type>
        <pages>797-798</pages>
        <resume>Cet article décrit une interface graphique de visualisation de chronologies événementielles construites automatiquement à partir de requêtes thématiques en utilisant un corpus de dépêches fourni par l’Agence France Pressse (AFP). Cette interface permet également la validation des chronologies par des journalistes qui peuvent ainsi les éditer et les modifier.</resume>
        <mots_cles>chronologie événementielle, évaluation, validation</mots_cles>
        <title>An Interface for Validating and Evaluating Thematic Timelines</title>
        <abstract>This demo paper presents a graphical interface for the visualization and evaluation of event timelines built automatically from a search query on a newswire article corpus provided by the Agence France Pressse (AFP). This interface also enables journalists to validate chronologies by editing and modifying them.</abstract>
        <keywords>event timeline, evaluation, validation</keywords>
      </article>
      <article id="taln-2013-demo-007" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Denis Maurel</nom>
            <email>denis.maurel@univ-tours.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Nathalie Friburger</nom>
            <email>nathalie.friburger@univ-tours.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université François Rabelais Tours</affiliation>
        </affiliations>
        <titre>CasSys Un système libre de cascades de transducteurs</titre>
        <type>démonstration</type>
        <pages>799-800</pages>
        <resume>CasSys est un système de création et de mise en oeuvre de cascades de transducteurs intégré à la plateforme Unitex. Nous présentons dans cette démonstration la nouvelle version implantée fin 2012. En particulier ont été ajoutées une interface plus conviviale et la possibilité d’itérer un même transducteur jusqu’à ce qu’il n’ait plus d’influence sur le texte. Un premier exemple concernera le traitement de texte avec une gestion complexe de balises XML et un deuxième présentera la cascade CasEN de reconnaissance des entités nommées.</resume>
        <mots_cles>cascade de transducteurs, graphes Unitex, texte avec balises XML, reconnaissance d'entités nommées</mots_cles>
        <title>CasSys, a free transducer cascade system</title>
        <abstract>CasSys is a free toolkit integrated in the Unitex platform to create and use transducer cascades. We are presenting the new version implemented at the end of 2012. The system interface has been improved and the Kleen star operation has been added: this operation allows applying the same transducer until it no longer produces changes in the text. The first example deals with complex XML text parsing and the second with CasEN, a free cascade for French Named Entity Recognition.</abstract>
        <keywords>transducer cascade, Unitex graphs, XML text, French Named Entity Recognition</keywords>
      </article>
      <article id="taln-2013-demo-008" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Lingxiao Wang</nom>
            <email>Lingxiao.Wang@imag.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Ying Zhang</nom>
            <email>Ying.Zhang@imag.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">GETALP - LIG, 41 rue des Mathématiques, BP 53, 38041 Grenoble Cedex 9</affiliation>
        </affiliations>
        <titre>iMAG : post-édition, évaluation de qualité de TA et production d'un corpus parallèle</titre>
        <type>démonstration</type>
        <pages>801-802</pages>
        <resume>Une passerelle interactive d’accès multilingue (iMAG) dédiée à un site Web S (iMAG-­‐S) est un bon outil pour rendre S accessible dans beaucoup de langues, immédiatement et sans responsabilité éditoriale. Les visiteurs de S ainsi que des post-­‐éditeurs et des modérateurs payés ou non contribuent à l’amélioration continue et incrémentale des segments textuels les plus importants, et éventuellement de tous. Dans cette approche, les pré-­‐traductions sont produites par un ou plusieurs systèmes de Traduction Automatique (TA) gratuits. Il y a deux effets de bord intéressants, obtenables sans coût additionnel : les iMAGs peuvent être utilisées pour produire des corpus parallèles de haute qualité, et pour mettre en place une évaluation permanente et finalisée de multiples systèmes de TA.</resume>
        <mots_cles>post-édition, évaluation de systèmes de TA, production d’un corpus parallèle</mots_cles>
        <title>iMAG : MT-postediting, translation quality evaluation and parallel corpus production</title>
        <abstract>An interactive Multilingual Access Gateway (iMAG) dedicated to a web site S (iMAG-­‐S) is a good tool to make S accessible in many languages immediately and without editorial responsibility. Visitors of S as well as paid or unpaid post-­‐editors and moderators contribute to the continuous and incremental improvement of the most important textual segments, and eventually of all. In this approach, pre-­‐translations are produced by one or more free machine translation systems. There are two interesting side effects obtainable without any added cost: iMAGs can be used to produce high-­‐quality parallel corpora and to set up a permanent task-­‐based evaluation of multiple MT systems.</abstract>
        <keywords>post-edition, evaluation of MT systems, production of parallel corpora</keywords>
      </article>
      <article id="taln-2013-demo-009" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>David Rouquet</nom>
            <email>david.rouquet@imag.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIG-GETALP</affiliation>
        </affiliations>
        <titre>Technologies du Web Sémantique pour l'exploitation de données lexicales en réseau (Lexical Linked Data)</titre>
        <type>démonstration</type>
        <pages>803-804</pages>
        <resume>Nous présentons un système basé sur les technologies du Web Sémantique pour la gestion, le développement et l'exploitation de données lexicales en réseau (Lexical Linked Data, LLD).</resume>
        <mots_cles>Lexical Linked Data, Lexique Multilingue, Pivot, Axies, Sparql, Spin</mots_cles>
        <title>Semantic Web technologies for Lexical Linked Data management</title>
        <abstract>We present a system based on Semantic Web technologies for Lexical Linked Data management.</abstract>
        <keywords>Lexical Linked Data, Multilingual Lexicon, Pivot, Axies, Sparql, Spin</keywords>
      </article>
      <article id="taln-2013-demo-010" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Achille Falaise</nom>
            <email>achille.falaise@imag.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université Grenoble Alpes, LIGGETALP, F38040 Grenoble</affiliation>
        </affiliations>
        <titre>Adaptation de la plateforme corporale ScienQuest pour l'aide à la rédaction en langue seconde</titre>
        <type>démonstration</type>
        <pages>805-806</pages>
        <resume>La plateforme ScienQuest fut initialement créée pour l'étude linguistique du positionnement et du raisonnement dans le corpus Scientext. Cette démonstration présente les modifications apportées à cette plateforme, pour en faire une base phraséologique adaptée à l'aide à la rédaction en langue seconde. Cette adaptation est utilisée dans le cadre de deux expérimentations en cours : l'aide à la rédaction en anglais pour les scientifiques, et l'aide à la rédaction académique en français pour les apprenants.</resume>
        <mots_cles>Aide à la rédaction, langue seconde, ScienQuest, Scientext</mots_cles>
        <title>Adaptation of the corpus platform ScienQuest for assistance to writing in a second language</title>
        <abstract>The ScienQuest platform was initially created for the linguistic study of positioning and reasoning in the Scientext corpus. This demonstration introduces modifications to this platform, transforming it into a phraseological database adapted for assistance to writing in a second language. This adaptation is used as part of two ongoing experiments: an assistance to writing in English for scientists, and an assistance to academic writing in French for learners.</abstract>
        <keywords>Writing assistance, second language, ScienQuest, Scientext</keywords>
      </article>
      <article id="taln-2013-demo-011" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Sébastian Peña Saldarriaga</nom>
            <email>sebastian.pena-saldarriaga@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Damien Vintache</nom>
            <email>damien.vintache@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Béatrice Daille</nom>
            <email>beatrice.daille @univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LINA, 44322 Nantes Cedex 03</affiliation>
        </affiliations>
        <titre>Démonstrateur Apopsis pour l’analyse des tweets</titre>
        <type>démonstration</type>
        <pages>807-808</pages>
        <resume>Le démonstrateur Apopsis permet de délimiter et de catégoriser les opinions émises sur les tweets en temps réel pour un sujet choisi par l’utilisateur au travers d’une interface web.</resume>
        <mots_cles>fouille d’opinion, polarité, twitter</mots_cles>
        <title>Apopsis Demonstrator for Tweet Analysis</title>
        <abstract>Apopsis web demonstrator detects and categorizes opinion expressions appearing in Twitter in real time thorigh a web interface.</abstract>
        <keywords>opinion mining, polarity, twitter</keywords>
      </article>
      <article id="taln-2013-demo-012" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Frederik Cailliau</nom>
            <email>cailliau@sinequa.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Ariane Cavet</nom>
            <email>cavet@sinequa.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Sinequa, 12 rue d’Athènes 75009 Paris</affiliation>
        </affiliations>
        <titre>L’analyse des sentiments au service des centres d’appels</titre>
        <type>démonstration</type>
        <pages>809-811</pages>
        <resume>Les conversations téléphoniques qui contiennent du sentiment négatif sont particulièrement intéressantes pour les centres d’appels, aussi bien pour évaluer la perception d’un produit par les clients que pour améliorer la formation des télé-conseillers. Néanmoins, ces conversations sont peu nombreuses et difficiles à trouver dans la masse d’enregistrements. Nous présentons un module d’analyse des sentiments qui permet de visualiser le déroulement émotionnel des conversations. Il se greffe sur un moteur de recherche, ce qui permet de trouver rapidement les conversations problématiques grâce à l’ordonnancement par score de négativité.</resume>
        <mots_cles>analyse des sentiments, conversations téléphoniques, recherche d’information, parole spontanée, parole conversationnelle</mots_cles>
        <title>Sentiment Analysis for Call-centers</title>
        <abstract>Phone conversations in which negative sentiment is expressed are particularly interesting for call centers, both to evaluate the clients’ perception of a product and for the training of the agents. However, these conversations are scarce and hard to find in the mass of the recorded calls. We present a module for sentiment analysis that allows the user to visualize the emotional course of each conversation. In combination with a search engine, a user can rapidly find the problematic calls using the ranking by negativity score.</abstract>
        <keywords>sentiment analysis, information retrieval, spontaneous speech, conversational speech</keywords>
      </article>
      <article id="taln-2013-demo-013" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Béatrice Daille</nom>
            <email>beatrice.daille@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Rima Harastani</nom>
            <email>rima.harastani@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LINA, 44322 Nantes Cedex 03</affiliation>
        </affiliations>
        <titre>TTC TermSuite alignement terminologique à partir de corpus comparables</titre>
        <type>démonstration</type>
        <pages>812-813</pages>
        <resume>TermSuite est outil libre multilingue réalisant une extraction terminologique monolingue et une extraction terminologique bilingue à partir de corpus comparables.</resume>
        <mots_cles>corpus comparable, extraction terminologique, alignement, UIMA</mots_cles>
        <title>TTC TermSuite - Terminological Alignment from Comparable Corpora</title>
        <abstract>TermSuite is based on a UIMA framework and performs monolingual and bilingual term extraction from comparable corpora for a range of languages.</abstract>
        <keywords>comparable corpora, terminology extraction, terminology alignment, UIMA</keywords>
      </article>
    </articles>
  </conference>
  <conference>
	<edition>
		<acronyme>TALN'2014</acronyme>
		<titre>21e conférence sur le Traitement Automatique des Langues Naturelles</titre>
		<ville>Marseille</ville>
		<pays>France</pays>
		<dateDebut>2014-07-01</dateDebut>
		<dateFin>2014-07-04</dateFin>
		<presidents>
			<president>
				<prenom>Philippe</prenom>
				<nom>Blache</nom>
			</president>
			<president>
				<prenom>Frédéric</prenom>
				<nom>Béchet</nom>
			</president>
		</presidents>
		<editeurs>
			<editeur>
				<prenom>Brigitte</prenom>
				<nom>Bigi</nom>
			</editeur>
		</editeurs>
		<typeArticles>
			<type id="long">Papiers longs</type>
			<type id="court">Papiers courts</type>
			<type id="démonstration">Démonstrations</type>
		</typeArticles>
		<siteWeb>http://www.taln2014.org</siteWeb>
	</edition>
	<articles>
		<article id="taln-2014-long-001" session="Fouille de données et TAL">
			<auteurs>
				<auteur>
					<prenom>Morgane</prenom>
					<nom>Marchand</nom>
					<email>morgane.marchand@cea.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Romaric</prenom>
					<nom>Besançon</nom>
					<email>romaric.besancon@cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Olivier</prenom>
					<nom>Mesnard</nom>
					<email>olivier.mesnard@cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Anne</prenom>
					<nom>Vilnat</nom>
					<email>anne.vilnat@limsi.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CEA, LIST, Laboratoire Vision et Ingénierie des Contenus Centre Nano-Innov Saclay, 91191 Gif-sur-Yvette Cedex</affiliation>
				<affiliation affiliationId="2">LIMSI-CNRS, Université Paris-Sud, 91403 Orsay Cedex</affiliation>
			</affiliations>
			<titre>Influence des marqueurs multi-polaires dépendant du domaine pour la fouille d’opinion au niveau du texte</titre>
			<type>long</type>
			<pages>1-12</pages>
			<resume>Les méthodes de détection automatique de l’opinion dans des textes s’appuient sur l’association d’une polarité d’opinion aux mots des textes, par lexique ou par apprentissage. Or, certains mots ont des polarités qui peuvent varier selon le domaine thématique du texte. Nous proposons dans cet article une étude des mots ou groupes de mots marqueurs d’opinion au niveau du texte et qui ont une polarité changeante en fonction du domaine. Les expériences, effectuées à la fois sur des corpus français et anglais, montrent que la prise en compte de ces marqueurs permet d’améliorer de manière significative la classification de l’opinion au niveau du texte lors de l’adaptation d’un domaine source à un domaine cible. Nous montrons également que ces marqueurs peuvent être utiles, de manière limitée, lorsque l’on est en présence d’un mélange de domaines. Si les domaines ne sont pas explicites, utiliser une séparation automatique des documents permet d’obtenir les mêmes améliorations.</resume>
			<mots_cles>Fouille d’opinion, adaptation au domaine, marqueurs multi-polaires</mots_cles>
			<title>Study of Domain Dependant Multi-Polarity Words for Document Level Opinion Mining</title>
			<abstract>In this article, we propose a study on the words or multi-words which are good indicators of the opinion polarity of a text but have different polarity depending on the domain. We have performed experiments on French and English corpora, which show that taking these multi-polarity words into account improve the opinion classification at text level in a domain adaptation framework. We also show that these words are useful when the corpus contains several domains. If these domains are not explicit, using a automatic domain characterization (e.g. wich Topic Modeling approaches) allows to achieve the same results.</abstract>
			<keywords>Opinion mining, domain adaptation, multi-polarity markers</keywords>
		</article>
		<article id="taln-2014-long-002" session="Fouille de données et TAL">
			<auteurs>
				<auteur>
					<prenom>Adrien</prenom>
					<nom>Bougouin</nom>
					<email>adrien.bougouin@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Florian</prenom>
					<nom>Boudin</nom>
					<email>florian.boudin@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Béatrice</prenom>
					<nom>Daille</nom>
					<email>beatrice.daille@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LINA – UMR CNRS 6241, 2 rue de la Houssinière 44322 Nantes Cedex 3, France</affiliation>
			</affiliations>
			<titre>Influence des domaines de spécialité dans l’extraction de termes-clés</titre>
			<type>long</type>
			<pages>13-24</pages>
			<resume>Les termes-clés sont les mots ou les expressions polylexicales qui représentent le contenu principal d’un document. Ils sont utiles pour diverses applications, telles que l’indexation automatique ou le résumé automatique, mais ne sont pas toujours disponibles. De ce fait, nous nous intéressons à l’extraction automatique de termes-clés et, plus particulièrement, à la difficulté de cette tâche lors du traitement de documents appartenant à certaines disciplines scientifiques. Au moyen de cinq corpus représentant cinq disciplines différentes (archéologie, linguistique, sciences de l’information, psychologie et chimie), nous déduisons une échelle de difficulté disciplinaire et analysons les facteurs qui influent sur cette difficulté.</resume>
			<mots_cles>Extraction de termes-clés, articles scientifiques, domaines de spécialité, méthodes non-supervisées</mots_cles>
			<title>The impact of domains for Keyphrase extraction</title>
			<abstract>Keyphrases are single or multi-word expressions that represent the main content of a document. Keyphrases are useful in many applications such as document indexing or text summarization. However, most documents are not provided with keyphrases. To tackle this problem, researchers propose methods to automatically extract keyphrases from documents of various nature. In this paper, we focus on the difficulty of automatic keyphrase extraction in scientific papers from various areas. Using five corpora representing five areas (archaeology, linguistics, information sciences, psychology and chemistry), we observe the difficulty scale and analyze factors inducing a higher or a lower difficulty</abstract>
			<keywords>Keyphrase extraction, scientific papers, specific domain, unsupervised methods</keywords>
		</article>
		<article id="taln-2014-long-003" session="Fouille de données et TAL">
			<auteurs>
				<auteur>
					<prenom>Emanuela</prenom>
					<nom>Boros</nom>
					<email>emanuela.boros@cea.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Romaric</prenom>
					<nom>Besançon</nom>
					<email>romaric.besancon@cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Olivier</prenom>
					<nom>Ferret</nom>
					<email>olivier.ferret@cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Brigitte</prenom>
					<nom>Grau</nom>
					<email>brigitte.grau@limsi.fr</email>
					<affiliationId>2</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CEA, LIST, Laboratoire Vision et Ingénierie des Contenus, F-91191, Gif-sur-Yvette</affiliation>
				<affiliation affiliationId="2">LIMSI, Rue John von Neumann, Campus Universitaire d’Orsay, F-91405 Orsay cedex</affiliation>
				<affiliation affiliationId="3">ENSIIE, 1 square de la résistance F-91025 Évry cedex</affiliation>
			</affiliations>
			<titre>Étiquetage en rôles événementiels fondé sur l’utilisation d’un modèle neuronal</titre>
			<type>long</type>
			<pages>25-35</pages>
			<resume>Les systèmes d’extraction d’information doivent faire face depuis toujours à une double difficulté : d’une part, ils souffrent d’une dépendance forte vis-à-vis du domaine pour lesquels ils ont été développés ; d’autre part, leur coût de développement pour un domaine donné est important. Le travail que nous présentons dans cet article se focalise sur la seconde problématique en proposant néanmoins une solution en relation avec la première. Plus précisément, il aborde la tâche d’étiquetage en rôles événementiels dans le cadre du remplissage de formulaire (template filling) en proposant pour ce faire de s’appuyer sur un modèle de représentation distribuée de type neuronal. Ce modèle est appris à partir d’un corpus représentatif du domaine considéré sans nécessiter en amont l’utilisation de prétraitements linguistiques élaborés. Il fournit un espace de représentation permettant à un classifieur supervisé traditionnel de se dispenser de l’utilisation de traits complexes et variés (traits morphosyntaxiques, syntaxiques ou sémantiques). Par une série d’expérimentations menées sur le corpus de la campagne d’évaluation MUC-4, nous montrons en particulier que cette approche permet de dépasser les performances de l’état de l’art et que cette différence est d’autant plus importante que la taille du corpus d’entraînement est faible. Nous montrons également l’intérêt de l’adaptation de ce type de modèle au domaine traité par rapport à l’utilisation de représentations distribuées à usage générique.</resume>
			<mots_cles>Extraction d’information, extraction de rôles événementiels, modèles de langage neuronaux</mots_cles>
			<title>Event Role Labelling using a Neural Network Model</title>
			<abstract>Information Extraction systems must cope with two problems : they heavily depend on the considered domain but the cost of development for a domain-specific system is important. We propose a new solution for role labeling in the event-extraction task that relies on using unsupervised word representations (word embeddings) as word features. We automatically learn domain-relevant distributed representations from a domain-specific unlabeled corpus without complex linguistic processing and use these features in a supervised classifier. Our experimental results on the MUC-4 corpus show that this system outperforms state-of-the-art systems on this event extraction task, especially when the amount of annotated data is small.We also show that using word representations induced on a domain-relevant dataset achieves better results than using more general word embeddings.</abstract>
			<keywords>Information extraction, event role filler detection, neural language models</keywords>
		</article>
		<article id="taln-2014-long-004" session="Sémantique">
			<auteurs>
				<auteur>
					<prenom>William</prenom>
					<nom>Léchelle</nom>
					<email>lechellw@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Philippe</prenom>
					<nom>Langlais</nom>
					<email>felipe@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">DIRO, Université de Montréal</affiliation>
			</affiliations>
			<titre>Utilisation de représentations de mots pour l’étiquetage de rôles sémantiques suivant FrameNet</titre>
			<type>long</type>
			<pages>36-45</pages>
			<resume>D’après la sémantique des cadres de Fillmore, les mots prennent leur sens par rapport au contexte événementiel ou situationnel dans lequel ils s’inscrivent. FrameNet, une ressource lexicale pour l’anglais, définit environ 1000 cadres conceptuels couvrant l’essentiel des contextes possibles. Dans un cadre conceptuel, un prédicat appelle des arguments pour remplir les différents rôles sémantiques associés au cadre. Nous cherchons à annoter automatiquement ces rôles sémantiques, étant donné le cadre sémantique et le prédicat, à l’aide de modèles à maximum d’entropie. Nous montrons que l’utilisation de représentations distribuées de mots pour situer sémantiquement les arguments apporte une information complémentaire au modèle, et améliore notamment l’étiquetage de cadres avec peu d’exemples d’entrainement</resume>
			<mots_cles>rôles sémantiques, représentations distribuées, maximum d’entropie</mots_cles>
			<title>Using distributed word representations for robust semantic role labeling</title>
			<abstract>According to Frame Semantics (Fillmore 1976), words’ meaning are best understood considering the semantic frame they play a role in, for the frame is what gives them context. FrameNet defines about 1000 such semantic frames, along with the roles arguments can fill in this frame. Our task is to automatically label arguments’ roles, given their span, the frame, and the predicate, using maximum entropy models. We make use of distributed word representations to improve generalisation over the few training exemples available for each frame.</abstract>
			<keywords>semantic role labelling, distributed word representations</keywords>
		</article>
		<article id="taln-2014-long-005" session="Sémantique">
			<auteurs>
				<auteur>
					<prenom>Lonneke</prenom>
					<nom>van der Plas</nom>
					<email>Lonneke.vanderPlas@ims.uni-stuttgart.de</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Marianna</prenom>
					<nom>Apidianaki</nom>
					<email>Marianna.Apidianaki@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">IMS, Pfaffenwaldring 5B, 70569 Stuttgart, Germany</affiliation>
				<affiliation affiliationId="2">LIMSI-CNRS, Rue John von Neumann, Campus Universitaire d’Orsay Bât 508, 91405 Orsay Cedex, France</affiliation>
			</affiliations>
			<titre></titre>
			<type>long</type>
			<pages>46-55</pages>
			<resume>Nous abordons la question du transfert d’annotations sémantiques, et plus spécifiquement d’étiquettes sur les prédicats, d’une langue à l’autre sur la base de corpus parallèles. Des travaux antérieurs ont transféré ces annotations directement au niveau des tokens, conduisant à un faible rappel. Nous présentons une approche globale de transfert qui agrège des informations repérées dans l’ensemble du corpus parallèle. Nous montrons que la performance de la méthode globale est supérieure aux résultats antérieurs en termes de rappel sans trop affecter la précision.</resume>
			<mots_cles>transfert inter-langue, annotation sémantique automatique, prédicats, désambiguïsation lexicale, corpus parallèles</mots_cles>
			<title>Cross-lingual Word Sense Disambiguation for Predicate Labelling of French</title>
			<abstract>We address the problem of transferring semantic annotations, more specifically predicate labellings, from one language to another using parallel corpora. Previous work has transferred these annotations directly at the token level, leading to low recall. We present a global approach to annotation transfer that aggregates information across the whole parallel corpus.We show that this global method outperforms previous results in terms of recall without sacrificing precision too much.</abstract>
			<keywords>cross-lingual transfer, automatic semantic annotation, predicates, Word Sense Disambiguation, parallel corpora</keywords>
		</article>
		<article id="taln-2014-long-006" session="Parsing 1">
			<auteurs>
				<auteur>
					<prenom>Assaf</prenom>
					<nom>Urieli</nom>
					<email>assaf.urieli@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CLLE-ERSS: CNRS &amp; Université de Toulouse, Toulouse, France</affiliation>
				<affiliation affiliationId="2">Joliciel Informatique SARL, 2 avenue du Cardié, 09000 Foix, France</affiliation>
			</affiliations>
			<titre>Améliorer l’étiquetage de “que” par les descripteurs ciblés et les règles</titre>
			<type>long</type>
			<pages>56-66</pages>
			<resume>Les outils TAL statistiques robustes, et en particulier les étiqueteurs morphosyntaxiques, utilisent souvent des descripteurs “pauvres”, qui peuvent être appliqués facilement à n’importe quelle langue, mais qui ne regarde pas plus loin que 1 ou 2 tokens à droite et à gauche et ne prennent pas en compte des classes d’équivalence syntaxiques. Bien que l’étiquetage morphosyntaxique atteint des niveaux élevés d’exactitude (autour de 97 %), les 3 % d’erreurs qui subsistent induisent systématiquement une baisse de 3 % dans l’exactitude du parseur. Parmi les phénomènes les plus faciles à cibler à l’aide de l’injection de connaissances linguistiques plus riches sont les mots fonctionnels ambigus, tels que le mot “que” en français. Dans cette étude, nous cherchons à améliorer l’étiquetage morphosyntaxique de “que” par l’utilisation de descripteurs ciblés et riches lors de l’entraînement, et par l’utilisation de règles symboliques qui contournent le modèle statistique lors de l’analyse. Nous atteignons une réduction du taux d’erreur de 45 % par les descripteurs riches, et de 55 % si on ajoute des règles.</resume>
			<mots_cles>étiquetage morphosyntaxique, apprentissage automatique supervisé, descripteurs riches, systèmes statistiques robustes</mots_cles>
			<title>Better pos-tagging for "que" through targeted features and rules</title>
			<abstract>Robust statistical NLP tools, and in particular pos-taggers, often use knowledge-poor features, which are easily applicable to any language but do not look beyond 1 or 2 tokens to the right and left and do not make use of syntactic equivalence classes. Although pos-tagging tends to get high accuracy scores (around 97%), the remaining 3% errors systematically result in a 3% loss in parsing accuracy. Some of the easiest phenomena to target via the injection of richer linguistic knowledge are ambiguous function words, such as “que” in French. In this study, we attempt to improve the pos-tagging of “que” through the use of targeted knowledge-rich features during training, and symbolic rules which override the statistical model during analysis. We reduce the error rate by 45% using targeted knowledge-rich features, and 55% if we add rules.</abstract>
			<keywords>pos-tagging, supervised machine learning, knowledge-rich features, robust statistical systems</keywords>
		</article>
		<article id="taln-2014-long-007" session="Parsing 1">
			<auteurs>
				<auteur>
					<prenom>Éric</prenom>
					<nom>Villemonte de la Clergerie</nom>
					<email>Eric.De_La_Clergerie@inria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">INRIA - Rocquencourt - B.P. 105 78153 Le Chesnay Cedex, FRANCE</affiliation>
			</affiliations>
			<titre>Jouer avec des analyseurs syntaxiques</titre>
			<type>long</type>
			<pages>67-78</pages>
			<resume>Nous présentons DYALOG-SR, un analyseur syntaxique statistique par dépendances développé dans le cadre de la tâche SPRML 2013 portant sur un jeu de 9 langues très différentes. L’analyseur DYALOG-SR implémente un algorithme d’analyse par transition (à la MALT), étendu par utilisation de faisceaux et de techniques de programmation dynamique. Une des particularité de DYALOG-SR provient de sa capacité à prendre en entrée des treillis de mots, particularité utilisée lors de SPMRL13 pour traiter des treillis en Hébreu et reprise plus récemment sur des treillis produits par SXPIPE pour le français. Disposant par ailleurs avec FRMG d’un analyseur alternatif pour le français, nous avons expérimenté un couplage avec DYALOG-SR, nous permettant ainsi d’obtenir les meilleurs résultats obtenus à ce jour sur le French TreeBank.</resume>
			<mots_cles>Analyse syntaxique, Analyse syntaxique par dépendances, faisceaux, Programmation Dynamique, Treillis de mots, Couplage d’analyseurs</mots_cles>
			<title>Playing with parsers</title>
			<abstract>We present DYALOG-SR, a statistical dependency parser developed for the SPRML 2013 shared task over 9 very different languages. DYALOG-SR implements a shift-reduce parsing algorithm (a la MALT), extended with beams and dynamic programming techniques. One of the specificities of DYALOG-SR is its ability to handle word lattices as input, which was used for handling Hebrew lattices and more recently French ones produced by SXPIPE. Having access to FRMG, an alternative parser for French, we also tried a coupling with DYALOG-SR, providing us the best results so far on the French TreeBank</abstract>
			<keywords>Parsing, Dependency Parsing, Beams, Dynamic Programming, Word Lattice, Parser coupling</keywords>
		</article>
		<article id="taln-2014-long-008" session="Lexique 1">
			<auteurs>
				<auteur>
					<prenom>Alain</prenom>
					<nom>Polguère</nom>
					<email>alain.polguere@univ-lorraine.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ATILF, CNRS &amp; Université de Lorraine, 44 av. de la Libération, BP 30687 , 54063 Nancy Cedex</affiliation>
			</affiliations>
			<titre>Principes de modélisation systémique des réseaux lexicaux</titre>
			<type>long</type>
			<pages>79-90</pages>
			<resume>Nous présentons une approche de la construction manuelle des ressources lexicales à large couverture fondée sur le recours à un type particulier de réseau lexical appelé système lexical. En nous appuyant sur l’expérience acquise dans le cadre de la construction du Réseau Lexical du Français (RL-fr), nous offrons tout d’abord une caractérisation formelle des systèmes lexicaux en tant que graphes d’unités lexicales de type « petits mondes » principalement organisés à partir du système des fonctions lexicales Sens-Texte. Nous apportons ensuite des arguments pour justifier la pertinence du modèle proposé, tant du point de vue théorique qu’applicatif.</resume>
			<mots_cles>système lexical, base de données lexicale, structure du lexique, réseau lexical, graphe petit monde, proxémie, Lexicologie Explicative et Combinatoire, fonction lexicale, Réseau Lexical du Français (RL-fr)</mots_cles>
			<title>Principles of Lexical Network Systemic Modeling</title>
			<abstract>We introduce a new approach for manually constructing broad-coverage lexical ressources based on a specific type of lexical network called lexical system. Drawing on experience gained from the construction of the French Lexical Network (fr-LN), we begin by formally characterizing lexical systems as “small-world” graphs of lexical units that are primarily organized around the system of Meaning-Text lexical functions. We then give arguments in favor of the proposed model that are both theory- and application-oriented.</abstract>
			<keywords>lexical system, lexical database, structure of the lexicon, lexical network, small-world graph, proxemy, Explanatory Combinatorial Lexicology, lexical function, French Lexical Network (fr-LN)</keywords>
		</article>
		<article id="taln-2014-long-009" session="Lexique 1">
			<auteurs>
				<auteur>
					<prenom>Núria</prenom>
					<nom>Gala</nom>
					<email>nuria.gala@lif.univ-mrs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Thomas</prenom>
					<nom>François</nom>
					<email>tfrancois@uclouvain.be</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Delphine</prenom>
					<nom>Bernhard</nom>
					<email>dbernhard@unistra.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Cédrick</prenom>
					<nom>Fairon</nom>
					<email>cfairon@uclouvain.be</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIF-CNRS UMR 7279, Aix Marseille Université</affiliation>
				<affiliation affiliationId="2">CENTAL, Université Catholique de Louvain</affiliation>
				<affiliation affiliationId="3">LILPA, Université de Strasbourg</affiliation>
			</affiliations>
			<titre>Un modèle pour prédire la complexité lexicale et graduer les mots</titre>
			<type>long</type>
			<pages>91-102</pages>
			<resume>Analyser la complexité lexicale est une tâche qui, depuis toujours, a principalement retenu l’attention de psycholinguistes et d’enseignants de langues. Plus récemment, cette problématique a fait l’objet d’un intérêt grandissant dans le domaine du traitement automatique des langues (TAL) et, en particulier, en simplification automatique de textes. L’objectif de cette tâche est d’identifier des termes et des structures difficiles à comprendre par un public cible et de proposer des outils de simplification automatisée de ces contenus. Cet article aborde la question lexicale en identifiant un ensemble de prédicteurs de la complexité lexicale et en évaluant leur efficacité via une analyse corrélationnelle. Les meilleures de ces variables ont été intégrées dans un modèle capable de prédire la difficulté lexicale dans un contexte d’apprentissage du français.</resume>
			<mots_cles>complexité lexicale, analyse morphologique, mots gradués, ressources lexicales</mots_cles>
			<title>A model to predict lexical complexity and to grade words</title>
			<abstract>Analysing lexical complexity is a task that has mainly attracted the attention of psycholinguists and language teachers. More recently, this issue has seen a growing interest in the field of Natural Language Processing (NLP) and, in particular, that of automatic text simplification. The aim of this task is to identify words and structures which may be difficult to understand by a target audience and provide automated tools to simplify these contents. This article focuses on the lexical issue by identifying a set of predictors of the lexical complexity whose efficiency are assessed with a correlational analysis. The best of those variables are integrated into a model able to predict the difficulty of words for learners of French.</abstract>
			<keywords>lexical complexity, morphological analysis, graded words, lexical resources</keywords>
		</article>
		<article id="taln-2014-long-010" session="Lexique 1">
			<auteurs>
				<auteur>
					<prenom>Lionel</prenom>
					<nom>Ramadier</nom>
					<email>lionel.ramadier@lirmm.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Manel</prenom>
					<nom>Zarrouk</nom>
					<email>manel.zarrouk@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mathieu</prenom>
					<nom>Lafourcade</nom>
					<email>mathieu.lafourcade@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Antoine</prenom>
					<nom>Micheau</nom>
					<email>antoine.micheau@imaios.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIRMM, 161, rue ADA 34392 Montpellier Cedex 5</affiliation>
				<affiliation affiliationId="2">IMAIOS, 34090 Montpellier</affiliation>
			</affiliations>
			<titre>Annotations et inférences de relations dans un réseau lexico-sémantique: application à la radiologie</titre>
			<type>long</type>
			<pages>103-112</pages>
			<resume>Les ontologies spécifiques à un domaine ont une valeur inestimable malgré les nombreux défis liés à leur développement. Dans la plupart des cas, les bases de connaissances spécifiques à un domaine sont construites avec une portée limitée. En effet, elles ne prennent pas en compte les avantages qu’il pourrait y avoir à combiner une ontologie de spécialité à une ontologie générale. En outre, la plupart des ressources existantes manque de méta-informations sur les annotations (informations fréquentielles : de fréquent à rare ; ou des informations de pertinence : pertinent, non pertinent et inférable). Nous présentons dans cet article un réseau lexical dédié à la radiologie construit sur un réseau lexical généraliste (JeuxDeMots). Ce réseau combine poids et annotations sur des relations typées entre des termes et des concepts, un mécanisme d’inférence et de réconciliation dans le but d’améliorer la qualité et la couverture du réseau. Nous étendons ce mécanisme afin de prendre en compte non seulement les relations mais aussi les annotations. Nous décrivons la manière de laquelle les annotations améliorent le réseau en imposant de nouvelles contraintes spécialement celles basées sur la connaissance médicale. Nous présentons par la suite des résultats préliminaires.</resume>
			<mots_cles>réseau lexical, inférence, annotation, radiologie</mots_cles>
			<title>Annotations and inference of relations in a lexical semantic network : Applied to radiology</title>
			<abstract>Relations annotation and inference in a lexical-semantic network : application to radiology Domain specific ontologies are invaluable despite many challenges associated with their development. In most cases, domain knowledge bases are built with very limited scope without considering the benefits of plunging domain knowledge to a general ontology. Furthermore, most existing resources lack meta-information about association strength (weights) and annotations (frequency information like frequent, rare ... or relevance information (pertinent or irrelevant)). In this paper, we are presenting a semantic resource for radiology built over an existing general semantic lexical network (JeuxDeMots). This network combines weight and annotations on typed relations between terms and concepts. Some inference mechanisms are applied to the network to improve its quality and coverage. We extend this mechanism to relation annotation. We describe how annotations are handled and how they improve the network by imposing new constraints especially those founded on medical knowledge. We present then some results</abstract>
			<keywords>relation inference, lexical semantic network, relation annotation, radiology</keywords>
		</article>
		<article id="taln-2014-long-011" session="Gestion des erreurs en TAL">
			<auteurs>
				<auteur>
					<prenom>Maud</prenom>
					<nom>Pironneau</nom>
					<email>developpement@druide.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Éric</prenom>
					<nom>Brunelle</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Simon</prenom>
					<nom>Charest</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Druide informatique, 1435 rue Saint-Alexandre, bureau 1040, Montréal (Québec)</affiliation>
			</affiliations>
			<titre>Correction automatique par résolution d’anaphores pronominales</titre>
			<type>long</type>
			<pages>113-124</pages>
			<resume>Cet article décrit des travaux réalisés dans le cadre du développement du correcteur automatique d’un logiciel commercial d’aide à la rédaction du français. Nous voulons corriger des erreurs uniquement détectables lorsque l’antécédent de certains pronoms est connu. Nous décrivons un algorithme de résolution des anaphores pronominales intra- et interphrastiques s’appuyant peu sur la correspondance de la morphologie, puisque celle-ci est possiblement erronée, mais plutôt sur des informations robustes comme l’analyse syntaxique fine et des cooccurrences fiables. Nous donnons un aperçu de nos résultats sur un vaste corpus de textes réels et, tout en tentant de préciser les critères décisifs, nous montrons que certains types de corrections anaphoriques sont d’une précision respectable.</resume>
			<mots_cles>Correcteur, Anaphores, Pronom, Saillance, Approche multistratégique, Cooccurrences</mots_cles>
			<title>Pronoun Anaphora Resolution for Automatic Correction of Grammatical Errors</title>
			<abstract>This article relates work done in order to expand the performance of a commercial French grammar checker. We try to achieve the correction of errors only detectable when an anaphora pronoun is linked with its antecedent. The algorithm searches for the antecedent within the same sentence as the pronoun as well as in previous sentences. It relies only slightly on morphology agreement, since it is what we are trying to correct, and uses instead information from a robust syntactic parsing and reliable cooccurrences. We give examples of our results on a vast corpus, and try to identify the key criteria for successful detection. We show that some types of corrections are precise enough to be integrated in a large scale commercial software.</abstract>
			<keywords>Grammar checker, Anaphora, Pronoun, Salience, Multi-Strategy Approach, Cooccurrences</keywords>
		</article>
		<article id="taln-2014-long-012" session="Gestion des erreurs en TAL">
			<auteurs>
				<auteur>
					<prenom>Isabelle</prenom>
					<nom>Tellier</nom>
					<email>isabelle.tellier@univ-paris3.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Iris</prenom>
					<nom>Eshkol-Taravella</nom>
					<email>iris.eshkol@univ-orleans.fr</email>
					<affiliationId>3</affiliationId>
					<affiliationId>4</affiliationId>
				</auteur>
				<auteur>
					<prenom>Yoann</prenom>
					<nom>Dupont</nom>
					<email>yoann.dupont@etu.univ-paris3.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Ilaine</prenom>
					<nom>Wang</nom>
					<email>i.wang@u-paris10.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">université Paris 3 – Sorbonne Nouvelle</affiliation>
				<affiliation affiliationId="2">Lattice, UMR 8094</affiliation>
				<affiliation affiliationId="3">université d’Orléans</affiliation>
				<affiliation affiliationId="4">LLL, UMR 7270</affiliation>
			</affiliations>
			<titre>Peut-on bien chunker avec de mauvaises étiquettes POS ?</titre>
			<type>long</type>
			<pages>125-136</pages>
			<resume>Dans cet article, nous testons deux approches distinctes pour chunker un corpus oral transcrit, en cherchant à minimiser les étapes de correction manuelle. Nous ré-utilisons tout d’abord un chunker appris sur des données écrites, puis nous tentons de ré-apprendre un chunker spécifique de l’oral à partir de données annotées et corrigées manuellement, mais en faible quantité. L'objectif est d'atteindre les meilleurs résultats possibles pour le chunker en se passant autant que possible de la correction manuelle des étiquettes POS. Nos expériences montrent qu’il est possible d’apprendre un nouveau chunker performant pour l’oral à partir d’un corpus de référence annoté de petite taille, sans intervention sur les étiquettes POS.</resume>
			<mots_cles>chunker, étiquetage POS, apprentissage automatique, corpus oral, disfluences</mots_cles>
			<title>Can we chunk well with bad POS labels?</title>
			<abstract>In this paper, we test two distinct approaches to chunk transcribed oral data, trying to minimize the phases of manual correction. First, we use an existing chunker, learned from written texts, then we try to learn a new specific chunker from a small amount of manually corrected labeled oral data. The purpose is to reach the best possible results for the chunker with as few manual corrections of the POS labels as possible. Our experiments show that it is possible to learn a new effective chunker for oral data from a labeled reference corpus of small size, without any manual correction of POS labels.</abstract>
			<keywords>chunker, POS labeling, machine learning, oral corpus, disfluencies</keywords>
		</article>
		<article id="taln-2014-long-013" session="Gestion des erreurs en TAL">
			<auteurs>
				<auteur>
					<prenom>Marion</prenom>
					<nom>Baranes</nom>
					<email>Marion.Baranes@inria.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Benoît</prenom>
					<nom>Sagot</nom>
					<email>Benoit.Sagot@inria.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">viavoo, 92100 Boulogne Billancourt</affiliation>
				<affiliation affiliationId="2">Alpage, INRIA &amp; Université Paris-Diderot, 75013 Paris</affiliation>
			</affiliations>
			<titre>Normalisation de textes par analogie: le cas des mots inconnus</titre>
			<type>long</type>
			<pages>137-148</pages>
			<resume>Dans cet article, nous proposons et évaluons un système permettant d’améliorer la qualité d’un texte bruité notamment par des erreurs orthographiques. Ce système a vocation à être intégré à une architecture complète d’extraction d’information, et a pour objectif d’améliorer les résultats d’une telle tâche. Pour chaque mot qui est inconnu d’un lexique de référence et qui n’est ni une entité nommée ni une création lexicale, notre système cherche à proposer une ou plusieurs normalisations possibles (une normalisation valide étant un mot connu dont le lemme est le même que celui de la forme orthographiquement correcte). Pour ce faire, ce système utilise des techniques de correction automatique lexicale par règle qui reposent sur un système d’induction de règles par analogie.</resume>
			<mots_cles>normalisation textuelle, correction orthographique, analogie</mots_cles>
			<title>Analogy-based Text Normalization : the case of unknowns words</title>
			<abstract>Analogy-based Text Normalization : the case of unknowns words. In this paper, we describe and evaluate a system for improving the quality of noisy texts containing non-word errors. It is meant to be integrated into a full information extraction architecture, and aims at improving its results. For each word unknown to a reference lexicon which is neither a named entity nor a neologism, our system suggests one or several normalization candidates (any known word which has the same lemma as the spell-corrected form is a valid candidate). For this purpose, we use an analogybased approach for acquiring normalisation rules and use them in the same way as lexical spelling correction rules.</abstract>
			<keywords>Text normalization, Spell checking, Analogy</keywords>
		</article>
		<article id="taln-2014-long-014" session="Modèles linguistiques">
			<auteurs>
				<auteur>
					<prenom>Antoine</prenom>
					<nom>Bride</nom>
					<email>Bride@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Tim</prenom>
					<nom>Van de Cruys</nom>
					<email>Cruys@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Nicolas</prenom>
					<nom>Asher</nom>
					<email>Asher@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">IRIT, Université Paul Sabatier, 118 route de Narbonne, F-31062 TOULOUSE CEDEX 9</affiliation>
			</affiliations>
			<titre>Une évaluation approfondie de différentes méthodes de compositionalité sémantique</titre>
			<type>long</type>
			<pages>149-160</pages>
			<resume>Au cours des deux dernières décennies, de nombreux algorithmes ont été développés pour capturer la sémantique des mots simples en regardant leur répartition dans un grand corpus, et en comparant ces distributions dans un modèle d’espace vectoriel. En revanche, il n’est pas trivial de combiner les objets algébriques de la sémantique distributionnelle pour arriver à une dérivation d’un contenu pour des expressions complexes, composées de plusieurs mots. Notre contribution a deux buts. Le premier est d’établir une large base de comparaison pour les méthodes de composition pour le cas adjectif_nom. Cette base nous permet d’évaluer en profondeur la performance des différentes méthodes de composition. Notre second but est la proposition d’une nouvelle méthode de composition, qui est une généralisation de la méthode de Baroni &amp; Zamparelli (2010). La performance de notre nouvelle méthode est également évaluée sur notre nouveau ensemble de test.</resume>
			<mots_cles>sémantique lexicale, sémantique distributionnelle, compositionalité</mots_cles>
			<title>An evaluation of various methods for adjective-nouns composition</title>
			<abstract>In the course of the last two decades, numerous algorithms have sprouted up that successfully capture the semantics of single words by looking at their distribution in text, and comparing these distributions in a vector space model. However, it is not straightforward to construct meaning representations beyond the level of individual words – i.e. the combination of words into larger units – using distributional methods. Our contribution is twofold. First of all, we carry out a large scale evaluation, comparing different composition methods within the distributional framework for the case of adjective-noun composition, making use of a newly developed dataset. Secondly, we propose a novel method for adjective-noun composition, which is a generalization of the approach by Baroni &amp; Zamparelli (2010). The performance of our novel method is equally evaluated on our new dataset.</abstract>
			<keywords>lexical semantics, distributional semantics, compositionality</keywords>
		</article>
		<article id="taln-2014-long-015" session="Modèles linguistiques">
			<auteurs>
				<auteur>
					<prenom>Laurence</prenom>
					<nom>Danlos</nom>
					<email>laurence.danlos@inria.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Aleksandre</prenom>
					<nom>Maskharashvili</nom>
					<email>aleksandre.maskharashvili@inria.fr</email>
					<affiliationId>4</affiliationId>
					<affiliationId>5</affiliationId>
					<affiliationId>6</affiliationId>
				</auteur>
				<auteur>
					<prenom>Sylvain</prenom>
					<nom>Pogodalla</nom>
					<email>sylvain.pogodalla@inria.fr</email>
					<affiliationId>4</affiliationId>
					<affiliationId>5</affiliationId>
					<affiliationId>6</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Paris Diderot (Paris 7), Paris, F-75013, France</affiliation>
				<affiliation affiliationId="2">ALPAGE, INRIA Paris–Rocquencourt, Paris, F-75013, France</affiliation>
				<affiliation affiliationId="3">Institut Universitaire de France, Paris, F-75005, France</affiliation>
				<affiliation affiliationId="4">INRIA, Villers-lès-Nancy, F-54600, France</affiliation>
				<affiliation affiliationId="5">Université de Lorraine, LORIA, UMR 7503, Vandoeuvre-lès-Nancy, F-54500, France</affiliation>
				<affiliation affiliationId="6">CNRS, LORIA, UMR 7503, Vandoeuvre-lès-Nancy, F-54500, France</affiliation>
			</affiliations>
			<titre>Génération de textes : G-TAG revisité avec les Grammaires Catégorielles Abstraites</titre>
			<type>long</type>
			<pages>161-172</pages>
			<resume>G-TAG est un formalisme dédié à la génération de textes. Il s’appuie sur les Grammaires d’Arbres Adjoints (TAG) qu’il étend avec des notions propres permettant de construire une forme de surface à partir d’une représentation conceptuelle. Cette représentation conceptuelle est indépendante de la langue, et le formalisme G-TAG a été conçu pour la mise en oeuvre de la synthèse dans une langue cible à partir de cette représentation. L’objectif de cet article est d’étudier G-TAG et les notions propres que ce formalisme introduit par le biais des Grammaires Catégorielles Abstraites (ACG) en exploitant leurs propriétés de réversibilité intrinsèque et leur propriété d’encodage des TAG. Nous montrons que les notions clefs d’arbre de g-dérivation et de lexicalisation en G-TAG s’expriment naturellement en ACG. La construction des formes de surface peut alors utiliser les algorithmes généraux associés aux ACG et certaines constructions absentes de G-TAG peuvent être prises en compte sans modification supplémentaire.</resume>
			<mots_cles>TAG, G-TAG, génération, réalisation syntaxique, grammaires catégorielles abstraites</mots_cles>
			<title>Text Generation: Reexamining G-TAG with Abstract Categorial Grammars</title>
			<abstract>G-TAG is a formalism dedicated to text generation. It relies on the Tree Adjoining Grammar (TAG) formalism and extends it with several specific notions allowing for the construction of a surface form from a conceptual representation. This conceptual representation is independent from the target language. The goal of this paper is to study G-TAG and its specific notions from the perspective given by Abstract Categorial Grammars (ACG). We use the reversibility property of ACG and the encoding of TAG they offer. We show that the key G-TAG notions of g-derivation tree and lexicalization are naturally expressed in ACG. The construction of surface forms can then rely on the general ACG algorithms and some operations that G-TAG is lacking can be freely accounted for.</abstract>
			<keywords>TAG, G-TAG, generation, syntactic realization, abstract categorial grammars</keywords>
		</article>
		<article id="taln-2014-long-016" session="Méthodes numériques pour le TAL">
			<auteurs>
				<auteur>
					<prenom>Guillaume</prenom>
					<nom>Wisniewski</nom>
					<email>Wisniewski.Guillaume@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Nicolas</prenom>
					<nom>Pécheux</nom>
					<email>Pecheux.Nicolas@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Elena</prenom>
					<nom>Knyazeva</nom>
					<email>Knyazeva.Elena@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Alexandre</prenom>
					<nom>Allauzen</nom>
					<email>Allauzen.Alexandre@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>François</prenom>
					<nom>Yvon</nom>
					<email>Yvon.François@limsi.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Paris Sud, 91 403 Orsay CEDEX</affiliation>
				<affiliation affiliationId="2">LIMSI-CNRS, 91 403 Orsay CEDEX</affiliation>
			</affiliations>
			<titre>Apprentissage partiellement supervisé d’un étiqueteur morpho-syntaxique par transfert cross-lingue</titre>
			<type>long</type>
			<pages>173-183</pages>
			<resume>Les méthodes de transfert cross-lingue permettent partiellement de pallier l’absence de corpus annotés, en particulier dans le cas de langues peu dotées en ressources linguistiques. Le transfert d’étiquettes morpho-syntaxiques depuis une langue riche en ressources, complété et corrigé par un dictionnaire associant à chaque mot un ensemble d’étiquettes autorisées, ne fournit cependant qu’une information de supervision incomplète. Dans ce travail, nous reformulons ce problème dans le cadre de l’apprentissage ambigu et proposons une nouvelle méthode pour apprendre un analyseur de manière faiblement supervisée à partir d’un modèle à base d’historique. L’évaluation de cette approche montre une amélioration sensible des performances par rapport aux méthodes de l’état de l’art pour trois langues sur quatre considérées, avec des gains jusqu’à 3,9% absolus ou 35,8% relatifs.</resume>
			<mots_cles>apprentissage partiellement supervisé, analyse morpho-syntaxique, transfert cross-lingue</mots_cles>
			<title>Cross-Lingual POS Tagging through Ambiguous Learning: First Experiments</title>
			<abstract>When Part-of-Speech annotated data is scarce, e.g. for under resourced languages, one can turn to crosslingual transfer and crawled dictionaries to collect partially supervised data. We cast this problem in the framework of ambiguous learning and show how to learn an accurate history-based model. This method is evaluated on four languages and yields improvements over state-of-the-art for three of them, with gains up to 3.9% absolute or 35.8% relative.</abstract>
			<keywords>Weakly Supervised Learning, Part-of-Speech Tagging, Cross-Lingual Transfer</keywords>
		</article>
		<article id="taln-2014-long-017" session="Méthodes numériques pour le TAL">
			<auteurs>
				<auteur>
					<prenom>Nicolas</prenom>
					<nom>Hernandez</nom>
					<email>nicolas.hernandez@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Nantes</affiliation>
			</affiliations>
			<titre>Construire un corpus monolingue annoté comparable Expérience à partir d’un corpus annoté morpho-syntaxiquement</titre>
			<type>long</type>
			<pages>184-195</pages>
			<resume>Motivé par la problématique de construction automatique d’un corpus annoté morpho-syntaxiquement distinct d’un corpus source, nous proposons une définition générale et opérationnelle de la relation de la comparabilité entre des corpus monolingues annotés. Cette définition se veut indépendante du domaine applicatif. Nous proposons une mesure de la relation de comparabilité et une procédure de construction d’un corpus comparable. Enfin nous étudions la possibilité d’utiliser la mesure de la perplexité définie dans la théorie de l’information comme moyen de prioriser les phrases à sélectionner pour construire un corpus comparable. Nous montrons que cette mesure joue un rôle mais qu’elle n’est pas suffisante.</resume>
			<mots_cles>Corpus comparable, Corpus monolingue, Corpus annoté, Mesure de la comparabilité, Construction de corpus comparable, Analyse morpho-syntaxique</mots_cles>
			<title>Building Monolingual Comparable and Annotated Corpora: An experimental study from a pos tagged corpus</title>
			<abstract>This work is motivated by the will of creating a new part-of-speech annotated corpus in French from an existing one. In this context, we proprose a general and operational definition of the comparability relation between annotated monolingual corpora.We propose a comparability measure and a procedure to build semi-automatically a comparable corpus from a source one. We study the use of the perplexity (information theory motivated measure) as a way to rank the sentences to select for building a comparable corpus. We show that the measure can play a role but that it is not sufficient.</abstract>
			<keywords>Comparable corpus, Monolingual corpus, Annotated corpus,Measuring comparability, Building comparable corpus, Part-of-Speech tagging</keywords>
		</article>
		<article id="taln-2014-long-018" session="Méthodes numériques pour le TAL">
			<auteurs>
				<auteur>
					<prenom>Hatim</prenom>
					<nom>Khouzaimi</nom>
					<email>hatim.khouzaimi@orange.com</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Romain</prenom>
					<nom>Laroche</nom>
					<email>romain.laroche@orange.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Fabrice</prenom>
					<nom>Lefèvre</nom>
					<email>fabrice.lefevre@univ-avignon.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Orange Labs, 38-40 rue du Général Leclerc 92794 Issy-les-Moulineaux, France</affiliation>
				<affiliation affiliationId="2">Laboratoire Informatique d’Avignon, 339 chemin des Meinajaries 84911 Avignon, France</affiliation>
			</affiliations>
			<titre>Vers une approche simplifiée pour introduire le caractère incrémental dans les systèmes de dialogue</titre>
			<type>long</type>
			<pages>196-207</pages>
			<resume>Le dialogue incrémental est au coeur de la recherche actuelle dans le domaine des systèmes de dialogue. Plusieurs architectures et modèles ont été publiés comme (Allen et al., 2001; Schlangen &amp; Skantze, 2011). Ces approches ont permis de comprendre différentes facettes du dialogue incrémental, cependant, les implémenter nécessite de repartir de zéro car elles sont fondamentalement différentes des architectures qui existent dans les systèmes de dialogue actuels. Notre approche se démarque par sa réutilisation de l’existant pour tendre vers une nouvelle génération de systèmes de dialogue qui ont un comportement incrémental mais dont le fonctionnement interne est basé sur les principes du dialogue traditionnel. Ce papier propose d’intercaler un module, appelé Scheduler, entre le service et le client. Ce Scheduler se charge de la gestion des événements asynchrones, de manière à reproduire le comportement des systèmes incrémentaux vu du client. Le service, de son côté, ne se comporte pas de manière incrémentale.</resume>
			<mots_cles>Systèmes de Dialogue, Traitement Incrémental, Architecture des Systèmes de Dialogue</mots_cles>
			<title>A simple approach to make dialogue systems incremental</title>
			<abstract>Incremental dialogue is at the heart of current research in the field of dialogue systems. Several architectures and models have been published such as (Allen et al., 2001; Schlangen &amp; Skantze, 2011). This work has made it possible to understand many aspects of incremental dialogue, however, in order to implement these solutions, one needs to start from scratch as the existing architectures are inherently different. Our approach is different as it tends towards a new generation of incremental systems that behave incrementally but work internally in a traditional way. This paper suggests inserting a new module, called the Scheduler, between the service and the client. This Scheduler manages the asynchronous events, hence reproducing the behaviour of incremental systems from the client’s point of view. On the other end, the service does not work incrementally.</abstract>
			<keywords>Dialogue Systems, Incremental Processing, Dialogue Systems Architecture</keywords>
		</article>
		<article id="taln-2014-long-019" session="Lexique 2">
			<auteurs>
				<auteur>
					<prenom>Nabil</prenom>
					<nom>Hathout</nom>
					<email>Nabil.Hathout@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Fiammetta</prenom>
					<nom>Namer</nom>
					<email>Fiammetta.Namer@univ-lorraine.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">UMR 5263 CLLE/ERSS, CNRS &amp; Université Toulouse Le Mirail, Toulouse</affiliation>
				<affiliation affiliationId="2">UMR 7118 ATILF, CNRS &amp; Université de Lorraine, Nancy</affiliation>
			</affiliations>
			<titre>La base lexicale Démonette : entre sémantique constructionnelle et morphologie dérivationnelle</titre>
			<type>long</type>
			<pages>208-219</pages>
			<resume>Démonette est une base de données lexicale pour le français dont les sommets (entrées lexicales) et les arcs (relations morphologiques entre les sommets) sont annotés au moyen d’informations morpho-sémantiques. Elle résulte d’une conception originale intégrant deux approches radicalement opposées : Morphonette, une ressource basée sur les analogies dérivationnelles, et DériF, un analyseur à base de règles linguistiques. Pour autant, Démonette n’est pas la simple fusion de deux ressources pré-existantes : cette base possède une architecture compatible avec l’approche lexématique de la morphologie ; son contenu peut être étendu au moyen de données issues de sources diverses. L'article présente le modèle Démonette et le contenu de sa version actuelle : 31 204 verbes, noms d'action, noms d’agent, et adjectifs de propriété dont les liens morphologiques donnent à voir des définitions bi-orientées entre ascendants et entre lexèmes en relation indirecte. Nous proposons enfin une évaluation de Démonette qui comparée à Verbaction obtient un score de 84% en rappel et de 90% en précision.</resume>
			<mots_cles>Réseau lexical, Morphologie dérivationnelle, Famille morphologique, Sémantique lexicale, Français</mots_cles>
			<title>The Démonette Lexical Database: between Constructional Semantics and Word Formation</title>
			<abstract>Démonette is a lexical database whose vertices (lexical entries) and edges (morphological relations between the vertices) are annotated with morpho-semantic information. It results from an original design incorporating two radically different approaches: Morphonette, a resource based on derivational analogies and DériF, an analyzer based on linguistic rules. However, Daemonette is not a simple merger of two pre-existing ressources: its architecture is fully compatible with the lexematic approach to morphology; its contents can be extended using data from various other sources. The article presents the Démonette model and the content of its current version, including 31,204 verbs, action nouns, agent nouns and property adjectives, where morphological links between both direct ascendants and indirectly related words have bi-oriented definitions. Finally, Démonette is assessed with respect to Verbaction with a recall of 84% and a precision of 90%.</abstract>
			<keywords>Lexical Network, Derivational morphology, Morphological family, Lexical semantics, French</keywords>
		</article>
		<article id="taln-2014-long-020" session="Lexique 2">
			<auteurs>
				<auteur>
					<prenom>Vincent</prenom>
					<nom>Claveau</nom>
					<email>vincent.claveau@irisa.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Ewa</prenom>
					<nom>Kijak</nom>
					<email>ewa.kijak@irisa.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Olivier</prenom>
					<nom>Ferret</nom>
					<email>olivier.ferret@cea.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">IRISA - CNRS - Univ Rennes 1, Campus de Beaulieu, F-35042 Rennes</affiliation>
				<affiliation affiliationId="2">CEA, LIST, Laboratoire Vision et Ingénierie des Contenus, F-91191 Gif-sur-Yvette</affiliation>
			</affiliations>
			<titre>Explorer le graphe de voisinage pour améliorer les thésaurus distributionnels</titre>
			<type>long</type>
			<pages>220-231</pages>
			<resume>Dans cet article, nous abordons le problème de construction et d’amélioration de thésaurus distributionnels. Nous montrons d’une part que les outils de recherche d’information peuvent être directement utilisés pour la construction de ces thésaurus, en offrant des performances comparables à l’état de l’art. Nous nous intéressons d’autre part plus spécifiquement à l’amélioration des thésaurus obtenus, vus comme des graphes de plus proches voisins. En tirant parti de certaines des informations de voisinage contenues dans ces graphes nous proposons plusieurs contributions. 1) Nous montrons comment améliorer globalement les listes de voisins en prenant en compte la réciprocité de la relation de voisinage, c’est-à-dire le fait qu’un mot soit un voisin proche d’un autre et vice-versa. 2) Nous proposons également une méthode permettant d’associer à chaque liste de voisins (i.e. à chaque entrées du thésaurus construit) un score de confiance. 3) Enfin, nous montrons comment utiliser ce score de confiance pour réordonner les listes de voisins les plus proches. Ces différentes contributions sont validées expérimentalement et offrent des améliorations significatives sur l’état de l’art.</resume>
			<mots_cles>thésaurus distributionnel, graphe de k proches voisins, fenêtre de Parzen, algorithme hongrois, Tnormes, recherche d’information</mots_cles>
			<title>Exploring the neighbor graph to improve distributional thesauri</title>
			<abstract>In this paper, we address the issue of building and improving a distributional thesaurus.We first show that existing tools from the information retrieval domain can be directly used in order to build a thesaurus with state-of-the-art performance. Secondly, we focus more specifically on improving the obtained thesaurus, seen as a graph of k-nearest neighbors. By exploiting information about the neighborhood contained in this graph, we propose several contributions. 1)We show how the lists of neighbors can be globally improved by examining the reciprocity of the neighboring relation, that is, the fact that a word can be close of another and vice-versa. 2) We also propose a method to associate a confidence score to any lists of nearest neighbors (i.e. any entry of the thesaurus). 3) Last, we demonstrate how these confidence scores can be used to reorder the closest neighbors of a word. These different contributions are validated through experiments and offer significant improvement over the state-of-theart.</abstract>
			<keywords>distributional thesaurus, k nearest neighbor graph, Parzen window, Hungarian algorithm, T-norms, information retrieval</keywords>
		</article>
		<article id="taln-2014-long-021" session="Lexique 2">
			<auteurs>
				<auteur>
					<prenom>Amandine</prenom>
					<nom>Périnet</nom>
					<email>amandine.perinet@edu.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Thierry</prenom>
					<nom>Hamon</nom>
					<email>hamon@limsi.fr</email>
					<affiliationId>2</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">INSERM, U1142, LIMICS, F-75006, Paris, France; Sorbonne Universités, UPMC Univ Paris 06, UMR_S 1142, LIMICS, F-75006, Paris, France; Université Paris 13, Sorbonne Paris Cité, LIMICS, (UMR_S 1142), F-93430, Villetaneuse, France</affiliation>
				<affiliation affiliationId="2">LIMSI-CNRS, 91403 Orsay, France</affiliation>
				<affiliation affiliationId="3">Université Paris 13, Sorbonne Paris Cité, 93430 Villetaneuse, France</affiliation>
			</affiliations>
			<titre>Réduction de la dispersion des données par généralisation des contextes distributionnels : application aux textes de spécialité</titre>
			<type>long</type>
			<pages>232-243</pages>
			<resume>Les modèles d’espace vectoriels mettant en oeuvre l’analyse distributionnelle s’appuient sur la redondance d’informations se trouvant dans le contexte des mots à associer. Cependant, ces modèles souffrent du nombre de dimensions considérable et de la dispersion des données dans la matrice des vecteurs de contexte. Il s’agit d’un enjeu majeur sur les corpus de spécialité pour lesquels la taille est beaucoup plus petite et les informations contextuelles moins redondantes. Nous nous intéressons au problème de la limitation de la dispersion des données sur des corpus de spécialité et proposons une méthode permettant de densifier la matrice en généralisant les contextes distributionnels. L’évaluation de la méthode sur un corpus médical en français montre qu’avec une petite fenêtre graphique et l’indice de Jaccard, la généralisation des contextes avec des relations fournies par des patrons lexico-syntaxiques permet d’améliorer les résultats, alors qu’avec une large fenêtre et le cosinus, il est préférable de généraliser avec des relations obtenues par inclusion lexicale.</resume>
			<mots_cles>Analyse distributionnelle, textes de spécialité, hyperonymie, dispersion des données, modèle d’espace vectoriel, méthode hybride</mots_cles>
			<title>Reducing data sparsity by generalising distributional contexts: application to specialised texts</title>
			<abstract>Vector space models implement the distributional hypothesis relying on the repetition of information occurring in the contexts of words to associate. However, these models suffer from a high number of dimensions and data sparseness in the matrix of contextual vectors. This is a major issue with specialized corpora that are of much smaller size and with much lower context frequencies.We tackle the problem of data sparseness on specialized texts and we propose a method that allows to make the matrix denser, by generalizing of distributional contexts. The evaluation of the method is performed on a French medical corpus, and shows that with a small graphical window and the Jaccard Index, the context generalization with lexico-syntactic patterns improves the results, while with a large window and the cosine measure, it is better to generalize with lexical inclusion.</abstract>
			<keywords>Distributional analysis, specialized texts, hypernymy, data sparseness, Vector Space Model, hybrid method</keywords>
		</article>
		<article id="taln-2014-long-022" session="Lexique 2">
			<auteurs>
				<auteur>
					<prenom>Juliette</prenom>
					<nom>Conrath</nom>
					<email>Conrath.Juliette@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Stergos</prenom>
					<nom>Afantenos</nom>
					<email>Afantenos.Stergos@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Nicholas</prenom>
					<nom>Asher</nom>
					<email>Asher.Nicholas@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Philippe</prenom>
					<nom>Muller</nom>
					<email>Muller.Philippe@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">IRIT, Université Toulouse &amp; CNRS, Univ. Paul Sabatier, 118 Route de Narbonne, 31062 Toulouse</affiliation>
			</affiliations>
			<titre>Extraction non supervisée de relations sémantiques lexicales</titre>
			<type>long</type>
			<pages>244-255</pages>
			<resume>Nous présentons une base de connaissances comportant des triplets de paires de verbes associés avec une relation sémantique/discursive, extraits du corpus français frWaC par une méthode s’appuyant sur la présence d’un connecteur discursif reliant deux verbes. Nous détaillons plusieurs mesures visant à évaluer la pertinence des triplets et la force d’association entre la relation sémantique/discursive et la paire de verbes. L’évaluation intrinsèque est réalisée par rapport à des annotations manuelles. Une évaluation de la couverture de la ressource est également réalisée par rapport au corpus Annodis annoté discursivement. Cette étude produit des résultats prometteurs démontrant l’utilité potentielle de notre ressource pour les tâches d’analyse discursive mais aussi des tâches de nature sémantique.</resume>
			<mots_cles>discours, sémantique, sémantique lexicale</mots_cles>
			<title>Unsupervised extraction of semantic relations</title>
			<abstract>This paper presents a knowledge base containing triples involving pairs of verbs associated with semantic or discourse relations. The relations in these triples are marked by discourse connectors between two adjacent instances of the verbs in the triple in the large French corpus, frWaC. We detail several measures that evaluate the relevance of the triples and the strength of their association. We use manual annotations to evaluate our method, and also study the coverage of our ressource with respect to the discourse annotated corpus Annodis. Our positive results show the potential impact of our ressource for discourse analysis tasks as well as semantically oriented tasks.</abstract>
			<keywords>discourse, semantics, lexical semantics</keywords>
		</article>
		<article id="taln-2014-long-023" session="Traduction Automatique">
			<auteurs>
				<auteur>
					<prenom>Quoc-Khanh</prenom>
					<nom>Do</nom>
					<email>Quoc-Khanh.Do@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Alexandre</prenom>
					<nom>Allauzen</nom>
					<email>Alexandre.Allauzen@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>François</prenom>
					<nom>Yvon</nom>
					<email>François.Yvon@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI/CNRS, rue John von Neumann, Campus Universitaire Orsay 91 403 Orsay</affiliation>
				<affiliation affiliationId="2">Université Paris Sud, 91 403 Orsay</affiliation>
			</affiliations>
			<titre>Modèles de langue neuronaux: une comparaison de plusieurs stratégies d’apprentissage</titre>
			<type>long</type>
			<pages>256-267</pages>
			<resume>Alors que l’importance des modèles neuronaux dans le domaine du traitement automatique des langues ne cesse de croître, les difficultés de leur apprentissage continue de freiner leur diffusion au sein de la communauté. Cet article étudie plusieurs stratégies, dont deux sont originales, pour estimer des modèles de langue neuronaux, en se focalisant sur l’ajustement du pas d’apprentissage. Les résultats expérimentaux montrent, d’une part, l’importance que revêt la conception de cette stratégie. D’autre part, le choix d’une stratégie appropriée permet d’apprendre efficacement des modèles de langue donnant lieu à des résultats à l’état de l’art en traduction automatique, avec un temps de calcul réduit et une faible influence des hyper-paramètres.</resume>
			<mots_cles>Réseaux de neurones, modèles de langue n-gramme, traduction automatique statistique</mots_cles>
			<title>Comparison of scheduling methods for the learning rate of neural network language models</title>
			<abstract>If neural networks play an increasingly important role in natural language processing, training issues still hinder their dissemination in the community. This paper studies different learning strategies for neural language models (including two new strategies), focusing on the adaptation of the learning rate. Experimental results show the impact of the design of such strategy. Moreover, provided the choice of an appropriate training regime, it is possible to efficiently learn language models that achieves state of the art results in machine translation with a lower training time and a reduced impact of hyper-parameters.</abstract>
			<keywords>Neural networks, n-gram language models, statistical machine translation</keywords>
		</article>
		<article id="taln-2014-long-024" session="Traduction Automatique">
			<auteurs>
				<auteur>
					<prenom>Nasredine</prenom>
					<nom>Semmar</nom>
					<email>nasredine.semmar@cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Houda</prenom>
					<nom>Saadane</nom>
					<email>houda.saadane@e.u-grenoble3.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Institut CEA LIST, DIASI, Laboratoire Vision et Ingénierie des Contenus, CEA Saclay – Nano-INNOV, 91191 Gif-sur-Yvette Cedex</affiliation>
				<affiliation affiliationId="2">LIDILEM, Université Stendhal-Grenoble III, Domaine Universitaire, 1180, avenue centrale, 38400 Saint Martin d'Hères</affiliation>
			</affiliations>
			<titre>Etude de l’impact de la translittération de noms propres sur la qualité de l’alignement de mots à partir de corpus parallèles français-arabe</titre>
			<type>long</type>
			<pages>268-279</pages>
			<resume>Les lexiques bilingues jouent un rôle important en recherche d'information interlingue et en traduction automatique. La construction manuelle de ces lexiques est lente et coûteuse. Les techniques d’alignement de mots sont généralement utilisées pour automatiser le processus de construction de ces lexiques à partir de corpus de textes parallèles. L’alignement de formes simples et de syntagmes nominaux à partir de corpus parallèles est une tâche relativement bien maîtrisée pour les langues à écriture latine, mais demeure une opération complexe pour l’appariement de textes n’utilisant pas la même écriture. Dans la perspective d’utiliser la translittération de noms propres de l’arabe vers l’écriture latine en alignement de mots et d’étudier son impact sur la qualité d’un lexique bilingue français-arabe construit automatiquement, cet article présente, d’une part, un système de translittération de noms propres de l’arabe vers l’écriture latine, et d’autre part, un outil d’alignement de mots simples et composés à partir de corpus de textes parallèles français-arabe. Le lexique bilingue produit par l’outil d'alignement de mots intégrant la translittération a été évalué en utilisant deux approches : une évaluation de la qualité d’alignement à l’aide d’un alignement de référence construit manuellement et une évaluation de l’impact de ce lexique bilingue sur la qualité de traduction du système de traduction automatique statistique Moses. Les résultats obtenus montrent que la translittération améliore aussi bien la qualité de l’alignement de mots que celle de la traduction.</resume>
			<mots_cles>Lexique bilingue, translittération, alignement de mots, traduction automatique statistique, évaluation</mots_cles>
			<title>Study of the impact of proper name transliteration on the performance of word alignment in French-Arabic parallel corpora</title>
			<abstract>Bilingual lexicons play a vital role in cross-language information retrieval and machine translation. The manual construction of these lexicons is often costly and time consuming. Word alignment techniques are generally used to construct bilingual lexicons from parallel texts. Aligning single words and nominal syntagms from parallel texts is relatively a well controlled task for languages using Latin script but it is complex when the source and target languages do not share the same written script. A solution to this issue consists in writing the proper names present in the parallel corpus in the same written script. This paper presents, on the one hand, a system for automatic transliteration of proper names from Arabic to Latin script, and on the other hand, a tool to align single and compound words from French- Arabic parallel text corpora. We have evaluated the word alignment tool integrating transliteration using two methods: A manual evaluation of the alignment quality and an evaluation of the impact of this alignment on the translation quality by using the statistical machine translation system Moses. The obtained results show that transliteration of proper names from Arabic to Latin improves the quality of both alignment and translation.</abstract>
			<keywords>Bilingual lexicon, transliteration, word alignment, statistical machine translation, evaluation</keywords>
		</article>
		<article id="taln-2014-long-025" session="Traduction Automatique">
			<auteurs>
				<auteur>
					<prenom>Souhir</prenom>
					<nom>Gahbiche-Braham</nom>
					<email>souhir@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Hélène</prenom>
					<nom>Bonneau-Maynard</nom>
					<email>hbm@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>François</prenom>
					<nom>Yvon</nom>
					<email>yvon@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS, B.P. 133, F-91403 Orsay Cedex, France</affiliation>
				<affiliation affiliationId="2">Université Paris Sud</affiliation>
			</affiliations>
			<titre>Adaptation thématique pour la traduction automatique de dépêches de presse</titre>
			<type>long</type>
			<pages>280-291</pages>
			<resume>L’utilisation de méthodes statistiques en traduction automatique (TA) implique l’exploitation de gros corpus parallèles représentatifs de la tâche de traduction visée. La relative rareté de ces ressources fait que la question de l’adaptation au domaine est une problématique centrale en TA. Dans cet article, une étude portant sur l’adaptation thématique des données journalistiques issues d’une même source est proposée. Dans notre approche, chaque phrase d’un document est traduite avec le système de traduction approprié (c.-à-d. spécifique au thème dominant dans la phrase). Deux scénarios de traduction sont étudiés : (a) une classification manuelle, reposant sur la codification IPTC ; (b) une classification automatique. Nos expériences montrent que le scénario (b) conduit à des meilleures performances (à l’aune des métriques automatiques), que le scénario (a). L’approche la meilleure pour la métrique BLEU semble toutefois consister à ne pas réaliser d’adaptation ; on observe toutefois qu’adapter permet de lever certaines ambiguïtés sémantiques.</resume>
			<mots_cles>adaptation thématique, classification automatique, traduction automatique</mots_cles>
			<title>Topic Adaptation for the Automatic Translation of News Articles</title>
			<abstract>Statistical approaches used in machine translation (MT) require the availability of large parallel corpora for the task at hand. The relative scarcity of thes resources makes domain adaptation a central issue in MT. In this paper, a study of thematic adaptation for News texts is presented. All data are produced by the same source : News articles. In our approach, each sentence is translated with the appropriate translation system (specific to the dominant theme for the sentence). Two machine translation scenarios are considered : (a) a manual classification, based on IPTC codification ; (b) an automatic classification. Our experiments show that scenario (b) leads to better performance (in terms of automatic metrics) than scenario (a) . The best approach for the BLEU metric however seems to dispense with adaptation alltogether. Nonetheless, we observe that domain adaptation sometimes resolves some semantic ambiguities.</abstract>
			<keywords>domain adaptation, automatic classification, machine translation</keywords>
		</article>
		<article id="taln-2014-long-026" session="Traitement de corpus">
			<auteurs>
				<auteur>
					<prenom>Maxime</prenom>
					<nom>Amblard</nom>
					<email>maxime.amblard@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Karën</prenom>
					<nom>Fort</nom>
					<email>karen.fort@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Lorraine, LORIA, INRIA, CNRS UMR 7503, Vandoeuvre-lès-Nancy 54500 France</affiliation>
			</affiliations>
			<titre>Étude quantitative des disfluences dans le discours de schizophrènes : automatiser pour limiter les biais</titre>
			<type>long</type>
			<pages>292-303</pages>
			<resume>Nous présentons dans cet article les résultats d’expériences que nous avons menées concernant les disfluences dans le discours de patients schizophrènes (en remédiation). Ces expériences ont eu lieu dans le cadre d’une étude plus large recouvrant d’autres niveaux d’analyse linguistique, qui devraient aider à l’identification d’indices linguistiques conduisant au diagnostic de schizophrénie. Cette étude fait la part belle aux outils de traitement automatique des langues qui permettent le traitement rapide de grandes masses de données textuelles (ici, plus de 375 000 mots). La première phase de l’étude, que nous présentons ici, a confirmé la corrélation entre l’état schizophrène et le nombre de disfluences présentes dans le discours.</resume>
			<mots_cles>discours pathologique, schizophrénie, disfluences</mots_cles>
			<title>Quantitative study of disfluencies in schizophrenics' speech: Automatize to limit biases</title>
			<abstract>We present in this article the results of experiments we led concerning disfluencies in the discourse of schizophrenic patients (in remediation). These experiments are part of a larger study dealing with other levels of linguistic analysis, that could eventually help identifying clues leading to the diagnostic of the disease. This study largely relies on natural language processing tools, which allow for the rapid processing of massive textual data (here, more than 375,000 words). The first phase of the study, which we present here, confirmed the correlation between schizophrenia and the number of disfluences appearing in the discourse.</abstract>
			<keywords>pathological discourse, schizophrenia, disfluencies</keywords>
		</article>
		<article id="taln-2014-long-027" session="Traitement de corpus">
			<auteurs>
				<auteur>
					<prenom>Iris</prenom>
					<nom>Eshkol-Taravella</nom>
					<email>iris.eshkol@univ-orleans.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Natalia</prenom>
					<nom>Grabar</nom>
					<email>natalia.grabar@univ-lille3.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CNRS UMR 7270 LLL, Université d’Orléans, 45100 Orléans, France</affiliation>
				<affiliation affiliationId="2">CNRS UMR 8163 STL, Université Lille 3, 59653 Villeneuve d’Ascq, France</affiliation>
			</affiliations>
			<titre>Repérage et analyse de la reformulation paraphrastique dans les corpus oraux</titre>
			<type>long</type>
			<pages>304-315</pages>
			<resume>Notre travail porte sur la détection automatique de la reformulation paraphrastique dans les corpus oraux. L’approche proposée est une approche syntagmatique qui tient compte des marqueurs de reformulation paraphrastique et des spécificités de l’oral. L’annotation manuelle effectuée par deux annotateurs permet d’obtenir une description fine et multidimensionnelle des données de référence. Une méthode automatique est proposée afin de décider si les tours de parole comportent ou ne comportent pas des reformulations paraphrastiques. Les résultats obtenus montrent jusqu’à 66,4 % de précision. L’analyse de l’annotation manuelle indique qu’il existe peu de segments paraphrastiques avec des modifications morphologiques (flexion, dérivation ou composition) ou de segments qui montrent l’équivalence syntaxique.</resume>
			<mots_cles>Paraphrase, reformulation, corpus oral, marqueurs de reformulation paraphrastique</mots_cles>
			<title>Detection and Analysis of Paraphrastic Reformulations in Spoken Corpora</title>
			<abstract>Our work addresses the automatic detection of paraphrastic rephrasing in spoken corpus. The proposed approach is syntagmatic. It is based on paraphrastic rephrasing markers and the specificities of the spoken language. Manual annotation performed by two annotators provides fine-grained and multi-dimensional description of the reference data. Automatic method is proposed in order to decide whether sentences contain or not the paraphrases. The obtained results show up to 66.4% precision. The analysis of the manual annotations indicates that there are few cases in which paraphrastic segments show morphological modifications (inflection, derivation or compounding) or syntactic equivalence.</abstract>
			<keywords>Paraphrase, reformulation, spoken corpus, markers of paraphrastic rephrasing</keywords>
		</article>
		<article id="taln-2014-long-028" session="Traitement de corpus">
			<auteurs>
				<auteur>
					<prenom>Raja</prenom>
					<nom>Ayed</nom>
					<email>ayed.raja@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Ibrahim</prenom>
					<nom>Bounhas</nom>
					<email>bounhas.ibrahim@yahoo.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Bilel</prenom>
					<nom>Elayeb</nom>
					<email>bilel.elayeb@riadi.rnu.tn</email>
					<affiliationId>1</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Narjès</prenom>
					<nom>Bellamine Ben Saoud</nom>
					<email>narjes.bellamine@ensi.rnu.tn</email>
					<affiliationId>1</affiliationId>
					<affiliationId>4</affiliationId>
				</auteur>
				<auteur>
					<prenom>Fabrice</prenom>
					<nom>Evrard</nom>
					<email>fabrice.evrard@enseeiht.fr</email>
					<affiliationId>5</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire de recherche RIADI, ENSI, Université de la Manouba, 2010, Tunisie</affiliation>
				<affiliation affiliationId="2">Laboratoire de l'informatique pour les systèmes industriels, Institut Supérieur de Documentation, Université de la Manouba, 2010,Tunisie</affiliation>
				<affiliation affiliationId="3">Institut de technologies des Émirats, P.O. Box: 41009, Abu Dhabi, Émirats arabes unis</affiliation>
				<affiliation affiliationId="4">Institut supérieur de l’informatique, ISI, Université de Tunis El Manar, 1002, Tunisie</affiliation>
				<affiliation affiliationId="5">Institut de recherche en informatique de Toulouse (IRIT), 02 rue Camichel, 31071 Toulouse, France</affiliation>
			</affiliations>
			<titre>Evaluation d’une approche de classification possibiliste pour la désambiguïsation des textes arabes</titre>
			<type>long</type>
			<pages>316-327</pages>
			<resume>La désambiguïsation morphologique d’un mot arabe consiste à identifier l’analyse morphologique appropriée correspondante à ce mot. Dans cet article, nous présentons trois modèles de désambiguïsation morphologique de textes arabes non voyellés basés sur la classification possibiliste. Cette approche traite les données imprécises dans les phases d’apprentissage et de test, étant donné que notre modèle apprend à partir de données non étiquetés. Nous testons notre approche sur deux corpus, à savoir le corpus du Hadith et le Treebank Arabe. Ces corpus contiennent des données de types différents classiques et modernes. Nous comparons nos modèles avec des classifieurs probabilistes et statistiques. Pour ce faire, nous transformons la structure des ensembles d’apprentissage et de test pour remédier au problème d’imperfection des données.</resume>
			<mots_cles>Traitement Automatique des Langues Naturelles, Désambiguïsation Morphologique de l’Arabe, Théorie des Possibilités, Classification Possibiliste</mots_cles>
			<title>Evaluation of a possibilistic classification approach for Arabic texts disambiguation</title>
			<abstract>Morphological disambiguation of Arabic words consists in identifying their appropriate morphological analysis. In this paper, we present three models of morphological disambiguation of non-vocalized Arabic texts based on possibilistic classification. This approach deals with imprecise training and testing datasets, as we learn from untagged texts. We experiment our approach on two corpora i.e. the Hadith corpus and the Arabic Treebank. These corpora contain data of different types: traditional and modern. We compare our models to probabilistic and statistical classifiers. To do this, we transform the structure of the training and the test sets to deal with imprecise data.</abstract>
			<keywords>Natural Language Processing, Arabic Morphological Disambiguation, Possibility Theory, Possibilistic Classification</keywords>
		</article>
		<article id="taln-2014-long-029" session="Parsing 2">
			<auteurs>
				<auteur>
					<prenom>Benoît</prenom>
					<nom>Crabbé</nom>
					<email>bcrabbe@linguist.univ-paris-diderot.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ALPAGE, INRIA, Université Paris Diderot Place Paul Ricoeur , 75013 Paris</affiliation>
			</affiliations>
			<titre>Un analyseur discriminant de la famille LR pour l’analyse en constituants</titre>
			<type>long</type>
			<pages>328-339</pages>
			<resume>On propose un algorithme original d’analyse syntaxique déterministe en constituants pour le langage naturel inspiré de LR (Knuth, 1965). L’algorithme s’appuie sur un modèle d’apprentissage discriminant pour réaliser la désambiguisation (Collins, 2002). On montre que le modèle discriminant permet de capturer plus finement de l’information morphologique présente dans les données, ce qui lui permet d’obtenir des résultats état de l’art en temps comme en exactitude pour l’analyse syntaxique du français.</resume>
			<mots_cles>Analyse guidée par les têtes, analyse LR, temps linéaire, modèle discriminant, inférence approximative</mots_cles>
			<title>A discriminative parser of the LR family for phrase structure parsing</title>
			<abstract>We provide a new weighted parsing algorithm for deterministic context free grammar parsing inspired by LR (Knuth, 1965). The parser is weighted by a discriminative model that allows determinism (Collins, 2002).We show that the discriminative model allows to take advantage of morphological information available in the data, hence allowing to achieve state of the art results both in time and in accurracy for parsing French.</abstract>
			<keywords>Head driven parsing, LR parsing, linear time, discriminative modelling, approximate inference</keywords>
		</article>
		<article id="taln-2014-long-030" session="Parsing 2">
			<auteurs>
				<auteur>
					<prenom>Jean-Philippe</prenom>
					<nom>Fauconnier</nom>
					<email>Jean-Philippe.Fauconnier@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Laurent</prenom>
					<nom>Sorin</nom>
					<email>Laurent.Sorin@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mouna</prenom>
					<nom>Kamel</nom>
					<email>Mouna.Kamel@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mustapha</prenom>
					<nom>Mojahid</nom>
					<email>Mustapha.Mojahid@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Nathalie</prenom>
					<nom>Aussenac-Gilles</nom>
					<email>Nathalie.Aussenac-Gilles@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">IRIT, Université Paul Sabatier, 118 Route de Narbonne, 31062 Toulouse Cedex 9</affiliation>
			</affiliations>
			<titre>Détection automatique de la structure organisationnelle de documents à partir de marqueurs visuels et lexicaux</titre>
			<type>long</type>
			<pages>340-351</pages>
			<resume>La compréhension d’un texte s’opère à travers les niveaux d’information visuelle, logique et discursive, et leurs relations d’interdépendance. La majorité des travaux ayant étudié ces relations a été menée dans le cadre de la génération de textes, où les propriétés visuelles sont inférées à partir des éléments logiques et discursifs. Les travaux présentés ici adoptent une démarche inverse en proposant de générer automatiquement la structure organisationnelle du texte (structure logique) à partir de sa forme visuelle. Le principe consiste à (i) labelliser des blocs visuels par apprentissage afin d’obtenir des unités logiques et (ii) relier ces unités par des relations de coordination ou de subordination pour construire un arbre. Pour ces deux tâches, des Champs Aléatoires Conditionnels et un Maximum d’Entropie sont respectivement utilisés. Après apprentissage, les résultats aboutissent à une exactitude de 80,46% pour la labellisation et 97,23% pour la construction de l’arbre.</resume>
			<mots_cles>discours, structure organisationnelle, mise en forme matérielle, marqueurs métadiscursifs, champs aléatoires conditionnels, maximum d’entropie</mots_cles>
			<title>Automatic Detection of Document Organizational Structure from Visual and Lexical Markers</title>
			<abstract>The process of understanding a document uses both visual, logic and discursive information along with the mutual relationships between those levels. Most approaches studying those relationships were conducted in the frame of text generation, where the text visual properties are infered from logical and discursive elements. We chose in our work to take the opposite path by trying to infer the logical structure of texts using their visual forms. To do so, we (i) assign a logical label to each visual block and (ii) we try to connect those logical units with coordination or subordination relationships, in order to build a logical tree. We used respectively a Conditional Random Fields and a Maximum Entropy algorithms for those two tasks. After a learning phase, the obtained models give us a 80,46% accuracy for task (i) and a 97,23% accuracy for task (ii).</abstract>
			<keywords>discourse, organizational structure, text formating, metadiscursive markers, conditional random fields, maximum entropy</keywords>
		</article>
		<article id="taln-2014-long-031" session="Parsing 2">
			<auteurs>
				<auteur>
					<prenom>Jean-Philippe</prenom>
					<nom>Prost</nom>
					<email>Prost@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIRMM, CNRS – Université Montpellier 2, 161 rue Ada, 34090 Montpellier, France</affiliation>
			</affiliations>
			<titre>Jugement exact de grammaticalité d’arbre syntaxique probable</titre>
			<type>long</type>
			<pages>352-362</pages>
			<resume>La robustesse de l’analyse probabiliste s’obtient généralement au détriment du jugement de grammaticalité sur la phrase analysée. Les analyseurs comme le Stanford Parser, ou les Reranking Parsers ne sont, en effet, pas capables de dissocier une analyse probable grammaticale d’une analyse probable erronée, et ce qu’elle porte sur une phrase elle-même grammaticale ou non. Dans cet article nous montrons que l’adoption d’une représentation syntaxique basée sur la théorie logique des modèles, accompagnée d’une structure syntaxique classique (par exemple de type syntagmatique), est de nature à permettre la résolution exacte de différents problèmes tels que celui du jugement de grammaticalité. Afin de démontrer la praticité et l’utilité d’une alliance entre symbolique et stochastique, nous nous appuyons sur une représentation de la syntaxe par modèles, ainsi que sur une grammaire de corpus, pour présenter une méthode de résolution exacte pour le jugement de grammaticalité d’un arbre syntagmatique probable. Nous présentons des résultats expérimentaux sur des arbres issus d’un analyseur probabiliste, qui corroborent l’intérêt d’une telle alliance.</resume>
			<mots_cles>Jugement de grammaticalité, syntaxe par modèles, Grammaires de Propriétés, analyse syntaxique probabiliste</mots_cles>
			<title>Exact Grammaticality of Likely Parse Tree</title>
			<abstract>The robustness of probabilistic parsing generally comes at the expense of grammaticality judgment – the grammaticality of the most probable output parse remaining unknown. Parsers, such as the Stanford or the Reranking ones, can not discriminate between grammatical and ungrammatical probable parses, whether their surface realisations are themselves grammatical or not. In this paper we show that a Model-Theoretic representation of Syntax alleviates the grammaticality judgment on a parse tree. In order to demonstrate the practicality and usefulness of an alliance between stochastic parsing and knowledge-based representation, we introduce an exact method for putting a binary grammatical judgment on a probable phrase structure. We experiment with parse trees generated by a probabilistic parser. We show experimental evidence on parse trees generated by a probabilistic parser to confirm our hypothesis.</abstract>
			<keywords>Grammaticality judgement, Model-Theoretic Syntax, Property Grammar, probabilistic syntactic parsing</keywords>
		</article>
		<article id="taln-2014-long-032" session="Lexique 3">
			<auteurs>
				<auteur>
					<prenom>Mokhtar-Boumeyden</prenom>
					<nom>Billami</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>José</prenom>
					<nom>Camacho-Collados</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Evelyne</prenom>
					<nom>Jacquey</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Laurence</prenom>
					<nom>Kister</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
			</affiliations>
			<titre>Annotation sémantique et validation terminologique en texte intégral en SHS</titre>
			<type>long</type>
			<pages>363-376</pages>
			<resume>Nos travaux se focalisent sur la validation d'occurrences de candidats termes en contexte. Les contextes d'occurrences proviennent d'articles scientifiques des sciences du langage issus du corpus SCIENTEXT1. Les candidats termes sont identifiés par l'extracteur automatique de termes de la plate-forme TTC-TermSuite et sont ensuite projetés dans les textes. La problématique générale de cet article est d'étudier dans quelle mesure les contextes sont à même de fournir des critères linguistiques pertinents pour valider ou rejeter chaque occurrence de candidat terme selon qu'elle relève d'un usage terminologique en sciences du langage ou non (langue générale, transdisciplinaire, autre domaine scientifique). Pour répondre à cette question, nous comparons deux méthodes d'exploitation (l'une inspirée de la textométrie et l'autre de Lesk) avec des contextes d'occurrences du même corpus annotés manuellement et mesurons si une annotation sémantique des contextes améliore l'exactitude des choix réalisés automatiquement.</resume>
			<mots_cles>Annotation sémantique, extraction et désambiguïsation terminologique, textométrie, texte intégral</mots_cles>
			<title>Semantic Annotation and Terminology Validation in full scientific articles in Social Sciences and Humanities</title>
			<abstract>Our work is in the field of the validation of term candidates occurrences in context. The textual data used in this article comes from the freely available corpus SCIENTEXT. The term candidates are computed by the platform TTC-TermSuite and their occurrences are projected in the texts. The main issue of this article is to examine how contexts are able to provide relevant linguistic criteria to validate or reject each occurrence of term candidates according to the distinction between a terminological and a non terminological use (general language, transdisciplinary use, use coming from another science). To answer this question, we compare two methods (a textometric one and another inspired from Lesk) with the manual annotation of the same corpus and we evaluate if a semantic annotation of contexts improves the accuracy of the choices made automatically.</abstract>
			<keywords>Semantic Annotation, Terminological Extraction and Disambiguation, Textual Metrics (Specificity), Full Text</keywords>
		</article>
		<article id="taln-2014-long-033" session="Lexique 3">
			<auteurs>
				<auteur>
					<prenom>Charlotte</prenom>
					<nom>Roze</nom>
					<email>charlotte.roze@unicaen.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Thierry</prenom>
					<nom>Charnois</nom>
					<email>thierry.charnois@lipn.univ-paris13.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Dominique</prenom>
					<nom>Legallois</nom>
					<email>dominique.legallois@unicaen.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Stéphane</prenom>
					<nom>Ferrari</nom>
					<email>stephane.ferrari@unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mathilde</prenom>
					<nom>Salles</nom>
					<email>mathilde.salles@unicaen.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GREYC, Université de Caen Basse-Normandie, Campus 2, 14000 Caen, France</affiliation>
				<affiliation affiliationId="2">CRISCO, Université de Caen Basse-Normandie, Campus 1, 14000 Caen, France</affiliation>
				<affiliation affiliationId="3">LIPN, Université Paris 13 Sorbonne Paris Cité, 93430 Villetaneuse, France</affiliation>
			</affiliations>
			<titre>Identification des noms sous-spécifiés, signaux de l’organisation discursive</titre>
			<type>long</type>
			<pages>377-388</pages>
			<resume>Dans cet article, nous nous intéressons aux noms sous-spécifiés, qui forment une classe d’indices de l’organisation discursive. Ces indices ont été peu étudiés dans le cadre de l’analyse du discours et en traitement automatique des langues. L’objectif est d’effectuer une étude linguistique de leur participation à la structuration discursive, notamment lorsqu’ils interviennent dans des séquences organisationnelles fréquentes (e.g. le patron Problème-Solution). Dans cet article, nous présentons les différentes étapes mises en oeuvre pour identifier automatiquement ces noms en corpus. En premier lieu, nous détaillons la construction d’un lexique de noms sous-spécifiés pour le français à partir d’un corpus constitué de 7 années du journal Le Monde. Puis nous montrons comment utiliser des techniques fondées sur la fouille de données séquentielles pour acquérir de nouvelles constructions syntaxiques caractéristiques des emplois de noms sousspécifiés. Enfin, nous présentons une méthode d’identification automatique des occurrences de noms sous-spécifiés et son évaluation.</resume>
			<mots_cles>noms sous-spécifiés, motifs séquentiels, structure discursive</mots_cles>
			<title>Identification of Shell Nouns, Signals of Discourse Organisation</title>
			<abstract>In this paper, we focus on shell nouns, a class of items involved in the signaling of discourse organisation. These signals have been little studied in Natural Language Processing and within discourse analysis theories. The main goal is to study their participation to discourse organisation, especially when they occur in Problem-Solution patterns. In this paper, we present the different steps involved in shell nouns identification of these nouns. First, we present the lexical acquisition of shell nouns from a large corpus. Second, we show how a method based on the extraction of sequential patterns (sequential data mining techniques) allows to discover new syntactic patterns specific to the use of shell nouns. Finally, we present a shell nouns identification system that we evaluate.</abstract>
			<keywords>shell nouns, sequential patterns, discourse structure</keywords>
		</article>
		<article id="taln-2014-court-001" session="Traduction">
			<auteurs>
				<auteur>
					<prenom>Laurent</prenom>
					<nom>Besacier</nom>
					<email>laurent.besacier@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIG, Université de Grenoble, UJF - BP 53, 38041 Grenoble Cedex 9</affiliation>
			</affiliations>
			<titre>Traduction automatisée d’une oeuvre littéraire: une étude pilote</titre>
			<type>court</type>
			<pages>389-394</pages>
			<resume>Les techniques actuelles de traduction automatique (TA) permettent de produire des traductions dont la qualité ne cesse de croitre. Dans des domaines spécifiques, la post-édition (PE) de traductions automatiques permet, par ailleurs, d’obtenir des traductions de qualité relativement rapidement. Mais un tel pipeline (TA+PE) est il envisageable pour traduire une oeuvre littéraire ? Cet article propose une ébauche de réponse à cette question. Un essai de l’auteur américain Richard Powers, encore non disponible en français, est traduit automatiquement puis post-édité et révisé par des traducteurs non-professionnels. La plateforme de post-édition du LIG utilisée permet de lire et éditer l’oeuvre traduite en français continuellement, suggérant (pour le futur) une communauté de lecteurs-réviseurs qui améliorent en continu les traductions de leur auteur favori. En plus de la présentation des résultats d’évaluation expérimentale du pipeline TA+PE (système de TA utilisé, scores automatiques), nous discutons également la qualité de la traduction produite du point de vue d’un panel de lecteurs (ayant lu la traduction en français, puis répondu à une enquête). Enfin, quelques remarques du traducteur français de R. Powers, sollicité à cette occasion, sont présentées à la fin de cet article.</resume>
			<mots_cles>traduction automatique, TA, oeuvre littéraire, post-édition</mots_cles>
			<title>Machine translation for litterature: a pilot study</title>
			<abstract>Current machine translation (MT ) techniques are continuously improving. In specific areas, post-editing (PE) allows to obtain high-quality translations relatively quickly. But is such a pipeline (MT+PE) usable to translate a literary work (fiction, short story) ? This paper tries to bring a preliminary answer to this question. A short story by American writer Richard Powers, still not available in French, is automatically translated and post-edited and then revised by nonprofessional translators. The LIG post-editing platform allows to read and edit the short story suggesting (for the future) a community of readers-editors that continuously improve the translations of their favorite author. In addition to presenting experimental evaluation results of the pipeline MT+PE (MT system used, auomatic evaluation), we also discuss the quality of the translation output from the perspective of a panel of readers (who read the translated short story in French, and answered to a survey afterwards). Finally, some remarks of the official french translator of R. Powers, requested on this occasion, are given at the end of this article.</abstract>
			<keywords>machine translation, MT, litterature, fiction, post-edition</keywords>
		</article>
		<article id="taln-2014-court-002" session="Traduction">
			<auteurs>
				<auteur>
					<prenom>Li</prenom>
					<nom>Gong</nom>
					<email>li.gong@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Aurélien</prenom>
					<nom>Max</nom>
					<email>aurelien.max@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>François</prenom>
					<nom>Yvon</nom>
					<email>francois.yvon@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS, Orsay, France</affiliation>
				<affiliation affiliationId="2">Univ. Paris-Sud, Orsay, France</affiliation>
			</affiliations>
			<titre>Vers un développement plus efficace des systèmes de traduction statistique : un peu de vert dans un monde de BLEU</titre>
			<type>court</type>
			<pages>395-400</pages>
			<resume>Dans cet article, nous montrons comment l’utilisation conjointe d’une technique d’alignement de phrases parallèles à la demande et d’estimation de modèles de traduction à la volée permet une réduction en temps très notable (jusqu’à 93% dans nos expériences) par rapport à un système à l’état de l’art, tout en offrant un compromis en termes de qualité très intéressant dans certaines configurations. En particulier, l’exploitation immédiate de documents traduits permet de compenser très rapidement l’absence d’un corpus de développement.</resume>
			<mots_cles>traduction automatique statistique, développement efficace, temps de calcul</mots_cles>
			<title>Towards a More Efficient Development of Statistical Machine Translation Systems</title>
			<abstract>In this article, we show how using both on-demand alignment of parallel sentences and on-the-fly estimation of translation models can yield massive reduction (up to 93% in our experiments) in development time as compared to a state-of-the-art system, while offering an interesting tradeoff as regards translation quality under some configurations. We show in particular that the absence of a development set can be quickly compensated by immediately using translated documents.</abstract>
			<keywords>statistical machine translation, efficient development, computation time</keywords>
		</article>
		<article id="taln-2014-court-003" session="Traduction">
			<auteurs>
				<auteur>
					<prenom>Yidong</prenom>
					<nom>Chen</nom>
					<email>ydchen@xmu.edu.cn</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Lingxiao</prenom>
					<nom>Wang</nom>
					<email></email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Christian</prenom>
					<nom>Boitet</nom>
					<email></email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Xiaodong</prenom>
					<nom>Shi</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">School of Information Science and Technology, Xiamen University, Xiamen, Fujian, China</affiliation>
				<affiliation affiliationId="2">GETALP, Laboratoire d’Informatique Grenoble (LIG), Université Joseph Fourier, Grenoble, France</affiliation>
			</affiliations>
			<titre></titre>
			<type>court</type>
			<pages>401-406</pages>
			<resume>Nous présentons un projet collaboratif en cours mené par l'université de Grenoble et l'université de Xiamen, et visant à créer des instances d'un nouveau type de système de traduction automatique statistique utilisant des ressources lexico-sémantiques et discursives. Le but concret est de développer des systèmes de TAS chinois-français pour des sites boursiers et économiques. Comme très peu de corpus et de dictionnaires bilingues chinois-français sont disponibles sur Internet, l'anglais est utilisé comme "pivot" pour construire les équivalents chinois-français par transitivité. Outre la description générale de ce projet, nous décrivons les progrès sur deux axes de recherche liés à ce projet. Pour cela, nous utilisons une méthode, proposée par XMU, d'induction de probabilité fondée sur la similarité thématique, qui produit des tables de traduction C-F à partir de tables de traduction C-E et E-F. Pour disposer de bons corpus parallèles C-F, nous utilisons un système Web de post-édition collaborative qui peut déclencher l'amélioration incrémentale du système de TA en utilisant des métriques d'évaluation de TA et en extrayant la "meilleure partie" de la mémoire de traductions courante.</resume>
			<mots_cles>traduction automatique statistique (SMT), chinois-français, domaine économique</mots_cles>
			<title>On-going Cooperative Research towards Developing Economy-Oriented Chinese-French SMT Systems with a New SMT Framework</title>
			<abstract>We present an on-going collaborative project pursued by Grenoble University and Xiamen University and aiming at creating instances of a new kind of SMT system using semantics and discourse-related resources. The concrete goal is to develop Chinese-French systems specialized to stock option and economic websites. Since very few Chinese-French bilingual corpora and dictionaries are freely available on Internet, English is used as a “pivot” for constructing the Chinese-French translation equivalents by transitivity. For this, we use a method, proposed by XMU, of probability induction based on topic similarity, which produces C-F translation tables from C-E and E-F translation tables. For getting good C-F parallel corpora, we use a web-based collaborative post-editing system that can trigger the incremental improvement of the MT system by using MT evaluation metrics and extracting the "best part" of the current translation memory.</abstract>
			<keywords>SMT, Chinese-French, Economic Domain</keywords>
		</article>
		<article id="taln-2014-court-004" session="Lexique 1">
			<auteurs>
				<auteur>
					<prenom>Juan Antonio</prenom>
					<nom>Lossio-Ventura</nom>
					<email>juan.lossio@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Clement</prenom>
					<nom>Jonquet</nom>
					<email>clement.jonquet@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mathieu</prenom>
					<nom>Roche</nom>
					<email>mathieu.roche@cirad.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Maguelonne</prenom>
					<nom>Teisseire</nom>
					<email>teisseire@teledetection.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIRMM, Université de Montpellier 2, CNRS, Montpellier - France</affiliation>
				<affiliation affiliationId="2">Irstea, Cirad, TETIS, Montpellier - France</affiliation>
			</affiliations>
			<titre>Extraction automatique de termes combinant différentes informations</titre>
			<type>court</type>
			<pages>407-412</pages>
			<resume>Pour une communauté, la terminologie est essentielle car elle permet de décrire, échanger et récupérer les données. Dans de nombreux domaines, l’explosion du volume des données textuelles nécessite de recourir à une automatisation du processus d’extraction de la terminologie, voire son enrichissement. L’extraction automatique de termes peut s’appuyer sur des approches de traitement du langage naturel. Des méthodes prenant en compte les aspects linguistiques et statistiques proposées dans la littérature, résolvent quelques problèmes liés à l’extraction de termes tels que la faible fréquence, la complexité d’extraction de termes de plusieurs mots, ou l’effort humain pour valider les termes candidats. Dans ce contexte, nous proposons deux nouvelles mesures pour l’extraction et le “ranking” des termes formés de plusieurs mots à partir des corpus spécifiques d’un domaine. En outre, nous montrons comment l’utilisation du Web pour évaluer l’importance d’un terme candidat permet d’améliorer les résultats en terme de précision. Ces expérimentations sont réalisées sur le corpus biomédical GENIA en utilisant des mesures de la littérature telles que C-value.</resume>
			<mots_cles>Extraction Automatique de Termes, Mesure basée sur le Web, Mesure Linguistique, Mesure Statistique, Traitement Automatique du Langage Biomédical</mots_cles>
			<title>Automatic Term Extraction Combining Different Information</title>
			<abstract>Comprehensive terminology is essential for a community to describe, exchange, and retrieve data. In multiple domain, the explosion of text data produced has reached a level for which automatic terminology extraction and enrichment is mandatory. Automatic Term Extraction (or Recognition) methods use natural language processing to do so. Methods featuring linguistic and statistical aspects as often proposed in the literature, rely some problems related to term extraction as low frequency, complexity of the multi-word term extraction, human effort to validate candidate terms. In contrast, we present two new measures for extracting and ranking muli-word terms from domain-specific corpora, covering the all mentioned problems. In addition we demonstrate how the use of the Web to evaluate the significance of a multi-word term candidate, helps us to outperform precision results obtain on the biomedical GENIA corpus with previous reported measures such as C-value.</abstract>
			<keywords>Automatic Term Extraction, Web-based measure, Linguistic-based measure, Statistic-based measure, Biomedical Natural Language Processing</keywords>
		</article>
		<article id="taln-2014-court-005" session="Lexique 1">
			<auteurs>
				<auteur>
					<prenom>Gilles</prenom>
					<nom>Boyé</nom>
					<email>gboye@u-bordeaux-montaigne.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Anna</prenom>
					<nom>Kupsc</nom>
					<email>akupsc@u-bordeaux-montaigne.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Bordeaux-Montaigne, Domaine universitaire, 33607 Pessac Cedex CLLE-ERSS, UMR5263, CNRS, 5 allées Antonio Machado, 31058 Toulouse Cedex 9</affiliation>
			</affiliations>
			<titre>Analyse automatique d’espaces thématiques</titre>
			<type>court</type>
			<pages>413-418</pages>
			<resume>Basé sur les calculs d’entropie conditionnelle de (Bonami &amp; Boyé, à paraître), nous proposons un analyseur automatique de la flexion dans le cadre de la morphologie thématique qui produit le graphe de régularités du paradigme. Le traitement se base sur un lexique de 6440 verbes extraits du BDLex (de Calmès &amp; Pérennou, 1998) associés à leurs fréquences dans Lexique3 (New et al., 2001). L’algorithme se compose de trois éléments : calcul de l’entropie conditionnelle entre paires de formes fléchies, distillation des paradigmes, construction du graphe de régularités. Pour l’entropie, nous utilisons deux modes de calcul différents, l’un se base sur la distribution de l’effectif des verbes entre leurs différentes options, l’autre sur la distribution des lexèmes verbaux en fonction de leurs fréquences pour contrebalancer l’influence des verbes ultra-fréquents sur les calculs.</resume>
			<mots_cles>morphologie flexionnelle, espaces thématiques, graphe des régularité, français, verbes</mots_cles>
			<title>Automated Analysis for Stem Spaces: the case of French verbs</title>
			<abstract>Based on the entropy calculations of (Bonami &amp; Boyé, à paraître), we propose an automatic analysis of inflection couched in the stem spaces framework. Our treatment is based on a lexicon of 6440 verbs present in BDLex (de Calmès &amp; Pérennou, 1998) and associated with their frequencies from Lexique3 (New et al., 2001). The algorithm we propose consists in three steps : computing conditional entropy between all pairs of inflected forms, distilling the paradigms and constructing a regularity graph. For computing entropy, we use two methods : the first one is based on count of verbs in a given distribution whereas the second one takes into account the frequency of each verbal lemma in the distribution to compensate for the bias introduced by the ultra-frequent verbs in the calculation.</abstract>
			<keywords>Inflectional morphology, stem spaces, regularity graph, French, verbs</keywords>
		</article>
		<article id="taln-2014-court-006" session="Lexique 1">
			<auteurs>
				<auteur>
					<prenom>Sandra</prenom>
					<nom>Milena Castellanos Páez</nom>
					<email>sandra.castellanos@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIG - GETALP. Grenoble, France</affiliation>
			</affiliations>
			<titre>Extraction et représentation des constructions à verbe support en espagnol</titre>
			<type>court</type>
			<pages>419-424</pages>
			<resume>Le traitement informatique de constructions à verbe support (prendre une photo, faire une présentation) est une tâche difficile en TAL. Cela est également vrai en espagnol, où ces constructions sont fréquentes dans les textes, mais ne font pas souvent partie des lexiques exploitables par une machine. Notre objectif est d'extraire des constructions à verbe support à partir d’un très grand corpus de l'espagnol. Nous peaufinons un ensemble de motifs morphosyntaxiques fondés sur un grand nombre de verbe support possibles. Ensuite, nous filtrons cette liste en utilisant des seuils et des mesures d'association. Bien que tout à fait classique, cette méthode permet l'extraction de nombreuses expressions de bonne qualité. À l’avenir, nous souhaitons étudier les représentations sémantiques de ces constructions dans des lexiques multilingues.</resume>
			<mots_cles>Expressions à verbe support, extraction, corpus, expressions polylexicales</mots_cles>
			<title>Extraction and representation of support verb constructions in Spanish</title>
			<abstract>The computational treatment of support verb constructions (take a picture, make a presentation) is a challenging task in NLP. This is also true in Spanish, where these constructions are frequent in texts, but not frequently included in machine-readable lexicons. Our goal is to extract support verb constructions from a very large corpus of Spanish. We fine-tune a set of morpho-syntactic patterns based on a large set of possible support verbs. Then, we filter this list using thresholds and association measures. While quite standard, this methodology allows the extraction of many good-quality expressions. As future work, we would like to investigate semantic representations for these constructions in multilingual lexicons.</abstract>
			<keywords>Support verb expressions, extraction, corpus, multiword expressions</keywords>
		</article>
		<article id="taln-2014-court-007" session="Lexique 1">
			<auteurs>
				<auteur>
					<prenom>Benoît</prenom>
					<nom>Sagot</nom>
					<email>Benoit.Sagot@inria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Laurence</prenom>
					<nom>Danlos</nom>
					<email>Laurence.Danlos@inria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Margot</prenom>
					<nom>Colinet</nom>
					<email>Margot.Colinet@inria.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Alpage, INRIA &amp; Université Paris-Diderot, 75013 Paris</affiliation>
				<affiliation affiliationId="2">LLF, CNRS &amp; Université Paris-Diderot, 75013 Paris</affiliation>
			</affiliations>
			<titre>Sous-catégorisation en pour et syntaxe lexicale</titre>
			<type>court</type>
			<pages>425-430</pages>
			<resume>La sous-catégorisation d’arguments introduits par la préposition pour a été sous-étudiée par le passé, comme en témoigne l’incomplétude des ressources lexico-syntaxiques existantes sur ce point. Dans cet article, nous présentons rapidement les différents types de sous-catégorisation en pour, qui contrastent avec les emplois de pour comme connecteur de discours. Nous décrivons l’intégration des arguments en pour au lexique syntaxique Lefff , enrichissant ainsi les informations de sous-catégorisation de nombreuses entrées verbales, nominales, adjectivales et adverbiales.</resume>
			<mots_cles>Sous-catégorisation, Arguments en pour, Lexiques syntaxiques</mots_cles>
			<title>Sub-categorization in 'pour' and lexical syntax</title>
			<abstract>Sub-categorized arguments introduced by the French preposition pour has been under-studied in previous work, as can be seen from the incompleteness of existing lexical-syntactic resources in that regard. In this paper, we briefly introduce the various types of sub-categorization in pour, which are to be distinguished from occurrences of pour as a discourse connective. We describe how we added arguments in pour within the syntactic lexicon Lefff , thus refining sub-categorization information for many verbal, nominal, adjectival and adverbial entries.</abstract>
			<keywords>Sub-categorization, Arguments in pour, Syntactic lexicons</keywords>
		</article>
		<article id="taln-2014-court-008" session="Étiquetage 1">
			<auteurs>
				<auteur>
					<prenom>Ingrid</prenom>
					<nom>Falk</nom>
					<email>ifalk@unistra.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Delphine</prenom>
					<nom>Bernhard</nom>
					<email>dbernhard@unistra.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Christophe</prenom>
					<nom>Gérard</nom>
					<email>christophegerard@unistra.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Romain</prenom>
					<nom>Potier-Ferry</nom>
					<email>romainpotierferry@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LiLPa, Université de Strasbourg</affiliation>
			</affiliations>
			<titre>Étiquetage morpho-syntaxique pour des mots nouveaux</titre>
			<type>court</type>
			<pages>431-436</pages>
			<resume>Les outils d’étiquetage automatique sont plus ou moins robustes en ce qui concerne l’étiquetage de mots inconnus, non rencontrés dans le corpus d’apprentissage. Il est important de connaître de manière précise la performance de ces outils lorsqu’on cible plus particulièrement l’étiquetage de néologismes formels. En effet, la catégorie grammaticale constitue un critère important à la fois pour leur identification et leur documentation. Nous présentons une évaluation et une comparaison de 7 étiqueteurs morphosyntaxiques du français, à partir d’un corpus issu du Wiktionnaire. Les résultats montrent que l’utilisation de traits de forme ou morphologiques est favorable à l’étiquetage correct des mots nouveaux.</resume>
			<mots_cles>étiquetage morphosyntaxique, évaluation, néologie formelle</mots_cles>
			<title>Part of Speech Tagging for New Words</title>
			<abstract>Part-of-speech (POS) taggers are more or less robust with respect to the labeling of unknown words not found in the training corpus. It is important to know precisely how these tools perfom when we target part-of-speech tagging for formal neologisms. Indeed, grammatical category is an important criterion for both their identification and documentation. We present an evaluation and comparison of 7 POS taggers for French, based on a corpus built from Wiktionary. The results show that the use of form-related or morphological features supports the accurate tagging of new words.</abstract>
			<keywords>part-of-speech tagging, evaluation, formal neologisms</keywords>
		</article>
		<article id="taln-2014-court-009" session="Étiquetage 1">
			<auteurs>
				<auteur>
					<prenom>Benoît</prenom>
					<nom>Sagot</nom>
					<email>Benoit.Sagot@inria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Kata</prenom>
					<nom>Gábor</nom>
					<email>Kata.Gabor@inria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Alpage, INRIA &amp; Université Paris-Diderot, 75013 Paris</affiliation>
			</affiliations>
			<titre>Détection et correction automatique d’entités nommées dans des corpus OCRisés</titre>
			<type>court</type>
			<pages>437-442</pages>
			<resume>La correction de données textuelles obtenues par reconnaissance optique de caractères (OCR) pour atteindre une qualité éditoriale reste aujourd’hui une tâche coûteuse, car elle implique toujours une intervention humaine. La détection et la correction automatiques d’erreurs à l’aide de modèles statistiques ne permettent de traiter de façon utile que les erreurs relevant de la langue générale. C’est pourtant dans certaines entités nommées que résident les erreurs les plus nombreuses, surtout dans des données telles que des corpus de brevets ou des textes juridiques. Dans cet article, nous proposons une architecture d’identification et de correction par règles d’un large éventail d’entités nommées (non compris les noms propres). Nous montrons que notre architecture permet d’atteindre un bon rappel et une excellente précision en correction, ce qui permet de traiter des fautes difficiles à traiter par les approches statistiques usuelles.</resume>
			<mots_cles>OCR, Entités nommées, Détection d’erreurs, Correction d’erreurs</mots_cles>
			<title>Named Entity Recognition and Correction in OCRized Corpora</title>
			<abstract>Correction of textual data obtained by optical character recognition (OCR) for reaching editorial quality is an expensive task, as it still involves human intervention. The coverage of statistical models for automated error detection and correction is inherently limited to errors that resort to general language. However, a large amount of errors reside in domain-specific named entities, especially when dealing with data such as patent corpora or legal texts. In this paper, we propose a rule-based architecture for the identification and correction of a wide range of named entities (proper names not included). We show that our architecture achieves a good recall and an excellent correction accuracy on error types that are difficult to adress with statistical approaches.</abstract>
			<keywords>OCR, Named entities, Error Detection, Error Correction</keywords>
		</article>
		<article id="taln-2014-court-010" session="Étiquetage 1">
			<auteurs>
				<auteur>
					<prenom>Amine</prenom>
					<nom>Chennoufi</nom>
					<email>chennoufi.amin@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Azzeddine</prenom>
					<nom>Mazroui</nom>
					<email>azze.mazroui@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Mohamed I / Faculté des Sciences / Département de Mathématiques et Informatiques Oujda, Maroc</affiliation>
			</affiliations>
			<titre>Méthodes de lissage d’une approche morpho-statistique pour la voyellation automatique des textes arabes</titre>
			<type>court</type>
			<pages>443-448</pages>
			<resume>Nous présentons dans ce travail un nouveau système de voyellation automatique des textes arabes en utilisant trois étapes. Durant la première phase, nous avons intégré une base de données lexicale contenant les mots les plus fréquents de la langue arabe avec l’analyseur morphologique AlKhalil Morpho Sys pour fournir les voyellations possibles pour chaque mot. Le second module dont l’objectif est d’éliminer l'ambiguïté repose sur une approche statistique dont l’apprentissage a été effectué sur un corpus constitué de textes de livres arabes et utilisant les modèles de Markov cachés (HMM) où les mots non voyellés représentent les états observés et les mots voyellés sont ses états cachés. Le système utilise les techniques de lissage pour contourner le problème des transitions des mots absentes et l'algorithme de Viterbi pour sélectionner la solution optimale. La troisième étape utilise un modèle HMM basé sur les caractères pour traiter le cas des mots non analysés.</resume>
			<mots_cles>Langue arabe, voyellation automatique, analyse morphologique, modèle de Markov caché, corpus, lissage, algorithme de Viterbi</mots_cles>
			<title>Smoothing methods for a morpho-statistical approach of automatic diacritization Arabic texts</title>
			<abstract>We present in this work a new approach for the Automatic diacritization for Arabic texts using three stages. During the first phase, we integrated a lexical database containing the most frequent words of Arabic with morphological analysis by Alkhalil Morpho Sys which provided possible diacritization for each word. The objective of the second module is to eliminate the ambiguity using a statistical approach in which the learning phase was performed on a corpus composed by several Arabic books. This approach uses the hidden Markov models (HMM) with Arabic unvowelized words taken as observed states and vowelized words are considered as hidden states. The system uses smoothing techniques to circumvent the problem of unseen word transitions in the corpus and the Viterbi algorithm to select the optimal solution. The third step uses a HMM model based on the characters to deal with the case of unanalyzed words.</abstract>
			<keywords>Arabic language, Automatic diacritization, morphological analysis, hidden Markov model, corpus, smoothing, Viterbi Algorithm</keywords>
		</article>
		<article id="taln-2014-court-011" session="Traitement de corpus 1">
			<auteurs>
				<auteur>
					<prenom>Mathieu-Henri</prenom>
					<nom>Falco</nom>
					<email>Mathieu-Henri.Falco@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Véronique</prenom>
					<nom>Moriceau</nom>
					<email>Veronique.Moriceau@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Anne</prenom>
					<nom>Vilnat</nom>
					<email>Anne.Vilnat@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS, Université Paris-Sud, 91405 Orsay, France</affiliation>
			</affiliations>
			<titre>Évaluation d’un système d’extraction de réponses multiples sur le Web par comparaison à des humains</titre>
			<type>court</type>
			<pages>449-454</pages>
			<resume>Dans cet article, nous proposons une évaluation dans un cadre utilisateur de Citron, un système de question-réponse en français capable d’extraire des réponses à des questions à réponses multiples (questions possédant plusieurs réponses correctes différentes) en domaine ouvert à partir de documents provenant du Web. Nous présentons ici le protocole expérimental et les résultats pour nos deux expériences utilisateurs qui visent à (1) comparer les performances de Citron par rapport à celles d’un être humain pour la tâche d’extraction de réponses multiples et (2) connaître la satisfaction d’un utilisateur devant différents formats de présentation de réponses.</resume>
			<mots_cles>système de question-réponse, réponses multiples, évaluation utilisateur</mots_cles>
			<title>User evaluation of a multiple answer extraction system on the Web</title>
			<abstract>In this paper, we propose a user evaluation of Citron, a question-answering system in French which extracts answers for multiple answer questions (expecting different correct answers) in open domain from Web documents. We present here our experimental protocol and results for user evaluations which aim at (1) comparing multiple answer extraction performances of Citron and users, and (2) knowing user preferences about multiple answer presentation.</abstract>
			<keywords>question-answering system, multiple answers, user evaluation</keywords>
		</article>
		<article id="taln-2014-court-012" session="Traitement de corpus 1">
			<auteurs>
				<auteur>
					<prenom>Natalie</prenom>
					<nom>Schluter</nom>
					<email>natalie.schluter@mah.se</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Department of Computer Science, School of Technology, Malmö University, Malmö, Sweden</affiliation>
			</affiliations>
			<titre></titre>
			<type>court</type>
			<pages>455-460</pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title>Centrality Measures for Non-Contextual Graph-Based Unsupervised Single Document Keyword Extraction</title>
			<abstract>The manner in which keywords fulfill the role of being central to a document is frustratingly still an open question. In this paper, we hope to shed some light on the essence of keywords in scientific articles and thereby motivate the graph-based approach to keyword extraction. We identify the document model captured by the text graph generated as input to a number of centrality metrics, and overview what these metrics say about keywords. In doing so, we achieve state-of-the-art results in unsupervised non-contextual single document keyword extraction.</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2014-court-013" session="Traitement de corpus 1">
			<auteurs>
				<auteur>
					<prenom>Rémy</prenom>
					<nom>Kessler</nom>
					<email>remy.kessler@umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Nicolas</prenom>
					<nom>Béchet</nom>
					<email>nicolas.bechet@irisa.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Audrey</prenom>
					<nom>Laplante</nom>
					<email>audrey.laplante@umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Dominic</prenom>
					<nom>Forest</nom>
					<email>dominic.forest@umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">C.P. 6128, succursale Centre-ville, Montréal H3C 3J7, Canada</affiliation>
				<affiliation affiliationId="2">IRISA, UMR 6074, INSA Rennes</affiliation>
			</affiliations>
			<titre>Détection de périodes musicales d’une collection de musique par apprentissage</titre>
			<type>court</type>
			<pages>461-466</pages>
			<resume>Dans ces travaux, nous présentons une approche afin d’étiqueter une large collection de chansons francophones. Nous avons développé une interface utilisant les paroles comme point d’entrée afin d’explorer cette collection de musique avec des filtres en fonction de chaque période musicale. Dans un premier temps, nous avons collecté paroles et métadonnées de différentes sources sur leWeb. Nous présentons dans cet article une méthode originale permettant d’attribuer de manière automatique la décennie de sortie des chansons de notre collection. Basée sur un système évalué au cours d’une des campagnes DEFT, l’approche combine fouille de textes et apprentissage supervisé et aborde la problématique comme une tâche de classification multi classes. Nous avons par la suite enrichi le modèle d’un certain nombre de traits supplémentaires tels que les tags sociaux afin d’observer leur influence sur les résultats.</resume>
			<mots_cles>Fouille de textes, apprentissage supervisé, paroles de chansons, tags sociaux</mots_cles>
			<title>Music period detection of music collections using learning techniques</title>
			<abstract>In this paper, we present an approach to label a large collection of songs in French by decade. We have developed an information visualization interface that allows users to browse the collection searching for lyrics with musical periods dependent filters. We first harvested lyrics and metadata from various sources on the web. We present in this article an original method to automatically assign the decade of songs for our collection. The original system was developed for a DEFT challenge and combined text mining and machine learning with a multi class approach. We subsequently enriched the model with additional features such as social tags to determine their impact on the results.</abstract>
			<keywords>text mining, machine learning, lyrics, social tagging</keywords>
		</article>
		<article id="taln-2014-court-014" session="Traitement de corpus 1">
			<auteurs>
				<auteur>
					<prenom>Thomas</prenom>
					<nom>François</nom>
					<email>thomas.francois@uclouvain.be</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Laetitia</prenom>
					<nom>Brouwers</nom>
					<email>laetitia.brouwers@uclouvain.be</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Hubert</prenom>
					<nom>Naets</nom>
					<email>hubert.naets@uclouvain.be</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Cédrick</prenom>
					<nom>Fairon</nom>
					<email>cedrick.fairon@uclouvain.be</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CENTAL, IL&amp;C, UCLouvain 1, Place Blaise Pascal, 1348 Louvain-la-Neuve</affiliation>
			</affiliations>
			<titre>AMESURE: une plateforme de lisibilité pour les textes administratifs</titre>
			<type>court</type>
			<pages>467-472</pages>
			<resume>Cet article présente une plateforme dédiée à l’évaluation de la difficulté des textes administratifs, dans un but d’aide à la rédaction. La plateforme propose d’une part une formule de lisibilité spécialisée pour les textes administratifs, dont la conception repose sur une nouvelle méthode d’annotation. Le modèle classe correctement 58% des textes sur une échelle à 5 niveaux et ne commet d’erreurs graves que dans 9% des cas. La plateforme propose d’autre part un diagnostic plus précis des difficultés spécifiques d’un texte, sous la forme d’indicateurs numériques, mais aussi d’une localisation de ces difficultés directement dans le document.</resume>
			<mots_cles>formule de lisibilité, textes administratifs, aide à la rédaction</mots_cles>
			<title>AMesure: a readability formula for administrative texts</title>
			<abstract>This paper presents a platform aiming to assess the difficulty of administrative texts, mostly for editorial assistance purposes. The platform first offers a readability formula specialized for administrative texts, the development of which required the design of a dedicated annotation procedure. The resulting model correctly classifies 58% of the texts on a 5-levels scale and commits serious misclassifications in only 9% of the cases. Moreover, the platform offers a more accurate diagnosis of the difficulty of a text in the form of numerical indicators corresponding to various textual characteristics. It also locates specific local difficulties directly in the text.</abstract>
			<keywords>readability formula, administrative texts, editorial assistance</keywords>
		</article>
		<article id="taln-2014-court-015" session="Sentiments">
			<auteurs>
				<auteur>
					<prenom>Caroline</prenom>
					<nom>Brun</nom>
					<email>caroline.brun@xerox.xrce.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Claude</prenom>
					<nom>Roux</nom>
					<email>claude.roux@xerox.xrce.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">XRCE, 6, chemin de Maupertuis, 38240 Meylan</affiliation>
			</affiliations>
			<titre>Décomposition des « hash tags » pour l’amélioration de la classification en polarité des « tweets »</titre>
			<type>court</type>
			<pages>473-478</pages>
			<resume>Les « mots dièses» ou « hash tags » sont le moyen naturel de lier entre eux différents tweets. Certains « hash tags » sont en fait de petites phrases dont la décomposition peut se révéler particulièrement utile lors d’une analyse d’opinion des tweets. Nous allons montrer dans cet article comment l’on peut automatiser cette décomposition et cette analyse de façon à améliorer la détection de la polarité des tweets.</resume>
			<mots_cles>hash tag, tweet, analyse d’opinion, TAL</mots_cles>
			<title>Decomposing Hashtags to Improve Tweet Polarity Classification</title>
			<abstract>Hash tags are the natural way through which tweets are linked to each other. Some of these hash tags are actually little sentences, whose decompositions can prove quite useful when mining opinions from tweets. We will show in this article how we can automatically detect the inner polarity of these hash tags, through their decomposition and analysis.</abstract>
			<keywords>hash tag, tweet, opinion mining, NLP</keywords>
		</article>
		<article id="taln-2014-court-016" session="Sentiments">
			<auteurs>
				<auteur>
					<prenom>Caroline</prenom>
					<nom>Langlet</nom>
					<email>caroline.langlet@telecom-paristech.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Chloé</prenom>
					<nom>Clavel</nom>
					<email>chloe.clavel@telecom-paristech.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Institut Mines-Télécom ; Télécom ParisTech ; CNRS LTCI, Paris</affiliation>
			</affiliations>
			<titre>Modélisation des questions de l’agent pour l’analyse des affects, jugements et appréciations de l’utilisateur dans les interactions humain-agent</titre>
			<type>court</type>
			<pages>479-484</pages>
			<resume>Cet article aborde la question des expressions d’attitudes (affects, jugements, appréciations) chez l’utilisateur dans le cadre d’échanges avec un agent virtuel. Il propose une méthode pour l’analyse des réponses à des questions fermées destinée à interroger les attitudes de l’utilisateur. Cette méthode s’appuie sur une formalisation des questions de l’agent – sous la forme d’une fiche linguistique – et sur une analyse de la réponse de l’utilisateur, pour créer un modèle utilisateur. La fiche linguistique de l’agent est structurée par un ensemble d’attributs relatifs, d’une part, à l’attitude à laquelle réfère la question, d’autre part, à sa forme morphosyntaxique. L’analyse de la réponse, quant à elle, repose sur un ensemble de règles sémantiques et syntaxiques définies par une grammaire formelle. A partir des résultats fournis par cette analyse et des informations contenues dans la fiche linguistique de l’agent, des calculs sémantiques sont effectués pour définir la valeur de la réponse et construire le modèle utilisateur.</resume>
			<mots_cles>affect, jugement, appréciation, interaction humain-agent, questions-réponses</mots_cles>
			<title>Modelling agent's questions for analysing user's affects, appreciations and judgements in human-agent interaction</title>
			<abstract>This paper tackles the issue of user’s attitudinal expressions in an human-agent interaction. It introduces a method for analysing the user’s anwers to the agent’s yes/no questions about attitude. In order to build a user model, this method relies on a formalization of the agent’s questions and a linguistic analysis of the user’s answer. This formalization comprises a set of attributes regarding the attitude expressed and the morphosyntaxic form of the question. The answer is then analyzed by using a set of semantic and syntactic rules included in a formal grammar. Finally, the semantic value of the answer can be calculated by using the results provided by this analysis and the information given by the question formalization. This computation is next integrated in the user model.</abstract>
			<keywords>affect, jugement, appreciation, human-agent interaction, questions-answers</keywords>
		</article>
		<article id="taln-2014-court-017" session="Outils">
			<auteurs>
				<auteur>
					<prenom>François</prenom>
					<nom>Barthélemy</nom>
					<email>francois.barthelemy@inria.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">INRIA – Alpage, domaine de Voluceau 78153 Rocquencourt</affiliation>
				<affiliation affiliationId="2">CNAM – Cedric, 292 rue Saint-Martin, 75003 Paris</affiliation>
			</affiliations>
			<titre>KNG: un outil pour l’écriture facile de cascades de transducteurs</titre>
			<type>court</type>
			<pages>485-490</pages>
			<resume>Cet article présente une bibliothèque python appelée KNG permettant d’écrire facilement des automates et transducteurs finis. Grâce à une gestion soigneuse des codages et des entrées-sorties, cette bibliothèque permet de réaliser une cascade de transducteurs au moyen de tubes unix reliant des scripts python.</resume>
			<mots_cles>Machines finies à états, cascade de transducteur, expression régulière</mots_cles>
			<title>KNG: a Tool for Writing Easily Transducer Cascades</title>
			<abstract>This paper presents a Python library called KNG which provides facilities to write easily finite state automata and transducers. Through careful management of encodings and input/output, a transducer cascade may be implented using unix pipes connecting python scripts.</abstract>
			<keywords>Finite State Machines, Transducer Cascade, Regular Expression</keywords>
		</article>
		<article id="taln-2014-court-018" session="Outils">
			<auteurs>
				<auteur>
					<prenom>Raoul</prenom>
					<nom>Blin</nom>
					<email>blin@ehess.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Centre de Recherches Linguistiques sur l’Asie Orientale, EHESS 131 bd St-Michel, 75005 Paris, France.</affiliation>
			</affiliations>
			<titre>Comparaison de deux outils d'analyse de corpus japonais pour l'aide au linguiste, Sagace et Mecab</titre>
			<type>court</type>
			<pages>491-498</pages>
			<resume>L'objectif est de comparer deux outils d'analyse de corpus de textes bruts pour l'aide à la recherche en linguistique japonaise. Les deux outils représentent chacun une approche spécifique. Le premier, Sagace, recherche un patron sans prise en compte de son environnement. Le second, un dispositif à base de Mecab, recherche les patrons après analyse morphologique complète des phrases. Nous comparons les performances en temps et en précision. Il ressort de cette analyse que les performances de Sagace sont globalement un peu inférieures à celles des dispositifs à base de Mecab, mais qu'elles restent tout à fait honorables voire meilleures pour certaines tâches.</resume>
			<mots_cles>Japonais, Corpus, Analyseurs, Mecab, Sagace</mots_cles>
			<title> Comparing two analyzers of Japanese corpora for helping linguists: MeCab and Sagace</title>
			<abstract>The purpose is to compare two tools used for helping linguist to analyze large corpora of raw japanese text. Each tool is representative of a specific approach. The first one, Sagace, search a pattern without taking into account its distribution. The second one is based on the morphological analyzer Mecab. It first analyzes the whole sentence before counting the searched pattern. We compare the processing time, needed ressources, and the quality of the results. It appears that performances of Sagace are globaly slightly lower than the Mecab system, but it doesn't defer so much. It may even be punctually better.</abstract>
			<keywords>Japanese, Corpus, Comparison, Mecab, Sagace</keywords>
		</article>
		<article id="taln-2014-court-019" session="Outils">
			<auteurs>
				<auteur>
					<prenom>Giulia</prenom>
					<nom>Barreca</nom>
					<email>giulia.barreca@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>George</prenom>
					<nom>Christodoulides</nom>
					<email>george@mycontent.gr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire MoDyCo, CNRS, Université Paris Ouest Nanterre La Défense 200, avenue de la République, FR-92001 Nanterre, France</affiliation>
				<affiliation affiliationId="2">Centre Valibel, Institut Langue &amp; Communication, Université de Louvain, Place Blaise Pascal 1, 1348 Louvain-la-Neuve, Belgique</affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>Un concordancier multi-niveaux et multimédia pour des corpus oraux</titre>
			<type>court</type>
			<pages>499-504</pages>
			<resume>Les concordanciers jouent depuis longtemps un rôle important dans l’analyse des corpus linguistiques, tout comme dans les domaines de la philologie, de la littérature, de la traduction et de l’enseignement des langues. Toutefois, il existe peu de concordanciers qui soient capables d’associer des annotations à plusieurs niveaux et synchronisées avec le signal sonore. L’essor des grands corpus de français parlé introduit une augmentation des exigences au niveau de la performance. Dans ce travail à caractère préliminaire, nous avons développé un prototype de concordancier multi-niveaux et multimédia, que nous avons testé sur le corpus de français parlé du projet Phonologie du Français Contemporain (PFC, 1,5 million de tokens de transcription alignée au niveau de l’énoncé). L’outil permet non seulement d’enrichir les résultats des concordances grâce aux données relevant de plusieurs couches d’annotation du corpus (annotation morphosyntaxique, lemme, codage de la liaison, codage du schwa etc.), mais aussi d’élargir les modalités d’accès au corpus.</resume>
			<mots_cles>concordancier, annotation multi-niveaux, linguistique de corpus, didactique du FLE</mots_cles>
			<title>A multi-level multimedia concordancer for spoken language corpora</title>
			<abstract>Concordances have always played an important role in the analysis of language corpora, for studies in humanities, literature, linguistics, translation and language teaching. However, very few of the available systems support multi-level queries against a richly-annotated, sound-aligned spoken corpus. The rapid growth in the development of spoken corpora, particularly for French, increases the need for scalable, high-performance solutions. We present the preliminary results of our project to develop a multi-level multimedia concordancer for spoken language corpora. We test our prototype on the PFC corpus of spoken French (1.5 million tokens, transcriptions aligned to the utterance level). Our tool allows researchers to query the corpus and produce concordances correlating several annotation levels (part-of-speech tags, lemmas, annotation of phonological phenomena such as the liaison and schwa, etc.) while allowing for multi-modal access to the associated sound recordings and other data.</abstract>
			<keywords>concordance tool, multi-level annotation, corpus linguistics, French language teaching</keywords>
		</article>
		<article id="taln-2014-court-020" session="Étiquetage 2">
			<auteurs>
				<auteur>
					<prenom>Perrine</prenom>
					<nom>Brusini</nom>
					<email>pbrusini@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Pascal</prenom>
					<nom>Amsili</nom>
					<email>amsili@linguist.univ-paris-diderot.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Emmanuel</prenom>
					<nom>Chemla</nom>
					<email></email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Anne</prenom>
					<nom>Christophe</nom>
					<email></email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Language, Cognition and Development Lab, Int. School for Advanced Studies (SISSA), Trieste</affiliation>
				<affiliation affiliationId="2">Laboratoire de Linguistique Formelle, CNRS &amp; Université Paris Diderot</affiliation>
				<affiliation affiliationId="3">Laboratoire de Sciences Cognitives et Psycholinguistique (CNRS &amp; ENS, EHESS)</affiliation>
			</affiliations>
			<titre>Simulation de l’apprentissage des contextes nominaux/verbaux par n-grammes</titre>
			<type>court</type>
			<pages>505-510</pages>
			<resume>On présente une étude d’apprentissage visant à montrer que les contextes locaux dans un corpus de parole adressée aux enfants peuvent être exploités, avec des méthodes statistiques simples, pour prédire la catégorie (nominale vs. verbale) d’un mot inconnu. Le modèle présenté, basé sur la mémorisation de n-grammes et sur une « graine sémantique » (un petit nombre de noms et verbes supposés connus et catégorisés) montre une excellente précision à toutes les tailles de graine sémantique, et un rappel plus faible, qui croît avec la taille de la graine sémantique. Les contextes les plus utilisés sont ceux qui contiennent des mots fonctionnels. Cette étude de faisabilité démontre que les très jeunes enfants pourraient exploiter les contextes de mots inconnus pour prédire leur catégorie syntaxique.</resume>
			<mots_cles>apprentissage, modélisation de l’acquisition du langage, n-grammes</mots_cles>
			<title>Learning simulation of nominal/verbal contexts through n-grams</title>
			<abstract>A learning study is presented whose aim is to show that local contexts, in a child-directed speech corpus, can be exploited, with simple statistical methods, to predict the category (noun vs. verb) of unknown words. The model we present here is based on the memorisation of n-grams and on a “semantic seed” (a small number of nouns and verbs supposedly known and well categorised). It shows an excellent precision for every size of the semantic seed, and its recall grows along with the size of the semantic seed. The most useful contexts are the ones that include function words. This feasibility study shows that very young children could exploit the contexts of unknown words to predict their syntactic category.</abstract>
			<keywords>learning, language acquisition modeling, n-gram</keywords>
		</article>
		<article id="taln-2014-court-021" session="Étiquetage 2">
			<auteurs>
				<auteur>
					<prenom>Anaïs</prenom>
					<nom>Ollagnier</nom>
					<email>anais.ollagnier@openedition.org</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Sébastien</prenom>
					<nom>Fournier</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Patrice</prenom>
					<nom>Bellot</nom>
					<email></email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Frédéric</prenom>
					<nom>Béchet</nom>
					<email></email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Aix-Marseille Université, CNRS, ENSAM, Université de Toulon LSIS UMR 7296, 13397, Marseille, France</affiliation>
				<affiliation affiliationId="2">Aix-Marseille Université, CNRS, CLEO OpenEdition UMS 3287, 13451, Marseille, France</affiliation>
				<affiliation affiliationId="3">Aix-Marseille Université, CNRS, LIF UMR 7279, 13288, Marseille, France</affiliation>
			</affiliations>
			<titre>Impact de la nature et de la taille des corpus d'apprentissage sur les performances dans la détection automatique des entités nommées</titre>
			<type>court</type>
			<pages>511-516</pages>
			<resume>Nous présentons une étude comparative sur l’impact de la nature et de la taille des corpus d’apprentissage sur les performances dans la détection automatique des entités nommées. Cette évaluation se présente sous la forme de multiples modulations de trois corpus français. Deux des corpus sont issus du catalogue des ressources linguistiques d’ELRA et le troisième est composé de documents extraits de la plateforme OpenEdition.org.</resume>
			<mots_cles>Reconnaissance d'entités nommées, Adaptation au domaine, comparaison d'outils</mots_cles>
			<title>Impact of the nature and size of the training set on performance in the automatic detection of named entities</title>
			<abstract>We present a comparative study on the impact of the nature and size of the training corpus on performance in automatic named entities recognition. This evaluation is in the form of multiple modulations on three French corpus. Two corpora are from the catalog of the European Language Resources Association (ELRA) and the third is composed of documents extract from the OpenEdition.org platform.</abstract>
			<keywords>Named entity recognition, Domain adptation, performance comparison</keywords>
		</article>
		<article id="taln-2014-court-022" session="Étiquetage 2">
			<auteurs>
				<auteur>
					<prenom>Meryem</prenom>
					<nom>Talha</nom>
					<email>meriem.talha@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Siham</prenom>
					<nom>Boulaknadel</nom>
					<email>boulaknadel@ircam.ma</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Driss</prenom>
					<nom>Aboutajdine</nom>
					<email>aboutaj@fsr.ac.ma</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LRIT, Unité Associée au CNRST (URAC 29), Faculté des Sciences, Mohammed V-Agdal, Rabat, Maroc</affiliation>
				<affiliation affiliationId="2">IRCAM, Avenue Allal El Fassi, Madinat Al Irfane, Rabat-Instituts, Maroc</affiliation>
			</affiliations>
			<titre>RENAM: Système de Reconnaissance des Entités Nommées Amazighes</titre>
			<type>court</type>
			<pages>517-524</pages>
			<resume>La reconnaissance des Entités Nommées (REN) en langue amazighe est un prétraitement potentiellement utile pour de nombreuses applications du traitement de la langue amazighe. Cette tâche représente toutefois un sévère challenge, compte tenu des particularités de cette langue. Dans cet article, nous présentons le premier système d’extraction d’entités nommées amazighes (RENAM) fondé sur une approche symbolique qui utilise le principe de transducteur à états finis disponible sous la plateforme GATE.</resume>
			<mots_cles>Reconnaissance des entités nommées (REN), Langue Amazighe, Règles d’annotation, JAPE, GATE</mots_cles>
			<title>NERAM : Named Entity Recognition for AMazighe language</title>
			<abstract>Named Entity Recognition (NER) for Amazigh language is a potentially useful pretreatment for many processing applications for the Amazigh language. However, this task represents a tough challenge, given the specificities of this language. In this paper, we present (NERAM) the first named entity system for the Amazigh language based on a symbolic approach that uses linguistic rules built manually by using an information extraction tool available within the platform GATE.</abstract>
			<keywords>Named Entities Recognition (NER), Amazigh Language, Annotation Rules, JAPE, GATE</keywords>
		</article>
		<article id="taln-2014-court-023" session="Étiquetage 2">
			<auteurs>
				<auteur>
					<prenom>Ingrid</prenom>
					<nom>Falk</nom>
					<email>ifalk@unistra.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Delphine</prenom>
					<nom>Bernhard</nom>
					<email>dbernhard@unistra.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Christophe</prenom>
					<nom>Gérard</nom>
					<email>christophegerard@unistra.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LiLPa, Université de Strasbourg</affiliation>
			</affiliations>
			<titre>De la quenelle culinaire à la quenelle politique : identification de changements sémantiques à l’aide des Topic Models</titre>
			<type>court</type>
			<pages>525-530</pages>
			<resume>Dans cet article nous employons le « topic modeling » pour explorer des chemins vers la détection automatique de l’apparition de nouveaux sens pour des mots connus. Nous appliquons les méthodes introduites dans (Lau et al., 2012, 2014) à un cas de néologie sémantique récent, l’apparition du nouveau sens de geste pour le mot « quenelle ». Nos expériences mettent en évidence le potentiel de cette approche pour l’apprentissage des sens du mot, l’alignement des topics à des sens de dictionnaire et enfin la détection de nouveaux sens.</resume>
			<mots_cles>topic models, induction de sens, néologie sémantique</mots_cles>
			<title>From the Culinary to the Political Meaning of "quenelle" : Using Topic Models For Identifying Novel Senses</title>
			<abstract>In this study we explore topic modeling for the automatic detection of new senses of known words. We apply methods developed in previous work for English (Lau et al., 2012, 2014) on a recent case of new word sense induction in French, namely the appearence of the new meaning of gesture for the word « quenelle ». Our experiments illustrate the potential of this approach at learning word senses, aligning the topics with dictionary senses and finally at detecting the new senses.</abstract>
			<keywords>topic models, word sense induction, semantic neologism</keywords>
		</article>
		<article id="taln-2014-court-024" session="Langue des signes">
			<auteurs>
				<auteur>
					<prenom>Michael</prenom>
					<nom>Filhol</nom>
					<email>michael.filhol@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI–CNRS, B. P. 133, 91403 Orsay cedex</affiliation>
			</affiliations>
			<titre>Grammaire récursive non linéaire pour les langues des signes</titre>
			<type>court</type>
			<pages>531-536</pages>
			<resume>Cet article propose une approche pour la formalisation de grammaires pour les langues des signes, rendant compte de leurs particularités linguistiques. Comparable aux grammaires génératives en termes de récursivité productive, le système présente des propriétés nouvelles comme la multi-linéarité permettant la spécification simultanée des articulateurs. Basé sur l’analyse des liens entre formes produites/observées et fonctions linguistiques au sens large, on observe un décloisonnement des niveaux traditionnels de construction de la langue, inhérent à la méthodologie employée. Nous présentons un ensemble de règles trouvées suivant la démarche présentée et concluons avec une perspective intéressante en traduction automatique vers la langue des signes.</resume>
			<mots_cles>Grammaire formelle, multi-linéarité, langue des signes</mots_cles>
			<title>Non-linear recursive grammar for Sign languages</title>
			<abstract>This article presents a formal approach to Sign Language grammars, with the aim of capturing their specificities. The system is comparable to generative grammar in the sense that it is recursively productive, but it has quite different properties such as multilinearity, enabling simultaneous articulator specification. As it is based on the analysis of systematic links between observable form features and interpreted linguistic functions in the general sense, the traditionally separate linguistic levels all end up covered by the same model. We present the results found for a set of linguistic structures, following the presented methodology, and conclude with an interesting prospect in the field of text-to-Sign machine translation.</abstract>
			<keywords>Formal grammar, multilinearity, Sign Language</keywords>
		</article>
		<article id="taln-2014-court-025" session="Langue des signes">
			<auteurs>
				<auteur>
					<prenom>Rémi</prenom>
					<nom>Dubot</nom>
					<email>remi.dubot@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Arturo</prenom>
					<nom>Curiel</nom>
					<email>arturo.curiel@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Christophe</prenom>
					<nom>Collet</nom>
					<email>christophe.collet@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">IRIT, Université de Toulouse, 118 route de Narbonne, 31062 Toulouse Cedex 9, France</affiliation>
			</affiliations>
			<titre>Vers un traitement automatique en soutien d’une linguistique exploratoire des Langues des Signes</titre>
			<type>court</type>
			<pages>537-542</pages>
			<resume>Les langues des signes sont les langues naturelles utilisées dans les communautés sourdes. Elles ont suscitées, ces dernières années, de l’intérêt dans le domaine du traitement automatique des langues naturelles. Néanmoins, il y a un manque général de données de terrain homogènes. Notre réflexion porte sur les moyens de soutenir la maturation des modèles linguistiques avant d’entamer un large effort d’annotation. Cet article présente des pré-requis pour la réalisation d’outils s’inscrivant dans cette démarche. L’exposé est illustré avec deux outils développés pour les langues des signes : le premier utilise une logique adaptée pour la représentation de modèles phonologiques et le second utilise des grammaires formelles pour la représentation de modèles syntaxiques.</resume>
			<mots_cles>Langues des Signes, Formalismes de modélisation, Reconnaissance, Annotation semi-automatique</mots_cles>
			<title>Supporting Sign Languages Exploratory Linguistics with an Automatization of the Annotation Process</title>
			<abstract>Sign Languages (SLs) are the vernaculars of deaf communities, they have drawn interests in the natural language processing research in recent years. However, the field suffers a general lack of homogeneous information. Our reflexion is about how to support the maturation of models before starting a consequent annotation effort. In this paper, we describe the requirements of tools supporting such an approach. It is illustrated with two examples of work developed following these guidelines. The first one aims at phonological representation using logic and the second one targets supra-lexical features recognition.</abstract>
			<keywords>Sign Languages, Modeling formalisms, Recognition, semi-automatic annotation</keywords>
		</article>
		<article id="taln-2014-court-026" session="Résumé automatique">
			<auteurs>
				<auteur>
					<prenom>Houda</prenom>
					<nom>Oufaida</nom>
					<email>h_oufaida@esi.dz</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Omar</prenom>
					<nom>Nouali</nom>
					<email>onouali@mail.cerist.dz</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Philippe</prenom>
					<nom>Blache</nom>
					<email>blache@lpl-aix.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Ecole Nationale Supérieure d’Informatique ESI, BP 68M Oued Smar, 16270, El Harrach Alger Algérie</affiliation>
				<affiliation affiliationId="2">Centre de Recherche sur l'Information Scientifique et Technique CERIST, Rue Des 3 Frères Aissou, Ben Aknoun Alger Algérie</affiliation>
				<affiliation affiliationId="3">LPL, AMU, CNRS, 5 avenue Pasteur, 13100 Aix-en-Provence</affiliation>
			</affiliations>
			<titre>Résumé Automatique Multilingue Expérimentations sur l’Anglais, l’Arabe et le Français</titre>
			<type>court</type>
			<pages>543-549</pages>
			<resume>La tâche du résumé multilingue vise à concevoir des systèmes de résumé très peu dépendants de la langue. L’approche par extraction est au coeur de ces systèmes, elle permet à l’aide de méthodes statistiques de sélectionner les phrases les plus pertinentes dans la limite de la taille du résumé. Dans cet article, nous proposons une approche de résumé multilingue, elle extrait les phrases dont les termes sont des plus discriminants. De plus, nous étudions l'impact des différents traitements linguistiques de base : le découpage en phrases, l'analyse lexicale, le filtrage des mots vides et la racinisation sur la couverture ainsi que la notation des phrases. Nous évaluons les performances de notre approche dans un contexte multilingue : l'anglais, l'arabe et le français en utilisant le jeu de données TAC MultiLing 2011.</resume>
			<mots_cles>Résumé multilingue, analyse discriminante, TAL, évaluation multilingue</mots_cles>
			<title>Multilingual Summarization Experiments on English, Arabic and French</title>
			<abstract>The task of multilingual summarization aims to design free-from language systems. Extractive methods are in the core of multilingual summarization systems. In this paper, we discuss the influence of various basic NLP tasks: sentence splitting, tokenization, stop words removal and stemming on sentence scoring and summaries' coverage. Hence, we propose a statistical method which extracts most relevant sentences on the basis of their terms discriminant power. We conduct several experimentations in a multilingual context: English, Arabic and French using the TAC MultiLing 2011 dataset.</abstract>
			<keywords>Multilingual summarization, Discriminant analysis, NLP, Multilingual evaluation</keywords>
		</article>
		<article id="taln-2014-court-027" session="Résumé automatique">
			<auteurs>
				<auteur>
					<prenom>Rémi</prenom>
					<nom>Bois</nom>
					<email>remi.bois@etu.univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Johannes</prenom>
					<nom>Leveling</nom>
					<email>jleveling@computing.dcu.ie</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Lorraine</prenom>
					<nom>Goeuriot</nom>
					<email>lgoeuriot@computing.dcu.ie</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Gareth J. F.</prenom>
					<nom>Jones</nom>
					<email>gjones@computing.dcu.ie</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Liadh</prenom>
					<nom>Kelly</nom>
					<email>lkelly@computing.dcu.ie</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LINA, Université de Nantes, France</affiliation>
				<affiliation affiliationId="2">CNGL, School of Computing, Dublin City University, Dublin 9, Ireland</affiliation>
			</affiliations>
			<titre></titre>
			<type>court</type>
			<pages>550-555</pages>
			<resume>Nous présentons dans cet article l’adaptation de l’outil de résumé automatique REZIME à la langue française. REZIME est un outil de résumé automatique mono-document destiné au domaine médical et s’appuyant sur des critères statistiques, syntaxiques et lexicaux pour extraire les phrases les plus pertinentes. Nous décrivons dans cet article le système REZIME tel qu’il a été conçu et les différentes étapes de son adaptation à la langue française. Les performances de l’outil adapté au français sont mesurées et comparées à celle de la version anglaise. Les résultats montrent que l’adaptation au français ne dégrade pas les performances de REZIME, qui donne des résultats équivalents dans les deux langues.</resume>
			<mots_cles>Résumé automatique, multilangue, domaine médical</mots_cles>
			<title>Porting a Summarizer to the French Language</title> 
			<abstract>We describe the porting of the English language REZIME text summarizer to the French language. REZIME is a single-document summarizer particularly focused on summarization of medical documents. Summaries are created by extracting key sentences from the original document. The sentence selection employs machine learning techniques, using statistical, syntactic and lexical features which are computed based on specialized language resources. The REZIME system was initially developed for English documents. In this paper we present the summarizer architecture, and describe the steps required to adapt it to the French language. The summarizer performance is evaluated for English and French datasets. Results show that the adaptation to French results in system performance comparable to the initial English system.</abstract>
			<keywords>single-document summarization, multilingual, medical domain</keywords>
		</article>
		<article id="taln-2014-court-028" session="Corpus">
			<auteurs>
				<auteur>
					<prenom>Brigitte</prenom>
					<nom>Bigi</nom>
					<email>brigitte.bigi@lpl-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Tatsuya</prenom>
					<nom>Watanabe</nom>
					<email>tatsuya.watanabe@atilf.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire Parole et Langage, AMU, CNRS, 5 avenue Pasteur, 13100 Aix-en-Provence</affiliation>
				<affiliation affiliationId="2">Ortolang</affiliation>
			</affiliations>
			<titre>Extraction de données orales multi-annotées</titre>
			<type>court</type>
			<pages>556-561</pages>
			<resume>Cet article aborde le problème de l’extraction de données orales multi-annotées : nous proposons une solution intermédiaire, entre d’une part les systèmes de requêtages très évolués mais qui nécessitent des données structurées, d’autre part les données (multi-)annotées des utilisateurs qui sont hétérogènes. Notre proposition s’appuie sur 2 fonctions principales : une fonction booléenne pour filtrer sur le contenu, et une fonction de relation qui implémente l’algèbre de Allen. Le principal avantage de cette approche réside dans sa généricité : le fonctionnement sera identique que les annotations proviennent de Praat, Transcriber, Elan ou tout autre logiciel d’annotation. De plus, deux niveaux d’utilisation ont été développés : une interface graphique qui ne nécessite aucune compétence ou connaissance spécifique de la part de l’utilisateur, et un interrogation par scripts en langage Python. L’approche a été implémentée dans le logiciel SPPAS, distribué sous licence GPL.</resume>
			<mots_cles>multimodalité, corpus, extraction</mots_cles>
			<title>Extracting multi-annotated speech data</title>
			<abstract>This paper addresses the problem of extracting multimodal annotated data in the linguistic field ranging from general linguistic to domain specific information. Our proposal can be considered as a solution or a least an intermediary solution that can link together requesting systems and expert data from various annotation tools. The system is partly based on the Allen algebra and consists in creating filters based on two functions : a boolean function and a relation function. The main advantage of this approach lies in its genericity : it will work identically with annotations from Praat, Transcriber, Elan or from any other annotation software. Furthermore, two levels of usage have been developed : a graphical user interface graph that not requires any skill or knowledge, and a query form in Python. This system is included in the software SPPAS and is distributed under the terms of the GPL license.</abstract>
			<keywords>multimodality, corpus, extraction</keywords>
		</article>
		<article id="taln-2014-court-029" session="Corpus">
			<auteurs>
				<auteur>
					<prenom>Anaïs</prenom>
					<nom>Lefeuvre</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jean-Yves</prenom>
					<nom>Antoine</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Agata</prenom>
					<nom>Savary</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Emmanuel</prenom>
					<nom>Schang</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Lotfi</prenom>
					<nom>Abouda</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Denis</prenom>
					<nom>Maurel</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Iris</prenom>
					<nom>Eshkol</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université François Rabelais de Tours, laboratoire LI</affiliation>
				<affiliation affiliationId="2">Université d’Orléans, laboratoire LLL, UMR 7270</affiliation>
			</affiliations>
			<titre>Annotation de la temporalité en corpus : contribution à l'amélioration de la norme TimeML</titre>
			<type>court</type>
			<pages>562-567</pages>
			<resume>Cet article propose une analyse critique de la norme TimeML à la lumière de l’expérience d’annotation temporelle d’un corpus de français parlé. Il montre que certaines adaptations de la norme seraient conseillées pour répondre aux besoins du TAL et des sciences du langage. Sont étudiées ici les questions de séparation des niveaux d’annotation, de délimitation des éventualités dans le texte et de l’ajout d’une relation temporelle de type associative.</resume>
			<mots_cles>annotation temporelle, TimeML, éventualités, relations temporelles, expressions temporelles</mots_cles>
			<title>Tense and Time Annotations : a Contribution to TimeML Improvement</title>
			<abstract>This paper reports a critical analysis of the TimeML standard, in the light of a temporal annotation that was conducted on spoken French. It shows that the norm suffers from weaknesses that must be corrected to fit the needs of NLP and corpus linguistics. These limitations concern mainly 1) the separation of different levels of linguistic annotation, 2) the delimitation in the text of the events, and 3) the absence of a bridging temporal relation in the norm.</abstract>
			<keywords>temporal annotation, TimeML, eventualities, temporal relations, time expressions</keywords>
		</article>
		<article id="taln-2014-court-030" session="Corpus">
			<auteurs>
				<auteur>
					<prenom>Louise</prenom>
					<nom>Deléger</nom>
					<email>louise.deleger@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Aurélie</prenom>
					<nom>Névéol</nom>
					<email>aurelie.neveol@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI – CNRS UPR 3251, Orsay, France</affiliation>
			</affiliations>
			<titre>Identification automatique de zones dans des documents pour la constitution d’un corpus médical en français</titre>
			<type>court</type>
			<pages>568-573</pages>
			<resume>De nombreuses informations cliniques sont contenues dans le texte des dossiers électroniques de patients et ne sont pas directement accessibles à des fins de traitement automatique. Pour pallier cela, nous préparons un large corpus annoté de documents cliniques. Une première étape de ce travail consiste à séparer le contenu médical des documents et les informations administratives contenues dans les en-têtes et pieds de page. Nous présentons un système d’identification automatique de zones dans les documents cliniques qui offre une F-mesure de 0,97, équivalente à l’accord inter-annoteur de 0,98. Notre étude montre que le contenu médical ne représente que 60% du contenu total de notre corpus, ce qui justifie la nécessité d’une segmentation en zones. Le travail d’annotation en cours porte sur les sections médicales identifiées.</resume>
			<mots_cles>Traitement Automatique de la Langue Biomédicale, segmentation de documents, identification de zones</mots_cles>
			<title>Automatic identification of document sections for designing a French clinical corpus</title>
			<abstract>Much clinical information is contained in the free text of Electronic Health Records (EHRs) and is not available for automatic processing. To advance Natural Language Processing of the French clinical narrative, we are building a richly annotated large-scale corpus of French clinical documents. To access the most medically relevant content of EHRs we develop an automatic system to separate the core medical content from other document sections, such as headers and footers. The performance of automatic content extraction achieves 96.6% F-measure, on par with human inter-annotator agreement of 98%. We find that medically relevant content covers only 60% of clinical documents in our corpus. Future annotation work will focus on these sections.</abstract>
			<keywords>BioNLP, Automatic document segmentation, section identification</keywords>
		</article>
		<article id="taln-2014-court-031" session="Corpus">
			<auteurs>
				<auteur>
					<prenom>Guy</prenom>
					<nom>Perrier</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Marie</prenom>
					<nom>Candito</nom>
					<email></email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Bruno</prenom>
					<nom>Guillaume</nom>
					<email></email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Corentin </prenom>
					<nom>Ribeyre</nom>
					<email></email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Karën</prenom>
					<nom>Fort</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Djamé</prenom>
					<nom>Seddah</nom>
					<email></email>
					<affiliationId>4</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Lorraine/LORIA</affiliation>
				<affiliation affiliationId="2">Université Paris Diderot/INRIA</affiliation>
				<affiliation affiliationId="3">Inria Nancy Grand-Est/LORIA</affiliation>
				<affiliation affiliationId="4">Université Paris Sorbonne/INRIA</affiliation>
			</affiliations>
			<titre>Un schéma d’annotation en dépendances syntaxiques profondes pour le français</titre>
			<type>court</type>
			<pages>574-579</pages>
			<resume>À partir du schéma d’annotation en dépendances syntaxiques de surface du corpus Sequoia, nous proposons un schéma en dépendances syntaxiques profondes qui en est une abstraction exprimant les relations grammaticales entre mots sémantiquement pleins. Quand ces relations grammaticales sont partie prenante de diathèses verbales, ces diathèses sont vues comme le résultat de redistributions à partir d’une diathèse canonique et c’est cette dernière qui est retenue dans notre schéma d’annotation syntaxique profonde.</resume>
			<mots_cles>schéma d’annotation, syntaxe profonde, grammaires de dépendance</mots_cles>
			<title>Annotation scheme for deep dependency syntax of French</title>
			<abstract>We describe in this article an annotation scheme for deep dependency syntax, built from the surface annotation scheme of the Sequoia corpus, abstracting away from it and expressing the grammatical relations between content words. When these grammatical relations take part into verbal diatheses, we consider the diatheses as resulting from redistributions from the canonical diathesis, which we retain in our annotation scheme.</abstract>
			<keywords>annotation scheme, deep syntax, dependency grammar</keywords>
		</article>
		<article id="taln-2014-court-032" session="Traitement de corpus 2">
			<auteurs>
				<auteur>
					<prenom>Elisa</prenom>
					<nom>Omodei</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Yufan</prenom>
					<nom>Guo</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jean-Philippe</prenom>
					<nom>Cointet</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Thierry</prenom>
					<nom>Poibeau</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
			</affiliations>
			<titre>Analyse argumentative du corpus de l’ACL (ACL Anthology)</titre>
			<type>court</type>
			<pages>580-585</pages>
			<resume>Cet article présente un essai d’application de l’analyse argumentative (text zoning) à l’ACL Anthology. Il s’agit ainsi de mieux caractériser le contenu des articles du domaine de la linguistique informatique afin de pouvoir en faire une analyse fine par la suite. Nous montrons que des technique récentes d’analyse argumentative fondées sur l’apprentissage faiblement supervisé permettent d’obtenir de bons résultats.</resume>
			<mots_cles>Analyse argumentative, corpus de textes scientifiques, ACL Anthology</mots_cles>
			<title>Argumentative analysis of the ACL Anthology</title>
			<abstract>This paper presents an application of Text Zoning to the ACL Anthology. Text Zoning is known to be useful to characterize the content of papers, especially in the scientific domain. We show that recent techniques based on weakly supervised learning obtain excellent results on the ACL Anthology. Although these kinds of techniques is known in the domain, it is the first time it is applied to the whole ACL Anthology.</abstract>
			<keywords>Text Zoning, Corpus of scientific texts, ACL Anthology</keywords>
		</article>
		<article id="taln-2014-court-033" session="Lexique 2">
			<auteurs>
				<auteur>
					<prenom>Veronika</prenom>
					<nom>Lux-Pogodalla</nom>
					<email>Veronika.Lux@atilf.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CNRS, ATILF, UMR 7118 Nancy, F-54000, France</affiliation>
			</affiliations>
			<titre>Intégration relationnelle des exemples lexicographiques dans un réseau lexical</titre>
			<type>court</type>
			<pages>586-591</pages>
			<resume>Nous présentons un ensemble d’exemples lexicographiques intégré dans le Réseau Lexical du Français et explorons son intérêt potentiel en tant que corpus annoté pour la recherche en désambiguisation sémantique automatique.</resume>
			<mots_cles>Réseau Lexical du Français, exemples lexicographiques, corpus annoté sémantiquement</mots_cles>
			<title>Integrating lexicographic examples in a lexical network</title>
			<abstract>This paper presents a set of lexicographic examples which is being developped along the French Lexical Network. The possibility of using this set as an annotated corpus for research on automatic Word Sense Disambiguation is examined.</abstract>
			<keywords>French Lexical Network, lexicographic examples, semantically annotated corpus</keywords>
		</article>
		<article id="taln-2014-court-034" session="Lexique 2">
			<auteurs>
				<auteur>
					<prenom>Mathieu</prenom>
					<nom>Lafourcade</nom>
					<email>mathieu.lafourcade@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Nathalie</prenom>
					<nom>Le Brun</nom>
					<email>imaginat@imaginat.name</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Virginie</prenom>
					<nom>Zampa</nom>
					<email>virginie.zampa@u-grenoble3.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Lirmm, Université Montpellier 2, France</affiliation>
				<affiliation affiliationId="2">Imagin@t, 34400 Lunel</affiliation>
				<affiliation affiliationId="3">Lidilem, Grenoble 3 BP25, 38040 Grenoble cedex 9, France</affiliation>
			</affiliations>
			<titre>Les couleurs des gens</titre>
			<type>court</type>
			<pages>592-597</pages>
			<resume>En TAL et plus particulièrement en analyse sémantique, les informations sur la couleur peuvent être importantes pour traiter correctement des informations textuelles (sens des mots, désambiguïsation et indexation). Plus généralement, connaître la ou les couleurs habituellement associée(s) à un terme est une information cruciale. Dans cet article, nous montrons comment le crowdsourcing, à travers un jeu, peut être une bonne stratégie pour collecter ces données lexico-sémantiques.</resume>
			<mots_cles>association couleur-mot, réseau lexical, crowdsourcing</mots_cles>
			<title>Colors of People</title>
			<abstract>In Natural Language Processing and semantic analysis in particular, color information may be important in order to properly process textual information (word sense disambiguation, and indexing). More specifically, knowing which colors are generally associated to terms is a crucial information. In this paper, we explore how crowdsourcing through a game with a purpose (GWAP) can be an adequate strategy to collect such lexico-semantic data.</abstract>
			<keywords>Word Color Associations, Lexical Network, Crowdsourcing</keywords>
		</article>
		<article id="taln-2014-court-035" session="Lexique 2">
			<auteurs>
				<auteur>
					<prenom>Mohammad</prenom>
					<nom>Nasiruddin</nom>
					<email>Mohammad.Nasiruddin@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Didier</prenom>
					<nom>Schwab</nom>
					<email>Didier.Schwab@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Andon</prenom>
					<nom>Tchechmedjiev</nom>
					<email>Andon.Tchechmedjiev@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Gilles</prenom>
					<nom>Sérasset</nom>
					<email>Gilles.Serasset@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Hervé</prenom>
					<nom>Blanchon</nom>
					<email>Hervé.Blanchon@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Univ. Grenoble Alpes</affiliation>
			</affiliations>
			<titre>Induction de sens pour enrichir des ressources lexicales</titre>
			<type>court</type>
			<pages>598-603</pages>
			<resume>En traitement automatique des langues, les ressources lexico-sémantiques ont été incluses dans un grand nombre d’applications. La création manuelle de telles ressources est consommatrice de temps humain et leur couverture limitée ne permet pas toujours de couvrir les besoins des applications. Ce problème est encore plus important pour les langues moins dotées que le français ou l’anglais. L’induction de sens présente dans ce cadre une piste intéressante. À partir d’un corpus de texte, il s’agit d’inférer les sens possibles pour chacun des mots qui le composent. Nous étudions dans cet article une approche basée sur une représentation vectorielle pour chaque occurrence d’un mot correspondant à ses voisins. À partir de cette représentation, construite sur un corpus en bengali, nous comparons plusieurs approches de classification non-supervisées (k-moyennes, regroupement hiérarchique et espérance-maximisation) des occurrences d’un mot pour déterminer les différents sens qu’il peut prendre. Nous comparons nos résultats au Bangla WordNet ainsi qu’à une référence établie pour l’occasion. Nous montrons que cette méthode permet de trouver des sens qui ne se trouvent pas dans le Bangla WordNet.</resume>
			<mots_cles>Induction de sens, bengali, Weka, Classification non-supervisée</mots_cles>
			<title>Word Sense Induction for Lexical Resource Enrichment</title>
			<abstract>In natural language processing, lexico-semantic resources are used in many applications. The manual creation of such resources is very time consuming and their limited coverage does not always satisfy the needs of applications. This problem is further exacerbated with lesser resourced languages. However, in that context, Word Sense Induction (WSI) offers an interesting avenue towards a solution. The purpose of WSI is, from a text corpus, to infer the possible senses for each word contained therein. In this paper, we study an approach based on a vectorial representation of the cooccurrence of word with their neighbours across each usage context. We first build the vectorial representation on a Bangla (also known as Bengali) corpus and then apply and compare three clustering algorithms (k-Means, Hierarchical Clustering and Expectation Maximisation) that elicit clusters corresponding to the different senses of each word as used within a corpus. We wanted to use Bangla WordNet to evaluate the clusters, however, the coverage of Bangla WordNet being restrictive compared to Princeton WordNet ( 23.65%), we find that the clustering algorithms induce correct senses that are not present in Bangla WordNet. Therefore we created a gold standard that we manually extended to include the senses not covered in Bangla WordNet.</abstract>
			<keywords>Word Sense Induction, Bangla, Weka, Clustering</keywords>
		</article>
		<article id="taln-2014-court-036" session="Lexique 2">
			<auteurs>
				<auteur>
					<prenom>François</prenom>
					<nom>Trouilleux</nom>
					<email>francois.trouilleux@univ-bpclermont.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Clermont Université, Université Blaise-Pascal, EA 999, LRL</affiliation>
			</affiliations>
			<titre>Un dictionnaire et une grammaire de composés français</titre>
			<type>court</type>
			<pages>604-609</pages>
			<resume>L’article présente deux ressources pour le TAL, distribuées sous licence GPL : un dictionnaire de mots composés français et une grammaire NooJ spécifiant un sous-ensemble des schémas de composés.</resume>
			<mots_cles>open source, ressources, dictionnaire, grammaire, mots composés</mots_cles>
			<title>A dictionary and a grammar of French compounds</title>
			<abstract>The paper introduces two resources for NLP, available with a GPL license: a dictionary of French compound words and a NooJ grammar which specifies a subset of compound patterns.</abstract>
			<keywords>open source, resources, dictionary, grammar, compound words</keywords>
		</article>
		<article id="taln-2014-demo-001" session="Démonstrations 1">
			<auteurs>
				<auteur>
					<prenom>Jean-Marie</prenom>
					<nom>Pierrel</nom>
					<email>Jean-Marie.Pierrel@atilf.fr</email>
					<email>contact@ortolang.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Lorraine, ATILF, 44 avenue de la Libération 54063 Nancy Cedex</affiliation>
				<affiliation affiliationId="2">CNRS, ATILF, 44 avenue de la Libération 54063 Nancy Cedex</affiliation>
			</affiliations>
			<titre>ORTOLANG : une infrastructure de mutualisation de ressources linguistiques écrites et orales</titre>
			<type>démonstration</type>
			<pages>1-2</pages>
			<resume>Nous proposons une démonstration de la Plateforme de l’Equipex ORTOLANG (Open Resources and Tools for LANGuage : www.ortolang.fr) en cours de mise en place dans le cadre du programme d’investissements d’avenir (PIA) lancé par le gouvernement français. S’appuyant entre autres sur l’existant des centres de ressources CNRTL (Centre National de Ressources Textuelles et Lexicales : www.cnrtl.fr) et SLDR (Speech and Language Data Repository : http://sldr.org/), cette infrastructure a pour objectif d’assurer la gestion, la mutualisation, la diffusion et la pérennisation de ressources linguistiques de type corpus, dictionnaires, lexiques et outils de traitement de la langue, avec une focalisation particulière sur le français et les langues de France.</resume>
			<mots_cles>Ortolang, plateforme, mutualisation, corpus, ressources linguistiques</mots_cles>
			<title>ORTOLANG an infrastructure for sharing of written and speech language resources</title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2014-demo-002" session="Démonstrations 1">
			<auteurs>
				<auteur>
					<prenom>Baptiste</prenom>
					<nom>Chardon</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Louis</prenom>
					<nom>Saint-Maxent</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Patrick</prenom>
					<nom>Séguéla</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
			</affiliations>
			<titre>Utilisabilité d'une ressource propriétaire riche dans le cadre de la classification de documents</titre>
			<type>démonstration</type>
			<pages>3-8</pages>
			<resume>Dans ce papier, nous nous intéressons à l’utilisation d’une ressource linguistique propriétaire riche pour une tâche de classification. L'objectif est ici de mesurer l'impact de l'ajout de ces ressources sur cette tâche en termes de performances. Nous montrons que l’utilisation de cette ressource en temps que traits supplémentaires de classification apporte un réel avantage pour un ajout très modéré en termes de nombre de traits.</resume>
			<mots_cles>classification de documents, classification automatique, ressources</mots_cles>
			<title></title>
			<abstract>In this paper, we focus on the use of a proprietary resource for a document classification task. The objective is here to measure the impact of the addition of this resource as input for classification features. We show that the use of this resource impacts positively the classification results, for a limited impact on the feature number.</abstract>
			<keywords>document level classification, automatic classification, resources</keywords>
		</article>
		<article id="taln-2014-demo-003" session="Démonstrations 1">
			<auteurs>
				<auteur>
					<prenom>Romain</prenom>
					<nom>Laroche</nom>
					<email>romain.laroche@orange.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Orange Labs, 38-40 avenue du Général Leclerc, 92130 Issy-les-Moulineaux / France</affiliation>
			</affiliations>
			<titre>CFAsT: Content-Finder AssistanT</titre>
			<type>démonstration</type>
			<pages>9-10</pages>
			<resume>Cette démonstration de CFAsT s’intéresse à “comment concevoir un système de dialogue avec un effort minimal”. Cet assistant virtuel repose sur un nouveau modèle pour la génération automatique de système de dialogue construite à partir de contenus. Cette approche utilise un moteur de recherche auquel on a ajouté des fonctionnalités de dialogue : à chaque tour, le système propose trois mots-clefs de manière à optimiser l’espérance de gain d’information.</resume>
			<mots_cles>Systèmes de dialogue, Traitement automatique des langues naturelles, Assistant virtuel</mots_cles>
			<title></title>
			<abstract>This CFAsT demonstration focuses on “how to design and develop a dialogue system with a minimal effort”. This virtual assistant embeds a novel model for automatic generation of dialogue systems built from contents. This approach is similar to and relies on a search engine, but with augmented dialogue capabilities : at each dialogue turn, the system propose three keywords, in order to optimise the information gain expectation.</abstract>
			<keywords>Dialogue systems, Natural language processing, Virtual assistant</keywords>
		</article>
		<article id="taln-2014-demo-004" session="Démonstrations 1">
			<auteurs>
				<auteur>
					<prenom>André</prenom>
					<nom>Jaccarini</nom>
					<email>jaccarini@mmsh.univ-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Christian</prenom>
					<nom>Gaubert</nom>
					<email>cgaubert@ifao.egnet.net</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">MMSH, CNRS, 5 rue du Château de l’Horloge, 13094 Aix-en-Provence</affiliation>
				<affiliation affiliationId="2">IFAO, 37 rue Cheikh Aly Yousef, Qasr al Ayni, Le Caire, Egypte</affiliation>
			</affiliations>
			<titre>Démonstration de Kawâkib, outil permettant d’assurer le feedback entre grammaire et corpus arabe pour l’élaboration d’un modèle théorique</titre>
			<type>démonstration</type>
			<pages>11-12</pages>
			<resume>Kawâkib est un outil assurant le feedback entre corpus arabe et grammaire. Ce logiciel interactif en ligne démontre le bien fondé de la méthode de variation des grammaires arabes pour l'obtention de l'algorithme optimal tant au niveau de l'analyse morphologique, cruciale étant donnée la structure du système sémitique, que syntaxique ou dans le domaine de la recherche de critères pertinents et discriminants pour le filtrage des textes.</resume>
			<mots_cles>arabe, automates, analyseurs, opérateurs linguistiques, mots-outils, filtrage de corpus</mots_cles>
			<title>Kawâkib, a Tool Providing Feedback Between Grammar and Arabic Corpus for the Building of a Theoritical Model</title>
			<abstract>Kawâkib is a tool allowing feedback between arabic corpus and grammar. As far as methodology is concerned, this interactive online software implements and illustrates the grammar variation method that aims to determine the optimal algorithm, either for morphology – which is essential in semitic languages - or for syntax. The software also permits the search for criteria for text filtering.</abstract>
			<keywords>arabic, automata, parsers, lingistic operators, tool words, corpus filtering</keywords>
		</article>
		<article id="taln-2014-demo-005" session="Démonstrations 1">
			<auteurs>
				<auteur>
					<prenom>Christophe</prenom>
					<nom>Dany</nom>
					<email>christophe.dany@owi-tech.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Ilhème</prenom>
					<nom>Ghalamallah</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">OWI, 31 avenue du Général Leclerc, 92340 Bourg-la-Reine</affiliation>
			</affiliations>
			<titre>OWI.Chat : Assistance sémantique pour un conseiller Chat, grâce à la théorie OWI</titre>
			<type>démonstration</type>
			<pages>13-14</pages>
			<resume>La canal chat permet aux entreprises de transformer leur site web en un véritable lieu d’achat et de service. OWI a développé un outil d’assistance aux conversations en ligne (OWI.Chat), qui analyse les messages des internautes et conseille les conseillers en temps réel.</resume>
			<mots_cles>Analyse sémantique, moteur sémantique Chat, Live Chat, Conversation en ligne, Traitement Automatique du Langage (TAL), Base de connaissance, Relation client</mots_cles>
			<title>OWI.Chat : a semantic assistance for chat customer service agents</title>
			<abstract>Chat channel enables companies to transform their website into a real place of purchase and service. OWI developed an online conversations assistance solution (OWI.Chat). Its main task is to analyze the Chat requests and help agents in real time.</abstract>
			<keywords>Semantic analysis, semantic engine, Chat, Live Chat, Online conversation, Natural Language Procession (NLP), Agent Knowledge Base, Customer relationship</keywords>
		</article>
		<article id="taln-2014-demo-006" session="Démonstrations 1">
			<auteurs>
				<auteur>
					<prenom>Karën</prenom>
					<nom>Fort</nom>
					<email>karen.fort@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Bruno</prenom>
					<nom>Guillaume</nom>
					<email>bruno.guillaume@loria.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Valentin</prenom>
					<nom>Stern</nom>
					<email>valentin.stern@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LORIA, Université de Lorraine</affiliation>
				<affiliation affiliationId="2">LORIA, Inria Nancy Grand-Est</affiliation>
			</affiliations>
			<titre>ZOMBILINGO : manger des têtes pour annoter en syntaxe de dépendances</titre>
			<type>démonstration</type>
			<pages>15-16</pages>
			<resume>Cet article présente ZOMBILINGO un jeu ayant un but (Game with a purpose) permettant d’annoter des corpus en syntaxe de dépendances. Les annotations créées sont librement disponibles sur le site du jeu.</resume>
			<mots_cles>jeux ayant un but, complexité, annotation, syntaxe en dépendances</mots_cles>
			<title>ZOMBILINGO: eating heads to perform dependency syntax annotation</title>
			<abstract>This paper presents ZOMBILINGO, a Game With A Purpose (GWAP) that allows for the dependency syntax annotation of French corpora. The created resource is freely available on the game Web site.</abstract>
			<keywords>GWAP, complexity, annotation, dependency syntax</keywords>
		</article>
		<article id="taln-2014-demo-007" session="Démonstrations 1">
			<auteurs>
				<auteur>
					<prenom>François-Régis</prenom>
					<nom>Chaumartin</nom>
					<email>frc@proxem.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Proxem, 19 boulevard de Magenta, 75010 Paris</affiliation>
			</affiliations>
			<titre>Ubiq : une plateforme de collecte, analyse et valorisation des corpus</titre>
			<type>démonstration</type>
			<pages>17-18</pages>
			<resume>Proxem édite Ubiq, une plateforme de collecte de documents et d’analyse sémantique, capable d'extraire des informations pertinentes à partir du contenu de vastes corpus. Les documents analysés sont d’une grande diversité : opinions collectées sur des sites web, emails de réclamation ou de demande d’information, réponse à des questions ouvertes dans des sondages, offres ou demandes d’emploi, etc. La reconnaissance des entités nommées joue un rôle central car c’est un préalable à d’autres traitements sémantiques. La conception d’un module de reconnaissance d’entités nommées nécessite généralement un investissement important en amont, avec une adaptation de domaine. Ubiq propose une approche d’apprentissage faiblement supervisé de l’extraction d’entités nommées qui tient compte du corpus collecté et de ressources externes (Wikipédia). La méthode et l’outillage développés permettent de déterminer à la volée, en interaction avec l’utilisateur, la granularité des types d’entités adaptée à un corpus de texte tout-venant.</resume>
			<mots_cles>entités nommées, désambiguïsation, apprentissage, Wikipédia, catégorisation</mots_cles>
			<title>Ubiq: a platform for crawling, analyzing and exploiting corpora</title>
			<abstract>Proxem publishes Ubiq, a platform for web crawling and semantic analysis, which can extract relevant information from large corpus. Documents are of great variety: reviews crawled from websites, emails about complaints or requests for information, answers to open questions in surveys, employment offers or job applications, etc. Named Entity Recognition plays a key role since it is a prerequisite to further semantic processing. The design of a NER module generally requires a significant upfront investment with some domain adaptation. Ubiq proposes a semi-supervised approach to NER that takes into account the crawled corpus and external resources (Wikipedia). The proposed method and tools allow to get on the fly, with some user interaction, the type granularity of entities suitable for a given corpus.</abstract>
			<keywords>named entities, disambiguation, machine learning, Wikipedia, categorization</keywords>
		</article>
		<article id="taln-2014-demo-008" session="Démonstrations 2">
			<auteurs>
				<auteur>
					<prenom>Fabrizio</prenom>
					<nom>Gotti</nom>
					<email>gottif@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Guy</prenom>
					<nom>Lapalme</nom>
					<email>lapalme@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">RALI, Université de Montréal, CP 6128 Succursale Centre-Ville, Montréal, Canada, H3C 3J7</affiliation>
			</affiliations>
			<titre>Zodiac : Insertion automatique des signes diacritiques du français</titre>
			<type>démonstration</type>
			<pages>19-20</pages>
			<resume>Nous proposons dans cette démonstration de présenter le logiciel Zodiac, permettant l’insertion automatique de diacritiques (accents, cédilles, etc.) dans un texte français. Zodiac prend la forme d’un complément Microsoft Word sous Windows permettant des corrections automatiques du texte au cours de la frappe. Sous Linux et Mac OS X, il est implémenté comme un programme sur ligne de commande, se prêtant naturellement à lire ses entrées sur un « pipeline » et écrire ses sorties sur la sortie standard. Implémenté en UTF-8, il met en oeuvre diverses librairies C++ utiles à certaines tâches du TAL, incluant la manipulation de modèles de langue statistiques.</resume>
			<mots_cles>aide à la rédaction, diacritiques, modèles de langue probabilistes</mots_cles>
			<title>A Tool for the Automatic Insertion of Diacritics in French</title>
			<abstract>In this demo session, we propose to show how the software module Zodiac works. It allows the automatic insertion of diacritical marks (accents, cedillas, etc.) in text written in French. Zodiac is implemented as a Microsoft Word add-in under Windows, allowing automatic corrections as the user is typing. Under Linux and Mac OS X, it is implemented as a command-line utility, lending itself naturally to be used in a text-processing pipeline. Zodiac handles UTF-8, and showcases some useful C++ libraries for natural language processing, including statistical language modeling.</abstract>
			<keywords>text editing, diacritical marks, statistical language models</keywords>
		</article>
		<article id="taln-2014-demo-009" session="Démonstrations 2">
			<auteurs>
				<auteur>
					<prenom>Mehdi</prenom>
					<nom>Embarek</nom>
					<email>embarekm@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">MK SOFT, 11 rue des fossés St Marcel, 75005 Paris</affiliation>
			</affiliations>
			<titre>Le système STAM</titre>
			<type>démonstration</type>
			<pages>21-22</pages>
			<resume>Le projet STAM aborde la problématique de la transcription automatique du langage texto (SMS) et plus particulièrement la traduction des messages écrits en arabe dialectal. L’objectif du système STAM est de traduire automatiquement des textes rédigés en langage SMS dans un dialecte parlé dans le monde arabe (langue source) en un texte facilement interprétable, compréhensible et en bon français (langue cible).</resume>
			<mots_cles>Dialecte, SMS, Transcription, STAM</mots_cles>
			<title>The STAM System</title>
			<abstract>The STAM project addresses the problem of automatic transcription of SMS language and especially the translation of messages written in Arabic dialect. The objective of STAM system is to automatically translate texts written in SMS language in a dialect spoken in the Arab World (source language) into a French text (target language), interpretable and understandable.</abstract>
			<keywords>Dialect, SMS, Transcription, STAM</keywords>
		</article>
		<article id="taln-2014-demo-010" session="Démonstrations 2">
			<auteurs>
				<auteur>
					<prenom>Hatim</prenom>
					<nom>Khouzaimi</nom>
					<email>hatim.khouzaimi@orange.com</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Romain</prenom>
					<nom>Laroche</nom>
					<email>romain.laroche@orange.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Fabrice</prenom>
					<nom>Lefèvre</nom>
					<email>fabrice.lefevre@univ-avignon.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Orange Labs, 38-40 Avenue du Général Leclerc, 92794 Issy-les-Moulineaux, France</affiliation>
				<affiliation affiliationId="2">Laboratoire Informatique d’Avignon, 339 Chemin des Meinajaries, 84911 Avignon, France</affiliation>
			</affiliations>
			<titre>DictaNum : système de dialogue incrémental pour la dictée de numéros.</titre>
			<type>démonstration</type>
			<pages>23-25</pages>
			<resume>Les stratégies de dialogue incrémentales offrent une meilleure réactivité, une expérience utilisateur plus aboutie et une réduction du risque de désynchronisation. Cependant, les systèmes de dialogue incrémentaux sont basés sur une architecture logicielle dont l’implantation est longue, difficile et donc coûteuse. Pour faciliter cette évolution d’architecture, nous proposons de simuler un comportement incrémental en ajoutant une surcouche à un service de dialogue traditionnel existant. DictaNum est un démonstrateur de dialogue incrémental mettant en oeuvre cette démarche. Sa tâche consiste à recueillir des numéros auprès des utilisateurs. Grâce à son fonctionnement incrémental, il autorise une correction rapide des erreurs au fil de la dictée.</resume>
			<mots_cles>Systèmes de Dialogue, Traitement Incrémental, Architecture des Systèmes de Dialogue</mots_cles>
			<title>DictaNum: a dialogue system for numbers dictation</title>
			<abstract>Incremental dialogue strategies are more reactive, offer a better user experience and reduce desynchronisation risks. However, incremental dialogue systems are based on architectures that are long, difficult and hence costly to implement. In order to make this architecture evolution easier, we suggest to simulate incremental behavior by adding a new layer to an existing traditional service. DictaNum is an incremental dialogue demonstrator that uses this approach. It collects numbers dictated by the user. Thanks to its incremental behavior, it makes it possible to rapidly correct errors on the fly.</abstract>
			<keywords>Dialogue Systems, Incremental Processing, Dialogue Systems Architectures</keywords>
		</article>
		<article id="taln-2014-demo-011" session="Démonstrations 2">
			<auteurs>
				<auteur>
					<prenom>Li</prenom>
					<nom>Gong</nom>
					<email>li.gong@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Aurélien</prenom>
					<nom>Max</nom>
					<email>aurelien.max@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>François</prenom>
					<nom>Yvon</nom>
					<email>francois.yvon@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS, Orsay, France</affiliation>
				<affiliation affiliationId="2">Univ. Paris Sud, Orsay, France</affiliation>
			</affiliations>
			<titre>Construction (très) rapide de tables de traduction à partir de grands bi-textes</titre>
			<type>démonstration</type>
			<pages>26-27</pages>
			<resume>Dans cet article de démonstration, nous introduisons un logiciel permettant de construire des tables de traduction de manière beaucoup plus rapide que ne le font les techniques à l’état de l’art. Cette accélération notable est obtenue par le biais d’un double échantillonnage : l’un permet la sélection d’un nombre limité de bi-phrases contenant les segments à traduire, l’autre réalise un alignement à la volée de ces bi-phrases pour extraire des exemples de traduction.</resume>
			<mots_cles>traduction automatique statistique, développement efficace, temps de calcul</mots_cles>
			<title>(Much) Faster Construction of SMT Phrase Tables from Large-scale Parallel Corpora</title>
			<abstract></abstract>
			<keywords>statistical machine translation, efficient development, computation time</keywords>
		</article>
		<article id="taln-2014-demo-012" session="Démonstrations 2">
			<auteurs>
				<auteur>
					<prenom>Tatiana</prenom>
					<nom>Ekeinhor-Komi</nom>
					<email>Tatiana.EkeinhorKomi@orange.com</email>
					<affiliationId>1</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Hajar</prenom>
					<nom>Falih</nom>
					<email>Hajar.Falih@orange.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Christine</prenom>
					<nom>Chardenon</nom>
					<email>Christine.Chardenon@orange.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Romain</prenom>
					<nom>Laroche</nom>
					<email>Romain.Laroche@orange.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Fabrice</prenom>
					<nom>Lefèvre</nom>
					<email>fabrice.lefevre@univ-avignon.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Orange Labs, 2 Avenue Pierre Marzin, 22300 Lannion</affiliation>
				<affiliation affiliationId="2">Orange Labs, 38-40 Rue du Général Leclerc, 92130 Issy les Moulineaux</affiliation>
				<affiliation affiliationId="3">LIA-CERI, Université d’Avignon, France</affiliation>
			</affiliations>
			<titre>Un assistant vocal personnalisable</titre>
			<type>démonstration</type>
			<pages>28-29</pages>
			<resume>Nous proposons la démonstration d’un assistant personnel basé sur une architecture distribuée. Un portail vocal relie l’utilisateur à des applications. Celles-ci sont installées par l’utilisateur qui compose de ce fait son propre assistant personnel selon ses besoins.</resume>
			<mots_cles>Système de dialogue, applications du traitement automatique du langage naturel, assistant personnel</mots_cles>
			<title>Enia : A customizable multi-domain assistant</title>
			<abstract>We introduce a personal assistant based on a distributed architecture. A portal connects user to applications. Applications are installed by a user who compose his own assistant according to his needs.</abstract>
			<keywords>Dialogue system, natural language processing applications, personal assistant</keywords>
		</article>
		<article id="taln-2014-demo-013" session="Démonstrations 2">
			<auteurs>
				<auteur>
					<prenom>Bruno</prenom>
					<nom>Gaume</nom>
					<email>gaume@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Emmanuel</prenom>
					<nom>Navarro</nom>
					<email>navarro@irit.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Yann</prenom>
					<nom>Desalle</nom>
					<email>yann.desalle@gmail.com</email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Benoît</prenom>
					<nom>Gaillard</nom>
					<email>benoit.gd@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CLLE-ERSS, CNRS, Universié de Toulouse</affiliation>
				<affiliation affiliationId="2">IRIT, CNRS, Université de Toulouse</affiliation>
				<affiliation affiliationId="3">ATILF, CNRS, Université de Lorraine</affiliation>
			</affiliations>
			<titre>Mesurer la similarité structurelle entre réseaux lexicaux</titre>
			<type>démonstration</type>
			<pages>30-39</pages>
			<resume>Dans cet article, nous comparons la structure topologique des réseaux lexicaux avec une méthode fondée sur des marches aléatoires. Au lieu de caractériser les paires de sommets selon un critère binaire de connectivité, nous mesurons leur proximité structurelle par la probabilité relative d’atteindre un sommet depuis l’autre par une courte marche aléatoire. Parce que cette proximité rapproche les sommets d’une même zone dense en arêtes, elle permet de comparer la structure topologique des réseaux lexicaux.</resume>
			<mots_cles>Réseaux lexicaux, réseaux petits mondes, comparaison de graphes, marches aléatoires</mots_cles>
			<title traduction="Florian">Measuring the structural similarity between lexical networks</title>
			<abstract>In this paper, we compare the topological structure of lexical networks with a method based on random walks. Instead of characterising pairs of vertices according only to whether they are connected or not, we measure their structural proximity by evaluating the relative probability of reaching one vertex from the other via a short random walk. This proximity between vertices is the basis on which we can compare the topological structure of lexical networks because it outlines the similar dense zones of the graphs.</abstract>
			<keywords>Lexical networks, small worlds, comparison graphs, random walks</keywords>
		</article>
		<article id="taln-2014-demo-014" session="Démonstrations 2">
			<auteurs>
				<auteur>
					<prenom>Yoann</prenom>
					<nom>Dupont</nom>
					<email>yoann.dupont@etud.sorbonne-nouvelle.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Isabelle</prenom>
					<nom>Tellier</nom>
					<email>isabelle.tellier@univ-paris3.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Paris 3 Sorbonne Nouvelle, 13, rue de Santeuil - 75231 Paris Cedex 05</affiliation>
			</affiliations>
			<titre>Un reconnaisseur d’entités nommées du Français</titre>
			<type>démonstration</type>
			<pages>40-41</pages>
			<resume>Nous proposons une démonstration d’un reconnaisseur d’entités nommées du Français appris automatiquement sur le French TreeBank annoté en entités nommées.</resume>
			<mots_cles>REN, POS, apprentissage automatique, French Treebank, extraction d’information, CRF</mots_cles>
			<title>A Named Entity recognizer for French</title>
			<abstract>We propose to demonstrate a french named entity recognizer trained on the French TreeBank enriched with named entity annotations.</abstract>
			<keywords>NER, POS, machine learning, French Treebank, information extraction, CRF</keywords>
		</article>
	</articles>
</conference>
<conference>
    <edition>
      <acronyme>TALN'2007</acronyme>
      <titre>conférence sur le Traitement Automatique des Langues Naturelles</titre>
      <ville>Toulouse</ville>
      <pays>France</pays>
      <dateDebut>2007-06-05</dateDebut>
      <dateFin>2007-06-08</dateFin>
      <presidents>
        <nom>Nabil Hathout</nom>
        <nom>Philippe Muller</nom>
      </presidents>
      <typeArticles>
        <type id="long">Papiers longs</type>
        <type id="poster">Posters</type>
        <type id="démonstration">Démonstrations</type>
      </typeArticles>
      <statistiques>
        <!-- <acceptations id="long" soumissions=""></acceptations> -->
        <!-- <acceptations id="court" soumissions=""></acceptations> -->
      </statistiques>
      <siteWeb>http://www.irit.fr/taln07/</siteWeb>
      <meilleurArticle>
        <articleId>taln-2007-long-009</articleId>
      </meilleurArticle>
    </edition>
    <articles>
      <article id="taln-2007-long-001" session="Segmentation">
        <auteurs>
          <auteur>
            <nom>Maria Georgescul</nom>
            <email>maria.georgescul@eti.unige.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Alexander Clarck</nom>
            <email>alexc@cs.rhul.ac.uk</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Susan Armstrong</nom>
            <email>susan.armstrong@issco.unige.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">ISSCO/TIM/ETI, University of Geneva</affiliation>
          <affiliation affiliationId="2">Department of Computer Science, Royal Holloway University of London</affiliation>
        </affiliations>
        <titre/>
        <type>long</type>
        <pages>15-24</pages>
        <resume>Dans cet article, nous traitons de la segmentation automatique des textes en épisodes thématiques non superposés et ayant une structure linéaire. Notre étude porte sur l’utilisation des traits lexicaux, acoustiques et syntaxiques et sur l’influence de ces traits sur la performance d’un système automatique de segmentation thématique. Nous appliquons notre approche, basée sur des machines à vecteurs support, à des transcriptions des dialogues multilocuteurs.</resume>
        <mots_cles>segmentation automatique en épisodes thématiques, machines à vecteurs support, dialogues multi-locuteurs</mots_cles>
        <title>Exploiting structural meeting-specific features for topic segmentation</title>
        <abstract>In this article we address the task of automatic text structuring into linear and non-overlapping thematic episodes. Our investigation reports on the use of various lexical, acoustic and syntactic features, and makes a comparison of how these features influence performance of automatic topic segmentation. Using datasets containing multi-party meeting transcriptions, we base our experiments on a proven state-of-the-art approach using support vector classification.</abstract>
        <keywords>automatic topic segmentation, support vector machines, multi-party dialogues</keywords>
      </article>
      <article id="taln-2007-long-002" session="Segmentation">
        <auteurs>
          <auteur>
            <nom>Silvia Fernández</nom>
            <email>silvia.fernandez@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Eric Sanjuan</nom>
            <email>eric.sanjuan@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Juan-Manuel Torres-Moreno</nom>
            <email>juan-manuel.torres@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire Informatique d’Avignon, BP 1228 84911 Avignon FRANCE</affiliation>
          <affiliation affiliationId="2">LPM UHP-Nancy, BP 239 54506 Vandoeuvre les Nancy FRANCE</affiliation>
          <affiliation affiliationId="3">École Polytechnique de Montréal, CP 6079 Centre-ville, Montréal, Québec, CANADA H3C3A7</affiliation>
        </affiliations>
        <titre>Énergie textuelle de mémoires associatives</titre>
        <type>long</type>
        <pages>25-34</pages>
        <resume>Dans cet article, nous présentons une approche de réseaux de neurones inspirée de la physique statistique de systèmes magnétiques pour étudier des problèmes fondamentaux du Traitement Automatique de la Langue Naturelle. L’algorithme modélise un document comme un système de neurones où l’on déduit l’énergie textuelle. Nous avons appliqué cette approche aux problèmes de résumé automatique et de détection de frontières thématiques. Les résultats sont très encourageants.</resume>
        <mots_cles>réseaux de neurones, réseaux de Hopfield, résumé, frontière thématiques</mots_cles>
        <title/>
        <abstract>In this paper we present a neural networks approach, inspired by statistical physics of magnetic systems, to study fundamental problems in Natural Language Processing. The algorithm models documents as neural network whose textual energy is studied. We obtained good results on the application of this method to automatic summarization and thematic borders detection.</abstract>
        <keywords>neural networks, Hopfield network, summarization, thematic boundary</keywords>
      </article>
      <article id="taln-2007-long-003" session="Acquisition">
        <auteurs>
          <auteur>
            <nom>Mehdi Embarek</nom>
            <email>embarekm@zoe.cea.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Olivier Ferret</nom>
            <email>ferreto@zoe.cea.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CEA LIST, LIC2M, 18 route du Panorama, BP6, FONTENAY AUX ROSES, F- 92265 France</affiliation>
        </affiliations>
        <titre>Une expérience d’extraction de relations sémantiques à partir de textes dans le domaine médical</titre>
        <type>long</type>
        <pages>37-46</pages>
        <resume>Dans cet article, nous présentons une méthode permettant d’extraire à partir de textes des relations sémantiques dans le domaine médical en utilisant des patrons linguistiques. La première partie de cette méthode consiste à identifier les entités entre lesquelles les relations visées interviennent, en l’occurrence les maladies, les examens, les médicaments et les symptômes. La présence d’une des relations sémantiques visées dans les phrases contenant un couple de ces entités est ensuite validée par l’application de patrons linguistiques préalablement appris de manière automatique à partir d’un corpus annoté. Nous rendons compte de l’évaluation de cette méthode sur un corpus en Français pour quatre relations.</resume>
        <mots_cles>extraction de relations sémantiques, patrons lexico-syntaxiques, domaine médical</mots_cles>
        <title/>
        <abstract>In this article, we present a method to extract semantic relations automatically in the medical domain using linguistic patterns. This method consists first in identifying the entities that are part of the relations to extract, that is to say diseases, exams, treatments, drugs and symptoms. Thereafter, sentences that contain these entities are extracted and the presence of a semantic relation is validated by applying linguistic patterns that were automatically learnt from an annotated corpus. We report the results of an evaluation of our extraction method on a French corpus for four relations.</abstract>
        <keywords>extraction of semantic relations, lexico-syntactic patterns, medical domain</keywords>
      </article>
      <article id="taln-2007-long-004" session="Acquisition">
        <auteurs>
          <auteur>
            <nom>Davy Weissenbacher</nom>
            <email>dw@lipn.univ-paris13.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Adeline Nazarenko</nom>
            <email>nazarenko@lipn.univ-paris13.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université Paris-Nord, LIPN, 99 av. J-B. Clément, F-93430 Villetaneuse</affiliation>
        </affiliations>
        <titre>Identifier les pronoms anaphoriques et trouver leurs antécédents : l’intérêt de la classification bayésienne</titre>
        <type>long</type>
        <pages>47-56</pages>
        <resume>On oppose souvent en TAL les systèmes à base de connaissances linguistiques et ceux qui reposent sur des indices de surface. Chaque approche a ses limites et ses avantages. Nous proposons dans cet article une nouvelle approche qui repose sur les réseaux bayésiens et qui permet de combiner au sein d’une même représentation ces deux types d’informations hétérogènes et complémentaires. Nous justifions l’intérêt de notre approche en comparant les performances du réseau bayésien à celles des systèmes de l’état de l’art, sur un problème difficile du TAL, celui de la résolution d’anaphore.</resume>
        <mots_cles>réseaux bayésiens, résolution des anaphores, connaissance linguistique, indice de surface</mots_cles>
        <title/>
        <abstract>In NLP, a traditional distinction opposes linguistically-based systems and knowledge-poor ones, which mainly rely on surface clues. Each approach has its drawbacks and its advantages. In this paper, we propose a new approach based on Bayes Networks that allows to combine both types of information. As a case study, we focus on the anaphora resolution which is known as a difficult NLP problem. We show that our bayesain system performs better than a state-of-the art one for this task.</abstract>
        <keywords>bayesian network, anaphora resolution, linguistic knowledge, surface clue</keywords>
      </article>
      <article id="taln-2007-long-005" session="Morphologie">
        <auteurs>
          <auteur>
            <nom>Bruno Cartoni</nom>
            <email>bruno.cartoni@eti.unige.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">ISSCO/TIM/ETI – Université de Genève, 40 bd du Pont d’Arve, 1205 Genève</affiliation>
        </affiliations>
        <titre>Régler les règles d’analyse morphologique</titre>
        <type>long</type>
        <pages>59-68</pages>
        <resume>Dans cet article, nous présentons différentes contraintes mécaniques et linguistiques applicables à des règles d’analyse des mots inconnus afin d’améliorer la performance d’un analyseur morphologique de l’italien. Pour mesurer l’impact de ces contraintes, nous présentons les résultats d’une évaluation de chaque contrainte qui prend en compte les gains et les pertes qu’elle engendre. Nous discutons ainsi de la nécessaire évaluation de chaque réglage apporté aux règles afin d’en déterminer la pertinence.</resume>
        <mots_cles>évaluation, analyse morphologique, mots inconnus, morphologie constructionnelle</mots_cles>
        <title/>
        <abstract>In this article, we present various constraints, mechanical and linguistic, that can be applied to analysing rules for unknown words in order to improve the performance of a morphological analyser for Italian. To measure the impact of these constraints, we present an evaluation for each constraint, taking into account the gains and losses which they generate. We then discuss the need to evaluate any fine-tuning of these kinds of rules in order to decide whether they are appropriate or not.</abstract>
        <keywords>evaluation, morphological analysis, unknown words, constructional morphology</keywords>
      </article>
      <article id="taln-2007-long-006" session="Morphologie">
        <auteurs>
          <auteur>
            <nom>François Barthélemy</nom>
            <email>barthe@cnam.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CNAM, Cédric, 292 rue Saint-Martin, 75003 Paris</affiliation>
          <affiliation affiliationId="2">INRIA, Atoll, 78153 Le Chesnay cedex</affiliation>
        </affiliations>
        <titre>Structures de traits typées et morphologie à partitions</titre>
        <type>long</type>
        <pages>69-78</pages>
        <resume>Les structures de traits typées sont une façon abstraite et agréable de représenter une information partielle. Dans cet article, nous montrons comment la combinaison de deux techniques relativement classiques permet de définir une variante de morphologie à deux niveaux intégrant harmonieusement des structures de traits et se compilant en une machine finie. La première de ces techniques est la compilation de structure de traits en expressions régulières, la seconde est la morphologie à partition. Nous illustrons au moyen de deux exemples l’expressivité d’un formalisme qui rapproche les grammaires à deux niveaux des grammaires d’unification.</resume>
        <mots_cles>morphologie à deux niveaux, transducteurs finis à états, structure de traits</mots_cles>
        <title/>
        <abstract>Feature Structures are an abstract and convenient way of representing partial information. In this paper, we show that the combination of two relatively classical techniques makes possible the definition of a variant of two-level morphology which integrates harmoniously feature structures and compiles into finite-state machines. The first technique is the compilation of feature structures into regular expressions, the second one is partition-based morphology. Two examples are given, which show that our formalism is close to unification grammars.</abstract>
        <keywords>two-level morphology, finite-state transducers, feature structures</keywords>
      </article>
      <article id="taln-2007-long-007" session="Morphologie">
        <auteurs>
          <auteur>
            <nom>Louise Deléger</nom>
            <email>louise.deleger@spim.jussieu.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Fiammetta Namer</nom>
            <email>fiammetta.namer@univ-nancy2.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Pierre Zweigenbaum</nom>
            <email>pz@limsi.fr</email>
            <affiliationId>3</affiliationId>
            <affiliationId>4</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">INSERM, UMR_S 872, Éq. 20, Les Cordeliers, 75006 Paris, Université Pierre et Marie Curie-Paris6, UMR_S 872, 75006 Paris, Université Paris Descartes, UMR_S 872, 75006 Paris</affiliation>
          <affiliation affiliationId="2">ATILF et Université Nancy 2, CLSH, 54015 Nancy</affiliation>
          <affiliation affiliationId="3">CNRS, UPR3251, LIMSI, 91403 Orsay</affiliation>
          <affiliation affiliationId="4">INALCO, CRIM, 75343 Paris Cedex 07</affiliation>
        </affiliations>
        <titre>Analyse morphosémantique des composés savants : transposition du français à l’anglais</titre>
        <type>long</type>
        <pages>79-88</pages>
        <resume>La plupart des vocabulaires spécialisés comprennent une part importante de lexèmes morphologiquement complexes, construits à partir de racines grecques et latines, qu’on appelle « composés savants ». Une analyse morphosémantique permet de décomposer et de donner des définitions à ces lexèmes, et semble pouvoir être appliquée de façon similaire aux composés de plusieurs langues. Cet article présente l’adaptation d’un analyseur morphosémantique, initialement dédié au français (DériF), à l’analyse de composés savants médicaux anglais, illustrant ainsi la similarité de structure de ces composés dans des langues européennes proches. Nous exposons les principes de cette transposition et ses performances. L’analyseur a été testé sur un ensemble de 1299 lexèmes extraits de la terminologie médicale WHO-ART : 859 ont pu être décomposés et définis, dont 675 avec succès. Outre une simple transposition d’une langue à l’autre, la méthode montre la potentialité d’un système multilingue.</resume>
        <mots_cles>analyse morphosémantique, composition savante, terminologie médicale</mots_cles>
        <title/>
        <abstract>Medical language, as many technical languages, is rich with morphologically complex words, many of which take their roots in Greek and Latin – in which case they are called neoclassical compounds. Morphosemantic analysis can help generate decompositions and definitions of such words, and is likely to be similarly applicable to compounds from different languages. This paper reports work on the adaptation of a morphosemantic analyzer dedicated to French (DériF) to analyze English medical neoclassical compounds, and shows the similarity in structure of compounds from related European languages. It presents the principles of this transposition and its current performance. The analyzer was tested on a set of 1,299 compounds extracted from theWHO-ART terminology: 859 could be decomposed and defined, 675 of which successfully. Aside from simple transposition from one language to another, the method also emphasizes the potentiality for a multilingual system.</abstract>
        <keywords>morphosemantic analysis, neo-classical compounding,medical terminology</keywords>
      </article>
      <article id="taln-2007-long-008" session="Traduction">
        <auteurs>
          <auteur>
            <nom>Oana Frunza</nom>
            <email>ofrunza@site.uottawa.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Diana Inkpen</nom>
            <email>diana@site.uottawa.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">School of Information Technology and Engineering, University of Ottawa, Ottawa, ON, K1N 6N5, Canada</affiliation>
        </affiliations>
        <titre/>
        <type>long</type>
        <pages>91-100</pages>
        <resume>Les congénères sont des mots qui ont au moins un sens en commun entre deux langues en plus d‘avoir une orthographie semblable. La reconnaissance de ce type de mots permet aux apprenants de langue seconde ou étrangère d‘enrichir plus rapidement leur vocabulaire et d‘améliorer leur compréhension écrite. Toutefois, les faux amis sont des paires de mots qui à l‘écrit ont des similarités, mais ils ont des significations différentes. Pour leur part, les congénères partiels sont des mots qui ont la même signification dans certains contextes dans chacune des deux langues. Cet article présente une méthode pour la classification automatique des paires des mots classées en congénères ou faux amis, en utilisant des mesures de similarité orthographiques et des méthodes d‘apprentissage automatique. Ainsi, nous construisons des listes complètes des congénères et des faux amis entre les deux langues. Nous désambiguisons les congénères partiels dans des contextes spécifiques. Nos méthodes sont évaluées pour le français et l‘anglais, mais elles seraient applicables à d‘autres paires des langues. Nous avons construit un outil qui prend ces listes et marque dans un texte français les mots qui ont des congénères ou des faux amis en anglais, dans le but d‘aider les apprenants en français langue seconde ou étrangère à améliorer leur compréhension écrite et à développer une meilleure rétention.</resume>
        <mots_cles>congénères, faux amis, congénères partiels, mesures de similarité orthographiques, apprentissage automatique, apprentissage des langues assisté par ordinateur</mots_cles>
        <title>A tool for detecting French-English cognates and false friends</title>
        <abstract>Cognates are pairs of words in different languages similar in spelling and meaning. They can help a second-language learner on the tasks of vocabulary expansion and reading comprehension. False friends are pairs of words that have similar spelling but different meanings. Partial cognates are pairs of words in two languages that have the same meaning in some, but not all contexts. In this article we present a method to automatically classify a pair of words as cognates or false friends, by using several measures of orthographic similarity as features for classification. We use this method to create complete lists of cognates and false friends between two languages. We also disambiguate partial cognates in context. We applied all our methods to French and English, but they can be applied to other pairs of languages as well. We built a tool that takes the produced lists and annotates a French text with equivalent English cognates or false friends, in order to help second-language learners improve their reading comprehension skills and retention rate.</abstract>
        <keywords>cognates, false friends, partial cognates, orthographic similarity measures, machine learning (ML), computer-assisted language learning (CALL)</keywords>
      </article>
      <article id="taln-2007-long-009" session="Traduction">
        <auteurs>
          <auteur>
            <nom>Philippe Langlais</nom>
            <email>felipe@iro.umontreal.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Alexandre Patry</nom>
            <email>patryale@iro.umontreal.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université de Montréal, CP. 6128 succursale centre-ville</affiliation>
        </affiliations>
        <titre>Enrichissement d’un lexique bilingue par analogie</titre>
        <type>long</type>
        <pages>101-110</pages>
        <resume>La présence de mots inconnus dans les applications langagières représente un défi de taille bien connu auquel n’échappe pas la traduction automatique. Les systèmes professionnels de traduction offrent à cet effet à leurs utilisateurs la possibilité d’enrichir un lexique de base avec de nouvelles entrées. Récemment, Stroppa et Yvon (2005) démontraient l’intérêt du raisonnement par analogie pour l’analyse morphologique d’une langue. Dans cette étude, nous montrons que le raisonnement par analogie offre également une réponse adaptée au problème de la traduction d’entrées lexicales inconnues.</resume>
        <mots_cles>analogie formelle, enrichissement de lexiques bilingues, traduction automatique</mots_cles>
        <title/>
        <abstract>Unknown words are a well-known hindrance to natural language applications. In particular, they drastically impact machine translation quality. An easy way out commercial translation systems usually offer their users is the possibility to add unknown words and their translations into a dedicated lexicon. Recently, Stroppa et Yvon (2005) shown how analogical learning alone deals nicely with morphology in different languages. In this study we show that analogical learning offers as well an elegant and efficient solution to the problem of identifying potential translations of unknown words.</abstract>
        <keywords>formal analogy, bilingual lexicon projection, machine translation</keywords>
      </article>
      <article id="taln-2007-long-010" session="Traduction">
        <auteurs>
          <auteur>
            <nom>Vincent Claveau</nom>
            <email>Vincent.Claveau@irisa.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">IRISA - CNRS, Campus de Beaulieu, 35042 Rennes cedex, France</affiliation>
        </affiliations>
        <titre>Inférence de règles de réécriture pour la traduction de termes biomédicaux</titre>
        <type>long</type>
        <pages>111-120</pages>
        <resume>Dans le domaine biomédical, le caractère multilingue de l’accès à l’information est un problème d’importance. Dans cet article nous présentons une technique originale permettant de traduire des termes simples du domaine biomédical de et vers de nombreuses langues. Cette technique entièrement automatique repose sur l’apprentissage de règles de réécriture à partir d’exemples et l’utilisation de modèles de langues. Les évaluations présentées sont menées sur différentes paires de langues (français-anglais, espagnol-portugais, tchèque-anglais, russe-anglais...). Elles montrent que cette approche est très efficace et offre des performances variables selon les langues mais très bonnes dans l’ensemble et nettement supérieures à celles disponibles dans l’état de l’art. Les taux de précision de traductions s’étagent ainsi de 57.5% pour la paire russe-anglais jusqu’à 85% pour la paire espagnol-portugais et la paire françaisanglais.</resume>
        <mots_cles>traduction artificielle, terminologie biomédicale, apprentissage artificiel, modèles de langue</mots_cles>
        <title/>
        <abstract>In the biomedical domain, offering a multilingual access to specialized information is a major issue. In this paper, we present an original approach to translate simple biomedical terms between several languages. This fully automatic approach is based on a machine learning technique inferring rewriting rules and on language models. The experiments that are presented are done onn several language pairs (French-English, Spanish-Portuguese, Czech-English, Russian-English...). They demonstrate the efficiency of our approach by yielding translation performances that vary according to the languages but are always very good and better than those of state-of-art techniques. Indeed, the translation precision rates go from 57.5% for translation from Russian to English up to 85% for Spanish-Portuguese and French-English language pairs.</abstract>
        <keywords>machine translation, biomedical terminology, machine learning, language models</keywords>
      </article>
      <article id="taln-2007-long-011" session="Outils">
        <auteurs>
          <auteur>
            <nom>Émilie Guimier De Neef</nom>
            <email>emilie.guimierdeneef@orange-ftgroup.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Arnaud Debeurme</nom>
            <email>arnaud.debeurme@orange-ftgroup.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Jungyeul Park</nom>
            <email>jungyeul.park@orange-ftgroup.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">FTR&amp;D/TECH/EASY – France Telecom R&amp;D, 2, avenue Pierre Marzin, 22300 Lannion Cedex, France</affiliation>
        </affiliations>
        <titre>TiLT correcteur de SMS : évaluation et bilan qualitatif</titre>
        <type>long</type>
        <pages>123-132</pages>
        <resume>Nous présentons le logiciel TiLT pour la correction des SMS et évaluons ses performances sur le corpus de SMS du DELIC. L'évaluation utilise la distance de Jaccard et la mesure BLEU. La présentation des résultats est suivie d'une analyse qualitative du système et de ses limites.</resume>
        <mots_cles>SMS, SMS corpus, correction orthographique, TiLT, evaluation</mots_cles>
        <title/>
        <abstract>This paper presents TiLT system which allows us to correct spelling errors in SMS messages to standard French. We perform Jaccard and Bleu metrics for its evaluation using the DELIC SMS corpus as a reference. We discuss qualitative analyses of system and its limits.</abstract>
        <keywords>SMS, SMS corpus, spelling correction, TiLT, evaluation</keywords>
      </article>
      <article id="taln-2007-long-012" session="Outils">
        <auteurs>
          <auteur>
            <nom>Hong-Thai Nguyen</nom>
            <email>Hong-Thai.Nguyen@imag.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Christian Boitet</nom>
            <email>Christian.Boitet@imag.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">GETALP, LIG, 385, av. de la Bibliothèque, BP 53 F-38041 Grenoble cedex 9</affiliation>
        </affiliations>
        <titre>Vers un méta-EDL complet, puis un EDL universel pour la TAO</titre>
        <type>long</type>
        <pages>133-142</pages>
        <resume>Un “méta-EDL” (méta-Environnement de Développement Linguiciel) pour la TAO permet de piloter à distance un ou plusieurs EDL pour construire des systèmes de TAO hétérogènes. Partant de CASH, un méta-EDL dédié à Ariane-G5, et de WICALE 1.0, un premier méta-EDL générique mais aux fonctionnalités minimales, nous dégageons les problèmes liés à l’ajout de fonctionnalités riches comme l’édition et la navigation en local, et donnons une solution implémentée dans WICALE 2.0. Nous y intégrons maintenant une base lexicale pour les systèmes à « pivot lexical », comme UNL/U++. Un but à plus long terme est de passer d’un tel méta-EDL générique multifonctionnel à un EDL « universel », ce qui suppose la réingénierie des compilateurs et des moteurs des langages spécialisés pour la programmation linguistique (LSPL) supportés par les divers EDL.</resume>
        <mots_cles>génie linguiciel, langages spécialisés pour la programmation linguistique, LSPL, environnement de développement, EDL, TAO, systèmes distribués hétérogènes</mots_cles>
        <title/>
        <abstract>A “meta-EDL” (meta-Environment for Developing Lingware) for MT allows to pilot one or more distant EDL in order to build heterogeneous MT systems. Starting from CASH, a meta-EDL dedicated to Ariane-G5, and from WICALE 1.0, a first meta-EDL, generic but offering minimal functionalities, we study the problems arising when adding rich functionalities such as local editing and navigation, and give a solution implemented in WICALE 2.0. We are now integrating to it a lexical database for MT systems relying on a “lexical pivot”, such as UNL/U++. A longer-term goal is to evolve from such a multifunctional generic meta-EDL to a “universal” EDL, which would imply the reengineering of the compilers and engines of the specialized languages (SLLPs) supported by the various EDLs.</abstract>
        <keywords>lingware engineering, specialized languages for linguistic programming, development environment, EDL, MT, heterogeneous distributed MT systems</keywords>
      </article>
      <article id="taln-2007-long-013" session="Outils">
        <auteurs>
          <auteur>
            <nom>Frederik Cailliau</nom>
            <email>cailliau@sinequa.com</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Claude De Loupy</nom>
            <email>loupy@syllabs.com</email>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIPN – Institut Galilée – Université Paris-Nord, 99, avenue Jean-Baptiste Clément, 93430 Villetaneuse</affiliation>
          <affiliation affiliationId="2">Sinequa Labs – 51 rue Ledru-Rollin, 94200 Ivry-sur-Seine</affiliation>
          <affiliation affiliationId="3">Syllabs – 3 rue Castex, c/o Agoranov, 75004 Paris</affiliation>
        </affiliations>
        <titre>Aides à la navigation dans un corpus de transcriptions d’oral</titre>
        <type>long</type>
        <pages>143-152</pages>
        <resume>Dans cet article, nous évaluons les performances de fonctionnalités d’aide à la navigation dans un contexte de recherche dans un corpus audio. Nous montrons que les particularités de la transcription et, en particulier les erreurs, conduisent à une dégradation parfois importante des performances des outils d’analyse. Si la navigation par concepts reste dans des niveaux d’erreur acceptables, la reconnaissance des entités nommées, utilisée pour l’aide à la lecture, voit ses performances fortement baisser. Notre remise en doute de la portabilité de ces fonctions à un corpus oral est néanmoins atténuée par la nature même du corpus qui incite à considérer que toute méthodes permettant de réduire le temps d’accès à l’information est pertinente, même si les outils utilisés sont imparfaits.</resume>
        <mots_cles>évaluation, moteur de recherche, corpus oral</mots_cles>
        <title/>
        <abstract>In this paper we evaluate the performances of navigation facilities within the context of information retrieval performed on an audio corpus. We show that the issues about transcription, especially the errors, lead to a sometimes important deterioration of the performances of the analysing tools. While the navigation by concepts remains within an acceptable error rate, the recognition of named entities used in fast reading undergo a performance drop. Our caution to the portability of these functions to a speech corpus is attenuated by the nature of the corpus: access time to a speech corpus can be very long, and therefore all methods that reduce access time are good to take.</abstract>
        <keywords>evaluation, search engine, speech corpus</keywords>
      </article>
      <article id="taln-2007-long-014" session="Syntaxe">
        <auteurs>
          <auteur>
            <nom>Marie-Laure Guénot</nom>
            <email>marie-laure.guenot@u-bordeaux3.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Signes, Université Michel de Montaigne Bordeaux 3</affiliation>
        </affiliations>
        <titre>Une grammaire du français pour une théorie descriptive et formelle de la langue</titre>
        <type>long</type>
        <pages>155-164</pages>
        <resume>Dans cet article, nous présentons une grammaire du français qui fait l’objet d’un modèle basé sur des descriptions linguistiques de corpus (provenant notamment des travaux de l’Approche Pronominale) et représentée selon le formalisme des Grammaires de Propriétés. Elle constitue une proposition nouvelle parmi les grammaires formelles du français, participant à la mise en convergence de la variété des travaux de description linguistique, et de la diversité des possibilités de représentation formelle. Cette grammaire est mise à disposition publique sur le Centre de Ressources pour la Description de l’Oral en tant que ressource pour la représentation et l’analyse.</resume>
        <mots_cles>développement de grammaire, ressource pour le TAL, grammaire du français, syntaxe, linguistique formelle, linguistique descriptive, grammaires de propriétés (GP)</mots_cles>
        <title/>
        <abstract>In this paper I present a grammar for French, which is the implementation of a linguistic model based on corpus descriptions (notably coming from Approche Pronominale) and represented into the Property Grammars formalism. It accounts for a new proposition among formal grammars, taking part into the works that aim to promote convergence between the various researchs of descriptive linguistics and the diversity of formal representation possibilities. It is freely available on the Spoken Data Resource Center (CRDO), as a representation and analysis resource.</abstract>
        <keywords>grammar development, resource for NLP, French grammar, syntax, formal linguistics, descriptive linguistics, property grammars (PG)</keywords>
      </article>
      <article id="taln-2007-long-015" session="Syntaxe">
        <auteurs>
          <auteur>
            <nom>Alexandre Dikovsky</nom>
            <email>Alexandre.Dikovsky@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LINA-FRE CNRS 2729, Université de Nantes</affiliation>
        </affiliations>
        <titre>Architecture compositionnelle pour les dépendances croisées</titre>
        <type>long</type>
        <pages>165-174</pages>
        <resume>L’article présente les principes généraux sous-jacent aux grammaires catégorielles de dépendances : une classe de grammaires de types récemment proposée pour une description compositionnelle et uniforme des dépendances continues et discontinues. Ces grammaires très expressives et analysées en temps polynomial, adoptent naturellement l’architecture multimodale et expriment les dépendances croisées illimitées.</resume>
        <mots_cles>grammaires catégorielles de dépendances, grammaires multimodales, analyseur syntaxique</mots_cles>
        <title/>
        <abstract>This article presents the general principles underlying the categorial dependency grammars : a class of type logical grammars recently introduced as a compositional and uniform definition of continuous and discontinuous dependences. These grammars are very expressive, are parsed in a reasonable polynomial time, naturally adopt the multimodal architecture and explain unlimited cross-serial dependencies.</abstract>
        <keywords>categorial dependency grammars, multimodal grammars, syntactic parser</keywords>
      </article>
      <article id="taln-2007-long-016" session="Syntaxe">
        <auteurs>
          <auteur>
            <nom>Claire Gardent</nom>
            <email>gardent@loria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Yannick Parmentier</nom>
            <email>parmenti@loria.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CNRS / LORIA, Campus scientifique – BP 259, F-54 506 Vandoeuvre-Lès-Nancy CEDEX</affiliation>
          <affiliation affiliationId="2">INRIA / LORIA – Nancy Université, Campus scientifique, BP 259, F-54 506 Vandoeuvre-Lès-Nancy CEDEX</affiliation>
        </affiliations>
        <titre>SemTAG, une architecture pour le développement et l’utilisation de grammaires d’arbres adjoints à portée sémantique</titre>
        <type>long</type>
        <pages>175-184</pages>
        <resume>Dans cet article, nous présentons une architecture logicielle libre et ouverte pour le développement de grammaires d’arbres adjoints à portée sémantique. Cette architecture utilise un compilateur de métagrammaires afin de faciliter l’extension et la maintenance de la grammaire, et intègre un module de construction sémantique permettant de vérifier la couverture aussi bien syntaxique que sémantique de la grammaire. Ce module utilise un analyseur syntaxique tabulaire généré automatiquement à partir de la grammaire par le système DyALog. Nous présentons également les résultats de l’évaluation d’une grammaire du français développée au moyen de cette architecture.</resume>
        <mots_cles>analyseur syntaxique, grammaires d’arbres adjoints, construction sémantique, architecture logicielle</mots_cles>
        <title/>
        <abstract>In this paper, we introduce a free and open software architecture for the development of Tree Adjoining Grammars equipped with semantic information. This architecture uses a metagrammar compiler to facilitate the grammar extension and maintnance, and includes a semantic construction module allowing to check both the syntactic and semantic coverage of the grammar. This module uses a tabular syntactic parser generated automatically from this grammar using the DyALog system. We also give the results of the evaluation of a real-size TAG for French developed using this architecture.</abstract>
        <keywords>syntactic parser, tree adjoining grammars, semantic construction, software architecture</keywords>
      </article>
      <article id="taln-2007-long-017" session="Désambiguïsation">
        <auteurs>
          <auteur>
            <nom>Fabienne Venant</nom>
            <email>fabienne.venant@ens.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LaLIC – Université Paris IV, Maison de la recherche, 28 rue Serpente 75006 Paris</affiliation>
        </affiliations>
        <titre>Utiliser des classes de sélection distributionnelle pour désambiguïser les adjectifs</titre>
        <type>long</type>
        <pages>187-196</pages>
        <resume>La désambiguïsation lexicale présente un intérêt considérable pour un nombre important d’applications, en traitement automatique des langues comme en recherche d'information. Nous proposons un modèle d’un genre nouveau, fondé sur la théorie de la construction dynamique du sens (Victorri et Fuchs, 1996). Ce modèle donne une place centrale à la polysémie et propose une représentation géométrique du sens. Nous présentons ici une application de ce modèle à la désambiguïsation automatique des adjectifs. La méthode utilisée s'appuie sur une pré-désambiguïsation du nom régissant l'adjectif, par le biais de classes de sélection distributionnelle. Elle permet aussi de prendre en compte les positions relatives du nom et de l'adjectif (postpostion ou antéposition) dans le calcul du sens.</resume>
        <mots_cles>traitement automatique des langues, désambiguïsation, sémantique, polysémie adjectivale, construction dynamique du sens, synonymie, classes distributionnelles, corpus, espace sémantique, espace distributionnel</mots_cles>
        <title/>
        <abstract>Automatic word sense disambiguation represents an important issue for many applications, in Natural Language Processing as in Information Retrieval. We propose a new kind of model, within the framework of Dynamical Construction of Meaning (Victorri and Fuchs, 1996). This model gives a central place to polysemy and proposes a geometric representation of meaning. We present here an application of this model to adjective sense disambiguation. The method we used relies on a pre-disambiguation of the noun used with the adjective under study, using distributionnal classes. It can also take into account the changes in the meaning of the adjective, whether it is placed before or after the noun.</abstract>
        <keywords>natural language processing, word sense disambiguation, semantics, adjectival polysemy, dynamical construction of meaning, synonymy, distributionnal classes, corpus, semantic space, distributional space</keywords>
      </article>
      <article id="taln-2007-long-018" session="Désambiguïsation">
        <auteurs>
          <auteur>
            <nom>Véronique Malaisé</nom>
            <email>vmalaise@few.vu.nl</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Luit Gazendam</nom>
            <email>Luit.Gazendam@telin.nl</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Hennie Brugman</nom>
            <email>Hennie.Brugman@mpi.nl</email>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Vrije Universiteit, Amsterdam</affiliation>
          <affiliation affiliationId="2">Telematica Institute, Enschedé, Netherlands</affiliation>
          <affiliation affiliationId="3">Max Planck Institute for Psycholinguistics, Nijmegen, Netherlands</affiliation>
        </affiliations>
        <titre/>
        <type>long</type>
        <pages>197-206</pages>
        <resume>La relation voir/employé pour d’un thesaurus est souvent plus complexe que la (para-)synonymie recommandée par l’ISO-2788, standard décrivant le contenu de ces vocabulaires contrôlés. Le fait qu’un non descripteur puisse renvoyer à plusieurs descripteurs (seuls les descripteurs sont pertinents dans le cadre de l’indexation contrôlée) fait que cette relation est complexe à utiliser dans un contexte d’annotation automatique : elle génère des cas d’ambiguité. Dans ce papier, nous présentons CARROT, un algorithme que nous avons mis au point pour classer les résultats de notre chaîne de traitements pour l’Extraction d’Information, et son utilisation dans le cadre de la sélection du descripteur pertinent lorsque plusieurs choix sont possibles. Cette sélection s’adresse à des documentalistes, dans le but de simplifier et d’accélérer leur travail, et se base sur la structure de leur thesaurus. Nous arrivons à un succès de 95 % dans nos suggestions ; nous discutons ces résultats et présentons des perspectives à cette expérimentation.</resume>
        <mots_cles>désambiguisation sémantique, algorithme de classement, annotation automatique</mots_cles>
        <title>Disambiguating automatic semantic annotation based on a thesaurus structure</title>
        <abstract>The use/use for relationship a thesaurus is usually more complex than the (para-) synonymy recommended in the ISO-2788 standard describing the content of these controlled vocabularies. The fact that a non preferred term can refer to multiple preferred terms (only the latter are relevant in controlled indexing) makes this relationship difficult to use in automatic annotation applications : it generates ambiguity cases. In this paper, we present the CARROT algorithm, meant to rank the output of our Information Extraction pipeline, and how this algorithm can be used to select the relevant preferred term out of different possibilities. This selection is meant to provide suggestions of keywords to human annotators, in order to ease and speed up their daily process and is based on the structure of their thesaurus. We achieve a 95 % success, and discuss these results along with perspectives for this experiment.</abstract>
        <keywords>word sense disambiguation, ranking algorithm, automatic annotation</keywords>
      </article>
      <article id="taln-2007-long-019" session="Désambiguïsation">
        <auteurs>
          <auteur>
            <nom>Marianna Apidianaki</nom>
            <email>Marianna.Apidianaki@linguist.jussieu.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Lattice, Université Paris 7, CNRS, ENS-1 rue Maurice Arnoux, F-92120, Montrouge</affiliation>
        </affiliations>
        <titre>Repérage de sens et désambiguïsation dans un contexte bilingue</titre>
        <type>long</type>
        <pages>207-216</pages>
        <resume>Les besoins de désambiguïsation varient dans les différentes applications du Traitement Automatique des Langues (TAL). Dans cet article, nous proposons une méthode de désambiguïsation lexicale opératoire dans un contexte bilingue et, par conséquent, adéquate pour la désambiguïsation au sein d’applications relatives à la traduction. Il s’agit d’une méthode contextuelle, qui combine des informations de cooccurrence avec des informations traductionnelles venant d’un bitexte. L’objectif est l’établissement de correspondances de traduction au niveau sémantique entre les mots de deux langues. Cette méthode étend les conséquences de l’hypothèse contextuelle du sens dans un contexte bilingue, tout en admettant l’existence d’une relation de similarité sémantique entre les mots de deux langues en relation de traduction. La modélisation de ces correspondances de granularité fine permet la désambiguïsation lexicale de nouvelles occurrences des mots polysémiques de la langue source ainsi que la prédiction de la traduction la plus adéquate pour ces occurrences.</resume>
        <mots_cles>désambiguïsation contextuelle, similarité sémantique, substituabilité, traduction</mots_cles>
        <title/>
        <abstract>Word Sense Disambiguation (WSD) needs vary greatly in different Natural Language Processing (NLP) applications. In this article, we propose a WSD method which operates in a bilingual context and is, thus, adequate for disambiguation in applications relative to translation. It is a contextual method which combines cooccurrence information with translation information found in a bitext. The goal is the establishment of translation correspondences at the sense level between the lexical items of two languages. This method extends the consequences of the contextual hypothesis in a bilingual framework assuming, at the same time, the existence of a semantic similarity relation between words of two languages being in a translation relation. The modelling of fine-grained correspondences allows for the disambiguation of new occurrences of the polysemous source language lexical items as well as for the prediction of the most adequate translation for those occurrences.</abstract>
        <keywords>contextual disambiguation, semantic similarity, substitutability, translation</keywords>
      </article>
      <article id="taln-2007-long-020" session="Syntaxe &amp; ressources">
        <auteurs>
          <auteur>
            <nom>Karën Fort</nom>
            <email>Karen.Fort@loria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Bruno Guillaume</nom>
            <email>Bruno.Guillaume@loria.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">projets Calligramme et TALARIS, LORIA/INRIA Lorraine, UMR 7503, Nancy</affiliation>
          <affiliation affiliationId="2">projet Calligramme, LORIA/INRIA Lorraine, UMR 7503, Nancy</affiliation>
        </affiliations>
        <titre>PrepLex : un lexique des prépositions du français pour l’analyse syntaxique</titre>
        <type>long</type>
        <pages>219-228</pages>
        <resume>PrepLex est un lexique des prépositions du français. Il contient les informations utiles à des systèmes d’analyse syntaxique. Il a été construit en comparant puis fusionnant différentes sources d’informations lexicales disponibles. Ce lexique met également en évidence les prépositions ou classes de prépositions qui apparaissent dans la définition des cadres de sous-catégorisation des ressources lexicales qui décrivent la valence des verbes.</resume>
        <mots_cles>prépositions, lexique, analyse syntaxique</mots_cles>
        <title/>
        <abstract>PrepLex is a lexicon of French prepositions which provides all the information needed for parsing. It was built by comparing and merging several authoritative lexical sources. This lexicon also shows the prepositions or classes of prepositions that appear in verbs subcategorization frames.</abstract>
        <keywords>prepositions, lexicon, parsing</keywords>
      </article>
      <article id="taln-2007-long-021" session="Syntaxe &amp; ressources">
        <auteurs>
          <auteur>
            <nom>Laurence Danlos</nom>
            <email>laurence.danlos@linguist.jussieu.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Benoît Sagot</nom>
            <email>benoit.sagot@inria.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Lattice - Université Paris 7 - Institut Universitaire de France, 2 place Jussieu, case 7003, 75251 Paris Cedex 05, France</affiliation>
          <affiliation affiliationId="2">Projet Signes - INRIA, Dom. Universitaire, 351 cours de la Libération, 33405 Talence Cedex, France</affiliation>
        </affiliations>
        <titre>Comparaison du Lexique-Grammaire des verbes pleins et de DICOVALENCE : vers une intégration dans le Lefff</titre>
        <type>long</type>
        <pages>229-238</pages>
        <resume>Cet article compare le Lexique-Grammaire des verbes pleins et DICOVALENCE, deux ressources lexicales syntaxiques pour le français développées par des linguistes depuis de nombreuses années. Nous étudions en particulier les divergences et les empiètements des modèles lexicaux sous-jacents. Puis nous présentons le Lefff , lexique syntaxique à grande échelle pour le TAL, et son propre modèle lexical. Nous montrons que ce modèle est à même d’intégrer les informations lexicales présentes dans le Lexique-Grammaire et dans DICOVALENCE. Nous présentons les résultats des premiers travaux effectués en ce sens, avec pour objectif à terme la constitution d’un lexique syntaxique de référence pour le TAL.</resume>
        <mots_cles>lexique syntaxique, Lexique-Grammaire, DICOVALENCE, Lefff</mots_cles>
        <title/>
        <abstract>This paper compares the Lexicon-Grammar of full verbs and DICOVALENCE, two syntactic lexical resources for French developed by linguists for numerous years. We focus on differences and overlaps between both underlying lexical models. Then we introduce the Lefff , large-coverage syntactic lexicon for NLP, and its own lexical model. We show that this model is able to integrate lexical information present in the Lexicon-Grammar and in DICOVALENCE. We describe the results of the first work done in this direction, the long term goal being the consitution of a high-quality syntactic lexicon for NLP.</abstract>
        <keywords>syntactic lexicon, Lexicon-Grammar, DICOVALENCE, Lefff</keywords>
      </article>
      <article id="taln-2007-long-022" session="Syntaxe &amp; ressources">
        <auteurs>
          <auteur>
            <nom>Pierre-André Buvet</nom>
            <email>Pierre-Andre.Buvet@lli.univ-paris13.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Emmanuel Cartier</nom>
            <email>Emmanuel.Cartier@lli.univ-paris13.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Fabrice Issac</nom>
            <email>Fabrice.Issac@lli.univ-paris13.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Salah Mejri</nom>
            <email>Salah.Mejri@lli.univ-paris13.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LDI UMR 7187– Université Paris 13</affiliation>
        </affiliations>
        <titre>Dictionnaires électroniques et étiquetage syntactico-sémantique</titre>
        <type>long</type>
        <pages>239-248</pages>
        <resume>Nous présentons dans cet article le prototype d’un système d’étiquetage syntactico-sémantique des mots qui utilise comme principales ressources linguistiques différents dictionnaires du laboratoire Lexiques, Dictionnaires, Informatique (LDI). Dans un premier temps, nous mentionnons des travaux sur le même sujet. Dans un deuxième temps, nous faisons la présentation générale du système. Dans un troisième temps, nous exposons les principales caractéristiques des dictionnaires syntactico-sémantiques utilisés. Dans un quatrième temps, nous détaillons un exemple de traitement.</resume>
        <mots_cles>étiqueteur sémantique, dictionnaire, LMF, XML, XPATH</mots_cles>
        <title/>
        <abstract>We present in this paper a syntactico-semantics tagger prototype which uses as first linguistic resources various dictionaries elaborated at LDI. First, we mention several related works. Second, we present the overall sketch of the system. Third, we expose the main characteristics of the syntactico-semantic dictionaries implied in the processes. Last, using an example, we explicit the main stages of the analysis.</abstract>
        <keywords>word sense disambiguation (WSD), dictionnary, LMF, XML, XPATH</keywords>
      </article>
      <article id="taln-2007-long-023" session="Sémantique">
        <auteurs>
          <auteur>
            <nom>Chiraz Ben Othmane Zribi</nom>
            <email>Chiraz.benothmane@riadi.rnu.tn</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Hanène Mejri</nom>
            <email>Hanene.mejri@riadi.rnu.tn</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Mohamed Ben Ahmed</nom>
            <email>Mohamed.benahmed@riadi.rnu.tn</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire de recherche RIADI, Université La Manouba, ENSI, La Manouba, Tunisie</affiliation>
        </affiliations>
        <titre>Un analyseur hybride pour la détection et la correction des erreurs cachées sémantiques en langue arabe</titre>
        <type>long</type>
        <pages>251-260</pages>
        <resume>Cet article s’intéresse au problème de la détection et de la correction des erreurs cachées sémantiques dans les textes arabes. Ce sont des erreurs orthographiques produisant des mots lexicalement valides mais invalides sémantiquement. Nous commençons par décrire le type d’erreur sémantique auquel nous nous intéressons. Nous exposons par la suite l’approche adoptée qui se base sur la combinaison de plusieurs méthodes, tout en décrivant chacune de ces méthodes. Puis, nous évoquons le contexte du travail qui nous a mené au choix de l’architecture multi-agent pour l’implémentation de notre système. Nous présentons et commentons vers la fin les résultats de l’évaluation dudit système.</resume>
        <mots_cles>erreur cachée, erreur sémantique, détection, correction, système multi-agent, langue arabe</mots_cles>
        <title/>
        <abstract>In this paper, we address the problem of detecting and correcting hidden semantic spelling errors in Arabic texts. Hidden semantic spelling errors are morphologically valid words causing invalid semantic irregularities. After the description of this type of errors, we propose and argue the combined method that we adopted in this work to realize a hybrid spell checker for detecting and correcting hidden spelling errors. Afterward, we present the context of this work and show the multi-agent architecture of our system. Finally, we expose and comment the obtained results.</abstract>
        <keywords>hidden error, semantic error, detection, correction, multi-agent system, Arabic language</keywords>
      </article>
      <article id="taln-2007-long-024" session="Sémantique">
        <auteurs>
          <auteur>
            <nom>Alexandre Denis</nom>
            <email>alexandre.denis@loria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Frédéric Béchet</nom>
            <email>frederic.bechet@univ-avignon.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Matthieu Quignard</nom>
            <email>matthieu.quignard@loria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">UMR 7503 LORIA/CNRS – Campus scientifique, 56 506 Vandoeuvre-lès-Nancy Cedex</affiliation>
          <affiliation affiliationId="2">LIA – 339, chemin des Meinajaries, BP 1228,, 84 911 Avignon Cedex 9</affiliation>
        </affiliations>
        <titre>Résolution de la référence dans des dialogues homme-machine : évaluation sur corpus de deux approches symbolique et probabiliste</titre>
        <type>long</type>
        <pages>261-270</pages>
        <resume>Cet article décrit deux approches, l’une numérique, l’autre symbolique, traitant le problème de la résolution de la référence dans un cadre de dialogue homme-machine. L’analyse des résultats obtenus sur le corpus MEDIA montre la complémentarité des deux systèmes développés : robustesse aux erreurs et hypothèses multiples pour l’approche numérique ; modélisation de phénomènes complexes et interprétation complète pour l’approche symbolique.</resume>
        <mots_cles>dialogue homme-machine, résolution de la référence, évaluation, compréhension dans le dialogue</mots_cles>
        <title/>
        <abstract>This paper presents two approaches, one symbolic, the other one probabilistic, for processing reference resolution in the framework of human-machine spoken dialogues. The results obtained by both systems on the French MEDIA corpus points out the complementarity of the two approaches : robustness and multiple hypotheses generation for the probabilistic one ; global interpretation and modeling of complex phenomenon for the symbolic one.</abstract>
        <keywords>human-machine dialogue, reference resolution, dialogue understanding, evaluation</keywords>
      </article>
      <article id="taln-2007-long-025" session="Sémantique">
        <auteurs>
          <auteur>
            <nom>Sebastian Padó</nom>
            <email>pado@coli.uni-sb.de</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Guillaume Pitel</nom>
            <email>Guillaume.Pitel@loria.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Computerlinguistik – Université de la Sarre</affiliation>
          <affiliation affiliationId="2">Équipe TALARIS, LORIA – INRIA</affiliation>
        </affiliations>
        <titre>Annotation précise du français en sémantique de rôles par projection cross-linguistique</titre>
        <type>long</type>
        <pages>271-280</pages>
        <resume>Dans le paradigme FrameNet, cet article aborde le problème de l’annotation précise et automatique de rôles sémantiques dans une langue sans lexique FrameNet existant. Nous évaluons la méthode proposée par Padó et Lapata (2005, 2006), fondée sur la projection de rôles et appliquée initialement à la paire anglais-allemand. Nous testons sa généralisabilité du point de vue (a) des langues, en l'appliquant à la paire (anglais-français) et (b) de la qualité de la source, en utilisant une annotation automatique du côté anglais. Les expériences montrent des résultats à la hauteur de ceux obtenus pour l'allemand, nous permettant de conclure que cette approche présente un grand potentiel pour réduire la quantité de travail nécessaire à la création de telles ressources dans de nombreuses langues.</resume>
        <mots_cles>multilingue, FrameNet, annotation sémantique automatique, sémantique lexicale, projection d’annotation de rôles, rôles sémantiques</mots_cles>
        <title/>
        <abstract>This paper considers the task of the automatic induction of role-semantic annotations for new languages with high precision. To this end we test the generalisability of the language-independent, projection-based annotation framework introduced by Padó and Lapata (2005, 2006) by (a) applying it to a new, more distant, language pair (English-French), and (b), using automatic, and thus noisy, input annotation. We show that even under these conditions, high-quality role annotations for French can be obtained that rival existing results for German. We conclude that the framework has considerable potential in reducing the manual effort involved in creating role-semantic resources for a wider range of languages.</abstract>
        <keywords>multilingual, FrameNet, automatic semantic annotation, lexical semantics, annotation projection, semantic roles</keywords>
      </article>
      <article id="taln-2007-long-026" session="Acquisition">
        <auteurs>
          <auteur>
            <nom>Simon Charest</nom>
            <email>developpement@druide.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Éric Brunelle</nom>
            <email>developpement@druide.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Jean Fontaine</nom>
            <email>developpement@druide.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Bertrand Pelletier</nom>
            <email>developpement@druide.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Druide informatique inc., 1435, rue St-Alexandre, bureau 1040, Montréal (Québec) H3A 2G4, Canada</affiliation>
        </affiliations>
        <titre>Élaboration automatique d’un dictionnaire de cooccurrences grand public</titre>
        <type>long</type>
        <pages>283-292</pages>
        <resume>Antidote RX, un logiciel d’aide à la rédaction grand public, comporte un nouveau dictionnaire de 800 000 cooccurrences, élaboré essentiellement automatiquement. Nous l’avons créé par l’analyse syntaxique détaillée d’un vaste corpus et par la sélection automatique des cooccurrences les plus pertinentes à l’aide d’un test statistique, le rapport de vraisemblance. Chaque cooccurrence est illustrée par des exemples de phrases également tirés du corpus automatiquement. Les cooccurrences et les exemples extraits ont été révisés par des linguistes. Nous examinons les choix d’interface que nous avons faits pour présenter ces données complexes à un public non spécialisé. Enfin, nous montrons comment nous avons intégré les cooccurrences au correcteur d’Antidote pour améliorer ses performances.</resume>
        <mots_cles>antidote, cooccurrences, collocations, corpus, analyseur, correcteur</mots_cles>
        <title/>
        <abstract>Antidote is a complete set of software reference tools for writing French that includes an advanced grammar checker. Antidote RX boasts a new dictionary of 800,000 cooccurrences created mostly automatically. The approach we chose is based on the syntactic parsing of a large corpus and the automatic selection of the most relevant co-occurrences using a statistical test, the log-likelihood ratio. Example sentences illustrating each cooccurrence in context are also automatically selected. The extracted co-occurrences and examples were revised by linguists. We examine the various choices that were made to present this complex data to a non-specialized public. We then show how we use the cooccurrence data to improve the performance of Antidote’s grammar checker.</abstract>
        <keywords>antidote, co-occurrences, collocations, corpus, parser, grammar checker</keywords>
      </article>
      <article id="taln-2007-long-027" session="Acquisition">
        <auteurs>
          <auteur>
            <nom>Didier Schwab</nom>
            <email>didier@cs.usm.my</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Lim Lian Tze</nom>
            <email>liantze@cs.usm.my</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Mathieu Lafourcade</nom>
            <email>lafourcade@lirmm.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Computer-Aided Translation Unit (UTMK), School of Computer Sciences, Universiti Sains Malaysia, Penang, Malaysia</affiliation>
          <affiliation affiliationId="2">TAL-LIRMM, Université Montpellier II – CNRS, 161 rue ada, 34392 Montpellier Cedex 5, France</affiliation>
        </affiliations>
        <titre>Les vecteurs conceptuels, un outil complémentaire aux réseaux lexicaux</titre>
        <type>long</type>
        <pages>293-302</pages>
        <resume>Fréquemment utilisés dans le Traitement Automatique des Langues Naturelles, les réseaux lexicaux font aujourd’hui l’objet de nombreuses recherches. La plupart d’entre eux, et en particulier le plus célèbre WordNet, souffrent du manque d’informations syntagmatiques mais aussi d’informations thématiques (« problème du tennis »). Cet article présente les vecteurs conceptuels qui permettent de représenter les idées contenues dans un segment textuel quelconque et permettent d’obtenir une vision continue des thématiques utilisées grâce aux distances calculables entre eux. Nous montrons leurs caractéristiques et en quoi ils sont complémentaires des réseaux lexico-sémantiques. Nous illustrons ce propos par l’enrichissement des données de WordNet par des vecteurs conceptuels construits par émergence.</resume>
        <mots_cles>WordNet, vecteurs conceptuels, informations lexicales, informations thématiques</mots_cles>
        <title/>
        <abstract>There is currently much research in natural language processing focusing on lexical networks. Most of them, in particular the most famous, WordNet, lack syntagmatic information and but also thematic information (« Tennis Problem »). This article describes conceptual vectors that allows the representation of ideas in any textual segment and offers a continuous vision of related thematics, based on the distances between these thematics. We show the characteristics of conceptual vectors and explain how they complement lexico-semantic networks. We illustrate this purpose by adding conceptual vectors to WordNet by emergence.</abstract>
        <keywords>WordNet, conceptual vectors, lexical information, thematic information</keywords>
      </article>
      <article id="taln-2007-long-028" session="Acquisition">
        <auteurs>
          <auteur>
            <nom>Julien Bourdaillet</nom>
            <email>julien.bourdaillet@lip6.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Jean-Gabriel Ganascia</nom>
            <email>jean-gabriel.ganascia@lip6.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire d’Informatique de Paris 6, Université Pierre et Marie Curie, 104 Quai Kennedy, 75016 Paris</affiliation>
        </affiliations>
        <titre>Alignements monolingues avec déplacements</titre>
        <type>long</type>
        <pages>303-312</pages>
        <resume>Ce travail présente une application d’alignement monolingue qui répond à une problématique posée par la critique génétique textuelle, une école d’études littéraires qui s’intéresse à la genèse textuelle en comparant les différentes versions d’une oeuvre. Ceci nécessite l’identification des déplacements, cependant, le problème devient ainsi NP-complet. Notre algorithme heuristique est basé sur la reconnaissance des homologies entre séquences de caractères. Nous présentons une validation expérimentale et montrons que notre logiciel obtient de bons résultats ; il permet notamment l’alignement de livres entiers.</resume>
        <mots_cles>alignement monolingue, distance d’édition avec déplacements, critique génétique textuelle</mots_cles>
        <title/>
        <abstract>This paper presents a monolingual alignment application that addresses a problem which occurs in textual genetic criticism, a humanities discipline of literary studies which compares texts’ versions to understand texts’ genesis. It requires the move detection, but this characteristic makes the problem NP-complete. Our heuristic algorithm is based on pattern matching in character sequences. We present an experimental validation where we show that our application obtains good results ; in particular it enables whole book alignment.</abstract>
        <keywords>monolingual alignment, edit distance with moves, textual genetic criticism</keywords>
      </article>
      <article id="taln-2007-long-029" session="Syntaxe">
        <auteurs>
          <auteur>
            <nom>Lionel Nicolas</nom>
            <email>lnicolas@i3s.unice.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Jacques Farré</nom>
            <email>jf@i3s.unice.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Éric Villemonte De La Clergerie</nom>
            <email>Eric.De_La_Clergerie@inria.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire I3S, Université de Nice-Sophia Antipolis, CNRS, 2000 route des Lucioles, B.P. 121, 06903 Sophia Antipolis Cedex, France</affiliation>
          <affiliation affiliationId="2">Projet ATOLL - INRIA, Domaine de Voluceau, B.P. 105, 78153 Le Chesnay Cedex, France</affiliation>
        </affiliations>
        <titre>Confondre le coupable : corrections d’un lexique suggérées par une grammaire</titre>
        <type>long</type>
        <pages>315-324</pages>
        <resume>Le succès de l’analyse syntaxique d’une phrase dépend de la qualité de la grammaire sous-jacente mais aussi de celle du lexique utilisé. Une première étape dans l’amélioration des lexiques consiste à identifier les entrées lexicales potentiellement erronées, par exemple en utilisant des techniques de fouilles d’erreurs sur corpus (Sagot &amp; Villemonte de La Clergerie, 2006). Nous explorons ici l’étape suivante : la suggestion de corrections pour les entrées identifiées. Cet objectif est atteint au travers de réanalyses des phrases rejetées à l’étape précédente, après modification des informations portées par les entrées suspectées. Un calcul statistique sur les nouveaux résultats permet ensuite de mettre en valeur les corrections les plus pertinentes.</resume>
        <mots_cles>analyse syntaxique, lexique, apprentissage, correction</mots_cles>
        <title/>
        <abstract>Successful parsing depends on the quality of the underlying grammar but also on the quality of the lexicon. A first step towards the improvement of lexica consists in identifying potentially erroneous lexical entries, for instance by using error mining techniques on corpora (Sagot &amp; Villemonte de La Clergerie, 2006). we explores the next step, namely the suggestion of corrections for those entries. This is achieved by parsing the sentences rejected at the previous step anew, after modifying the information carried by the suspected entries. Afterwards, a statistical computation on the parsing results exhibits the most relevant corrections.</abstract>
        <keywords>parsing, lexicon, machine learning, correction</keywords>
      </article>
      <article id="taln-2007-long-030" session="Syntaxe">
        <auteurs>
          <auteur>
            <nom>Sylvain Pogodalla</nom>
            <email>sylvain.pogodalla@loria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LORIA/INRIA Lorraine</affiliation>
        </affiliations>
        <titre>Ambiguïté de portée et approche fonctionnelle des grammaires d’arbres adjoints</titre>
        <type>long</type>
        <pages>325-334</pages>
        <resume>En s’appuyant sur la notion d’arbre de dérivation des Grammaires d’Arbres Adjoints (TAG), cet article propose deux objectifs : d’une part rendre l’interface entre syntaxe et sémantique indépendante du langage de représentation sémantique utilisé, et d’autre part offrir un noyau qui permette le traitement sémantique des ambiguïtés de portée de quantificateurs sans utiliser de langage de représentation sous-spécifiée.</resume>
        <mots_cles>interface syntaxe et sémantique, sémantique formelle, grammaires d’arbres adjoints, grammaires catégorielles</mots_cles>
        <title/>
        <abstract>Relying on the derivation tree of the Tree Adjoining Grammars (TAG), this paper has to goals : on the one hand, to make the syntax/semantics interface independant from the semantic representation language, and on the other hand to propose an architecture that enables the modeling of scope ambguities without using underspecified representation formalisms.</abstract>
        <keywords>syntax/semantics interface, formal semantics, tree adjoining grammars, categorial grammars</keywords>
      </article>
      <article id="taln-2007-long-031" session="Syntaxe">
        <auteurs>
          <auteur>
            <nom>Ingrid Falk</nom>
            <email>Ingrid.Falk-@loria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Gil Francopoulo</nom>
            <email>Gil.Francopoulo@wanadoo.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Claire Gardent</nom>
            <email>Claire.Gardent@loria.fr</email>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CNRS/ATILF, Nancy</affiliation>
          <affiliation affiliationId="2">INRIA/LORIA, Nancy</affiliation>
          <affiliation affiliationId="3">CNRS/LORIA, Nancy</affiliation>
        </affiliations>
        <titre>Évaluer SYNLEX</titre>
        <type>long</type>
        <pages>335-344</pages>
        <resume>SYNLEX est un lexique syntaxique extrait semi-automatiquement des tables du LADL. Comme les autres lexiques syntaxiques du français disponibles et utilisables pour le TAL (LEFFF, DICOVALENCE), il est incomplet et n’a pas fait l’objet d’une évaluation permettant de déterminer son rappel et sa précision par rapport à un lexique de référence. Nous présentons une approche qui permet de combler au moins partiellement ces lacunes. L’approche s’appuie sur les méthodes mises au point en acquisition automatique de lexique. Un lexique syntaxique distinct de SYNLEX est acquis à partir d’un corpus de 82 millions de mots puis utilisé pour valider et compléter SYNLEX. Le rappel et la précision de cette version améliorée de SYNLEX sont ensuite calculés par rapport à un lexique de référence extrait de DICOVALENCE.</resume>
        <mots_cles>lexique syntaxique, évaluation</mots_cles>
        <title/>
        <abstract>SYNLEX is a syntactic lexicon extracted semi-automatically from the LADL tables. Like the other syntactic lexicons for French which are both available and usable for NLP (LEFFF, DICOVALENCE), it is incomplete and its recall and precision wrt a gold standard are unknown. We present an approach which goes some way towards adressing these shortcomings. The approach draws on methods used for the automatic acquisition of syntactic lexicons. First, a new syntactic lexicon is acquired from an 82 million words corpus. This lexicon is then used to validate and extend SYNLEX. Finally, the recall and precision of the extended version of SYNLEX is computed based on a gold standard extracted from DICOVALENCE.</abstract>
        <keywords>syntactic lexicon, evaluation</keywords>
      </article>
      <article id="taln-2007-long-032" session="Morphologie">
        <auteurs>
          <auteur>
            <nom>Fathi Debili</nom>
            <email>fathi.debili@wanadoo.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Zied Ben Tahar</nom>
            <email>bentaharzied@gmail.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Emna Souissi</nom>
            <email>emna.souissi@planet.tn</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LLACAN, INALCO, CNRS, 7, rue Guy Môquet, 94801 Villejuif cedex, France</affiliation>
          <affiliation affiliationId="2">ESSTT, 5, Avenue Taha Hussein – 1008 Tunis</affiliation>
        </affiliations>
        <titre>Analyse automatique vs analyse interactive : un cercle vertueux pour la voyellation, l’étiquetage et la lemmatisation de l’arabe</titre>
        <type>long</type>
        <pages>347-356</pages>
        <resume>Comment produire de façon massive des textes annotés dans des conditions d’efficacité, de reproductibilité et de coût optimales ? Plutôt que de corriger les sorties d’analyse automatique moyennant des outils d’éditions éventuellement dédiés, ainsi qu’il estcommunément préconisé, nous proposons de recourir à des outils d’analyse interactive où la correction manuelle est au fur et à mesure prise en compte par l’analyse automatique. Posant le problème de l’évaluation de ces outils interactifs et du rendement de leur ergonomie linguistique, et proposant pour cela une métrique fondée sur le calcul du coût qu’exigent ces corrections exprimé en nombre de manipulations (frappe au clavier, clic de souris, etc.), nous montrons, au travers d’un protocole expérimental simple orienté vers la voyellation, l’étiquetage et la lemmatisation de l’arabe, que paradoxalement, les meilleures performances interactives d’un système ne sont pas toujours corrélées à ses meilleures performances automatiques. Autrement dit, que le comportement linguistique automatique le plus performant n’est pas toujours celui qui assure, dès lors qu’il y a contributions manuelles, le meilleur rendement interactif.</resume>
        <mots_cles>analyse automatique vs interactive, annotation séquentielle, parallèle, voyellation, lemmatisation, étiquetage de l’arabe, métrique pour l’évaluation de l’analyse interactive</mots_cles>
        <title/>
        <abstract>How can we massively produce annotated texts, with optimal efficiency, reproducibility and cost? Rather than correcting the output of automatic analysis by means of possibly dedicated tools, as is currently suggested, we find it more advisable to use interactive tools for analysis, where manual editing is fed in real time into automatic analysis. We address the issue of evaluating these tools, along with their performance in terms of linguistic ergonomy, and propose a metric for calculating the cost of editing as a number of keystrokes and mouse clicks. We show, by way of a simple protocol addressing Arabic vowellation, tagging and lemmatization, that, surprisingly, the best interactive performance of a system is not always correlated to its best automatic performance. In other words, the most performing automatic linguistic behavior of a system is not always yielding the best interactive behavior, when manual editing is involved.</abstract>
        <keywords>automatic versus interactive analysis of Arabic, proposal of metrics for evaluating the interactive analysis, design and implementation of software for interactive vowellation, lemmatisation and POS-tagging of Arabic, evaluation</keywords>
      </article>
      <article id="taln-2007-long-033" session="Morphologie">
        <auteurs>
          <auteur>
            <nom>Jonas Granfeldt</nom>
            <email>Jonas.Granfeldt@rom.lu.se</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Pierre Nugues</nom>
            <email>Pierre.Nugues@cs.lth.se</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Centre de langues et de littérature, Université de Lund, S-221 00 Lund</affiliation>
          <affiliation affiliationId="2">Institut d’informatique, Institut de Technologie de Lund, S-221 00 Lund</affiliation>
        </affiliations>
        <titre>Évaluation des stades de développement en français langue étrangère</titre>
        <type>long</type>
        <pages>357-366</pages>
        <resume>Cet article décrit un système pour définir et évaluer les stades de développement en français langue étrangère. L’évaluation de tels stades correspond à l’identification de la fréquence de certains phénomènes lexicaux et grammaticaux dans la production des apprenants et comment ces fréquences changent en fonction du temps. Les problèmes à résoudre dans cette démarche sont triples : identifier les attributs les plus révélateurs, décider des points de séparation entre les stades et évaluer le degré d’efficacité des attributs et de la classification dans son ensemble. Le système traite ces trois problèmes. Il se compose d’un analyseur morphosyntaxique, appelé Direkt Profil, auquel nous avons relié un module d’apprentissage automatique. Dans cet article, nous décrivons les idées qui ont conduit au développement du système et son intérêt. Nous présentons ensuite le corpus que nous avons utilisé pour développer notre analyseur morphosyntaxique. Enfin, nous présentons les résultats sensiblement améliorés des classificateurs comparé aux travaux précédents (Granfeldt et al., 2006). Nous présentons également une méthode de sélection de paramètres afin d’identifier les attributs grammaticaux les plus appropriés.</resume>
        <mots_cles>analyseur morphosyntaxique, apprentissage automatique, acquisition des langues</mots_cles>
        <title/>
        <abstract>This paper describes a system to define and evaluate stages of development in second language French. The task of identifying such stages can be formulated as identifying the frequency of some lexical and grammatical features in the learners’ production and how they vary over time. The problems in this procedure are threefold : identify the relevant features, decide on cutoff points for the stages, and evaluate the degree of efficiency of the attributes and of the overall classification. The system addresses these three problems. It consists of a morphosyntactic analyzer called Direkt Profil and a machine-learning module connected to it. We first describe the usefulness and rationale behind the development of the system. We then present the corpus we used to develop our morphosyntactic analyzer called Direkt Profil. Finally, we present new and substantially improved results on training machine-learning classifiers compared to previous experiments (Granfeldt et al., 2006). We also introduce a method of attribute selection in order to identify the most relevant grammatical features.</abstract>
        <keywords>morphosyntactic parser, machine learning, language acquisition</keywords>
      </article>
      <article id="taln-2007-long-034" session="Morphologie">
        <auteurs>
          <auteur>
            <nom>Delphine Bernhard</nom>
            <email>Delphine.Bernhard@imag.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">TIMC-IMAG, Institut d’Ingénierie de l’Information de Santé, Faculté de Médecine, 38706 La Tronche cedex</affiliation>
        </affiliations>
        <titre>Apprentissage non supervisé de familles morphologiques par classification ascendante hiérarchique</titre>
        <type>long</type>
        <pages>367-376</pages>
        <resume>Cet article présente un système d’acquisition de familles morphologiques qui procède par apprentissage non supervisé à partir de listes de mots extraites de corpus de textes. L’approche consiste à former des familles par groupements successifs, similairement aux méthodes de classification ascendante hiérarchique. Les critères de regroupement reposent sur la similarité graphique des mots ainsi que sur des listes de préfixes et de paires de suffixes acquises automatiquement à partir des corpus traités. Les résultats obtenus pour des corpus de textes de spécialité en français et en anglais sont évalués à l’aide de la base CELEX et de listes de référence construites manuellement. L’évaluation démontre les bonnes performances du système, indépendamment de la langue, et ce malgré la technicité et la complexité morphologique du vocabulaire traité.</resume>
        <mots_cles>familles morphologiques, classification, apprentissage non supervisé</mots_cles>
        <title/>
        <abstract>This article describes a method for the unsupervised acquisition of morphological families using lists of words extracted from text corpora. It proceeds by incrementally grouping words in families, similarly to agglomerative hierarchical clustering methods. Clustering criteria rely on graphical similarity as well as lists of prefixes and suffix pairs which are automatically acquired from the target corpus. Results obtained for specialised text corpora in French and English are evaluated using the CELEX database and manually built reference lists. The evaluation shows that the system perfoms well for both languages, despite the morphological complexity of the technical vocabulary used for the evaluation.</abstract>
        <keywords>morphological families, clustering, unsupervised learning</keywords>
      </article>
      <article id="taln-2007-long-035" session="Discours">
        <auteurs>
          <auteur>
            <nom>Catherine Recanati</nom>
            <email>Catherine.Recanati@lipn.univ-paris13.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Nicoleta Rogovschi</nom>
            <email>Nicoleta.Rogovschi@lipn.univ-paris13.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIPN – UMR 7030 du CNRS, Institut Galilée, Université Paris 13, 99, avenue J-B. Clément, 93430 Villetaneuse, France</affiliation>
        </affiliations>
        <titre>Enchaînements verbaux – étude sur le temps et l'aspect utilisant des techniques d’apprentissage non supervisé</titre>
        <type>long</type>
        <pages>379-388</pages>
        <resume>L’apprentissage non supervisé permet la découverte de catégories initialement inconnues. Les techniques actuelles permettent d'explorer des séquences de phénomènes alors qu'on a tendance à se focaliser sur l'analyse de phénomènes isolés ou sur la relation entre deux phénomènes. Elles offrent ainsi de précieux outils pour l'analyse de données organisées en séquences, et en particulier, pour la découverte de structures textuelles. Nous présentons ici les résultats d’une première tentative de les utiliser pour inspecter les suites de verbes provenant de phrases de récits d’accident de la route. Les verbes étaient encodés comme paires (cat, temps), où cat représente la catégorie aspectuelle d’un verbe, et temps son temps grammatical. L’analyse, basée sur une approche originale, a fourni une classification des enchaînements de deux verbes successifs en quatre groupes permettant de segmenter les textes. Nous donnons ici une interprétation de ces groupes à partir de statistiques sur des annotations sémantiques indépendantes.</resume>
        <mots_cles>temps, aspect, sémantique, apprentissage non supervisé, fouille de données</mots_cles>
        <title/>
        <abstract>Unsupervised learning allows the discovery of initially unknown categories. Current techniques make it possible to explore sequences of phenomena whereas one tends to focus on the analysis of isolated phenomena or on the relation between two phenomena. They offer thus invaluable tools for the analysis of sequential data, and in particular, for the discovery of textual structures. We report here the results of a first attempt at using them for inspecting sequences of verbs coming from sentences of French accounts of road accidents. Verbs were encoded as pairs (cat, tense) – where cat is the aspectual category of a verb, and tense its grammatical tense. The analysis, based on an original approach, provided a classification of the links between two successive verbs into four distinct groups (clusters) allowing texts segmentation. We give here an interpretation of these clusters by using statistics on semantic annotations independent of the training process.</abstract>
        <keywords>time, tense, aspect, semantics, unsupervised learning, data mining</keywords>
      </article>
      <article id="taln-2007-long-036" session="Discours">
        <auteurs>
          <auteur>
            <nom>Laurence Danlos</nom>
            <email>Laurence.Danlos@linguist.jussieu.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LATTICE – Université Paris 7, Institut Universitaire de France</affiliation>
        </affiliations>
        <titre>D-STAG : un formalisme pour le discours basé sur les TAG synchrones</titre>
        <type>long</type>
        <pages>389-398</pages>
        <resume>Nous proposons D-STAG, un formalisme pour le discours qui utilise les TAG synchrones. Les analyses sémantiques produites par D-STAG sont des structures de discours hiérarchiques annotées de relations de discours coordonnantes ou subordonnantes. Elles sont compatibles avec les structures de discours produites tant en RST qu’en SDRT. Les relations de discours coordonnantes et subordonnantes sont modélisées respectivement par les opérations de substitution et d’adjonction introduites en TAG.</resume>
        <mots_cles>discours, grammaires d’arbres adjoints (synchrones), interface syntaxe/sémantique</mots_cles>
        <title/>
        <abstract>We propose D-STAG, a framework which uses Synchronous TAG for discourse. D-STAG semantic analyses are hierarchical discourse structures richly annotated with coordinating and subordinating discourse relations. They are compatible both with RST and SDRT discourse structures. Coordinating and subordinating relations are respectively modeled with the TAG substitution and adjunction operations.</abstract>
        <keywords>discourse, (synchronous) tree adjoining grammars, syntax/semantic interface</keywords>
      </article>
      <article id="taln-2007-long-037" session="Traduction &amp; alignement">
        <auteurs>
          <auteur>
            <nom>Violeta Seretan</nom>
            <email>Violeta.Seretan@lettres.unige.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Éric Wehrli</nom>
            <email>Eric.Wehrli@lettres.unige.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Language Technology Laboratory (LATL) - University of Geneva, 2 Rue de Candolle, 1211 Geneva, Switzerland</affiliation>
        </affiliations>
        <titre>Collocation translation based on sentence alignment and parsing</titre>
        <type>long</type>
        <pages>401-410</pages>
        <resume>Bien que de nombreux efforts aient été déployés pour extraire des collocations à partir de corpus de textes, seule une minorité de travaux se préoccupent aussi de rendre le résultat de l’extraction prêt à être utilisé dans les applications TAL qui pourraient en bénéficier, telles que la traduction automatique. Cet article décrit une méthode précise d’identification de la traduction des collocations dans un corpus parallèle, qui présente les avantages suivants : elle peut traiter des collocation flexibles (et pas seulement figées) ; elle a besoin de ressources limitées et d’un pouvoir de calcul raisonnable (pas d’alignement complet, pas d’entraînement) ; elle peut être appliquée à plusieurs paires des langues et fonctionne même en l’absence de dictionnaires bilingues. La méthode est basée sur l’information syntaxique provenant du parseur multilingue Fips. L’évaluation effectuée sur 4000 collocations de type verbe-objet correspondant à plusieurs paires de langues a montré une précision moyenne de 89.8% et une couverture satisfaisante (70.9%). Ces résultats sont supérieurs à ceux enregistrés dans l’évaluation d’autres méthodes de traduction de collocations.</resume>
        <mots_cles>traduction de collocations, extraction de collocations, parsing, alignement de textes</mots_cles>
        <title/>
        <abstract>To date, substantial efforts have been devoted to the extraction of collocations from text corpora. However, only a few works deal with the subsequent processing of results in order for these to be successfully integrated into the NLP applications that could benefit from them (e.g., machine translation). This paper presents an accurate method for identifying translation equivalents of collocations in parallel text, whose main strengths are that : it can handle flexible (not only rigid) collocations ; it only requires limited resources and computation (no full alignment, no training needed) ; it deals with several language pairs, and it can even work when no bilingual dictionary is available. The method relies heavily on syntactic information provided by the Fips multilingual parser. Evaluation performed on 4000 verb-object collocations for different language pairs showed an average accuracy of 89.8% and a reasonable coverage (70.9%). These figures are higher that those reported in the evaluation of related work in collocation translation.</abstract>
        <keywords>collocation translation, collocation extraction, parsing, text alignment</keywords>
      </article>
      <article id="taln-2007-long-038" session="Traduction &amp; alignement">
        <auteurs>
          <auteur>
            <nom>Nasredine Semmar</nom>
            <email>nasredine.semmar@cea.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Christian Fluhr</nom>
            <email>christian.fluhr@cea.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CEA, LIST, LIC2M, 18 route du Panorama, BP6, FONTENAY AUX ROSES, F- 92265 France</affiliation>
        </affiliations>
        <titre>Utilisation d’une approche basée sur la recherche cross-lingue d’information pour l’alignement de phrases à partir de textes bilingues Arabe-Français</titre>
        <type>long</type>
        <pages>411-420</pages>
        <resume>L’alignement de phrases à partir de textes bilingues consiste à reconnaître les phrases qui sont traductions les unes des autres. Cet article présente une nouvelle approche pour aligner les phrases d’un corpus parallèle. Cette approche est basée sur la recherche crosslingue d’information et consiste à construire une base de données des phrases du texte cible et considérer chaque phrase du texte source comme une requête à cette base. La recherche crosslingue utilise un analyseur linguistique et un moteur de recherche. L’analyseur linguistique traite aussi bien les documents à indexer que les requêtes et produit un ensemble de lemmes normalisés, un ensemble d’entités nommées et un ensemble de mots composés avec leurs étiquettes morpho-syntaxiques. Le moteur de recherche construit les fichiers inversés des documents en se basant sur leur analyse linguistique et retrouve les documents pertinents à partir de leur indexes. L’aligneur de phrases a été évalué sur un corpus parallèle Arabe-Français et les résultats obtenus montrent que 97% des phrases ont été correctement alignées.</resume>
        <mots_cles>alignement de phrases, corpus parallèle, recherche cross-lingue d’information</mots_cles>
        <title/>
        <abstract>Sentence alignment consists in identifying correspondences between sentences in one language and sentences in the other language. This paper describes a new approach to aligning sentences from a parallel corpora. This approach is based on cross-language information retrieval and consists in building a database of sentences of the target text and considering each sentence of the source text as a query to that database. Cross-language information retrieval uses a linguistic analyzer and a search engine. The linguistic analyzer processes both documents to be indexed and queries to produce a set of normalized lemmas, a set of named entities and a set of nominal compounds with their morpho-syntactic tags. The search engine builds the inverted files of the documents on the basis of their linguistic analysis and retrieves the relevant documents from the indexes. An evaluation of the sentence aligner was performed based on a Arabic to French parallel corpus and results show that 97% of sentences were correctly aligned.</abstract>
        <keywords>sentence alignment, parallel corpora, cross-lingual information retrieval</keywords>
      </article>
      <article id="taln-2007-poster-001" session="Posters">
        <auteurs>
          <auteur>
            <nom>Laurent Audibert</nom>
            <email>laurent.audibert@lipn.univ-paris13.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire d’Informatique de l’université Paris-Nord (LIPN), 99, avenue Jean-Baptiste Clément – 93430 Villetaneuse, France</affiliation>
        </affiliations>
        <titre>Désambiguïsation lexicale automatique : sélection automatique d’indices</titre>
        <type>poster</type>
        <pages>13-22</pages>
        <resume>Nous exposons dans cet article une expérience de sélection automatique des indices du contexte pour la désambiguïsation lexicale automatique. Notre point de vue est qu’il est plus judicieux de privilégier la pertinence des indices du contexte plutôt que la sophistication des algorithmes de désambiguïsation utilisés. La sélection automatique des indices par le biais d’un algorithme génétique améliore significativement les résultats obtenus dans nos expériences précédentes tout en confortant des observations que nous avions faites sur la nature et la répartition des indices les plus pertinents.</resume>
        <mots_cles>désambiguïsation lexicale automatique, corpus sémantiquement étiqueté, cooccurrences, sélection d’indices, algorithmes génétiques</mots_cles>
        <title/>
        <abstract>This article describes an experiment on automatic features selection for word sense disambiguation. Our point of view is that word sense disambiguation success is more dependent on the features used to represent the context in which an ambiguous word occurs than on the sophistication of the learning techniques used. Automatic features selection using a genetic algorithm improves significantly our last experiment bests results and is consistent with the observations we have made on the nature and space distribution of the most reliable features.</abstract>
        <keywords>word sense disambiguation, sense tagged corpora, cooccurrences, features selection, genetic algorithms</keywords>
      </article>
      <article id="taln-2007-poster-002" session="Posters">
        <auteurs>
          <auteur>
            <nom>Delphine Battistelli</nom>
            <email>Delphine.Battistelli@paris4.sorbonne.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Marie Chagnoux</nom>
            <email>Marie.Chagnoux@free.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Lalic – Université Paris IV-Sorbonne, 28 rue Serpente 75006 Paris</affiliation>
          <affiliation affiliationId="2">France Télécom – Div. R&amp;D, TECH/EASY/Langues Naturelles, 2 avenue Pierre Marzin, 22307 Lannion Cedex</affiliation>
        </affiliations>
        <titre>Représenter la dynamique énonciative et modale de textes</titre>
        <type>poster</type>
        <pages>23-32</pages>
        <resume>Nous proposons d’exposer ici une méthodologie d’analyse et de représentation d’une des composantes de la structuration des textes, celle liée à la notion de prise en charge énonciative. Nous mettons l’accent sur la structure hiérarchisée des segments textuels qui en résulte ; nous la représentons d’une part sous forme d’arbre et d’autre part sous forme de graphe. Ce dernier permet d’appréhender la dynamique énonciative et modale de textes comme un cheminement qui s’opère entre différents niveaux de discours dans un texte au fur et à mesure de sa lecture syntagmatique.</resume>
        <mots_cles>linguistique textuelle, énonciation, représentation sémantique</mots_cles>
        <title/>
        <abstract>We propose a methodological framework for analyzing and representing the concept of commitment, which is one of the features characterizing textual structure. We emphasize the hierarchical structure of textual segments commitment conveys to. We represent it first as a tree and then as a graph. The latter enables us to access the modal and enunciative textual dynamics, as it shows the path followed through different discursive levels during the syntagmatic reading of a text.</abstract>
        <keywords>textual linguistics, enunciation, semantical representation</keywords>
      </article>
      <article id="taln-2007-poster-003" session="Posters">
        <auteurs>
          <auteur>
            <nom>Olivier Blanc</nom>
            <email>oblanc@univ-mlv.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Matthieu Constant</nom>
            <email>mconstan@univ-mlv.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Patrick Watrin</nom>
            <email>watrin@univ-mlv.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">IGM, Université de Marne-la-Vallée &amp; CNRS</affiliation>
        </affiliations>
        <titre>Segmentation en super-chunks</titre>
        <type>poster</type>
        <pages>33-42</pages>
        <resume>Depuis l’analyseur développé par Harris à la fin des années 50, les unités polylexicales ont peu à peu été intégrées aux analyseurs syntaxiques. Cependant, pour la plupart, elles sont encore restreintes aux mots composés qui sont plus stables et moins nombreux. Toutefois, la langue est remplie d’expressions semi-figées qui forment également des unités sémantiques : les expressions adverbiales et les collocations. De même que pour les mots composés traditionnels, l’identification de ces structures limite la complexité combinatoire induite par l’ambiguïté lexicale. Dans cet article, nous détaillons une expérience qui intègre ces notions dans un processus de segmentation en super-chunks, préalable à l’analyse syntaxique. Nous montrons que notre chunker, développé pour le français, atteint une précision et un rappel de 92,9 % et 98,7 %, respectivement. Par ailleurs, les unités polylexicales réalisent 36,6 % des attachements internes aux constituants nominaux et prépositionnels.</resume>
        <mots_cles>chunker, super-chunks, analyse syntaxique, patrons lexico-syntaxiques</mots_cles>
        <title/>
        <abstract>Since Harris’ parser in the late 50’s, multiword units have been progressively integrated in parsers. Nevertheless, in the most part, they are still restricted to compound words, that are more stable and less numerous. Actually, language is full of semi-frozen expressions that also form basic semantic units : semi-frozen adverbial expressions (e.g. time), collocations. Like compounds, the identification of these structures limits the combinatorial complexity induced by lexical ambiguity. In this paper, we detail an experiment that largely integrates these notions in a procedure of segmentation into super-chunks, preliminary to a parser. We show that the chunker, developped for French, reaches 92.9% precision and 98.7% recall. Moreover, multiword units realize 36.6% of the attachments within nominal and prepositional phrases.</abstract>
        <keywords>chunker, super-chunks, syntactic analysis, lexico-syntactic patterns</keywords>
      </article>
      <article id="taln-2007-poster-004" session="Posters">
        <auteurs>
          <auteur>
            <nom>Narjès Boufaden</nom>
            <email>Narjes.Boufaden@crim.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Truong Le Hoang</nom>
            <email>LeHoang.Truong@crim.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Pierre Dumouchel</nom>
            <email>Pierre.Dumouchel@crim.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">École de Technologie Supérieure et Centre de Recherche Informatique de Montréal</affiliation>
        </affiliations>
        <titre>Détection et prédiction de la satisfaction des usagers dans les dialogues Personne-Machine</titre>
        <type>poster</type>
        <pages>43-52</pages>
        <resume>Nous étudions le rôle des entités nommées et marques discursives de rétroaction pour la tâche de classification et prédiction de la satisfaction usager à partir de dialogues. Les expériences menées sur 1027 dialogues Personne-Machine dans le domaine des agences de voyage montrent que les entités nommées et les marques discursives n’améliorent pas de manière significative le taux de classification des dialogues. Par contre, elles permettent une meilleure prédiction de la satisfaction usager à partir des premiers tours de parole usager.</resume>
        <mots_cles>prédiction de la satisfaction usager, classification des dialogues Personne-Machine</mots_cles>
        <title/>
        <abstract>We study the usefulness of named entities and acknowldgment words for user satisfaction classification and prediction from Human-Computer dialogs. We show that named entities and acknowledgment words do not enhance baseline classification performance. However, they allow a better prediction of user satisfaction in the beginning of the dialogue.</abstract>
        <keywords>prediction of user satisfaction, Human-Computer dialog classification</keywords>
      </article>
      <article id="taln-2007-poster-005" session="Posters">
        <auteurs>
          <auteur>
            <nom>Pierrette Bouillon</nom>
            <email>Pierrette.Bouillon@issco.unige.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Manny Rayner</nom>
            <email>Emmanuel.Rayner@issco.unige.ch</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Marianne Starlander</nom>
            <email>Marianne.Starlander@eti.unige.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Marianne Santaholma</nom>
            <email>Marianne.Santaholma@eti.unige.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">University of Geneva, TIM/ISSCO, 40, bvd du Pont-d’Arve, CH-1211 Geneva 4, Switzerland</affiliation>
          <affiliation affiliationId="2">Powerset Inc, 475 Brannan Street, San Francisco, CA 94107, US</affiliation>
        </affiliations>
        <titre>Les ellipses dans un système de traduction automatique de la parole</titre>
        <type>poster</type>
        <pages>53-62</pages>
        <resume>Dans tout dialogue, les phrases elliptiques sont très nombreuses. Dans cet article, nous évaluons leur impact sur la reconnaissance et la traduction dans le système de traduction automatique de la parole MedSLT. La résolution des ellipses y est effectuée par une méthode robuste et portable, empruntée aux systèmes de dialogue homme-machine. Cette dernière exploite une représentation sémantique plate et combine des techniques linguistiques (pour construire la représentation) et basées sur les exemples (pour apprendre sur la base d’un corpus ce qu’est une ellipse bien formée dans un sous-domaine donné et comment la résoudre).</resume>
        <mots_cles>traduction automatique de la parole, reconnaissance de la parole, ellipses, évaluation, traitement du dialogue, modèle du language fondé sur les grammaire</mots_cles>
        <title/>
        <abstract>Elliptical phrases are frequent in all genres of dialogue. In this paper, we describe an evaluation of the speech understanding component of the MedSLT medical speech translation system, which focusses on the contrast between system performance on elliptical phrases and full utterances. Ellipsis resolution in the system is handled by a robust and portable method, adapted from similar methods commonly used in spoken dialogue systems, which exploits the flat representation structures used. The resolution module combines linguistic methods, used to construct the representations, with an example-based approach to defining the space of well-formed ellipsis resolutions in a subdomain.</abstract>
        <keywords>speech recognition, speech translation, ellipsis, dialogue processing, grammar-based language modelling, evaluation</keywords>
      </article>
      <article id="taln-2007-poster-006" session="Posters">
        <auteurs>
          <auteur>
            <nom>Nathalie Camelin</nom>
            <email>nathalie.camelin@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Frédéric Béchet</nom>
            <email>frederic.bechet@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Géraldine Damnati</nom>
            <email>geraldine.damnati@francetelecom.com</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Renato De Mori</nom>
            <email>renato.demori@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIA/CNRS, University of Avignon, BP1228, 84911 Avignon cedex 09 France</affiliation>
          <affiliation affiliationId="2">France Télécom R&amp;D – TECH/SSTP/RVA, 2 av. Pierre Marzin, 22307 Lannion Cedex 07, France</affiliation>
        </affiliations>
        <titre>Analyse automatique de sondages téléphoniques d’opinion</titre>
        <type>poster</type>
        <pages>63-72</pages>
        <resume>Cette étude présente la problématique de l’analyse automatique de sondages téléphoniques d’opinion. Cette analyse se fait en deux étapes : tout d’abord extraire des messages oraux les expressions subjectives relatives aux opinions de utilisateurs sur une dimension particulière (efficacité, accueil, etc.) ; puis sélectionner les messages fiables, selon un ensemble de mesures de confiance, et estimer la distribution des diverses opinions sur le corpus de test. Le but est d’estimer une distribution aussi proche que possible de la distribution de référence. Cette étude est menée sur un corpus de messages provenant de vrais utilisateurs fournis par France Télécom R&amp;D.</resume>
        <mots_cles>détection d’opinions, classification automatique, reconnaissance automatique de la parole, champs conditionnels aléatoires</mots_cles>
        <title/>
        <abstract>This paper introduces the context of the automatic analysis of opinion telephone surveys. This analysis is done by means of two stages : firstly the subjective expressions, related to the expression of an opinion on a particular dimension (efficiency, courtesy, . . . ), are extracted from the audio messages ; secondly the reliable messages, according to a set of confidence measures, are selected and the distribution of the positive and negative opinions in these messages is estimated. The goal is to obtain a distribution as close as possible to the reference one. This study is carried on a telephone survey corpus, provided by France Télécom R&amp;D, obtained in real field conditions.</abstract>
        <keywords>opinion extraction, automatic classification, automatic speech recognition, conditional random fields</keywords>
      </article>
      <article id="taln-2007-poster-007" session="Posters">
        <auteurs>
          <auteur>
            <nom>Claire Gardent</nom>
            <email>Claire.Gardent@loria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Éric Kow</nom>
            <email>Eric.Kow@loria.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CNRS/LORIA, Nancy</affiliation>
          <affiliation affiliationId="2">INRIA/LORIA, Nancy</affiliation>
        </affiliations>
        <titre>Une réalisateur de surface basé sur une grammaire réversible</titre>
        <type>poster</type>
        <pages>73-82</pages>
        <resume>En génération, un réalisateur de surface a pour fonction de produire, à partir d’une représentation conceptuelle donnée, une phrase grammaticale. Les réalisateur existants soit utilisent une grammaire réversible et des méthodes statistiques pour déterminer parmi l’ensemble des sorties produites la plus plausible ; soit utilisent des grammaires spécialisées pour la génération et des méthodes symboliques pour déterminer la paraphrase la plus appropriée à un contexte de génération donné. Dans cet article, nous présentons GENI, un réalisateur de surface basé sur une grammaire d’arbres adjoints pour le français qui réconcilie les deux approches en combinant une grammaire réversible avec une sélection symbolique des paraphrases.</resume>
        <mots_cles>réalisation de surface, grammaire d’arbres adjoints, réversibilité</mots_cles>
        <title/>
        <abstract>In generation, a surface realiser takes as input a conceptual representation and outputs a grammatical sentence. Existing realisers fall into two camps. Either they are based on a reversible grammar and use statistical filtering to determine among the several outputs the most plausible one. Or they combine a grammar tailored for generation and a symbolic means of choosing the paraphrase most appropriate to a given generation context. In this paper, we present GENI, a surface realiser based on a Tree Adjoining Grammar for French which reconciles both approaches in that (i) the grammar used is réversible and (ii) paraphrase selection is based on symbolic means.</abstract>
        <keywords>surface realisation, tree adjoining grammar, reversibility</keywords>
      </article>
      <article id="taln-2007-poster-008" session="Posters">
        <auteurs>
          <auteur>
            <nom>Laurent Gillard</nom>
            <email>laurent.gillard@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Patrice Bellot</nom>
            <email>patrice.bellot@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Marc El-Bèze</nom>
            <email>marc.elbeze@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire d'Informatique d'Avignon (LIA), Université d’Avignon et des Pays de Vaucluse, 339 ch. des Meinajaries, BP 1228, F-84911 Avignon Cedex 9 (France)</affiliation>
        </affiliations>
        <titre>Analyse des échecs d’une approche pour traiter les questions définitoires soumises à un système de questions/réponses</titre>
        <type>poster</type>
        <pages>83-92</pages>
        <resume>Cet article revient sur le type particulier des questions définitoires étudiées dans le cadre des campagnes d’évaluation des systèmes de Questions/Réponses. Nous présentons l’approche développée suite à notre participation à la campagne EQueR et son évaluation lors de QA@CLEF 2006. La réponse proposée est la plus représentative des expressions présentes en apposition avec l’objet à définir, sa sélection est faite depuis des indices dérivés de ces appositions. Environ 80% de bonnes réponses sont trouvées sur les questions définitoires des volets francophones de CLEF. Les cas d’erreurs rencontrés sont analysés et discutés en détail.</resume>
        <mots_cles>système de questions/réponses, questions définitoires</mots_cles>
        <title/>
        <abstract>This paper proposes an approach to deal with definitional question answering. Our system extracts answers to these questions from appositives appearing closed to the subject to define. Results are presented for CLEF campaigns. Next, failures are discussed.</abstract>
        <keywords>question answering, definitional question answering</keywords>
      </article>
      <article id="taln-2007-poster-009" session="Posters">
        <auteurs>
          <auteur>
            <nom>Lorraine Goeuriot</nom>
            <email>lorraine.goeuriot@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Natalia Grabar</nom>
            <email>natalia.grabar@biomath.jussieu.fr</email>
            <affiliationId>2</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Béatrice Daille</nom>
            <email>beatrice.daille@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LINA/Nantes</affiliation>
          <affiliation affiliationId="2">INSERM, UMR_S 872, Eq. 20, Paris, F-75006, Université René Descartes, Paris, F-75006</affiliation>
          <affiliation affiliationId="3">Health on the Net Foundation, SIM/HUG, Genève, Suisse</affiliation>
        </affiliations>
        <titre>Caractérisation des discours scientifiques et vulgarisés en français, japonais et russe</titre>
        <type>poster</type>
        <pages>93-102</pages>
        <resume>L’objectif principal de notre travail consiste à étudier la notion de comparabilité des corpus, et nous abordons cette question dans un contexte monolingue en cherchant à distinguer les documents scientifiques et vulgarisés. Nous travaillons séparément sur des corpus composés de documents du domaine médical dans trois langues à forte distance linguistique (le français, le japonais et le russe). Dans notre approche, les documents sont caractérisés dans chaque langue selon leur thématique et une typologie discursive qui se situe à trois niveaux de l’analyse des documents : structurel, modal et lexical. Le typage des documents est implémenté avec deux algorithmes d’apprentissage (SVMlight et C4.5). L’évaluation des résultats montre que la typologie discursive proposée est portable d’une langue à l’autre car elle permet en effet de distinguer les deux discours. Nous constatons néanmoins des performances très variées selon les langues, les algorithmes et les types de caractéristiques discursives.</resume>
        <mots_cles>linguistique des corpus, corpus comparable, algorithmes d’apprentissage, analyse stylistique, degré de comparabilité</mots_cles>
        <title/>
        <abstract>The main objective of our study consists to characterise the comparability of corpora, and we address this issue in the monolingual context through the disctinction of expert and non expert documents. We work separately with corpora composed of medical area documents in three languages, which show an important linguistic distance between them (French, Japanese and Russian). In our approach, documents are characterised in each language through their thematic topic and through a discursive typology positioned at three levels of document analysis : structural, modal and lexical. The document typology is implemented with two learning algorithms (SVMlight and C4.5). Evaluation of results shows that the proposed discursive typology can be transposed from one language to another, as it indeed allows to distinguish the two aimed discourses. However, we observe that performances vary a lot according to languages, algorithms and types of discursive characteristics.</abstract>
        <keywords>corpus linguistics, comparable corpora, learning algorithms, stylistic analysis, degree of comparability</keywords>
      </article>
      <article id="taln-2007-poster-010" session="Posters">
        <auteurs>
          <auteur>
            <nom>Thierry Hamon</nom>
            <email>Thierry.Hamon@lipn.univ-paris13.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Julien Derivière</nom>
            <email>Julien.Derivière@lipn.univ-paris13.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Adeline Nazarenko</nom>
            <email>Adeline.Nazarenko@lipn.univ-paris13.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIPN – UMR CNRS 7030, 99 av. J.B. Clément, F-93430 Villetaneuse, FRANCE</affiliation>
        </affiliations>
        <titre>OGMIOS : une plate-forme d’annotation linguistique de collection de documents issus du Web</titre>
        <type>poster</type>
        <pages>103-112</pages>
        <resume>L’un des objectifs du projet ALVIS est d’intégrer des informations linguistiques dans des moteurs de recherche spécialisés. Dans ce contexte, nous avons conçu une plate-forme d’enrichissement linguistique de documents issus du Web, OGMIOS, exploitant des outils de TAL existants. Les documents peuvent être en français ou en anglais. Cette architecture est distribuée, afin de répondre aux contraintes liées aux traitements de gros volumes de textes, et adaptable, pour permettre l’analyse de sous-langages. La plate-forme est développée en Perl et disponible sous forme de modules CPAN. C’est une structure modulaire dans lequel il est possible d’intégrer de nouvelles ressources ou de nouveaux outils de TAL. On peut ainsi définir des configuration différentes pour différents domaines et types de collections. Cette plateforme robuste permet d’analyser en masse des données issus du web qui sont par essence très hétérogènes. Nous avons évalué les performances de la plateforme sur plusieurs collections de documents. En distribuant les traitements sur vingt machines, une collection de 55 329 documents du domaine de la biologie (106 millions de mots) a été annotée en 35 heures tandis qu’une collection de 48 422 dépêches relatives aux moteurs de recherche (14 millions de mots) a été annotée en 3 heures et 15 minutes.</resume>
        <mots_cles>plateforme d’annotation linguistique, passage à l’échelle, robustesse</mots_cles>
        <title/>
        <abstract>In the context of the ALVIS project, which aims at integrating linguistic information in topic-specific search engines, we developed an NLP architecture, OGMIOS, to linguistically annotate large collections of web documents with existing NLP tools. Documents can be written in French or English. The distributed architecture allows us to take into account constraints related to the scalability problem of Natural Language Processing and the domain specific tuning of the linguistic analysis. The platform is developed in Perl and is available as CPAN modules. It is a modularized framework where new resources or NLP tools can be integrated. Then, various configurations are easy to define for various domains and collections. This platform is robust to massively analyse web document collections which are heterogeneous in essence. We carried out experiments on two different collections of web documents on 20 computers. A 55,329 web documents collection dealing with biology (106 millions of words) has been annotated in 35 hours, whereas a 48,422 search engine news collection (14 millions of word) has been annotated in 3 hours and 15 minutes.</abstract>
        <keywords>linguistic annotation, NLP platform, process scability, robustess</keywords>
      </article>
      <article id="taln-2007-poster-011" session="Posters">
        <auteurs>
          <auteur>
            <nom>Sébastien Haton</nom>
            <email>sebastien.haton@atilf.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Jean-Marie Pierrel</nom>
            <email>jean-marie.pierrel@atilf.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire ATILF Nancy Université – CNRS, 44 avenue de la Libération, BP 30687, 54063 Nancy CEDEX</affiliation>
        </affiliations>
        <titre>Les Lexiques-Miroirs. Du dictionnaire bilingue au graphe multilingue</titre>
        <type>poster</type>
        <pages>113-122</pages>
        <resume>On observe dans les dictionnaires bilingues une forte asymétrie entre les deux parties d’un même dictionnaire et l’existence de traductions et d’informations « cachées », i.e. pas directement visibles à l’entrée du mot à traduire. Nous proposons une méthodologie de récupération des données cachées ainsi que la « symétrisation » du dictionnaire grâce à un traitement automatique. L’étude d’un certain nombre de verbes et de leurs traductions en plusieurs langues a conduit à l’intégration de toutes les données, visibles ou cachées, au sein d’une base de données unique et multilingue. L’exploitation de la base de données a été rendue possible par l’écriture d’un algorithme de création de graphe synonymique qui lie dans un même espace les mots de langues différentes. Le programme qui en découle permettra de générer des dictionnaires paramétrables directement à partir du graphe.</resume>
        <mots_cles>dictionnaires bilingues, traduction automatique, graphe multilingue, algorithme, polysémie verbale, dissymétrie lexicographique</mots_cles>
        <title/>
        <abstract>Lexical asymmetry and hidden data, i.e. not directly visible into one lexical entry, are phenomena peculiar to most of the bilingual dictionaries. Our purpose is to establish a methodology to highlight both phenomena by extracting hidden data from the dictionary and by re-establishing symmetry between its two parts. So we studied a large number of verbs and integrated them into a unique multilingual database. At last, our database is turned into a “multilexical” graph thanks to an algorithm, which is binding together words from different languages into the same semantic space. This will allow us to generate automatically dictionaries straight from the graph.</abstract>
        <keywords>bilingual dictionaries, machine translation, multilingual graph, algorithm, verbal polysemy, lexicographical asymmetry</keywords>
      </article>
      <article id="taln-2007-poster-012" session="Posters">
        <auteurs>
          <auteur>
            <nom>Sylvain Kahane</nom>
            <email>sk@ccr.jussieu.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Modyco, Université Paris 10 - Nanterre &amp; CNRS</affiliation>
        </affiliations>
        <titre>Traduction, restructurations syntaxiques et grammaires de correspondance</titre>
        <type>poster</type>
        <pages>123-132</pages>
        <resume>Cet article présente une nouvelle formalisation du modèle de traduction par transfert de la Théorie Sens-Texte. Notre modélisation utilise les grammaires de correspondance polarisées et fait une stricte séparation entre les modèles monolingues, un lexique bilingue minimal et des règles de restructuration universelles, directement associées aux fonctions lexicales syntaxiques.</resume>
        <mots_cles>traduction automatique, paraphrase, restructuration syntaxique, TST (Théorie Sens-Texte), grammaire de dépendance, fonction lexicale, lexique bilingue, GUP (Grammaire d’Unification Polarisée), grammaire de correspondance, grammaires synchrones</mots_cles>
        <title/>
        <abstract>This paper presents a new formalisation of transfer-based translation model of the Meaning-Text Theory. Our modelling is based on polarized correspondence grammars and observes a strict separation between monolingual models, the bilingual lexicon and universal restructuring rules, directly associated with syntactic lexical functions.</abstract>
        <keywords>machine translation, paraphrase, syntactic restructuring, MTT (Meaning-Text Theory), dependency grammar, lexical function, bilingual lexicon, PUG (Polarized Unification Grammar), correspondence grammar, synchronous grammars</keywords>
      </article>
      <article id="taln-2007-poster-013" session="Posters">
        <auteurs>
          <auteur>
            <nom>Aïda Khemakhem</nom>
            <email>khemakhem.aida@gnet.tn</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Bilel Gargouri</nom>
            <email>Bilel.Gargouri@fsegs.rnu.tn</email>
            <affiliationId/>
          </auteur>
          <auteur>
            <nom>Abdelhamid Abdelwahed</nom>
            <email>abdelhamid.abdelwahed@yahoo.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Gil Francopoulo</nom>
            <email>gil.francopoulo@wanadoo.fr</email>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire MIRACL, FSEG-SFAX B.P. 1088, 3018 SFAX – TUNISIE</affiliation>
          <affiliation affiliationId="2">Unité de recherche LSCA, FLSH-SFAX B.P. 553, 3018 SFAX – TUNISIE</affiliation>
          <affiliation affiliationId="3">INRIA-Loria</affiliation>
        </affiliations>
        <titre>Modélisation des paradigmes de flexion des verbes arabes selon la norme LMF - ISO 24613</titre>
        <type>poster</type>
        <pages>133-142</pages>
        <resume>Dans cet article, nous spécifions les paradigmes de flexion des verbes arabes en respectant la version 9 de LMF (Lexical Markup Framework), future norme ISO 24613 qui traite de la standardisation des bases lexicales. La spécification de ces paradigmes se fonde sur une combinaison des racines et des schèmes. En particulier, nous mettons en relief les terminaisons de racines sensibles aux ajouts de suffixes et ce, afin de couvrir les situations non considérées dans les travaux existants. L’élaboration des paradigmes de flexion verbale que nous proposons est une description en intension d'ArabicLDB (Arabic Lexical DataBase) qui est une base lexicale normalisée pour la langue arabe. Nos travaux sont illustrés par la réalisation d’un conjugueur des verbes arabes à partir d'ArabicLDB.</resume>
        <mots_cles>langue arabe, paradigmes de flexion verbale, base lexicale, norme ISO 24613, LMF, lexical markup framework, conjugueur des verbes arabes</mots_cles>
        <title/>
        <abstract>In this paper, we specify the inflected paradigms of Arabic verbs with respect to the version 9 of LMF (Lexical Markup Framework) which is the expected ISO 24613 standard dealing with the standardisation of lexical databases. The specification of these paradigms is based on a combination of schemes and roots. In particular, we highlight the role of root endings that is not considered in other researches and that may generate erroneous forms while concatenating suffixes. The development of verbal inflected paradigms that we propose is an intentional component of ArabicLDB (Arabic Lexical DataBase) which is a normalized Arabic lexical database that we developed according to LMF. Our works are illustrated by the realization of a conjugation tool for Arabic verbs using ArabicLDB.</abstract>
        <keywords>Arabic, inflected paradigms of verb, lexical database, norm ISO 24613, LMF, lexical markup framework, conjugation of arabic verbs</keywords>
      </article>
      <article id="taln-2007-poster-014" session="Posters">
        <auteurs>
          <auteur>
            <nom>Olivier Kraif</nom>
            <email>Olivier.Kraif@u-grenoble3.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Claude Ponton</nom>
            <email>Claude.Ponton@u-grenoble3.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIDILEM - Laboratoire de linguistique et didactique des langues étrangères et maternelles (http://www.u-grenoble3.fr/lidilem/labo)</affiliation>
        </affiliations>
        <titre>Du bruit, du silence et des ambiguïtés : que faire du TAL pour l'apprentissage des langues ?</titre>
        <type>poster</type>
        <pages>143-152</pages>
        <resume>Nous proposons une nouvelle approche pour l’intégration du TAL dans les systèmes d’apprentissage des langues assisté par ordinateur (ALAO), la stratégie « moinsdisante ». Cette approche tire profit des technologies élémentaires mais fiables du TAL et insiste sur la nécessité de traitements modulaires et déclaratifs afin de faciliter la portabilité et la prise en main didactique des systèmes. Basé sur cette approche, ExoGen est un premier prototype pour la génération automatique d’activités lacunaires ou de lecture d’exemples. Il intègre un module de repérage et de description des réponses des apprenants fondé sur la comparaison entre réponse attendue et réponse donnée. L’analyse des différences graphiques, orthographiques et morphosyntaxiques permet un diagnostic des erreurs de type fautes d’orthographe, confusions, problèmes d’accord, de conjugaison, etc. La première évaluation d’ExoGen sur un extrait du corpus d’apprenants FRIDA produit des résultats prometteurs pour le développement de cette approche « moins-disante », et permet d'envisager un modèle d'analyse performant et généralisable à une grande variété d'activités.</resume>
        <mots_cles>ALAO, apprentissage des langues, diagnostic d'erreur, feed-back d'erreur</mots_cles>
        <title/>
        <abstract>This paper presents the so-called "moins-disante" strategy, a new approach for NLP integrating in Computer Assisted Language Learning (CALL) systems. It is based on the implementation of basic but reliable NLP techniques, and put emphasis on declarative and modular processing, for the sake of portability and didactic implementation. Based on this approach, ExoGen is a prototype for generating activities such as gap filling exercises. It integrates a module for error detection and description, which checks learners' answers against expected ones. Through the analysis of graphic, orthographic and morphosyntactic differences, it is able to diagnose problems like spelling errors, lexical mix-up, error prone agreement, bad conjugations, etc. The first evaluation of ExoGen outputs, based on the FRIDA learner corpus, has yielded very promising results, paving the way for the development of an efficient and general model tailored to a wide variety of activities.</abstract>
        <keywords>CALL, language learning, error diagnosis, error feedback</keywords>
      </article>
      <article id="taln-2007-poster-015" session="Posters">
        <auteurs>
          <auteur>
            <nom>Anna Kupsc</nom>
            <email>akupsc@univ-paris3.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université Paris3 / LLF, UMR 7110 et Académie Polonaise des Sciences</affiliation>
        </affiliations>
        <titre>Extraction automatique de cadres de sous-catégorisation verbale pour le français à partir d’un corpus arboré</titre>
        <type>poster</type>
        <pages>153-162</pages>
        <resume>Nous présentons une expérience d’extraction automatique des cadres de souscatégorisation pour 1362 verbes français. Nous exploitons un corpus journalistique richement annoté de 15 000 phrases dont nous extrayons 12 510 occurrences verbales. Nous évaluons dans un premier temps l’extraction des cadres basée sur la fonction des arguments, ce qui nous fournit 39 cadres différents avec une moyenne de 1.54 cadres par lemme. Ensuite, nous adoptons une approche mixte (fonction et catégorie syntaxique) qui nous fournit dans un premier temps 925 cadres différents, avec une moyenne de 3.44 cadres par lemme. Plusieurs méthodes de factorisation, neutralisant en particulier les variantes de réalisation avec le passif ou les pronoms clitiques, sont ensuite appliquées et nous permettent d’aboutir à 235 cadres différents avec une moyenne de 1.94 cadres par verbe. Nous comparons brièvement nos résultats avec les travaux existants pour le français et pour l’anglais.</resume>
        <mots_cles>français, corpus arboré, sous-catégorisation verbale, lexique-grammaire</mots_cles>
        <title/>
        <abstract>We present our work on automatic extraction of subcategorisation frames for 1362 French verbs. We use a treebank of 15000 sentences from which we extract 12510 verb occurrences. We evaluate the results based on a functional representation of frames and we acquire 39 different frames, 1.54 per lemma on average. Then, we adopt a mixed representation (functions and categories), which leads to 925 different frames, 3.44 frames on average. We investigate several methods to reduce the ambiguity (e.g., neutralisation of passive forms or clitic arguments), which allows us to arrive at 235 frames, with 1.94 frames per lemma on average. We present a brief comparison with the existing work on French and English.</abstract>
        <keywords>French, treebank, verbal subcategorization, lexicon grammar</keywords>
      </article>
      <article id="taln-2007-poster-016" session="Posters">
        <auteurs>
          <auteur>
            <nom>François Lareau</nom>
            <email>francois.lareau@umontreal.ca</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Lattice - U. Paris 7, UFRL, Case 7003, 2 pl. Jussieu, 75251 Paris cedex 5 </affiliation>
          <affiliation affiliationId="2">OLST - U. de Montréal, Ling., CP 6128 succ C.-V., Montréal QC, H3C 3J7</affiliation>
        </affiliations>
        <titre>Vers une formalisation des décompositions sémantiques dans la Grammaire d’Unification Sens-Texte</titre>
        <type>poster</type>
        <pages>163-172</pages>
        <resume>Nous proposons une formalisation de la décomposition du sens dans le cadre de la Grammaire d’Unification Sens-Texte. Cette formalisation vise une meilleure intégration des décompositions sémantiques dans un modèle global de la langue. Elle repose sur un jeu de saturation de polarités qui permet de contrôler la construction des représentations décomposées ainsi que leur mise en correspondance avec des arbres syntaxiques qui les expriment. Le formalisme proposé est illustré ici dans une perspective de synthèse, mais il s’applique également en analyse.</resume>
        <mots_cles>Grammaire d’Unification Sens-Texte, Théorie Sens-Texte, sémantique, représentation du sens, paraphrasage</mots_cles>
        <title/>
        <abstract>We propose a formal representation of meaning decomposition in the framework of the Meaning-Text Unification Grammar. The proposed technique aims at offering a better integration of such semantic decompositions into a global model of the language. It relies on the saturation of polarities to control the construction of decomposed respresentations as well as their mapping to the syntactic trees that express them. The proposed formalism is discussed from the viewpoint of generation, but it applies to analysis as well.</abstract>
        <keywords>Meaning-Text Unification Grammar, Meaning-Text Theory, semantics, representation of meaning, paraphrasing</keywords>
      </article>
      <article id="taln-2007-poster-017" session="Posters">
        <auteurs>
          <auteur>
            <nom>Anne-Laure Ligozat</nom>
            <email>Anne-Laure.Ligozat@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Brigitte Grau</nom>
            <email>Brigitte Grau@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Isabelle Robba</nom>
            <email>Isabelle.Robba@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Anne Vilnat</nom>
            <email>Anne.Vilnat@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIMSI-CNRS - BP 133, 91403 Orsay Cedex</affiliation>
        </affiliations>
        <titre>Systèmes de questions-réponses : vers la validation automatique des réponses</titre>
        <type>poster</type>
        <pages>173-182</pages>
        <resume>Les systèmes de questions-réponses (SQR) ont pour but de trouver une information précise extraite d’une grande collection de documents comme le Web. Afin de pouvoir comparer les différentes stratégies possibles pour trouver une telle information, il est important d’évaluer ces systèmes. L’objectif d’une tâche de validation de réponses est d’estimer si une réponse donnée par un SQR est correcte ou non, en fonction du passage de texte donné comme justification. En 2006, nous avons participé à une tâche de validation de réponses, et dans cet article nous présentons la stratégie que nous avons utilisée. Celle-ci est fondée sur notre propre système de questions-réponses. Le principe est de comparer nos réponses avec les réponses à valider. Nous présentons les résultats obtenus et montrons les extensions possibles. À partir de quelques exemples, nous soulignons les difficultés que pose cette tâche.</resume>
        <mots_cles>systèmes de questions-réponses, validation de réponses</mots_cles>
        <title/>
        <abstract>Question answering aims at retrieving precise information from a large collection of documents, typically theWeb. Different techniques can be used to find relevant information, and to compare these techniques, it is important to evaluate question answering systems. The objective of an Answer Validation task is to estimate the correctness of an answer returned by a QA system for a question, according to the text snippet given to support it.We participated in such a task in 2006. In this article, we present our strategy for deciding if the snippets justify the answers. We used a strategy based on our own question answering system, and compared the answers it returned with the answer to judge. We discuss our results, and show the possible extensions of our strategy. Then we point out the difficulties of this task, by examining different examples.</abstract>
        <keywords>question answering, answer validation</keywords>
      </article>
      <article id="taln-2007-poster-018" session="Posters">
        <auteurs>
          <auteur>
            <nom>Huei-Chi Lin</nom>
            <email>lin_huei_chi@yahoo.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Max Silberztein</nom>
            <email>max.silberztein@univ-fcomte.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire de Sémiolinguistique, Didactique, Informatique (LASELDI) – Université de Franche-Comté, 30 rue Mégevand 25000 Besançon</affiliation>
        </affiliations>
        <titre>Ressources lexicales chinoises pour le TALN</titre>
        <type>poster</type>
        <pages>183-192</pages>
        <resume>Nous voulons traiter des textes chinois automatiquement ; pour ce faire, nous formalisons le vocabulaire chinois, en utilisant principalement des dictionnaires et des grammaires morphologiques et syntaxiques formalisés avec le logiciel NooJ. Nous présentons ici les critères linguistiques qui nous ont permis de construire dictionnaires et grammaires, sachant que l’application envisagée (linguistique de corpus) nous impose certaines contraintes dans la formalisation des unités de la langue, en particulier des composés.</resume>
        <mots_cles>ressources linguistiques pour le chinois, linguistique de corpus, NooJ</mots_cles>
        <title/>
        <abstract>In order to parse Chinese texts automatically, we need to formalize the Chinese vocabulary by using electronic dictionaries and morphological and syntactic grammars. We have used the NooJ software to enter the formalization. We present here the set of linguistic criteria used to construct these dictionaries and grammars, so that they can be used by corpus-linguistic applications. We focus our discussion on the characterization of Chinese linguistic units, specifically compounds.</abstract>
        <keywords>linguistic resources for chinese, corpus linguistics, NooJ</keywords>
      </article>
      <article id="taln-2007-poster-019" session="Posters">
        <auteurs>
          <auteur>
            <nom>Sinikka Loikkanen</nom>
            <email>sinikka.loikkanen@helsinki.fi</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université d’Helsinki</affiliation>
        </affiliations>
        <titre>Étiquetage morpho-syntaxique de textes kabyles</titre>
        <type>poster</type>
        <pages>193-202</pages>
        <resume>Cet article présente la construction d’un étiqueteur morpho-syntaxique développé pour annoter un corpus de textes kabyles (1 million de mots). Au sein de notre projet, un étiqueteur morpho-syntaxique a été développé et implémenté. Ceci inclut un analyseur morphologique ainsi que l’ensemble de règles de désambiguïsation qui se basent sur l’approche supervisée à base de règles. Pour effectuer le marquage, un jeu d’étiquettes morpho-syntaxiques pour le kabyle est proposé. Les résultats préliminaires sont très encourageants. Nous obtenons un taux d’étiquetage réussi autour de 97 % des textes en prose.</resume>
        <mots_cles>Étiquetage morpho-syntaxique, corpus de textes, langue kabyle, berbère</mots_cles>
        <title/>
        <abstract>This paper describes the construction of a morpho-syntactic tagger developed to annotate our Kabyle text corpus (1 million words).Within our project, a part-of-speech tagger has been developed and implemented. That includes a morphological analyser and a set of disambiguation rules based on supervised rule-based tagging. To realise the annotation, a POS tagset for Kabyle is proposed. The first results of tests are very encouraging. At present stage, our tagger reaches 97 % of success.</abstract>
        <keywords>Part of speech tagging, text corpus, kabyle language, berber</keywords>
      </article>
      <article id="taln-2007-poster-020" session="Posters">
        <auteurs>
          <auteur>
            <nom>Athina Michou</nom>
            <email>Athina.Michou@lettres.unige.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LATL – Université de Genève</affiliation>
        </affiliations>
        <titre>Analyse syntaxique et traitement automatique du syntagme nominal grec moderne</titre>
        <type>poster</type>
        <pages>203-212</pages>
        <resume>Cet article décrit le traitement automatique du syntagme nominal en grec moderne par le modèle d’analyse syntaxique multilingue Fips. L’analyse syntaxique linguistique est focalisée sur les points principaux du DP grec : l’accord entre les constituants fléchis, l’ordre flexible des constituants, la cliticisation sur les noms et le phénomène de la polydéfinitude. Il est montré comment ces phénomènes sont traités et implémentés dans le cadre de l’analyseur syntaxique FipsGreek, qui met en oeuvre un formalisme inspiré de la grammaire générative chomskyenne.</resume>
        <mots_cles>analyseur grec, analyse morphosyntaxique, syntagme nominal, grec moderne</mots_cles>
        <title/>
        <abstract>This article describes an automatic treatment to the Modern Greek noun phrase in terms of the Fips multilingual syntactic parser. The syntactic analysis focuses on the main issues related to the Greek DP: the agreement among the inflected constituents, the relatively free constituent order, noun cliticisation, and the polydefiniteness phenomenon. The paper discusses how these processes are treated and implemented within the FipsGreek parser, which puts forth a formalism relying on Chomsky’s generative grammar.</abstract>
        <keywords>Greek parser, morphosyntactic analysis, determiner phrase, modern Greek</keywords>
      </article>
      <article id="taln-2007-poster-021" session="Posters">
        <auteurs>
          <auteur>
            <nom>Erwan Moreau</nom>
            <email>Erwan.Moreau@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LINA - FRE 2729, Université de Nantes, 2 rue de la Houssinière, BP 92208, F-44322 Nantes cedex 3</affiliation>
        </affiliations>
        <titre>Apprentissage symbolique de grammaires et traitement automatique des langues</titre>
        <type>poster</type>
        <pages>213-222</pages>
        <resume>Le modèle de Gold formalise le processus d’apprentissage d’un langage. Nous présentons dans cet article les avantages et inconvénients de ce cadre théorique contraignant, dans la perspective d’applications en TAL. Nous décrivons brièvement les récentes avancées dans ce domaine, qui soulèvent selon nous certaines questions importantes.</resume>
        <mots_cles>apprentissage symbolique, modèle de Gold, grammaires catégorielles</mots_cles>
        <title/>
        <abstract>Gold’s model formalizes the learning process of a language. In this paper we present the advantages and drawbacks of this restrictive theoretical framework, in the viewpoint of applications to NLP.We briefly describe recent advances in the domain which, in our opinion, raise some important questions.</abstract>
        <keywords>symbolic learning, Gold’s model, categorial grammars</keywords>
      </article>
      <article id="taln-2007-poster-022" session="Posters">
        <auteurs>
          <auteur>
            <nom>Yayoi Nakamura-Delloye</nom>
            <email>yayoi@free.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université Paris 7 – LATTICE, 1 rue Maurice Arnoux 92120 Montrouge</affiliation>
        </affiliations>
        <titre>Méthodes d’alignement des propositions : un défi aux traductions croisées</titre>
        <type>poster</type>
        <pages>223-232</pages>
        <resume>Le présent article décrit deux méthodes d’alignement des propositions : l’une basée sur les méthodes d’appariement des graphes et une autre inspirée de la classification ascendante hiérarchique (CAH). Les deux méthodes sont caractérisées par leur capacité d’alignement des traductions croisées, ce qui était impossible pour beaucoup de méthodes classiques d’alignement des phrases. Contrairement aux résultats obtenus avec l’approche spectrale qui nous paraissent non satisfaisants, l’alignement basé sur la méthode de classification ascendante hiérarchique est prometteur dans la mesure où cette technique supporte bien les traductions croisées.</resume>
        <mots_cles>alignement des corpus parallèles, appariement de graphes, classification ascendante hiérarchique, proposition syntaxique, mémoire de traduction, linguistique contrastive</mots_cles>
        <title/>
        <abstract>The present paper describes two methods for clauses alignment. The first one uses a graph matching approach, while the second one relies on agglomerative hirerarchical clustering (AHC). Both methods are characterized by the fact they can align cross translations, which was impossible for previous classic sentence alignment methods. Though the results given by the spectral method are unsatisfactory, the method based on AHC is very promising. It handles correctly cross translations.</abstract>
        <keywords>parallel corpora alignment, graph matching, agglomerative hierarchical clustering, syntactic clause, translation memory, contrastive linguistics</keywords>
      </article>
      <article id="taln-2007-poster-023" session="Posters">
        <auteurs>
          <auteur>
            <nom>Fiammetta Namer</nom>
            <email>fiammetta.namer@univ-nancy2.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Pierrette Bouillon</nom>
            <email>pierrette.bouillon@issco.unige.ch</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Evelyne Jacquey</nom>
            <email>evelyne.jacquey@atilf.fr</email>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université Nancy2 et ATILF</affiliation>
          <affiliation affiliationId="2">ISSCO</affiliation>
          <affiliation affiliationId="3">ATILF</affiliation>
        </affiliations>
        <titre>Un Lexique Génératif de référence pour le français</titre>
        <type>poster</type>
        <pages>233-242</pages>
        <resume>Cet article propose une approche originale visant la construction d’un lexique sémantique de référence sur le français. Sa principale caractéristique est de pouvoir s’appuyer sur les propriétés morphologiques des lexèmes. La méthode combine en effet des résultats d’analyse morphologique (Namer, 2002;2003), à partir de ressources lexicales de grande taille (nomenclatures du TLF) et des méthodologies d’acquisition d’information lexicale déjà éprouvées (Namer 2005; Sébillot 2002). Le format de représentation choisi, dans le cadre du Lexique Génératif, se distingue par ses propriétés d’expressivité et d’économie. Cette approche permet donc d’envisager la construction d’un lexique de référence sur le français caractérisé par une forte homogénéité tout en garantissant une couverture large, tant du point de vue de la nomenclature que du point de vue des contenus sémantiques. Une première validation de la méthode fournit une projection quantitative et qualitative des résultats attendus.</resume>
        <mots_cles>acquisition lexicale, lexique de référence du français, modèle du lexique génératif, morphologie constructionnelle, corpus, sémantique</mots_cles>
        <title/>
        <abstract>This paper describes an original approach aiming at building a reference semantic lexicon for French. Its main characteristic is that of being able to rely on morphological properties. The method thus combines morphological analyses results (Namer 2002;2003;2005) from large scale lexical resources (i.e. TLF word lists) with already tested acquisition methodologies on lexical information (Sébillot, 2002). The representation format, within the Generative Lexicon framework, has been chosen for its expressiveness and economy features. So, this approach allows us to consider building a reference lexicon for French, which is fundamentally homogeneous as well as of a large coverage. A feasability study of the described method provides a projection of expected results, from both quantitative and qualitative points of view.</abstract>
        <keywords>lexical acquisition, reference lexicon for French, generative lexicon model, word formation, corpora, semantics</keywords>
      </article>
      <article id="taln-2007-poster-024" session="Posters">
        <auteurs>
          <auteur>
            <nom>Patrick Paroubek</nom>
            <email>pap@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Anne Vilnat</nom>
            <email>anne@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Isabelle Robba</nom>
            <email>isabelle@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Christelle Ayache</nom>
            <email>ayache@elda.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIMSI-CNRS Bât. 508 Université Paris XI, BP 133 - 91403 ORSAY Cedex</affiliation>
          <affiliation affiliationId="2">ELRA-ELDA 55-57, rue Brillat Savarin 75013 Paris</affiliation>
        </affiliations>
        <titre>Les résultats de la campagne EASY d’évaluation des analyseurs syntaxiques du français</titre>
        <type>poster</type>
        <pages>243-252</pages>
        <resume>Dans cet article, nous présentons les résultats de la campagne d’évaluation EASY des analyseurs syntaxiques du français. EASY a été la toute première campagne d’évaluation comparative des analyseurs syntaxiques du français en mode boîte noire utilisant des mesures objectives quantitatives. EASY fait partie du programme TECHNOLANGUE du Ministère délégué à la Recherche et à l’Éducation, avec le soutien du ministère de délégué à l’industrie et du ministère de la culture et de la communication. Nous exposons tout d’abord la position de la campagne par rapport aux autres projets d’évaluation en analyse syntaxique, puis nous présentos son déroulement, et donnons les résultats des 15 analyseurs participants en fonction des différents types de corpus et des différentes annotations (constituants et relations). Nous proposons ensuite un ensemble de leçons à tirer de cette campagne, en particulier à propos du protocole d’évaluation, de la définition de la segmentation en unités linguistiques, du formalisme et des activités d’annotation, des critères de qualité des données, des annotations et des résultats, et finalement de la notion de référence en analyse syntaxique. Nous concluons en présentant comment les résultats d’EASY se prolongent dans le projet PASSAGE (ANR-06-MDCA-013) qui vient de débuter et dont l’objectif est d’étiqueter un grand corpus par plusieurs analyseurs en les combinant selon des paramètres issus de l’évaluation.</resume>
        <mots_cles>analyseur syntaxique, évaluation, français</mots_cles>
        <title/>
        <abstract>In this paper, we present the results of the EASY evaluation campaign on parsers of French. EASY has been the very first black-box comparative evaluation campaign for parsers of French, with objective quantitative performance measures. EASY was part of the TECHNOLANGUE program of the Delegate Ministry of Research, jointly supported by the Delegate Ministry of Industry and the ministry of Culture and Communication. After setting EASY in the context of parsing evaluation and giving an account of the campaign, we present the results obtained by 15 parsers according to syntactic relation and subcorpus genre. Then we propose some lessons to draw from this campaign, in particular about the evaluation protocole, the segmenting into linguistic units, the formalism and the annotation activities, the quality criteria to apply for data, annotations and results and finally about the notion of reference for parsing. We conclude by showing how EASY results extend through the PASSAGE project (ANR-06-MDCA-013), which has just started and whose aim is the automatic annotation of a large corpus by several parsers, the combination of which being parametrized by results stemming from evaluation.</abstract>
        <keywords>parser, evaluation, french</keywords>
      </article>
      <article id="taln-2007-poster-025" session="Posters">
        <auteurs>
          <auteur>
            <nom>Holger Schwenk</nom>
            <email>schwenk@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Daniel Déchelotte</nom>
            <email>dechelot@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Hélène Bonneau-Maynard</nom>
            <email>hbm@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Alexandre Allauzen</nom>
            <email>allauzen@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIMSI-CNRS, B.P. 133, 91403 Orsay cedex</affiliation>
        </affiliations>
        <titre>Modèles statistiques enrichis par la syntaxe pour la traduction automatique</titre>
        <type>poster</type>
        <pages>253-262</pages>
        <resume>La traduction automatique statistique par séquences de mots est une voie prometteuse. Nous présentons dans cet article deux évolutions complémentaires. La première permet une modélisation de la langue cible dans un espace continu. La seconde intègre des catégories morpho-syntaxiques aux unités manipulées par le modèle de traduction. Ces deux approches sont évaluées sur la tâche Tc-Star. Les résultats les plus intéressants sont obtenus par la combinaison de ces deux méthodes.</resume>
        <mots_cles>traduction automatique, approche statistique, modélisation linguistique dans un espace continu, analyse morpho-syntaxique, désambiguïsation lexicale</mots_cles>
        <title/>
        <abstract>Statistical phrase-based translation models are very efficient. In this paper, we present two complementary methods. The first one consists in a a statistical language model that is based on a continuous representation of the words in the vocabulary. By these means we expect to take better advantage of the limited amount of training data. In the second method, morpho-syntactic information is incorporated into the translation model in order to obtain lexical disambiguation. Both approaches are evaluated on the Tc-Star task. Most promising results are obtained by combining both methods.</abstract>
        <keywords>statistical machine translation, continuous space language model, POS tagging, lexical disambiguation</keywords>
      </article>
      <article id="taln-2007-poster-026" session="Posters">
        <auteurs>
          <auteur>
            <nom>Laurianne Sitbon</nom>
            <email>laurianne.sitbon@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Patrice Bellot</nom>
            <email>patrice.bellot@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Philippe Blache</nom>
            <email>blache@lpl.univ-aix.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire d’Informatique d’Avignon - Université d’Avignon</affiliation>
          <affiliation affiliationId="2">Laboratoire Parole et Langage - Université de Provence</affiliation>
        </affiliations>
        <titre>Traitements phrastiques phonétiques pour la réécriture de phrases dysorthographiées</titre>
        <type>poster</type>
        <pages/>
        <resume>Cet article décrit une méthode qui combine des hypothèses graphémiques et phonétiques au niveau de la phrase, à l’aide d’une réprésentation en automates à états finis et d’un modèle de langage, pour la réécriture de phrases tapées au clavier par des dysorthographiques. La particularité des écrits dysorthographiés qui empêche les correcteurs orthographiques d’être efficaces pour cette tâche est une segmentation en mots parfois incorrecte. La réécriture diffère de la correction en ce sens que les phrases réécrites ne sont pas à destination de l’utilisateur mais d’un système automatique, tel qu’un moteur de recherche. De ce fait l’évaluation est conduite sur des versions filtrées et lemmatisées des phrases. Le taux d’erreurs mots moyen passe de 51 % à 20 % avec notre méthode, et est de 0 % sur 43 % des phrases testées.</resume>
        <mots_cles>réécriture de phrases, dyslexie, automates, correction orthographique</mots_cles>
        <title/>
        <abstract>This paper introduces a sentence level method combining written correction and phonetic interpretation in order to automatically rewrite sentences typed by dyslexic spellers. The method uses a finite state automata framework and a language model. Dysorthographics refers to incorrect word segmentation which usually causes classical spelling correctors fail. Our approach differs from spelling correction in that we aim to use several rewritings as an expression of the user need in an information retrieval context. Our system is evaluated on questions collected with the help of an orthophonist. The word error rate on lemmatised sentences falls from 51 % to 20 % (falls to 0 % on 43 % of sentences).</abstract>
        <keywords>sentence level rewriting, dyslexia, FSM, spell checking</keywords>
      </article>
      <article id="taln-2007-poster-027" session="Posters">
        <auteurs>
          <auteur>
            <nom>Grégory Smits</nom>
            <email>gsmits@info.unicaen.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Christine Chardenon</nom>
            <email>christine.chardenon@orange-ftgroup.com</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">GREYC–Université de Caen, F-14032 CAEN cedex</affiliation>
          <affiliation affiliationId="2">France Télécom R&amp;D TECH/EASY/LN, 2, avenue Pierre Marzin, 22307 Lannion Cedex</affiliation>
        </affiliations>
        <titre>Vers une méthodologie générique de contrôle basée sur la combinaison de sources de jugement</titre>
        <type>poster</type>
        <pages>273-282</pages>
        <resume>Le contrôle des hypothèses concurrentes générées par les différents modules qui peuvent intervenir dans des processus de TALN reste un enjeu important malgré de nombreuses avancées en terme de robustesse. Nous présentons dans cet article une méthodologie générique de contrôle exploitant des techniques issues de l’aide multicritère à la décision. À partir de l’ensemble des critères de comparaison disponibles et la formalisation des préférences d’un expert, l’approche proposée évalue la pertinence relative des différents objets linguistiques générés et conduit à la mise en place d’une action de contrôle appropriée telle que le filtrage, le classement, le tri ou la propagation.</resume>
        <mots_cles>méthodologie de contrôle, aide multicritère à la décision, apprentissage automatique de métriques</mots_cles>
        <title/>
        <abstract>The control of concurrent hypotheses generated by the different modules which compose NLP processes is still an important issue despite advances concerning robustness. In this article, we present a generic methodology of control inspired from multicriteria decision aid methods. Based on available comparison criteria and formalized expert knowledge, the proposed approach evaluate the relevancy of each generated linguistic object and lead to the decision of an appropriate control action such as filtering, ordering, sorting or propagating.</abstract>
        <keywords>control methodology, multicriteria decision aid, metrics automatic learning</keywords>
      </article>
      <article id="taln-2007-poster-028" session="Posters">
        <auteurs>
          <auteur>
            <nom>Agnès Tutin</nom>
            <email>agnes.tutin@u-grenoble3.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIDILEM, Université Grenoble 3, BP 25, 38040 Grenoble Cedex 09</affiliation>
        </affiliations>
        <titre>Traitement sémantique par analyse distributionnelle des noms transdisciplinaires des écrits scientifiques</titre>
        <type>poster</type>
        <pages>283-292</pages>
        <resume>Dans cette étude sur le lexique transdisciplinaire des écrits scientifiques, nous souhaitons évaluer dans quelle mesure les méthodes distributionnelles de TAL peuvent faciliter la tâche du linguiste dans le traitement sémantique de ce lexique. Après avoir défini le champ lexical et les corpus exploités, nous testons plusieurs méthodes basées sur des dépendances syntaxiques et observons les proximités sémantiques et les classes établies. L’hypothèse que certaines relations syntaxiques - en particulier les relations de sous-catégorisation – sont plus appropriées pour établir des classements sémantiques n’apparaît qu’en partie vérifiée. Si les relations de sous-catégorisation génèrent des proximités sémantiques entre les mots de meilleure qualité, cela ne semble pas le cas pour la classification par voisinage.</resume>
        <mots_cles>corpus, écrits scientifiques, classes sémantiques, analyse distributionnelle</mots_cles>
        <title/>
        <abstract>In this study about general scientific lexicon, we aim at evaluating to what extent distributional methods in NLP can enhance the linguist’s task in the semantic treatment. After a definition of our lexical field and a presentation of our corpora, we evaluate several methods based on syntactic dependencies for establishing semantic similarities and semantic classes. Our hypothesis that some syntactic relations – namely subcategorized relations – is more relevant to establish semantic classes does not entirely appears valid. If subcategorized relations produce better semantic links between words, this is not the case with neighbour joigning clustering method.</abstract>
        <keywords>corpus, scientific writings, semantic classes, distributional analysis</keywords>
      </article>
      <article id="taln-2007-poster-029" session="Posters">
        <auteurs>
          <auteur>
            <nom>Jeanne Villaneau</nom>
            <email>Jeanne.Villaneau@univ-ubs.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Valoria Université de Bretagne Sud</affiliation>
        </affiliations>
        <titre>Une expérience de compréhension en contexte de dialogue avec le système LOGUS, approche logique de la compréhension de la langue orale</titre>
        <type>poster</type>
        <pages>293-302</pages>
        <resume>LOGUS est un système de compréhension de la langue orale dans le cadre d’un dialogue homme-machine finalisé. Il est la mise en oeuvre d’une approche logique qui utilise différents formalismes afin d’obtenir un système robuste mais néanmoins relativement extensible. Cet article décrit essentiellement l’étape de compréhension en contexte de dialogue implémentée sur LOGUS, développée et testée à partir d’un corpus de réservation hôtelière enregistré et annoté lors des travaux du groupe MEDIA du projet technolangue. Il décrit également les différentes interrogations et conclusions que peut susciter une telle expérience et les résultats obtenus par le système dans la résolution des références. Concernant l’approche elle-même, cette expérience semble montrer que le formalisme adopté pour la représentation sémantique des énoncés est bien adapté à la compréhension en contexte.</resume>
        <mots_cles>compréhension automatique de la parole, résolution des références, dialogue oral homme-machine</mots_cles>
        <title/>
        <abstract>LOGUS is a spoken language understanding system usable in a man-machine dialogue. It is based on a logical approach where various formalisms are used, in order to achieve a robust but generic and extensible system. Implementation of a context-sensitive understanding is the main topic of this paper. Processing and tests were carried out from a hotel reservation corpus which was recorded and annotated as part of the work handled by the technolangue consortium’s MEDIA subgroup. This paper also describes the various questions raised and conclusions drawn from such an experiment, as well as the results achieved by the system for anaphora resolution. This experiment shows that the formalism used in order to represent the meaning of the utterances is relevant for anaphora resolution and in-context understanding.</abstract>
        <keywords>man-machine dialogue, spoken language understanding, anaphora resolution</keywords>
      </article>
      <article id="taln-2007-poster-030" session="Posters">
        <auteurs>
          <auteur>
            <nom>Anis Zouaghi</nom>
            <email>Anis.Zouaghi@riadi.rnu.tn</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Mounir Zrigui</nom>
            <email>Mounir.Zrigui@fsm.rnu.tn</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Mohamed Ben Ahmed</nom>
            <email>Mohamed.Benahmed@riadi.rnu.tn</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Labo RIADI (Unité de Monastir), Université de Monastir, Faculté des sciences de Monastir</affiliation>
          <affiliation affiliationId="2">Labo RIADI – Université de la Mannouba, École nationale des sciences de l’informatique</affiliation>
        </affiliations>
        <titre>Évaluation des performances d’un modèle de langage stochastique pour la compréhension de la parole arabe spontanée</titre>
        <type>poster</type>
        <pages>303-312</pages>
        <resume>Les modèles de Markov cachés (HMM : Hidden Markov Models) (Baum et al., 1970), sont très utilisés en reconnaissance de la parole et depuis quelques années en compréhension de la parole spontanée latine telle que le français ou l’anglais. Dans cet article, nous proposons d’utiliser et d’évaluer la performance de ce type de modèle pour l’interprétation sémantique de la parole arabe spontanée. Les résultats obtenus sont satisfaisants, nous avons atteint un taux d’erreur de l’ordre de 9,9% en employant un HMM à un seul niveau, avec des probabilités tri_grammes de transitions.</resume>
        <mots_cles>analyse sémantique, modèle de langage stochastique, contexte pertinent, information mutuelle moyenne, parole arabe spontanée</mots_cles>
        <title/>
        <abstract>The HMM (Hidden Markov Models) (Baum et al., 1970), are frequently used in speech recognition and in the comprehension of foreign spontaneous speech such us the french or the english. In this article, we propose using and evaluating the performance of this model type for the semantic interpretation of the spontaneous arabic speech. The obtained results are satisfying; we have achieved an error score equal to 9.9%, by using HMM with trigrams probabilities transitions.</abstract>
        <keywords>semantic analysis, stochastic language model, pertinent context, overage mutual information, spontaneous arabic speech</keywords>
      </article>
      <article id="taln-2007-demo-001" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Éric Brunelle</nom>
            <email>developpement@druide.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Simon Charest</nom>
            <email>developpement@druide.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Druide informatique inc., 1435, rue St-Alexandre, bureau 1040, Montréal (Québec) H3A 2G4, Canada</affiliation>
        </affiliations>
        <titre>Présentation du logiciel Antidote RX</titre>
        <type>démonstration</type>
        <pages>315-317</pages>
        <resume>Antidote RX est la sixième édition d’Antidote, un logiciel d’aide à la rédaction développé et commercialisé par la société Druide informatique. Antidote RX comporte un correcteur grammatical avancé, dix dictionnaires de consultation et dix guides linguistiques. Il fonctionne sous les systèmes d’exploitation Windows, Mac OS X et Linux.</resume>
        <mots_cles/>
        <title/>
        <abstract/>
        <keywords/>
      </article>
      <article id="taln-2007-demo-002" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Dominique Laurent</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Sophie Nègre</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Patrick Séguéla</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Synapse Développement, 33 Rue Maynard, 31000 Toulouse</affiliation>
        </affiliations>
        <titre>Logiciel Cordial</titre>
        <type>démonstration</type>
        <pages>319-321</pages>
        <resume>Cordial est un correcteur efficace et discret enrichi d'un grand nombre de fonctions d'aide à la rédaction et d'analyse de documents. Très riche avec ces multiples dictionnaires et souvent pertinent dans ses propositions, Cordial est un compagnon précieux qui vous permet d'assurer la qualité de vos écrits. La version 2007 de Cordial s'intègre dans un vaste éventail de logiciels comme les traitements de texte (Word, Open Office, Word Perfect...), clients de messagerie (Outlook, Notes, Thunderbird, webmails...) ou navigateurs (Explorer, Mozilla).</resume>
        <mots_cles/>
        <title/>
        <abstract/>
        <keywords/>
      </article>
      <article id="taln-2007-demo-003" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Elliott Macklovitch</nom>
            <email>macklovi@iro.umontreal.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Guy Lapalme</nom>
            <email>lapalme@iro.umontreal.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire RALI – Université de Montréal, Montréal, Canada</affiliation>
        </affiliations>
        <titre>TransCheck : un vérificateur automatique de traductions</titre>
        <type>démonstration</type>
        <pages>323-325</pages>
        <resume>Nous offrirons une démonstration de la dernière version de TransCheck, un vérificateur automatique de traductions que le RALI est en train de développer. TransCheck prend en entrée deux textes, un texte source dans une langue et sa traduction dans une autre, les aligne au niveau de la phrase et ensuite vérifie les régions alignées pour s’assurer de la présence de certains équivalents obligatoires (p. ex. la terminologie normalisée) et de l’absence de certaines interdictions de traduction (p. ex. des interférences de la langue source). Ainsi, TransCheck se veut un nouveau type d’outil d’aide à la traduction qui pourra à réduire le fardeau de la révision et diminuer le coût du contrôle de la qualité.</resume>
        <mots_cles>traduction assistée par ordinateur, vérification automatique de traductions, révision de traduction</mots_cles>
        <title/>
        <abstract>We will present a demonstration of the latest version of TransCheck, an automatic translation checker that the RALI is currently developing. TransCheck takes as input two texts, a source text in one language and its translation in another, aligns them at the sentence level and then verifies the aligned regions to ensure that they contain certain obligatory equivalents (e.g. standardized terminology) and do not contain certain prohibited translations (e.g. source language interference). TransCheck is thus intended to be a new type of tool for assisting translators which has the potential to ease the burden of revision and diminish the costs of quality control.</abstract>
        <keywords>machine-aided translation, automatic translation checking, translation revision</keywords>
      </article>
      <article id="taln-2007-demo-004" session="Démonstrations">
        <auteurs>
          <auteur>
            <nom>Jean-Marie Pierrel</nom>
            <email>Jean-Marie.Pierrel@atilf.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Etienne Petitjean</nom>
            <email>Etienne.Petitjean@atilf.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CNRTL/ATILF CNRS – Nancy Université, 44 avenue de la Libération, BP 30687, 54063 Nancy CEDEX</affiliation>
        </affiliations>
        <titre>Le CNRTL, Centre National de Ressources Textuelles et Lexicales, un outil de mutualisation de ressources linguistiques</titre>
        <type>démonstration</type>
        <pages>327-329</pages>
        <resume>Créé en 2005 à l’initiative du Centre National de la Recherche Scientifique, le CNRTL propose une plate-forme unifiée pour l’accès aux ressources et documents électroniques destinés à l’étude et l’analyse de la langue française. Les services du CNRTL comprennent le recensement, la documentation (métadonnées), la normalisation, l’archivage, l’enrichissement et la diffusion des ressources. La pérennité du service et des données est garantie par le soutien institutionnel du CNRS, l’adossement à un laboratoire de recherche en linguistique et informatique du CNRS et de Nancy Université (ATILF – Analyse et Traitement Informatique de la Langue Française), ainsi que l’intégration dans le réseau européen CLARIN (common language resources and technology infrastructure european).</resume>
        <mots_cles/>
        <title/>
        <abstract/>
        <keywords/>
      </article>
    </articles>
  </conference>
  <conference>
    <edition>
      <acronyme>TALN'2008</acronyme>
      <titre>conférence sur le Traitement Automatique des Langues Naturelles</titre>
      <ville>Avignon</ville>
      <pays>France</pays>
      <dateDebut>2008-06-09</dateDebut>
      <dateFin>2008-06-13</dateFin>
      <presidents>
        <nom>Frédéric Bechet</nom>
        <nom>Jean-Francois Bonastre</nom>
      </presidents>
      <typeArticles>
        <type id="long">Papiers longs</type>
        <type id="court">Papiers courts</type>
      </typeArticles>
      <statistiques>
        <!-- <acceptations id="" soumissions=""></acceptations> -->
      </statistiques>
      <siteWeb>http://www.lia.univ-avignon.fr/jep-taln08/</siteWeb>
      <meilleurArticle>
        <articleId>taln-2008-long-014</articleId>
        <articleId>taln-2008-long-027</articleId>
      </meilleurArticle>
    </edition>
    <articles>
      <article id="taln-2008-long-001" session="Sémantique">
        <auteurs>
          <auteur>
            <nom>Amanda Rocha-Chaves</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Lucia-Helena Machado-Rino</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Universidade Federal de Sao Carlos</affiliation>
        </affiliations>
        <titre/>
        <type>long</type>
        <pages/>
        <resume/>
        <mots_cles/>
        <title>The Mitkov algorithm for anaphora resolution in Portuguese</title>
        <abstract>This paper reports on the use of the Mitkov´s algorithm for pronoun resolution in texts written in Brazilian Portuguese. Third person pronouns are the only ones focused upon here, with noun phrases as antecedents. A system for anaphora resolution in Brazilian Portuguese texts was built that embeds most of the Mitkov’s features. Some of his resolution factors were directly incorporated into the system; others had to be slightly modified for language adequacy. The resulting approach was intrinsically evaluated on hand-annotated corpora. It was also compared to Lappin &amp; Leass’s algorithm for pronoun resolution, also customized to Portuguese. Success rate was the evaluation measure used in both experiments. The results of both evaluations are discussed here.</abstract>
        <keywords>Pronoun resolution, anaphora resolution</keywords>
      </article>
      <article id="taln-2008-long-002" session="Sémantique">
        <auteurs>
          <auteur>
            <nom>Paul Bédaride</nom>
            <email>Paul.Bedaride@loria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Claire Gardent</nom>
            <email>Claire.Gardent@loria.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université Henri Poincaré/LORIA, Nancy</affiliation>
          <affiliation affiliationId="2">CNRS/LORIA, Nancy</affiliation>
        </affiliations>
        <titre>Réécriture et Détection d’Implication Textuelle</titre>
        <type>long</type>
        <pages/>
        <resume>Nous présentons un système de normalisation de la variation syntaxique qui permet de mieux reconnaître la relation d’implication textuelle entre deux phrases. Le système est évalué sur une suite de tests comportant 2 520 paires test et les résultats montrent un gain en précision par rapport à un système de base variant entre 29.8 et 78.5 points la complexité des cas considérés.</resume>
        <mots_cles>Normalisation syntaxique, Détection d’implication textuelle, Réécriture de graphe</mots_cles>
        <title/>
        <abstract>We present a system for dealing with syntactic variation which significantly improves the treatment of textual implication. The system is evaluated on a testsuite of 2 520 sentence pairs and the results show an improvment in precision over the baseline which varies between 29.8 and 78.5 points depending on the complexity of the cases being considered.</abstract>
        <keywords>Syntactic normalisation, Recognising Textual Entailment, Graph rewriting</keywords>
      </article>
      <article id="taln-2008-long-003" session="Sémantique">
        <auteurs>
          <auteur>
            <nom>Delphine Battistelli</nom>
            <email>delphine.battistelli@paris-sorbonne.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Javier Couto</nom>
            <email>jcouto@fing.edu.uy</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Jean-Luc Minel</nom>
            <email>jminel@u-paris10.fr</email>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Sylviane Schwer</nom>
            <email>schwer@lipn.univ-paris13.fr</email>
            <affiliationId>4</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Lalic – Université Paris-Sorbonne – 28 rue Serpente 75006 Paris</affiliation>
          <affiliation affiliationId="2">INCO – Universidad de la Republica – 565 Herrera y Reissig 11300 Montevideo – Uruguay</affiliation>
          <affiliation affiliationId="3">MoDyCo – Université Paris X, CNRS UMR 7114 – 200 avenue de la République, 92001 Nanterre Cedex</affiliation>
          <affiliation affiliationId="4">LIPN – Université Paris XIII, CNRS UMR 7030 – 99 Bd Jean-Baptiste Clément, 93240 Villetaneuse</affiliation>
        </affiliations>
        <titre>Représentation algébrique des expressions calendaires et vue calendaire d’un texte</titre>
        <type>long</type>
        <pages/>
        <resume>Cet article aborde l’étude des expressions temporelles qui font référence directement à des unités de temps relatives aux divisions courantes des calendriers, que nous qualifions d’expressions calendaires (EC). Nous proposons une modélisation de ces expressions en définissant une algèbre d’opérateurs qui sont liés aux classes de marqueurs linguistiques qui apparaissent dans les EC. A partir de notre modélisation, une vue calendaire est construite dans la plate-forme de visualisation et navigation textuelle NaviTexte, visant le support à la lecture de textes. Enfin, nous concluons sur les perspectives offertes par le développement d’une première application de navigation temporelle.</resume>
        <mots_cles>expressions temporelles calendaires, modélisation algébrique, visualisation</mots_cles>
        <title/>
        <abstract>In this paper we address the study of temporal expressions that refer directly to text units concerning common calendar divisions, that we name “calendar expressions” (EC). We propose to model these expressions by defining an operator algebra, the operators being related to different linguistics marker classes that occur in the EC. Based on our model, a calendar view is built up into the text visualization and navigation framework NaviTexte, aiming the support of text reading. Finally, we discuss the perspectives offered by the development of a first temporal navigation application.</abstract>
        <keywords>temporal calendar expressions, algebraic modelization, vizualisation</keywords>
      </article>
      <article id="taln-2008-long-004" session="Sémantique">
        <auteurs>
          <auteur>
            <nom>Gabriel Parent</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Michel Gagnon</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Philippe Muller</nom>
            <email/>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">École Polytechnique de Montréal</affiliation>
          <affiliation affiliationId="2">Université Paul-Sabatier, Toulouse</affiliation>
        </affiliations>
        <titre>Annotation d’expressions temporelles et d’événements en français</titre>
        <type>long</type>
        <pages/>
        <resume>Dans cet article, nous proposons une méthode pour identifier, dans un texte en français, l’ensemble des expressions adverbiales de localisation temporelle, ainsi que tous les verbes, noms et adjectifs dénotant une éventualité (événement ou état). Cette méthode, en plus d’identifier ces expressions, extrait certaines informations sémantiques : la valeur de la localisation temporelle selon la norme TimeML et le type des éventualités. Pour les expressions adverbiales de localisation temporelle, nous utilisons une cascade d’automates, alors que pour l’identification des événements et états nous avons recours à une analyse complète de la phrase. Nos résultats sont proches de travaux comparables sur l’anglais, en l’absence d’évaluation quantitative similaire sur le français.</resume>
        <mots_cles>Extraction d’informations temporelle, TimeML</mots_cles>
        <title/>
        <abstract>We propose a method to extract expressions of temporal location in French texts, and all verbs, nouns and adjectived that denote an event or a state. This method also computes some semantic information : the value of the temporal location according to the TimeML standard and the types of eventualities. For temporal location expression, we use a cascade of transducers, wheras event identification is based on full syntactic parsing. Our result are compared to similar work on English, as no other empirical evaluation has been done on French before.</abstract>
        <keywords>Temporal information extraction, TimeML</keywords>
      </article>
      <article id="taln-2008-long-005" session="Extraction d'information">
        <auteurs>
          <auteur>
            <nom>Stéphane Huet</nom>
            <email>stephane.huet@irisa.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Guillaume Gravier</nom>
            <email>guillaume.gravier@irisa.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Pascale Sébillot</nom>
            <email>pascale.sebillot@irisa.fr</email>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université de Rennes 1, Institut de Recherche en Informatique et Systèmes Aléatoires, Rennes</affiliation>
          <affiliation affiliationId="2">CNRS, Institut de Recherche en Informatique et Systèmes Aléatoires, Rennes</affiliation>
          <affiliation affiliationId="3">INSA de Rennes, Institut de Recherche en Informatique et Systèmes Aléatoires, Rennes</affiliation>
        </affiliations>
        <titre>Un modèle multi-sources pour la segmentation en sujets de journaux radiophoniques</titre>
        <type>long</type>
        <pages/>
        <resume>Nous présentons une méthode de segmentation de journaux radiophoniques en sujets, basée sur la prise en compte d’indices lexicaux, syntaxiques et acoustiques. Partant d’un modèle statistique existant de segmentation thématique, exploitant la notion de cohésion lexicale, nous étendons le formalisme pour y inclure des informations d’ordre syntaxique et acoustique. Les résultats expérimentaux montrent que le seul modèle de cohésion lexicale ne suffit pas pour le type de documents étudié en raison de la taille variable des segments et de l’absence d’un lien direct entre segment et thème. L’utilisation d’informations syntaxiques et acoustiques permet une amélioration substantielle de la segmentation obtenue.</resume>
        <mots_cles>segmentation en sujets, corpus oraux, cohésion lexicale, indices acoustiques, indices syntaxiques</mots_cles>
        <title/>
        <abstract>We present a method for story segmentation of radio broadcast news, based on lexical, syntactic and audio cues. Starting from an existing statistical topic segmentation model which exploits the notion of lexical cohesion, we extend the formalism to include syntactic and acoustic knwoledge sources. Experimental results show that the sole use of lexical cohesion is not efficient for the type of documents under study because of the variable size of the segments and the lack of direct relation between topics and stories. The use of syntactics and acoustics enables a consequent improvement of the quality of the segmentation.</abstract>
        <keywords>story segmentation, spoken documents, lexical cohesion, acoustic cues, syntactic cues</keywords>
      </article>
      <article id="taln-2008-long-006" session="Extraction d'information">
        <auteurs>
          <auteur>
            <nom>Cédric Vidrequin</nom>
            <email>cedric.vidrequin@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Juan-Manuel Torres-Moreno</nom>
            <email>juan-manuel.torres@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Jean-Jacques Schneider</nom>
            <email>jjschneider@semantia.com</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Marc El-Bèze</nom>
            <email>marc.elbeze@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire Informatique d'Avignon, Agroparc BP1228, 84911 Avignon CEDEX 9, France</affiliation>
          <affiliation affiliationId="2">Société SEMANTIA, Parc d'activité de Gémenos,30 avenue du château de Jouques, 13420 Gémenos, France</affiliation>
        </affiliations>
        <titre>Extraction automatique d'informations à partir de micro-textes non structurés</titre>
        <type>long</type>
        <pages/>
        <resume>Nous présentons dans cet article une méthode d'extraction automatique d'informations sur des textes de très petite taille, faiblement structurés. Nous travaillons sur des textes dont la rédaction n'est pas normalisée, avec très peu de mots pour caractériser chaque information. Les textes ne contiennent pas ou très peu de phrases. Il s'agit le plus souvent de morceaux de phrases ou d'expressions composées de quelques mots. Nous comparons plusieurs méthodes d'extraction, dont certaines sont entièrement automatiques. D'autres utilisent en partie une connaissance du domaine que nous voulons réduite au minimum, de façon à minimiser le travail manuel en amont. Enfin, nous présentons nos résultats qui dépassent ce dont il est fait état dans la littérature, avec une précision équivalente et un rappel supérieur.</resume>
        <mots_cles>extraction automatique, micro-texte, texte non structuré, petites annonces</mots_cles>
        <title/>
        <abstract>In this article, we present a method of automatic extraction of informations on very small-sized and weakly structured texts. We work on texts whose drafting is not normalised, with very few words to characterize each information. Texts does not contain sentences, or only few. There are mostly about fragments of sentences or about expressions of some words. We compare several extracting methods, some completely automatic and others using an small domain knowledge. We want this knowledge to be minimalistic to reduce as much as possible any manual work. Then, we present our results, witch are better than those published in the literature, with an equivalent precision and a greater recall.</abstract>
        <keywords>automatique extraction, micro-text, unstructured text, adds</keywords>
      </article>
      <article id="taln-2008-long-007" session="Extraction d'information">
        <auteurs>
          <auteur>
            <nom>Laurent Gillard</nom>
            <email>Laurent.Gillard@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Patrice Bellot</nom>
            <email>Patrice.Bellot@univ-avignon.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Marc El-Bèze</nom>
            <email>marc.elbeze@univ-avignon.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CEA, LIST, Laboratoire d’ingénierie de la connaissance multi-média multilingue, 18 route du Panorama, BP6, FONTENAY AUX ROSES, F- 92265 France</affiliation>
          <affiliation affiliationId="2">Laboratoire d'Informatique d'Avignon (LIA), Université d’Avignon et des Pays de Vaucluse, F-84911 Avignon Cedex 9 (France)</affiliation>
        </affiliations>
        <titre>Quelles combinaisons de scores et de critères numériques pour un système de Questions/Réponses ?</titre>
        <type>long</type>
        <pages/>
        <resume>Dans cet article, nous présentons une discussion sur la combinaison de différents scores et critères numériques pour la sélection finale d’une réponse dans la partie en charge des questions factuelles du système de Questions/Réponses développé au LIA. Ces scores et critères numériques sont dérivés de ceux obtenus en sortie de deux composants cruciaux pour notre système : celui de sélection des passages susceptibles de contenir une réponse et celui d’extraction et de sélection d’une réponse. Ils sont étudiés au regard de leur expressivité. Des comparaisons sont faites avec des approches de sélection de passages mettant en oeuvre des scores conventionnels en recherche d’information. Parallèlement, l’influence de la taille des contextes (en nombre de phrases) est évaluée. Cela permet de mettre en évidence que le choix de passages constitués de trois phrases autour d’une réponse candidate, avec une sélection des réponses basée sur une combinaison entre un score de passage de type Lucene ou Cosine et d’un score de compacité apparaît comme un compromis intéressant.</resume>
        <mots_cles>Système de Questions/Réponses, compacité, densité, combinaison de scores</mots_cles>
        <title/>
        <abstract>This article discusses combinations of scores for selecting the best answer in a factual question answering system. Two major components of our QA system: (i) relevant passage selection, and (ii) answer extraction, produce a variety of scores. Here we study the expressivity of these scores, comparing our passage density score (i) to more conventional ranking techniques in information retrieval. In addition, we study varying the length (in number of sentences) of context retained in the relevant passages. We find that a three sentences window, and a mixing of Lucene or Cosine ranking with our compactness score (ii) provides the best results.</abstract>
        <keywords>Question Answering, compactness, density, combination of scores</keywords>
      </article>
      <article id="taln-2008-long-008" session="Dialogue Homme-Machine">
        <auteurs>
          <auteur>
            <nom>Vladimir Popescu</nom>
            <email>vladimir.popescu@imag.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Jean Caelen</nom>
            <email>jean.caelen@imag.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">GETALP/LIG, Institut National Polytechnique de Grenoble, France</affiliation>
          <affiliation affiliationId="2">Université « Politehnica » de Bucarest, Roumanie</affiliation>
        </affiliations>
        <titre>Contrôle rhétorique de la génération des connecteurs concessifs en dialogue homme-machine</titre>
        <type>long</type>
        <pages/>
        <resume>Les connecteurs discursifs ont on rôle important dans l’interprétation des discours (dialogiques ou pas), donc lorsqu’il s’agit de produire des énoncés, le choix des mots qui relient les énoncés (par exemple, en dialogue oral) s’avère essentiel pour assurer la compréhension des visées illocutoires des locuteurs. En linguistique computationnelle, le problème a été abordé surtout au niveau de l’interprétation des discours monologiques, tandis que pour le dialogue, les recherches se sont limitées en général à établir une correspondance quasiment biunivoque entre relations rhétoriques et connecteurs. Dans ce papier nous proposons un mécanisme pour guider la génération des connecteurs concessifs en dialogue, à la fois du point de vue discursif et sémantique ; chaque connecteur considéré sera contraint par un ensemble de conditions qui prennent en compte la cohérence du discours et la pertinence sémantique de chaque mot concerné. Les contraintes discursives, exprimées dans un formalisme dérivé de la SDRT (« Segmented Discourse Representation Theory ») seront plongées dans des contraintes sémantiques sur les connecteurs, proposées par l’école genevoise (Moeschler), pour enfin évaluer la cohérence du discours résultant de l’emploi de ces connecteurs.</resume>
        <mots_cles>Dialogue homme-machine, cohérence discursive, connecteurs concessifs, sémantique, pragmatique</mots_cles>
        <title/>
        <abstract>Cue words play an important part in discourse interpretation (whether dialogues are concerned or not), hence when utterances have to be produced, the choice of the words that connect these utterances (for example, in spoken dialogue) is essential for ensuring the comprehension of the illocutionary goals of the speakers. In computational linguistics, the issue has been mitigated particularly in the context of monologue discourse interpretation, whereas for dialogues, research is usually limited to establishing an almost one-to-one mapping between rhetorical relations and cue words. In this paper we propose a mechanism for guiding concessive connectors in dialogue, at the same time from a discourse and from a semantic point of view; each considered connector will be constrained via a set of conditions that take into account discourse coherence and the semantic relevance of each word concerned. Discourse constraints, expressed in a formalism derived from SDRT (“Segmented Discourse Representation Theory”) will be mapped to semantic constraints on the connectors ; these semantic constraints are proposed by the Geneve linguistics school (Moeschler). Finally, the coherence of the discourse resulted from the use of these connectors will be assessed.</abstract>
        <keywords>Human-computer dialogue, discourse coherence, concessive connectors, semantics, pragmatics</keywords>
      </article>
      <article id="taln-2008-long-009" session="Dialogue Homme-Machine">
        <auteurs>
          <auteur>
            <nom>Alexandre Denis</nom>
            <email>alexandre.denis@loria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Matthieu Quignard</nom>
            <email>matthieu.quignard@loria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">UMR 7503 LORIA/CNRS – Campus scientifique, 56 506 Vandoeuvre-lès-Nancy Cedex</affiliation>
        </affiliations>
        <titre>Modélisation du principe d’ancrage pour la robustesse des systèmes de dialogue homme-machine finalisés</titre>
        <type>long</type>
        <pages/>
        <resume>Cet article présente une modélisation du principe d’ancrage (grounding) pour la robustesse des systèmes de dialogue finalisés. Ce principe, décrit dans (Clark &amp; Schaefer, 1989), suggère que les participants à un dialogue fournissent des preuves de compréhension afin d’atteindre la compréhension mutuelle. Nous explicitons une définition computationnelle du principe d’ancrage fondée sur des jugements de compréhension qui, contrairement à d’autres modèles, conserve une motivation pour l’expression de la compréhension. Nous déroulons enfin le processus d’ancrage sur un exemple tiré de l’implémentation du modèle.</resume>
        <mots_cles>dialogue homme-machine, robustesse, ancrage, compréhension mutuelle</mots_cles>
        <title/>
        <abstract>This paper presents a grounding model for robustness in dialogue systems. The grounding process (Clark &amp; Schaefer, 1989) suggests that dialogue participants provide evidence of understanding in order to reach mutual understanding. We propose here a computational definition of the grounding criterion based on understanding judgments that keeps a motivation for providing evidence of understanding, as opposed to some existing models. Eventually, we detail the grounding process on a dialogue generated by the actual implementation.</abstract>
        <keywords>human-machine dialogue, robustness, grounding, mutual understanding</keywords>
      </article>
      <article id="taln-2008-long-010" session="Résumé Automatique">
        <auteurs>
          <auteur>
            <nom>Silvia Fernández</nom>
            <email>silvia.fernandez@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Eric Sanjuan</nom>
            <email>eric.sanjuan@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Juan-Manuel Torres-Moreno</nom>
            <email>juan-manuel.torres@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire Informatique d’Avignon, BP 1228 84911 Avignon France</affiliation>
          <affiliation affiliationId="2">LPM UHP-Nancy, BP 239 54506 Vandoeuvre lès Nancy France</affiliation>
          <affiliation affiliationId="3">École Polytechnique de Montréal, CP 6079 Montréal, Canada H3C3A7</affiliation>
        </affiliations>
        <titre>Enertex : un système basé sur l’énergie textuelle</titre>
        <type>long</type>
        <pages/>
        <resume>Dans cet article, nous présentons des applications du système Enertex au Traitement Automatique de la Langue Naturelle. Enertex est basé sur l’énergie textuelle, une approche par réseaux de neurones inspirée de la physique statistique des systèmes magnétiques. Nous avons appliqué cette approche aux problèmes du résumé automatique multi-documents et de la détection de frontières thématiques. Les résultats, en trois langues : anglais, espagnol et français, sont très encourageants.</resume>
        <mots_cles>Énergie textuelle, Réseaux de neurones, Modèle de Hopfield, Résumé automatique, Frontières thématiques</mots_cles>
        <title/>
        <abstract>In this paper we present Enertex applications to study fundamental problems in Natural Language Processing. Enertex is based on textual energy, a neural networks approach, inspired by statistical physics of magnetic systems.We obtained good results on the application of this method to automatic multi-document summarization and thematic border detection in three languages : English, Spanish and French.</abstract>
        <keywords>Textual Energy, Neural Networks, Hopfield Model, Automatic Summarization, Thematic Boundaries</keywords>
      </article>
      <article id="taln-2008-long-011" session="Résumé Automatique">
        <auteurs>
          <auteur>
            <nom>Fatma Kallel Jaoua</nom>
            <email>fatma_fseg@yahoo.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Lamia Hadrich Belguith</nom>
            <email>l.belguith@fsegs.rnu.tn</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Maher Jaoua</nom>
            <email>maher.jaoua@fsegs.rnu.tn</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Abdelmajid Ben Hamadou</nom>
            <email>abdelmajid.benhamadou@fsegs.rnu.tn</email>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire LARIS-MIRACL, ISG Université de Gabès, Tunisie</affiliation>
          <affiliation affiliationId="2">Laboratoire LARIS-MIRACL, FSEGS, Université de Sfax, Tunisie</affiliation>
          <affiliation affiliationId="3">Laboratoire LARIM-MIRACL – ISIMS, Université de Sfax, Tunisie</affiliation>
        </affiliations>
        <titre>Intégration d’une étape de pré-filtrage et d’une fonction multiobjectif en vue d’améliorer le système ExtraNews de résumé de documents multiples</titre>
        <type>long</type>
        <pages/>
        <resume>Dans cet article, nous présentons les améliorations que nous avons apportées au système ExtraNews de résumé automatique de documents multiples. Ce système se base sur l’utilisation d’un algorithme génétique qui permet de combiner les phrases des documents sources pour former les extraits, qui seront croisés et mutés pour générer de nouveaux extraits. La multiplicité des critères de sélection d’extraits nous a inspiré une première amélioration qui consiste à utiliser une technique d’optimisation multi-objectif en vue d’évaluer ces extraits. La deuxième amélioration consiste à intégrer une étape de pré-filtrage de phrases qui a pour objectif la réduction du nombre des phrases des textes sources en entrée. Une évaluation des améliorations apportées à notre système est réalisée sur les corpus de DUC’04 et DUC’07.</resume>
        <mots_cles>Résumé automatique, pré-filtrage de phrases, optimisation multi-objectif, algorithme génétique</mots_cles>
        <title/>
        <abstract>In this paper, we present the improvements that we brought to the ExtraNews system dedicated for automatic summarisation of multiple documents. This system is based on the use of a genetic algorithm that combines sentences of the source documents to form the extracts. These extracts are crossed and transferred to generate new ones. The multiplicity of the extract selection criteria inspired us the first improvement that consists in the use of a multi-objectif optimization technique to evaluate these extracts. The second improvement consists of the integration of a sentence pre-filtering step which is based on the notion of dominance between sentences. Our objective is to reduce the sentence number of the source texts. An evaluation of the proposed improvements to our system is realized on DUC' 04 and DUC' 07 corpus.</abstract>
        <keywords>Automatic summarization, sentences pre-filtering, multi-objective optimization, genetic algorithm</keywords>
      </article>
      <article id="taln-2008-long-012" session="Session commune JEP/TALN">
        <auteurs>
          <auteur>
            <nom>Philippe Langlais</nom>
            <email>felipe@iro.umontreal.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Alexandre Patry</nom>
            <email>patryale@iro.umontreal.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Fabrizio Gotti</nom>
            <email>gottif@iro.umontreal.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Département d’Informatique et de Recherche Opérationnelle, Université de Montréal, C.P. 6128, succursale Centre-Ville, H3C 3J7, Montréal, Québec, Canada</affiliation>
        </affiliations>
        <titre>Recherche locale pour la traduction statistique à base de segments</titre>
        <type>long</type>
        <pages/>
        <resume>Dans cette étude, nous nous intéressons à des algorithmes de recherche locale pour la traduction statistique à base de segments (phrase-based machine translation). Les algorithmes que nous étudions s’appuient sur une formulation complète d’un état dans l’espace de recherche contrairement aux décodeurs couramment utilisés qui explorent l’espace des préfixes des traductions possibles. Nous montrons que la recherche locale seule, permet de produire des traductions proches en qualité de celles fournies par les décodeurs usuels, en un temps nettement inférieur et à un coût mémoire constant. Nous montrons également sur plusieurs directions de traduction qu’elle permet d’améliorer de manière significative les traductions produites par le système à l’état de l’art Pharaoh (Koehn, 2004).</resume>
        <mots_cles>Traduction statistique, recherche locale, post-traitement</mots_cles>
        <title/>
        <abstract>Most phrase-based statistical machine translation decoders rely on a dynamicprogramming technique for maximizing a combination of models, including one or several language models and translation tables. One implication of this choice is the design of a scoring function that can be computed incrementally on partial translations, a restriction a search engine using a complete-state formulation does not have. In this paper, we present experiments we conducted with a simple, yet effective greedy search engine.We report significant improvements in translation quality over a state-of-the-art beam-search decoder, for some configurations.</abstract>
        <keywords>Statistical Machine Translation, local search, post-processing</keywords>
      </article>
      <article id="taln-2008-long-013" session="Session commune JEP/TALN">
        <auteurs>
          <auteur>
            <nom>Catherine Kobus</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>François Yvon</nom>
            <email/>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Géraldine Damnati</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Orange Labs / 2, avenue Pierre Marzin, 22300 Lannion</affiliation>
          <affiliation affiliationId="2">Univ. Paris Sud 11 &amp; LIMSI-CNRS, BP 133, 91403 Orsay Cedex</affiliation>
        </affiliations>
        <titre>Transcrire les SMS comme on reconnaît la parole</titre>
        <type>long</type>
        <pages/>
        <resume>Cet article présente une architecture inspirée des systèmes de reconnaissance vocale pour effectuer une normalisation orthographique de messages en « langage SMS ». Nous décrivons notre système de base, ainsi que diverses évolutions de ce système, qui permettent d’améliorer sensiblement la qualité des normalisations produites.</resume>
        <mots_cles>SMS, décodage phonétique, modèles de langage, transducteurs finis</mots_cles>
        <title/>
        <abstract>This paper presents a system aiming at normalizing the orthography of SMS messages, using techniques that are commonly used in automatic speech recognition devices. We describe a baseline system and various evolutions, which are shown to improve significantly the quality of the output normalizations.</abstract>
        <keywords>SMS, phonetic decoding, language models, finite-state transducers</keywords>
      </article>
      <article id="taln-2008-long-014" session="Syntaxe">
        <auteurs>
          <auteur>
            <nom>Laura Kallmeyer</nom>
            <email>lk@sfs.uni-tuebingen.de</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Yannick Parmentier</nom>
            <email>parmenti@sfs.uni-tuebingen.de</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">SFB 441 / Université de Tübingen, Nauklerstr. 35, 72074 Tübingen, Allemagne</affiliation>
          <affiliation affiliationId="2">SfS-CL / SFB 441 / Université de Tübingen, Nauklerstr. 35, 72074 Tübingen, Allemagne</affiliation>
        </affiliations>
        <titre>Convertir des grammaires d’arbres adjoints à composantes multiples avec tuples d’arbres (TT-MCTAG) en grammaires à concaténation d’intervalles (RCG)</titre>
        <type>long</type>
        <pages/>
        <resume>Cet article étudie la relation entre les grammaires d’arbres adjoints à composantes multiples avec tuples d’arbres (TT-MCTAG), un formalisme utilisé en linguistique informatique, et les grammaires à concaténation d’intervalles (RCG). Les RCGs sont connues pour décrire exactement la classe PTIME, il a en outre été démontré que les RCGs « simples » sont même équivalentes aux systèmes de réécriture hors-contextes linéaires (LCFRS), en d’autres termes, elles sont légèrement sensibles au contexte. TT-MCTAG a été proposé pour modéliser les langages à ordre des mots libre. En général ces langages sont NP-complets. Dans cet article, nous définissons une contrainte additionnelle sur les dérivations autorisées par le formalisme TT-MCTAG. Nous montrons ensuite comment cette forme restreinte de TT-MCTAG peut être convertie en une RCG simple équivalente. Le résultat est intéressant pour des raisons théoriques (puisqu’il montre que la forme restreinte de TT-MCTAG est légèrement sensible au contexte), mais également pour des raisons pratiques (la transformation proposée ici a été utilisée pour implanter un analyseur pour TT-MCTAG).</resume>
        <mots_cles>Grammaires d’arbres adjoints à composantesmultiples, grammaires à concaténation d’intervalles, légère sensibilité au contexte</mots_cles>
        <title/>
        <abstract>This paper investigates the relation between TT-MCTAG, a formalism used in computational linguistics, and RCG. RCGs are known to describe exactly the class PTIME ; « simple » RCG even have been shown to be equivalent to linear context-free rewriting systems, i.e., to be mildly context-sensitive. TT-MCTAG has been proposed to model free word order languages. In general, it is NP-complete. In this paper, we will put an additional limitation on the derivations licensed in TT-MCTAG. We show that TT-MCTAG with this additional limitation can be transformed into equivalent simple RCGs. This result is interesting for theoretical reasons (since it shows that TT-MCTAG in this limited form is mildly context-sensitive) and also for practical reasons (the proposed transformation has been used for implementing a parser for TT-MCTAG).</abstract>
        <keywords>Multicomponent Tree Adjoining Grammars, Range Concatenation Grammars, mild context-sensitivity</keywords>
      </article>
      <article id="taln-2008-long-015" session="Syntaxe">
        <auteurs>
          <auteur>
            <nom>Piet Mertens</nom>
            <email>piet.mertens@arts.kuleuven.be</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Département de Linguistique, Université de Leuven, Belgique</affiliation>
        </affiliations>
        <titre>Factorisation des contraintes syntaxiques dans un analyseur de dépendance</titre>
        <type>long</type>
        <pages/>
        <resume>Cet article décrit un analyseur syntaxique pour grammaires de dépendance lexicalisées. Le formalisme syntaxique se caractérise par une factorisation des contraintes syntaxiques qui se manifeste dans la séparation entre dépendance et ordre linéaire, la spécification fonctionnelle (plutôt que syntagmatique) des dépendants, la distinction entre dépendants valenciels (la sous-catégorisation) et non valenciels (les circonstants) et la saturation progressive des arbres. Ceci résulte en une formulation concise de la grammaire à un niveau très abstrait et l’élimination de la reduplication redondante des informations due aux réalisations alternatives des dépendants ou à leur ordre. Les arbres élémentaires (obtenus à partir des formes dans l’entrée) et dérivés sont combinés entre eux par adjonction d’un arbre dépendant saturé à un arbre régissant, moyennant l’unification des noeuds et des relations. La dérivation est réalisée grâce à un analyseur chart bi-directionnel.</resume>
        <mots_cles>Analyseur syntaxique, dépendance</mots_cles>
        <title/>
        <abstract>We describe a parser for lexicalized dependency grammar. The formalism is characterized by a factorization of the syntactic constraints, based on the separation between dependency and word order, the functional (rather than phrasal) specification of dependents, the distinction between valency and non valency dependents, and the incremental saturation of the trees. These features enable a concise formulation of the grammar at a very abstract level and eliminate syntactic information redundancy due to alternative forms of dependents and word order. Each word form produces one or more elementary dependency trees. Trees, both elementary and derived, are combined by adjoining a saturated dependent to a governing tree, after unification of shared nodes and relations. This is achieved using a bi-directional chart parser.</abstract>
        <keywords>Syntactic parser, dependency</keywords>
      </article>
      <article id="taln-2008-long-016" session="Syntaxe">
        <auteurs>
          <auteur>
            <nom>Pascal Vaillant</nom>
            <email>pascal.vaillant@guyane.univ-ag.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">GRIMAAG, Université des Antilles-Guyane, B.P. 792, 97337 Cayenne cedex</affiliation>
        </affiliations>
        <titre>Grammaires factorisées pour des dialectes apparentés</titre>
        <type>long</type>
        <pages/>
        <resume>Pour la formalisation du lexique et de la grammaire de dialectes étroitement apparentés, il peut se révéler utile de factoriser une partie du travail de modélisation. Les soussystèmes linguistiques isomorphes dans les différents dialectes peuvent alors faire l’objet d’une description commune, les différences étant spécifiées par ailleurs. Cette démarche aboutit à un modèle de grammaire à couches : le noyau est commun à la famille de dialectes, et une couche superficielle détermine les caractéristiques de chacun. Nous appliquons ce procédé à la famille des langues créoles à base lexicale française de l’aire américano-caraïbe.</resume>
        <mots_cles>TAG, modélisation, grammaire, variation dialectale</mots_cles>
        <title/>
        <abstract>The task of writing formal lexicons and grammars for closely related dialects can benefit from factoring part of the modelling. Isomorphic linguistic subsystems from the different dialectsmay have a common description, while the differences are specified aside. This process leads to a layered grammar model: a kernel common to the whole family of dialects, and a superficial skin specifying the particular properties of each one of them. We apply this principle to the family of French-lexifier creole languages of the American-Caribbean area.</abstract>
        <keywords>TAG, grammar, modelling, dialectal variation</keywords>
      </article>
      <article id="taln-2008-long-017" session="Syntaxe">
        <auteurs>
          <auteur>
            <nom>Benoît Crabbé</nom>
            <email>bcrabbe@linguist.jussieu.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Marie Candito</nom>
            <email>candito@linguist.jussieu.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université Paris 7 (UFRL) et INRIA (Alpage), 30 rue du Château des Rentiers 75013 Paris</affiliation>
        </affiliations>
        <titre>Expériences d’analyse syntaxique statistique du français</titre>
        <type>long</type>
        <pages/>
        <resume>Nous montrons qu’il est possible d’obtenir une analyse syntaxique statistique satisfaisante pour le français sur du corpus journalistique, à partir des données issues du French Treebank du laboratoire LLF, à l’aide d’un algorithme d’analyse non lexicalisé.</resume>
        <mots_cles>Analyseur syntaxique statistique, Analyse syntaxique non lexicalisée, Analyse du français</mots_cles>
        <title/>
        <abstract>We show that we can acquire satisfactory parsing results for French from data induced from the French Treebank using an unlexicalised parsing algorithm.</abstract>
        <keywords>Statistical parser, Unlexicalised parsing, French parsing</keywords>
      </article>
      <article id="taln-2008-long-018" session="Ressources">
        <auteurs>
          <auteur>
            <nom>Benoît Sagot</nom>
            <email>benoit.sagot@inria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Darja Fišer</nom>
            <email>darja.fiser@guest.arnes.si</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Alpage, INRIA / Paris 7, 30 rue du Ch. des rentiers, 75013 Paris, France</affiliation>
          <affiliation affiliationId="2">Fac. des Lettres, Univ. de Ljubljana, Aškerˇceva 2, 1000 Ljubljana, Slovénie</affiliation>
        </affiliations>
        <titre>Construction d’un wordnet libre du français à partir de ressources multilingues</titre>
        <type>long</type>
        <pages/>
        <resume>Cet article décrit la construction d’un Wordnet Libre du Français (WOLF) à partir du Princeton WordNet et de diverses ressources multilingues. Les lexèmes polysémiques ont été traités au moyen d’une approche reposant sur l’alignement en mots d’un corpus parallèle en cinq langues. Le lexique multilingue extrait a été désambiguïsé sémantiquement à l’aide des wordnets des langues concernées. Par ailleurs, une approche bilingue a été suffisante pour construire de nouvelles entrées à partir des lexèmes monosémiques. Nous avons pour cela extrait des lexiques bilingues à partir deWikipédia et de thésaurus. Le wordnet obtenu a été évalué par rapport au wordnet français issu du projet EuroWordNet. Les résultats sont encourageants, et des applications sont d’ores et déjà envisagées.</resume>
        <mots_cles>Wordnet, corpus alignés, Wikipédia, sémantique lexicale</mots_cles>
        <title/>
        <abstract>This paper describes the construction of a freely-available wordnet for French (WOLF) based on Princeton WordNet by using various multilingual resources. Polysemous words were dealt with an approach in which a parallel corpus for five languages was wordaligned and the extracted multilingual lexicon was disambiguated with the existing wordnets for these languages. On the other hand, a bilingual approach sufficed to acquire equivalents for monosemous words. Bilingual lexicons were extracted from Wikipedia and thesauri. The merged wordnet was evaluated against the French WordNet. The results are promising, and applications are already intended.</abstract>
        <keywords>Wordnet, aligned corpora, Wikipedia, lexical semantics</keywords>
      </article>
      <article id="taln-2008-long-019" session="Ressources">
        <auteurs>
          <auteur>
            <nom>Mathieu Lafourcade</nom>
            <email>lafourcade@lirmm.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Alain Joubert</nom>
            <email>joubert@lirmm.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIRMM – Univ. Montpellier 2 - CNRS, Laboratoire d’Informatique, de Robotique et de Microélectronique de Montpellier, 161, rue Ada – 34392 Montpellier Cédex 5 – France</affiliation>
        </affiliations>
        <titre>Détermination des sens d’usage dans un réseau lexical construit à l’aide d’un jeu en ligne</titre>
        <type>long</type>
        <pages/>
        <resume>Les informations lexicales, indispensables pour les tâches réalisées en TALN, sont difficiles à collecter. En effet, effectuée manuellement, cette tâche nécessite la compétence d’experts et la durée nécessaire peut être prohibitive, alors que réalisée automatiquement, les résultats peuvent être biaisés par les corpus de textes retenus. L’approche présentée ici consiste à faire participer un grand nombre de personnes à un projet contributif en leur proposant une application ludique accessible sur le web. A partir d’une base de termes préexistante, ce sont ainsi les joueurs qui vont construire le réseau lexical, en fournissant des associations qui ne sont validées que si elles sont proposées par au moins une paire d’utilisateurs. De plus, ces relations typées sont pondérées en fonction du nombre de paires d’utilisateurs qui les ont proposées. Enfin, nous abordons la question de la détermination des différents sens d’usage d’un terme, en analysant les relations entre ce terme et ses voisins immédiats dans le réseau lexical, avant de présenter brièvement la réalisation et les premiers résultats obtenus.</resume>
        <mots_cles>Traitement Automatique du Langage Naturel, réseau lexical, relations typées pondérées, sens d’usage d’un terme, jeu en ligne</mots_cles>
        <title/>
        <abstract>Lexical information is indispensable for the tasks realized in NLP, but collecting lexical information is a difficult work. Indeed, when done manually, it requires the competence of experts and the duration can be prohibitive. When done automatically, the results can be biased by the corpus of texts. The approach we present here consists in having people take part in a collective project by offering them a playful application accessible on the web. From an already existing base of terms, the players themselves thus build the lexical network, by supplying associations which are validated only if they are suggested by a pair of users. Furthermore, these typed relations are weighted according to the number of pairs of users who provide them. Finally, we approach the question of the word usage determination for a term, by searching relations between this term and its neighbours in the network, before briefly presenting the realization and the first obtained results.</abstract>
        <keywords>Natural Language Processing, lexical network, typed and weighted relations, word usage, webbased game</keywords>
      </article>
      <article id="taln-2008-long-020" session="Ressources">
        <auteurs>
          <auteur>
            <nom>Feten Baccar</nom>
            <email>baccarf@yahoo.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Aïda Khemakhem</nom>
            <email>aida_khemakhem@yahoo.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Bilel Gargouri</nom>
            <email>bilel.gargouri@fsegs.rnu.tn</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Kais Haddar</nom>
            <email>kais.haddar@fss.rnu.tn</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Abdelmajid Ben Hamadou</nom>
            <email>abdelmajid.benhamadou@isimsf.rnu.tn</email>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire MIRACL, FSEGS, B.P. 1088, 3018 Sfax, Tunisie</affiliation>
          <affiliation affiliationId="2">Laboratoire MIRACL, FSS, B.P. 802, 3038 Sfax, Tunisie</affiliation>
          <affiliation affiliationId="3">Laboratoire MIRACL, ISIMS, B.P. 242, 3021 Sakiet-Ezzit Sfax, Tunisie</affiliation>
        </affiliations>
        <titre>Modélisation normalisée LMF des dictionnaires électroniques éditoriaux de l’arabe</titre>
        <type>long</type>
        <pages/>
        <resume>Le présent papier s’intéresse à l’élaboration des dictionnaires électroniques arabes à usage éditorial. Il propose un modèle unifié et normalisé de ces dictionnaires en se référant à la future norme LMF (Lexical Markup Framework) ISO 24613. Ce modèle permet de construire des dictionnaires extensibles, sur lesquels on peut réaliser, grâce à une structuration fine et standard, des fonctions de consultation génériques adaptées aux besoins des utilisateurs. La mise en oeuvre du modèle proposé est testée sur des dictionnaires existants de la langue arabe en utilisant, pour la consultation, le système ADIQTO (Arabic DIctionary Query TOols) que nous avons développé pour l’interrogation générique des dictionnaires normalisés de l’arabe.</resume>
        <mots_cles>Dictionnaire électronique Arabe, usage éditorial, modèle normalisé, LMF, interrogation générique</mots_cles>
        <title/>
        <abstract>This paper is interested in the development of the Arabic electronic dictionaries of human use. It proposes a unified and standardized model for these dictionaries according to the future standard LMF (Lexical Markup Framework) ISO 24613. Thanks to its subtle and standardized structure, this model allows the development of extendable dictionaries on which generic interrogation functions adapted to the user’s needs can be implemented. This model has already been carried out on some existing Arabic dictionaries using the ADIQTQ (Arabic DIctionary Query Tool) system, which we developed for the generic interrogation of standardized dictionaries of Arabic.</abstract>
        <keywords>Arabic electronic dictionary, human use, standardized model, LMF, generic interrogation</keywords>
      </article>
      <article id="taln-2008-long-021" session="Ressources">
        <auteurs>
          <auteur>
            <nom>Lucie Barque</nom>
            <email>lucie.barque@linguist.jussieu.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>François-Régis Chaumartin</nom>
            <email>frc@proxem.com</email>
            <affiliationId>2</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Lattice - Université Paris 7, UFRL, Case 7003, 75251 Paris cedex 5</affiliation>
          <affiliation affiliationId="2">Proxem - 7 impasse Dumur, 92110 Clichy</affiliation>
          <affiliation affiliationId="3">Alpage - Université Paris 7, UFRL, Case 7003, 75251 Paris cedex 5</affiliation>
        </affiliations>
        <titre>La polysémie régulière dans WordNet</titre>
        <type>long</type>
        <pages/>
        <resume>Cette étude propose une analyse et une modélisation des relations de polysémie dans le lexique électronique anglais WordNet. Elle exploite pour cela la hiérarchie des concepts (représentés par des synsets), et la définition associée à chacun de ces concepts. Le résultat est constitué d'un ensemble de règles qui nous ont permis d'identifier d’une façon largement automatisée, avec une précision voisine de 91%, plus de 2100 paires de synsets liés par une relation de polysémie régulière. Notre méthode permet aussi une désambiguïsation lexicale partielle des mots de la définition associée à ces synsets.</resume>
        <mots_cles>polysémie régulière, métaphore, métonymie, WordNet, désambiguïsation lexicale</mots_cles>
        <title/>
        <abstract>This paper presents an analysis and modeling of polysemy in the WordNet English lexical database. It exploits the concepts hierarchy (constituted by synsets), and the gloss defining each of these concepts. The result consists of rules set which enabled us to identify in a largely automated way, with a precision close to 91%, more than 2100 synsets pairs, connected by a regular polysemy relation. Our method also allows a partial word sense disambiguation of the definition associated with these synsets.</abstract>
        <keywords>regular polysemy, metaphor, metonymy, WordNet, word sense disambiguation</keywords>
      </article>
      <article id="taln-2008-long-022" session="Traduction automatique">
        <auteurs>
          <auteur>
            <nom>Caroline Lavecchia</nom>
            <email/>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Kamel Smaïli</nom>
            <email/>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>David Langlois</nom>
            <email/>
            <affiliationId>1</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LORIA/Speech Group, Campus scientifique, BP 239, 54506 Vandoeuvre lès Nancy Cedex, France</affiliation>
          <affiliation affiliationId="2">Université Nancy2</affiliation>
          <affiliation affiliationId="3">IUFM de Lorraine</affiliation>
        </affiliations>
        <titre>Une alternative aux modèles de traduction statistique d’IBM: Les triggers inter-langues</titre>
        <type>long</type>
        <pages/>
        <resume>Dans cet article, nous présentons une nouvelle approche pour la traduction automatique fondée sur les triggers inter-langues. Dans un premier temps, nous expliquons le concept de triggers inter-langues ainsi que la façon dont ils sont déterminés. Nous présentons ensuite les différentes expérimentations qui ont été menées à partir de ces triggers afin de les intégrer au mieux dans un processus complet de traduction automatique. Pour cela, nous construisons à partir des triggers inter-langues des tables de traduction suivant différentes méthodes. Nous comparons par la suite notre système de traduction fondé sur les triggers interlangues à un système état de l’art reposant sur le modèle 3 d’IBM (Brown &amp; al., 1993). Les tests menés ont montré que les traductions automatiques générées par notre système améliorent le score BLEU (Papineni &amp; al., 2001) de 2, 4% comparé à celles produites par le système état de l’art.</resume>
        <mots_cles>Traduction Automatique Statistique, Triggers Inter-Langues, Information Mutuelle, Corpus parallèle, Décodage</mots_cles>
        <title/>
        <abstract>In this paper, we present an original approach for machine translation based on inter-lingual triggers. First, we describe the idea of inter-lingual triggers and how to determine them. Then, we present the way to make good use of them in order to integrate them in an entire translation process. We used inter-lingual triggers to estimate different translation tables. Then we compared our translation system based on triggers to a state-of-the-art system based on IBM model 3 (Brown &amp; al., 1993). The experiments showed that automatic translations generated by our system outperform model 3 of IBM by 2.4% in terms of BLEU (Papineni &amp; al., 2001).</abstract>
        <keywords>StatisticalMachine Translation, Inter-Lingual Triggers,Mutual Information, Parallel corpus, Decoding process</keywords>
      </article>
      <article id="taln-2008-long-023" session="Traduction automatique">
        <auteurs>
          <auteur>
            <nom>Aurélien Max</nom>
            <email>aurelien.max@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIMSI-CNRS et Université Paris-Sud 11 Orsay, France</affiliation>
        </affiliations>
        <titre>Génération de reformulations locales par pivot pour l’aide à la révision</titre>
        <type>long</type>
        <pages/>
        <resume>Cet article présente une approche pour obtenir des paraphrases pour de courts segments de texte qui peuvent aider un rédacteur à reformuler localement des textes. La ressource principale utilisée est une table d’alignements bilingues de segments d’un système de traduction automatique statistique. Un segment marqué par le rédacteur est tout d’abord traduit dans une langue pivot avant d’être traduit à nouveau dans la langue d’origine, ce qui est permis par la nature même de la ressource bilingue utilisée sans avoir recours à un processus de traduction complet. Le cadre proposé permet l’intégration et la combinaison de différents modèles d’estimation de la qualité des paraphrases. Des modèles linguistiques tentant de prendre en compte des caractéristiques des paraphrases de courts segments de textes sont proposés, et une évaluation est décrite et ses résultats analysés. Les domaines d’application possibles incluent, outre l’aide à la reformulation, le résumé et la réécriture des textes pour répondre à des conventions ou à des préférences stylistiques. L’approche est critiquée et des perspectives d’amélioration sont proposées.</resume>
        <mots_cles>Paraphrase, Traduction Automatique Statistique basée sur les segments, Aide à la rédaction</mots_cles>
        <title/>
        <abstract>In this article, we present a method to obtain paraphrases for short text spans that can be useful to help a writer in reformulating text. The main resource used is a bilingual phrase table containing aligned phrases, a common resource in statistical machine translation. The writer can mark a segment for paraphrasing, and this segment is first translated into a pivot language before being back-translated into the original language, which is possible without performing a full translation of the input. Our proposed framework allows integrating and combining various models for estimating paraphrase quality. We propose linguistic models which permits to conduct empirical experiments about the characteristics of paraphrases for short text spans. Application domains include, in addition to paraphrasing aids, summarization and rephrasing of text for conforming to conventional or stylistic guidelines. We finally discuss the limitations of our work and describe possible ways of improvement.</abstract>
        <keywords>Paraphrasing, Phrase-Based Statistical Machine Translation (PBSMT), Authoring aids</keywords>
      </article>
      <article id="taln-2008-long-024" session="Traduction automatique">
        <auteurs>
          <auteur>
            <nom>Christian Boitet</nom>
            <email>Christian.Boitet@imag.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire LIG, GETALP – Université Joseph Fourier, 385 rue de la bibliothèque, BP 53, 38041 Grenoble, Cedex 9, France</affiliation>
        </affiliations>
        <titre>Les architectures linguistiques et computationnelles en traduction automatique sont indépendantes</titre>
        <type>long</type>
        <pages/>
        <resume>Contrairement à une idée répandue, les architectures linguistiques et computationnelles des systèmes de traduction automatique sont indépendantes. Les premières concernent le choix des représentations intermédiaires, les secondes le type d'algorithme, de programmation et de ressources utilisés. Il est ainsi possible d'utiliser des méthodes de calcul « expertes » ou « empiriques » pour construire diverses phases ou modules de systèmes d'architectures linguistiques variées. Nous terminons en donnant quelques éléments pour le choix de ces architectures en fonction des situations traductionnelles et des ressources disponibles, en termes de dictionnaires, de corpus, et de compétences humaines.</resume>
        <mots_cles>Traduction Automatique, TA, TAO, architecture linguistique, architecture computationnelle, TA experte, TA par règles, TA empirique, TA statistique, TA par l'exemple</mots_cles>
        <title/>
        <abstract>Contrary to a wide-spread idea, the linguistic and computational architectures of MT systems are independent. The former concern the choice of the intermediate representations, the latter the type of algorithm, programming, and resources used. It is thus possible to use "expert" or "empirical" computational methods to build various phases or modules of systems having various linguistic architectures. We finish by giving some elements for choosing these architectures depending on the translational situations and the available resources, in terms of dictionaries, corpora, and human competences.</abstract>
        <keywords>Machine Translation, MT, linguistic architecture, computational architecture, expert MT, rule-based MT, empirical MT, statistical MT, example-based MT</keywords>
      </article>
      <article id="taln-2008-long-025" session="Entités Nommées">
        <auteurs>
          <auteur>
            <nom>Caroline Brun</nom>
            <email>Caroline.Brun@xrce.xerox.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Caroline Hagège</nom>
            <email>Caroline.Hagege@xrce.xerox.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Xerox Research Centre Europe – 6, chemin de Maupertuis, 38240 Meylan France</affiliation>
        </affiliations>
        <titre>Vérification sémantique pour l’annotation d’entités nommées</titre>
        <type>long</type>
        <pages/>
        <resume>Dans cet article, nous proposons une méthode visant à corriger et à associer dynamiquement de nouveaux types sémantiques dans le cadre de systèmes de détection automatique d’entités nommées (EN). Après la détection des entités nommées et aussi de manière plus générale des noms propres dans les textes, une vérification de compatibilité de types sémantiques est effectuée non seulement pour confirmer ou corriger les résultats obtenus par le système de détection d’EN, mais aussi pour associer de nouveaux types non couverts par le système de détection d’EN. Cette vérification est effectuée en utilisant l’information syntaxique associée aux EN par un système d’analyse syntaxique robuste et en confrontant ces résultats avec la ressource sémantique WordNet. Les résultats du système de détection d’EN sont alors considérablement enrichis, ainsi que les étiquettes sémantiques associées aux EN, ce qui est particulièrement utile pour l’adaptation de systèmes de détection d’EN à de nouveaux domaines.</resume>
        <mots_cles>Entités nommées, Analyse syntaxique robuste, Types sémantiques</mots_cles>
        <title/>
        <abstract>In this paper we propose a new method that enables to correct and to associate new semantic types in the context of Named Entity (NE) Recognition Systems. After named entities (and more generally proper nouns) have been detected in texts, a semantic compatibility checking is performed. This checking can not only confirm or correct previous results of the NER system but also associate new NE types that have not been previously foreseen. This checking is performed using information associated to the NE by a robust syntactic analyzer and confronting this information to WordNet. After this checking is performed, final results of the NER system are better and new NE semantic tags are created. This second point is particularly useful when adapting existing NER systems to new domains.</abstract>
        <keywords>Named Entities, Robust Parsing, Semantic Types</keywords>
      </article>
      <article id="taln-2008-long-026" session="Entités Nommées">
        <auteurs>
          <auteur>
            <nom>Thomas Girault</nom>
            <email>thomas.girault@orange-ftgroup.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">France Télécom R&amp;D, 2, avenue Pierre Marzin 22307 Lannion Cedex</affiliation>
        </affiliations>
        <titre>Exploitation de treillis de Galois en désambiguïsation non supervisée d’entités nommées</titre>
        <type>long</type>
        <pages/>
        <resume>Nous présentons une méthode non supervisée de désambiguïsation d’entités nommées, basée sur l’exploitation des treillis de Galois. Nous réalisons une analyse de concepts formels à partir de relations entre des entités nommées et leurs contextes syntaxiques extraits d’un corpus d’apprentissage. Le treillis de Galois résultant fournit des concepts qui sont utilisés comme des étiquettes pour annoter les entités nommées et leurs contextes dans un corpus de test. Une évaluation en cascade montre qu’un système d’apprentissage supervisé améliore la classification des entités nommées lorsqu’il s’appuie sur l’annotation réalisée par notre système de désambiguïsation non supervisée.</resume>
        <mots_cles>Désambiguïsation non supervisée, treillis de Galois, entités nommées</mots_cles>
        <title/>
        <abstract>We present an unsupervised method for named entities disambiguation, based on concept lattice mining.We perform a formal concept analysis from relations between named entities and their syntactic contexts observed in a training corpora. The resulting lattice produces concepts which are considered as labels for named entities and context annotation. Our approach is validated through a cascade evaluation which shows that supervised named entity classification is improved by using the annotation produced by our unsupervised disambiguation system.</abstract>
        <keywords>Unsupervised word sense disambiguation, concept lattice, named entities</keywords>
      </article>
      <article id="taln-2008-long-027" session="Entités Nommées">
        <auteurs>
          <auteur>
            <nom>Caroline Brun</nom>
            <email>Caroline.Brun@xrce.xerox.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Maud Ehrmann</nom>
            <email>Maud.Ehrmann@xrce.xerox.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Guillaume Jacquet</nom>
            <email>Guillaume.Jacquet@xrce.xerox.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Xerox Research Center Europe - XRCE, 6, Chemin de Maupertuis, 38240 Meylan</affiliation>
        </affiliations>
        <titre>Résolution de Métonymie des Entités Nommées : proposition d’une méthode hybride</titre>
        <type>long</type>
        <pages/>
        <resume>Dans cet article, nous décrivons la méthode que nous avons développée pour la résolution de métonymie des entités nommées dans le cadre de la compétition SemEval 2007. Afin de résoudre les métonymies sur les noms de lieux et noms d’organisation, tel que requis pour cette tâche, nous avons mis au point un système hybride basé sur l’utilisation d’un analyseur syntaxique robuste combiné avec une méthode d’analyse distributionnelle. Nous décrivons cette méthode ainsi que les résultats obtenus par le système dans le cadre de la compétition SemEval 2007.</resume>
        <mots_cles>Entités Nommées, métonymie, méthode hybride, analyse syntaxique robuste, approche distributionnelle</mots_cles>
        <title/>
        <abstract>In this paper, we describe the method we develop in order to solve Named entity metonymy in the framework of the SemEval 2007 competition. In order to perform Named Entity metonymy resolution on location names and company names, as required for this task, we developed a hybrid system based on the use of a robust parser that extracts deep syntactic relations combined with a non supervised distributional approach, also relying on the relations extracted by the parser.We describe this methodology as well as the results obtained at SemEval 2007.</abstract>
        <keywords>Named Entities, metonymy, hybrid method, robust parsing, distributional approach</keywords>
      </article>
      <article id="taln-2008-long-028" session="Etiquetage et indexation">
        <auteurs>
          <auteur>
            <nom>Tatiana El-Khoury</nom>
            <email>tatiana.elkhoury@imag.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire TIMC–IMAG - Université Joseph Fourier-Grenoble 1, BP 53-38041 Grenoble</affiliation>
        </affiliations>
        <titre>Etude de la corrélation entre morphosyntaxe et sémantique dans une perspective d’étiquetage automatique de textes médicaux arabes</titre>
        <type>long</type>
        <pages/>
        <resume>Cet article se propose d’étudier les relations sémantiques reliant base et expansion au sein des termes médicaux arabes de type « N+N », particulièrement ceux dont la base est un déverbal. En étudiant les relations sémantiques établies par une base déverbale, ce travail tente d’attirer l’attention sur l’interpénétration du sémantique et du morphosyntaxique ; il montre que, dans une large mesure, la structure morphosyntaxique de la base détermine l’éventail des possibilités relationnelles. La découverte de régularités dans le comportement de la base déverbale permet de prédire le type de relations que peut établir cette base avec son expansion pavant ainsi la voie à un traitement automatique et un travail d’étiquetage sémantique des textes médicaux arabes.</resume>
        <mots_cles>étiquetage automatique, terminologie médicale arabe, morphosyntaxe, sémantique</mots_cles>
        <title/>
        <abstract>This paper examines the semantic relations existing in Arabic medical texts between the head and its extension in a two-noun compound, particularly when the head is a deverbal noun or a nominalization. By studying semantic relations encoded by nominalizations, this research work aims at underlining the correlation between morphosyntax and semantics notably the influence of the head noun structure on the set of semantic relations that can be established. The discovery of regularities in the functioning of the head noun allows thus to predict the type of relation that will be encoded. Such data are a pre-requisite for natural language processing and automatic part-to-speech tagging of medical Arabic texts.</abstract>
        <keywords>Part-of-speech tagging, Arabic medical terminology, morphosyntax, semantics</keywords>
      </article>
      <article id="taln-2008-long-029" session="Etiquetage et indexation">
        <auteurs>
          <auteur>
            <nom>Philippe Blache</nom>
            <email>philippe.blache@lpl-aix.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Stéphane Rauzy</nom>
            <email>stephane.rauzy@lpl-aix.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire Parole et Langage, CNRS &amp; Université de Provence</affiliation>
        </affiliations>
        <titre>Influence de la qualité de l’étiquetage sur le chunking : une corrélation dépendant de la taille des chunks</titre>
        <type>long</type>
        <pages/>
        <resume>Nous montrons dans cet article qu’il existe une corrélation étroite existant entre la qualité de l’étiquetage morpho-syntaxique et les performances des chunkers. Cette corrélation devient linéaire lorsque la taille des chunks est limitée. Nous appuyons notre démonstration sur la base d’une expérimentation conduite suite à la campagne d’évaluation Passage 2007 (de la Clergerie et al., 2008). Nous analysons pour cela les comportements de deux analyseurs ayant participé à cette campagne. L’interprétation des résultats montre que la tâche de chunking, lorsqu’elle vise des chunks courts, peut être assimilée à une tâche de “super-étiquetage”.</resume>
        <mots_cles>Analyse syntaxique, étiquetage morphosyntaxique, analyseur stochastique, analyseur symbolique superficiel, chunker</mots_cles>
        <title/>
        <abstract>We show in this paper that a strong correlation exists between the performance of chunk parsers and the quality of the tagging task in input. This dependency becomes linear when the size of the chunks is small. Our demonstration is based on an experiment conducted at the end of the Passage 2007 shared task evaluation initiative (de la Clergerie et al., 2008). The performance of two parsers which took part in this evaluation has been investigated. The results indicate that the chunking task, for sufficiently short chunks, is similar to a super-tagging task.</abstract>
        <keywords>Parsing, tagging, stochastic parser, symbolic shallow parser, chunker</keywords>
      </article>
      <article id="taln-2008-long-030" session="Etiquetage et indexation">
        <auteurs>
          <auteur>
            <nom>Aurélie Névéol</nom>
            <email>neveola@nlm.nih.gov</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Vincent Claveau</nom>
            <email>Vincent.Claveau@irisa.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">National Library of Medicine, 8600 Rockville Pike, Bethesda, MD 20894, USA</affiliation>
          <affiliation affiliationId="2">IRISA - CNRS, Campus de Beaulieu, 35042 Rennes cedex, France</affiliation>
        </affiliations>
        <titre>Apprentissage artificiel de règles d’indexation pour MEDLINE</titre>
        <type>long</type>
        <pages/>
        <resume>L’indexation est une composante importante de tout système de recherche d’information. Dans MEDLINE, la base documentaire de référence pour la littérature du domaine biomédical, le contenu des articles référencés est indexé à l’aide de descripteurs issus du thésaurus MeSH. Avec l’augmentation constante de publications à indexer pour maintenir la base à jour, le besoin d’outils automatiques se fait pressant pour les indexeurs. Dans cet article, nous décrivons l’utilisation et l’adaptation de la Programmation Logique Inductive (PLI) pour découvrir des règles d’indexation permettant de générer automatiquement des recommandations d’indexation pour MEDLINE. Les résultats obtenus par cette approche originale sont très satisfaisants comparés à ceux obtenus à l’aide de règles manuelles lorsque celles-ci existent. Ainsi, les jeux de règles obtenus par PLI devraient être prochainement intégrés au système produisant les recommandations d’indexation automatique pour MEDLINE.</resume>
        <mots_cles>Analyse et Indexation/méthodes, Medical Subject Headings, Apprentissage Artificiel, Programmation Logique Inductive</mots_cles>
        <title/>
        <abstract>Indexing is a crucial step in any information retrieval system. In MEDLINE, a widely used database of the biomedical literature, the indexing process involves the selection of Medical Subject Headings in order to describe the subject matter of articles. The need for automatic tools to assist human indexers in this task is growing with the increasing amount of publications to be referenced in MEDLINE. In this paper, we describe the use and the customization of Inductive Logic Programming (ILP) to infer indexing rules that may be used to produce automatic indexing recommendations for MEDLINE indexers. Our results show that this original ILP-based approach overperforms manual rules when they exist. We expect the sets of ILP rules obtained in this experiment to be integrated in the system producing automatic indexing recommendations for MEDLINE.</abstract>
        <keywords>Abstracting and Indexing/methods, Medical Subject Headings, Machine Learning, Inductive Logic Programming</keywords>
      </article>
      <article id="taln-2008-court-001" session="Poster">
        <auteurs>
          <auteur>
            <nom>Yayoi Nakamura-Delloye</nom>
            <email>yayoi@free.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université Paris VII - Lattice (UMR 8094), 75013 Paris</affiliation>
          <affiliation affiliationId="2">Université Paris X - MoDyCo (UMR 7114), 92001 Nanterre Cedex</affiliation>
        </affiliations>
        <titre>Y a-t-il une véritable équivalence entre les propositions syntaxiques du français et du japonais ?</titre>
        <type>court</type>
        <pages/>
        <resume>La présente contribution part de nos constats réalisés à partir des résultats d’évaluation de notre système d’alignement des propositions de textes français-japonais. La présence importante de structures fondamentalement difficiles à aligner et les résultats peu satisfaisants de différentes méthodes de mise en correspondance des mots nous ont finalement amenés à remettre en cause l’existence même d’équivalence au niveau des propositions syntaxiques entre le français et le japonais. Afin de compenser les défauts que nous avons découverts, nous proposons des opérations permettant de restaurer l’équivalence des propositions alignées et d’améliorer la qualité des corpus alignés.</resume>
        <mots_cles>Alignement, proposition syntaxique, études contrastives français-japonais, similarité lexicale</mots_cles>
        <title/>
        <abstract>This paper is based on our observations obtained from the results of our French-Japanese clause alignment system. Structures fundamentally difficult to align were so numerous and results obtained by various word-matching methods were so unsatisfactory that we questioned the existence of equivalence at the syntactic clause level between French and Japanese. In order to compensate the defect that we discovered, we propose some operations to restore aligned clause equivalence to improve the quality of aligned corpora.</abstract>
        <keywords>Alignment, syntactic clause, French-Japanese contrastive study, lexical similarty</keywords>
      </article>
      <article id="taln-2008-court-002" session="Poster">
        <auteurs>
          <auteur>
            <nom>Sylvain Schmitz</nom>
            <email>Sylvain.Schmitz@loria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Joseph Le Roux</nom>
            <email>Joseph.LeRoux@loria.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LORIA, INRIA Nancy Grand Est, Nancy</affiliation>
          <affiliation affiliationId="2">LORIA, Université Nancy 2, Nancy</affiliation>
        </affiliations>
        <titre>Calculs d’unification sur les arbres de dérivation TAG</titre>
        <type>court</type>
        <pages/>
        <resume>Nous définissons un formalisme, les grammaires rationnelles d’arbres avec traits, et une traduction des grammaires d’arbres adjoints avec traits vers ce nouveau formalisme. Cette traduction préserve les structures de dérivation de la grammaire d’origine en tenant compte de l’unification de traits. La construction peut être appliquée aux réalisateurs de surface qui se fondent sur les arbres de dérivation.</resume>
        <mots_cles>Unification, grammaire d’arbres adjoints, arbre de dérivation, grammaire rationnelle d’arbres</mots_cles>
        <title/>
        <abstract>The derivation trees of a tree adjoining grammar provide a first insight into the sentence semantics, and are thus prime targets for generation systems. We define a formalism, feature based regular tree grammars, and a translation from feature based tree adjoining grammars into this new formalism. The translation preserves the derivation structures of the original grammar, and accounts for feature unification.</abstract>
        <keywords>Unification, tree adjoining grammar, derivation tree, regular tree grammar</keywords>
      </article>
      <article id="taln-2008-court-003" session="Poster">
        <auteurs>
          <auteur>
            <nom>Alexandre Labadié</nom>
            <email>labadie@lirmm.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Violaine Prince</nom>
            <email>prince@lirmm.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIRMM, 161 rue ADA, 34392 Montpellier cedex</affiliation>
        </affiliations>
        <titre>Comparaison de méthodes lexicales et syntaxico-sémantiques dans la segmentation thématique de texte non supervisée</titre>
        <type>court</type>
        <pages/>
        <resume>Cet article présente une méthode basée sur des calculs de distance et une analyse sémantique et syntaxique pour la segmentation thématique de texte. Pour évaluer cette méthode nous la comparons à un un algorithme lexical très connu : c99. Nous testons les deux méthodes sur un corpus de discours politique français et comparons les résultats. Les deux conclusions qui ressortent de notre expérience sont que les approches sont complémentaires et que les protocoles d’évaluation actuels sont inadaptés.</resume>
        <mots_cles>Méthodes d’évaluation, segmentation de texte, segmentation thématique</mots_cles>
        <title/>
        <abstract>This paper present a semantic and syntactic distance based method in topic text segmentation and compare it to a very well known text segmentation algorithm : c99. To do so we ran the two algorithms on a corpus of twenty two French political discourses and compared their results. Our two conclusions are that the two approaches are complementary and that evaluation methods in this domain should be revised.</abstract>
        <keywords>Evaluation methods, text segmentation, topic segmentation</keywords>
      </article>
      <article id="taln-2008-court-004" session="Poster">
        <auteurs>
          <auteur>
            <nom>Jérôme Lehuen</nom>
            <email>Jerome.Lehuen@lium.univ-lemans.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIUM - Université du Maine, Avenue Laënnec, 72085 Le Mans Cedex 9</affiliation>
        </affiliations>
        <titre>Un modèle de langage pour le DHM : la Grammaire Sémantique Réversible</titre>
        <type>court</type>
        <pages/>
        <resume>Cet article propose un modèle de langage dédié au dialogue homme-machine, ainsi que des algorithmes d’analyse et de génération. L’originalité de notre approche est de faire reposer l’analyse et la génération sur les mêmes connaissances, essentiellement sémantiques. Celles-ci sont structurées sous la forme d’une bibliothèque de concepts, et de formes d’usage associées aux concepts. Les algorithmes, quant à eux, sont fondés sur un double principe de correspondance entre des offres et des attentes, et d’un calcul heuristique de score.</resume>
        <mots_cles>Grammaire Sémantique, Réversibilité, Analyse, Génération, Dialogue</mots_cles>
        <title/>
        <abstract>In this paper we present a language model for man-machine dialogue, as well as algorithms for analysis and text generation. The originality of our approach is to base analysis and generation on the same knowledge. These one is structured like a library of concepts and syntactic patterns. The algorithms are based on a principle of correspondence between offers and expectations, and calculation of a heuristic scoring.</abstract>
        <keywords>Semantic Grammar, Reversibility, Analysis, Generation, Dialogue</keywords>
      </article>
      <article id="taln-2008-court-005" session="Poster">
        <auteurs>
          <auteur>
            <nom>Maxime Amblard</nom>
            <email>maxime.amblard@orange-ftgroup.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Johannes Heinecke</nom>
            <email>johannes.heinecke@orange-ftgroup.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Estelle Maillebuau</nom>
            <email>estelle.maillebuau@orange-ftgroup.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Orange Labs, 2 av. Pierre Marzin, 22307 Lannion cedex</affiliation>
        </affiliations>
        <titre>Discourse Representation Theory et graphes sémantiques : formalisation sémantique en contexte industriel</titre>
        <type>court</type>
        <pages/>
        <resume>Ces travaux présentent une extension des représentations formelles pour la sémantique, de l’outil de traitement automatique des langues de Orange Labs1. Nous abordons ici uniquement des questions relatives à la construction des représentations sémantiques, dans le cadre de l’analyse linguistique. Afin d’obtenir des représentations plus fines de la structure argumentale des énoncés, nous incluons des concepts issus de la DRT dans le système de représentation basé sur les graphes sémantiques afin de rendre compte de la notion de portée.</resume>
        <mots_cles>Modèle sémantique, analyse syntaxique en dépendance, DRT</mots_cles>
        <title/>
        <abstract>This works present an extension of the formal representation of semantic for the natural language processing tool developped at Orange Labs. We cover here only issues relating to build the semantic representations, based on linguistic parsing. To obtain more detailed representations of the argumental structure statements, we include insights from the DRT in the system of representation based on the semantic graphs to give an account of scope.</abstract>
        <keywords>Semantic modelling, parsing, DRT</keywords>
      </article>
      <article id="taln-2008-court-006" session="Poster">
        <auteurs>
          <auteur>
            <nom>Karën Fort</nom>
            <email>Karen.Fort@loria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Bruno Guillaume</nom>
            <email>Bruno.Guillaume@loria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LORIA / INRIA Nancy Grand-Est</affiliation>
        </affiliations>
        <titre>Sylva : plate-forme de validation multi-niveaux de lexiques</titre>
        <type>court</type>
        <pages/>
        <resume>La production de lexiques est une activité indispensable mais complexe, qui nécessite, quelle que soit la méthode de création utilisée (acquisition automatique ou manuelle), une validation humaine. Nous proposons dans ce but une plate-forme Web librement disponible, appelée Sylva (Systematic lexicon validator). Cette plate-forme a pour caractéristiques principales de permettre une validation multi-niveaux (par des validateurs, puis un expert) et une traçabilité de la ressource. La tâche de l’expert(e) linguiste en est allégée puisqu’il ne lui reste à considérer que les données sur lesquelles il n’y a pas d’accord inter-validateurs.</resume>
        <mots_cles>Lexiques, plate-forme de validation, cadres de sous-catégorisation</mots_cles>
        <title/>
        <abstract>Lexicon production is essential but complex and all creation methods (automatic acquisition or manual creation) require human validation. For this purpose, we propose a freely available Web-based framework, named Sylva (Systematic lexicon validator). The main point of our framework is that it handles multi-level validations and keeps track of the resource’s history. The expert linguist task is made easier : (s)he has only to consider data on which validators disagree.</abstract>
        <keywords>Lexicons, validation framework, subcategorization frames</keywords>
      </article>
      <article id="taln-2008-court-007" session="Poster">
        <auteurs>
          <auteur>
            <nom>Rémy Kessler</nom>
            <email>remy.kessler@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Juan-Manuel Torres-Moreno</nom>
            <email>juan-manuel.torres@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Marc El-Bèze</nom>
            <email>marc.elbeze@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIA / Université d’Avignon, 339 chemin des Meinajariès, 84911 Avignon</affiliation>
          <affiliation affiliationId="2">AKTOR 12, allée Irène Joliot Curie 69800 Saint Priest</affiliation>
        </affiliations>
        <titre>E-Gen : Profilage automatique de candidatures</titre>
        <type>court</type>
        <pages/>
        <resume>La croissance exponentielle de l’Internet a permis le développement de sites d’offres d’emploi en ligne. Le système E-Gen (Traitement automatique d’offres d’emploi) a pour but de permettre l’analyse et la catégorisation d’offres d’emploi ainsi qu’une analyse et classification des réponses des candidats (Lettre de motivation et CV). Nous présentons les travaux réalisés afin de résoudre la seconde partie : on utilise une représentation vectorielle de texte pour effectuer une classification des pièces jointes contenus dans le mail à l’aide de SVM. Par la suite, une évaluation de la candidature est effectuée à l’aide de différents classifieurs (SVM et n-grammes de mots).</resume>
        <mots_cles>Classification de textes, Modèle probabiliste, Ressources humaines, Offres d’emploi</mots_cles>
        <title/>
        <abstract>The exponential growth of the Internet has allowed the development of a market of on-line job search sites. This paper presents the E-Gen system (Automatic Job Offer Processing system for Human Resources). E-Gen will perform two complex tasks : an analysis and categorisation of job postings, which are unstructured text documents, an analysis and a relevance ranking of the candidate answers (cover letter and curriculum vitae). Here we present the work related to the second task : we use vectorial representation before generating a classification with SVM to determine the type of the attachment. In the next step, we try to classify the candidate answers with different classifiers (SVM and ngrams of words).</abstract>
        <keywords>Text Classification, Probabilistic Model, Human Ressources, Job Offer</keywords>
      </article>
      <article id="taln-2008-court-008" session="Poster">
        <auteurs>
          <auteur>
            <nom>François Barthélemy</nom>
            <email>barthe@cnam.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CNAM, Cédric, 292 rue Saint-Martin, 75003 Paris</affiliation>
          <affiliation affiliationId="2">INRIA, Alpage, 78153 Le Chesnay cedex</affiliation>
        </affiliations>
        <titre>Typage, produit cartésien et unités d’analyse pour les modèles à états finis</titre>
        <type>court</type>
        <pages/>
        <resume>Dans cet article, nous présentons un nouveau langage permettant d’écrire des relations rationnelles compilées en automates finis. Les deux caractéristiques innovantes de ce langage sont de pourvoir décrire des relations à plusieurs niveaux, pas nécessairement deux et d’utiliser diverses unités d’analyse pour exprimer les liens entre niveaux. Cela permet d’aligner de façon fine des représentations multiples.</resume>
        <mots_cles>Machine finie à états, morphologie à deux niveau</mots_cles>
        <title/>
        <abstract>In this paper, we present a new language to write rational relations compiled into finite state automata. There are two main novelties in the language. Firstly, the descriptions may have more than two levels. Secondly, various units may be used to express the relationships between the levels. Using these features, it is possible to align finely multiple representations.</abstract>
        <keywords>Finite-state machine, two-level morphology</keywords>
      </article>
      <article id="taln-2008-court-009" session="Poster">
        <auteurs>
          <auteur>
            <nom>Frédéric Landragin</nom>
            <email>frederic.landragin@linguist.jussieu.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CNRS – Laboratoire LaTTICe (UMR 8094), 1 rue Maurice Arnoux – 92120 Montrouge</affiliation>
        </affiliations>
        <titre>Vers l’évaluation de systèmes de dialogue homme-machine : de l’oral au multimodal</titre>
        <type>court</type>
        <pages/>
        <resume>L’évaluation pour le dialogue homme-machine ne se caractérise pas par l’efficacité, l’objectivité et le consensus que l’on observe dans d’autres domaines du traitement automatique des langues. Les systèmes de dialogue oraux et multimodaux restent cantonnés à des domaines applicatifs restreints, ce qui rend difficiles les évaluations comparatives ou normées. De plus, les avancées technologiques constantes rendent vite obsolètes les paradigmes d’évaluation et ont pour conséquence une multiplication de ceux-ci. Des solutions restent ainsi à trouver pour améliorer les méthodes existantes et permettre des diagnostics plus automatisés des systèmes. Cet article se veut un ensemble de réflexions autour de l’évaluation de la multimodalité dans les systèmes à forte composante linguistique. Des extensions des paradigmes existants sont proposées, en particulier DQR/DCR, sachant que certains sont mieux adaptés que d’autres au dialogue multimodal. Des conclusions et perspectives sont tirées sur l’avenir de l’évaluation pour le dialogue homme-machine.</resume>
        <mots_cles>Dialogue finalisé, multimodalité, évaluation pour le dialogue hommemachine, paradigme d’évaluation, test utilisateur, diagnostic, paraphrase multimodale</mots_cles>
        <title/>
        <abstract>Evaluating human-machine dialogue systems is not so efficient, objective, and consensual than evaluating other natural language processing systems. Oral and multimodal dialogue systems are still working within reduced applicative domains. Comparative and normative evaluations are then difficult. Moreover, the continuous technological progress makes obsolete and numerous the evaluating paradigms. Some solutions are still to be identified to improve existing methods and to allow a more automatic diagnosis of systems. The aim of this paper is to provide a set of remarks dealing with the evaluation of multimodal spoken language dialogue systems. Some extensions of existing paradigms are presented, in particular DQR/DCR, considering that some paradigms fit better multimodal issues than others. Some conclusions and perspectives are then drawn on the future of the evaluation of human-machine dialogue systems.</abstract>
        <keywords>Task-driven dialogue, multimodality, evaluating human-machine dialogue, evaluation paradigm, user test, diagnosis</keywords>
      </article>
      <article id="taln-2008-court-010" session="Poster">
        <auteurs>
          <auteur>
            <nom>Nuria Gala</nom>
            <email>nuria.gala@univ-provence.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Véronique Rey</nom>
            <email>veronique.rey@univ-provence.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIF – CNRS, Aix – Marseille Univ., 163 av. de Luminy, 13288 Marseille</affiliation>
          <affiliation affiliationId="2">SHADYC – CNRS, Aix – Marseille Univ., 29 av. R. Schuman, 13100 Aix</affiliation>
        </affiliations>
        <titre>POLYMOTS : une base de données de constructions dérivationnelles en français à partir de radicaux phonologiques</titre>
        <type>court</type>
        <pages/>
        <resume>Cet article présente POLYMOTS, une base de données lexicale contenant huit mille mots communs en français. L’originalité de l’approche proposée tient à l'analyse des mots. En effet, à la différence d’autres bases lexicales représentant la morphologie dérivationnelle des mots à partir d’affixes, ici l’idée a été d’isoler un radical commun à un ensemble de mots d’une même famille. Nous avons donc analysé les formes des mots et, par comparaison phonologique (forme phonique comparable) et morphologique (continuité de sens), nous avons regroupé les mots par familles, selon le type de radical phonologique. L’article présente les fonctionnalités de la base et inclut une discussion sur les applications et les perspectives d’une telle ressource.</resume>
        <mots_cles>ressource lexicale, morphologie dérivationnelle, traitement automatique des familles de mots</mots_cles>
        <title/>
        <abstract>In this paper we present POLYMOTS, a lexical database containing eight thousand common nouns in French. Whereas most of the existing lexicons for derivational morphology take affixes as starting point for producing paradigms of words, we defend here the idea that it is possible to isolate a morpho-phonological stem and produce a paradigm of words belonging to the same family. This point leads us to describe three types of stems according to their phonological and morphological form. The article presents the different features of such a lexical database and discusses the applications and future work using and enriching this resource.</abstract>
        <keywords>lexical resource, derivational morphology, word families processing</keywords>
      </article>
      <article id="taln-2008-court-011" session="Poster">
        <auteurs>
          <auteur>
            <nom>Bruno Cartoni</nom>
            <email>bruno.cartoni@eti.unige.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">ISSCO/TIM-ETI – Université de Genève, 40 bd du Pont d’Arve, CH-1205 Genève</affiliation>
        </affiliations>
        <titre>Mesure de l’alternance entre préfixes pour la génération en traduction automatique</titre>
        <type>court</type>
        <pages/>
        <resume>La génération de néologismes construits pose des problèmes dans un système de traduction automatique, notamment au moment de la sélection du préfixe dans les formations préfixées, quand certains préfixes paraissent pouvoir alterner. Nous proposons une étude « extensive », qui vise à rechercher dans de larges ressources textuelles (l’Internet) des formes préfixées générées automatiquement, dans le but d’individualiser les paramètres qui favorisent l’un des préfixes ou qui, au contraire, permettent cette alternance. La volatilité de cette ressource textuelle nécessite certaines précautions dans la méthodologie de décompte des données extraites.</resume>
        <mots_cles>morphologie, traduction automatique, génération, néologisme, études empiriques</mots_cles>
        <title/>
        <abstract>Generating constructed neologisms in a machine translation system is confronted to the issue of selecting the right affixes, especially when some affixes can be used alternately. We propose here an “extensive” study that looks into large textual data collections (web) for prefixed forms that have been automatically generated, in order to find out parameters that allow the use of both prefixes or, on the contrary, that prevent one or the other prefixation. The volatility of web resources requires methodological precautions, especially in data counting.</abstract>
        <keywords>morphology, machine translation, generation, neologism, empirical studies</keywords>
      </article>
      <article id="taln-2008-court-012" session="Poster">
        <auteurs>
          <auteur>
            <nom>Abdenour Mokrane</nom>
            <email>Abdenour.Mokrane@univ-tours.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Nathalie Friburger</nom>
            <email>Nathalie.Friburger@univ-tours.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Jean-Yves Antoine</nom>
            <email>Jean-Yves.Antoine@univ-tours.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université François Rabelais Tours – LI, IUP Blois, France</affiliation>
        </affiliations>
        <titre>Cascades de transducteurs pour le chunking de la parole conversationnelle : l’utilisation de la plateforme CasSys dans le projet EPAC</titre>
        <type>court</type>
        <pages/>
        <resume>Cet article présente l’utilisation de la plate-forme CasSys pour la segmentation de la parole conversationnelle (chunking) à l’aide de cascades de transducteurs Unitex. Le système que nous présentons est utilisé dans le cadre du projet ANR EPAC. Ce projet a pour objectif l’indexation et l’annotation automatique de grands flux de parole issus d’émissions télévisées ou radiophoniques. Cet article présente tout d’abord l’adaptation à ce type de données d’un système antérieur de chunking (Romus) qui avait été développé pour le dialogue oral homme-machine. Il décrit ensuite les principaux problèmes qui se posent à l’analyse : traitement des disfluences de l’oral spontané, mais également gestion des erreurs dues aux étapes antérieures de reconnaissance de la parole et d’étiquetage morphosyntaxique.</resume>
        <mots_cles>Traitement Automatique du Langage Parlé (TALP), segmentation, chunks, parole conversationnelle, transducteurs, Unitex</mots_cles>
        <title/>
        <abstract>This paper describes the use of the CasSys platform in order to achieve the chunking of conversational speech transcripts by means of cascades of Unitex transducers. Our system is involved in the EPAC project of the French National Agency of Research (ANR). The aim of this project is to develop robust methods for the annotation of audio/multimedia document collections which contains conversational speech sequences such as TV or radio programs. At first, this paper presents the adaptation of a former chunking system (Romus) which was developed in the restricted framework of dedicated spoken manmachine dialogue. Then, it describes the problems that are arising due to 1) spontaneous speech disfluencies and 2) errors for the previous stages of processing (automatic speech recognition and POS tagging).</abstract>
        <keywords>Spoken Language Processing, chunking, conversational speech, transducers, Unitex</keywords>
      </article>
      <article id="taln-2008-court-013" session="Poster">
        <auteurs>
          <auteur>
            <nom>Aurélien Bossard</nom>
            <email>Aurelien.Bossard@lipn.univ-paris13.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Thierry Poibeau</nom>
            <email>Thierry.Poibeau@lipn.univ-paris13.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIPN - UMR 7030, CNRS - Université Paris 13, F-93430 Villetaneuse, France</affiliation>
        </affiliations>
        <titre>Regroupement automatique de documents en classes événementielles</titre>
        <type>court</type>
        <pages/>
        <resume>Cet article porte sur le regroupement automatique de documents sur une base événementielle. Après avoir précisé la notion d’événement, nous nous intéressons à la représentation des documents d’un corpus de dépêches, puis à une approche d’apprentissage pour réaliser les regroupements de manière non supervisée fondée sur k-means. Enfin, nous évaluons le système de regroupement de documents sur un corpus de taille réduite et nous discutons de l’évaluation quantitative de ce type de tâche.</resume>
        <mots_cles>Regroupement de documents, Suivi d’événement</mots_cles>
        <title/>
        <abstract>This paper analyses the problem of automatic document clustering based on events. We first specify the notion of event. Then, we detail the document modelling method and the learning approach for document clustering based on k-means. We finally evaluate our document clustering system on a small corpus and discuss the quantitative evaluation for this kind of task.</abstract>
        <keywords>Document clustering, Event tracking</keywords>
      </article>
      <article id="taln-2008-court-014" session="Poster">
        <auteurs>
          <auteur>
            <nom>Mary Hearne</nom>
            <email>mhearne@computing.dcu.ie</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Sylwia Ozdowska</nom>
            <email>sozdowska@computing.dcu.ie</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>John Tinsley</nom>
            <email>jtinsley@computing.dcu.ie</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">National Centre for Language Technology, Dublin City University, Glasnevin, Dublin 9, Ireland</affiliation>
        </affiliations>
        <titre/>
        <type>court</type>
        <pages/>
        <resume>Nous évaluons le recours à des techniques de traduction à base de segments syntaxiquement motivés, seules ou en combinaison avec des techniques à base de segments non motivés, et nous comparons les apports respectifs de l’analyse en constituants et de l’analyse en dépendances dans ce cadre. À partir d’un corpus parallèle Anglais–Français, nous construisons automatiquement deux corpus d’entraînement arborés, en constituants et en dépendances, alignés au niveau sous-phrastique et en extrayons des correspondances bilingues entre mots et syntagmes motivées syntaxiquement. Nous mesurons automatiquement la qualité de la traduction obtenue par un système à base de segments. Les résultats montrent que la combinaison des correspondances bilingues non motivées et motivées sur le plan syntaxique améliore la qualité de la traduction quel que soit le type d’analyse considéré. Par ailleurs, le gain en qualité est plus important avec le recours à l’analyse en dépendances au regard des constituants.</resume>
        <mots_cles>Traduction statistique à base de segments, annotation en constituants, annotation en dépendances, corpus parallèles arborés alignés au niveau sousphrastique</mots_cles>
        <title>Comparing Constituency and Dependency Representations for SMT Phrase-Extraction</title>
        <abstract>We consider the value of replacing and/or combining string-basedmethods with syntax-based methods for phrase-based statistical machine translation (PBSMT), and we also consider the relative merits of using constituency-annotated vs. dependency-annotated training data. We automatically derive two subtree-aligned treebanks, dependency-based and constituency-based, from a parallel English–French corpus and extract syntactically motivated word- and phrase-pairs. We automatically measure PB-SMT quality. The results show that combining string-based and syntax-based word- and phrase-pairs can improve translation quality irrespective of the type of syntactic annotation. Furthermore, using dependency annotation yields greater translation quality than constituency annotation for PB-SMT.</abstract>
        <keywords>PB-SMT, constituency annotation, dependency annotation, subtreealigned parallel treebanks</keywords>
      </article>
      <article id="taln-2008-court-015" session="Poster">
        <auteurs>
          <auteur>
            <nom>Fabien Poulard</nom>
            <email>fabien.poulard@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Thierry Waszak</nom>
            <email>thierry.waszak@univ-avignon.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Nicolas Hernandez</nom>
            <email>nicolas.hernandez@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Patrice Bellot</nom>
            <email>patrice.bellot@univ-avignon.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LINA UMR 6241 / Université de Nantes</affiliation>
          <affiliation affiliationId="2">LIA / Université d’Avignon</affiliation>
        </affiliations>
        <titre>Repérage de citations, classification des styles de discours rapporté et identification des constituants citationnels en écrits journalistiques</titre>
        <type>court</type>
        <pages/>
        <resume>Dans le contexte de la recherche de plagiat, le repérage de citations et de ses constituants est primordial puisqu’il peut amener à évaluer le caractère licite ou illicite d’une reprise (source citée ou non). Nous proposons ici une comparaison de méthodes automatiques pour le repérage de ces informations et rapportons une évaluation quantitative de celles-ci. Un corpus d’écrits journalistiques français a été manuellement annoté pour nous servir de base d’apprentissage et de test.</resume>
        <mots_cles>détection de citations, classification des styles de discours rapporté, identification du locuteur, techniques par apprentissage et base de règles, écrits journalistiques</mots_cles>
        <title/>
        <abstract>In the application context of reported content, that includes plagiarism and impact of textual information searched, citations finding and its fundamentals is essential as it may help estimating legal value of a citation (with or without specifying original source). We propose here a comparison between automatic methods for finding up those elements and we quantitatively evaluate them. A French journalistic corpus has been manually annotated to be used as learning base and for testing.</abstract>
        <keywords>detection of citations, reported speech style classification, source identification, machine learning and rules-based techniques, news corpus</keywords>
      </article>
      <article id="taln-2008-court-016" session="Poster">
        <auteurs>
          <auteur>
            <nom>Frédéric Landragin</nom>
            <email>frederic.landragin@linguist.jussieu.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CNRS – Laboratoire LaTTICe (UMR 8094), 1 rue Maurice Arnoux – 92120 Montrouge</affiliation>
        </affiliations>
        <titre>Vers l’identification et le traitement des actes de dialogue composites</titre>
        <type>court</type>
        <pages/>
        <resume>Il peut être difficile d’attribuer une seule valeur illocutoire à un énoncé dans un dialogue. En premier lieu, un énoncé peut comporter plusieurs segments de discours ayant chacun leur valeur illocutoire spécifique. De plus, un seul segment peut s’analyser en tant qu’acte de langage composite, regroupant par exemple la formulation d’une question et l’émission simultanée d’une information. Enfin, la structure du dialogue en termes d’échanges et de séquences peut être déterminante dans l’identification de l’acte, et peut également apporter une valeur illocutoire supplémentaire, comme celle de clore la séquence en cours. Dans le but de déterminer la réaction face à un tel acte de dialogue composite, nous présentons une approche théorique pour l’analyse des actes de dialogue en fonction du contexte de tâche et des connaissances des interlocuteurs. Nous illustrons sur un exemple nos choix de segmentation et d’identification des actes composites, et nous présentons les grandes lignes d’une stratégie pour déterminer la réaction qui semble être la plus pertinente.</resume>
        <mots_cles>Actes de langage complexes, structure du dialogue, terrain commun</mots_cles>
        <title/>
        <abstract>Attributing one illocutionary value to a utterance in a dialogue can be difficult. First, a utterance can include several discourse segments, each one with a specific illocutionary value. Moreover, one discourse segment can be linked to a complex speech act that groups for instance a question together with the assertion of new information. Finally, the dialogue structure with the various exchanges and sequences can be decisive when identifying the dialogue act, and can also bring an additional illocutionary value, for instance the one consisting of closing the current sequence. With the aim to determine how to react to such a composite dialogue act, we present a theoretical approach to dialogue act analysis considering the task context and the dialogue participants’ knowledge. We illustrate our choices in terms of segmentation and identification of composite acts, and we present the main features of a strategy for determining the most relevant reaction.</abstract>
        <keywords>Complex speech acts, dialogue structure, common ground</keywords>
      </article>
      <article id="taln-2008-court-017" session="Poster">
        <auteurs>
          <auteur>
            <nom>Manal El Zant</nom>
            <email>el.zant@medecine.univ-mrs.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Jean Royauté</nom>
            <email>jean.royaute@lif.univ-mrs.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Michel Roux</nom>
            <email>michel.roux@medecine.univ-mrs.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIF / Université de la Méditerranée, 27 Bd Jean Moulin, 13005 Marseille</affiliation>
        </affiliations>
        <titre>Représentation évènementielle des déplacements dans des dépêches épidémiologiques</titre>
        <type>court</type>
        <pages/>
        <resume>La représentation évènementielle des déplacements de personnes dans des dépêches épidémiologiques est d’une grande importance pour une compréhension détaillée du sens de ces dépêches. La dissémination des composants d’une telle représentation dans les dépêches rend difficile l’accès à leurs contenus. Ce papier décrit un système d’extraction d’information utilisant des cascades de transducteurs à nombre d’états fini qui ont permis la réalisation de trois tâches : la reconnaissance des entités nommées, l’annotation et la représentation des composants ainsi que la représentation des structures évènementielles. Nous avons obtenu une moyenne de rappel de 80, 93% pour la reconnaissance des entités nommées et de 97, 88% pour la représentation des composants. Ensuite, nous avons effectué un travail de normalisation de cette représentation par la résolution de certaines anaphores pronominales. Nous avons obtenu une valeur moyenne de précision de 81, 72% pour cette résolution.</resume>
        <mots_cles>Sous-langage, représentation évènementielle, extraction d’information, structure prédicative, structure predicate-arguments</mots_cles>
        <title/>
        <abstract>The representation of motion events is important for an automatic comprehension of disease outbreak reports. The dispersion of components in this type of reports makes it difficult to have such a representation. This paper describes an automatic extraction of event structures representation of these texts.We built an information extraction system by using cascaded finite state transducers which allowed the realization of three tasks : the named entity recognition, the component annotation and representation and the event structure representation. We obtained a recall of 80, 93% for the named entity recognition task and a recall of 97, 88% for argument representation task. Thereafter, we worked in anaphoric pronouns resolution where we obtained a precision of 81.83%.</abstract>
        <keywords>Sublanguage, event structure representation, information extraction, predicative structure, predicate-arguments structure</keywords>
      </article>
      <article id="taln-2008-court-018" session="Poster">
        <auteurs>
          <auteur>
            <nom>Éric Wehrli</nom>
            <email>Eric.Wehrli@lettres.unige.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Luka Nerima</nom>
            <email>Luka.Nerima@lettres.unige.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LATL-Département de linguistique, Université de Genève</affiliation>
        </affiliations>
        <titre>Traduction multilingue : le projet MulTra</titre>
        <type>court</type>
        <pages/>
        <resume>L’augmentation rapide des échanges et des communications pluriculturels, en particulier sur internet, intensifie les besoins d’outils multilingues y compris de traduction. Cet article décrit un projet en cours au LATL pour le développement d’un système de traduction multilingue basé sur un modèle linguistique abstrait et largement générique, ainsi que sur un modèle logiciel basé sur la notion d’objet. Les langues envisagées dans la première phase de ce projet sont l’allemand, le français, l’italien, l’espagnol et l’anglais.</resume>
        <mots_cles>Traduction automatique multilingue, approche par objets, génération de lexiques bilingues</mots_cles>
        <title/>
        <abstract>The increase of cross-cultural communication triggered notably by the Internet intensifies the needs for multilingual linguistic tools, in particular translation systems for several languages. The LATL has developed an efficient multilingual parsing technology based on an abstract and generic linguistic model and on object-oriented software design. The proposed project intends to apply a similar approach to the problem of multilingual translation (German, French, Italian and English).</abstract>
        <keywords>Multilingual machine translation, object design, bilingual dictionary derivation</keywords>
      </article>
      <article id="taln-2008-court-019" session="Poster">
        <auteurs>
          <auteur>
            <nom>Erwan Moreau</nom>
            <email>emoreau@enst.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>François Yvon</nom>
            <email>yvon@limsi.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Olivier Cappé</nom>
            <email>cappe@enst.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Institut Télécom ParisTech &amp; LTCI CNRS</affiliation>
          <affiliation affiliationId="2">Univ. Paris Sud &amp; LIMSI CNRS</affiliation>
        </affiliations>
        <titre>Appariement d’entités nommées coréférentes : combinaisons de mesures de similarité par apprentissage supervisé</titre>
        <type>court</type>
        <pages/>
        <resume>L’appariement d’entités nommées consiste à regrouper les différentes formes sous lesquelles apparaît une entité. Pour cela, des mesures de similarité textuelle sont généralement utilisées. Nous proposons de combiner plusieurs mesures afin d’améliorer les performances de la tâche d’appariement. À l’aide d’expériences menées sur deux corpus, nous montrons la pertinence de l’apprentissage supervisé dans ce but, particulièrement avec l’algorithme C4.5.</resume>
        <mots_cles>Entités nommées, Appariement, Mesures de similarité textuelle, Apprentissage supervisé</mots_cles>
        <title/>
        <abstract>Matching named entities consists in grouping the different forms under which an entity may occur. Textual similarity measures are the usual tools for this task. We propose to combine several measures in order to improve the performance. We show the relevance of supervised learning in this objective through experiences with two corpora, especially in the case of the C4.5 algorithm.</abstract>
        <keywords>Named entities,Matching, Textual similaritymeasures, Supervised learning</keywords>
      </article>
      <article id="taln-2008-court-020" session="Poster">
        <auteurs>
          <auteur>
            <nom>Renaud Marlet</nom>
            <email>renaud.marlet@inria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LaBRI / INRIA Bordeaux – Sud-Ouest</affiliation>
        </affiliations>
        <titre>Un sens logique pour les graphes sémantiques</titre>
        <type>court</type>
        <pages/>
        <resume>Nous discutons du sens des graphes sémantiques, notamment de ceux utilisés en Théorie Sens-Texte. Nous leur donnons un sens précis, éventuellement sous-spécifié, grâce à une traduction simple vers une formule de Minimal Recursion Semantics qui couvre les cas de prédications multiples sur plusieurs entités, de prédication d’ordre supérieur et de modalités.</resume>
        <mots_cles>Graphe sémantique, logique, quantification, Théorie Sens-Texte (TST)</mots_cles>
        <title/>
        <abstract>We discuss the meaning of semantic graphs, in particular of those used in Meaning-Text Theory. We provide a precise, possibly underspecified, meaning to such graphs through a simple translation into a Minimal Recursion Semantics formula. This translation covers cases of multiple predications over several entities, higher order predication and modalities.</abstract>
        <keywords>Semantic graph, logic, quantification, Meaning-Text Theory (MTT)</keywords>
      </article>
      <article id="taln-2008-court-021" session="Poster">
        <auteurs>
          <auteur>
            <nom>Marie-Jean Meurs</nom>
            <email>marie-jean.meurs@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Frédéric Duvert</nom>
            <email>frederic.duvert@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Frédéric Béchet</nom>
            <email>frederic.bechet@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Fabrice Lefèvre</nom>
            <email>fabrice.lefevre@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Renato De Mori</nom>
            <email>renato.demori@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université d’Avignon et des Pays de Vaucluse, Laboratoire Informatique d’Avignon (EA 931), F-84911 Avignon, France.</affiliation>
        </affiliations>
        <titre>Annotation en Frames Sémantiques du corpus de dialogue MEDIA</titre>
        <type>court</type>
        <pages/>
        <resume>Cet article présente un formalisme de représentation des connaissances qui a été utilisé pour fournir des annotations sémantiques de haut niveau pour le corpus de dialogue oral MEDIA. Ces annotations en structures sémantiques, basées sur le paradigme FrameNet, sont obtenues de manière incrémentale et partiellement automatisée. Nous décrivons le processus d’interprétation automatique qui permet d’obtenir des compositions sémantiques et de générer des hypothèses de frames par inférence. Le corpus MEDIA est un corpus de dialogues en langue française dont les tours de parole de l’utilisateur ont été manuellement transcrits et annotés (niveaux mots et constituants sémantiques de base). Le processus proposé utilise ces niveaux pour produire une annotation de haut niveau en frames sémantiques. La base de connaissances développée (définitions des frames et règles de composition) est présentée, ainsi que les résultats de l’annotation automatique.</resume>
        <mots_cles>compréhension automatique de la parole, système de dialogue oral, frames sémantiques, décodage conceptuel, annotation sémantique, inférence sémantique</mots_cles>
        <title/>
        <abstract>This paper introduces a knowledge representation formalism, used for incremental and partially automated annotation of the French MEDIA dialogue corpus in terms of semantic structures. We describe an automatic interpretation process for composing semantic structures from basic semantic constituents using patterns involving constituents and words. The process has procedures for obtaining semantic compositions and generating frame hypotheses by inference. This process is applied to MEDIA, a dialogue corpus manually annotated at the word and semantic constituent levels, and thus produces a higher level semantic frame annotation. The Knowledge Source defined and the results obtained on the automatically-derived annotation are reported.</abstract>
        <keywords>spoken language understanding, spoken dialogue system, semantic structures, semantic frames, conceptual decoding, semantic annotation, semantic inference</keywords>
      </article>
      <article id="taln-2008-court-022" session="Poster">
        <auteurs>
          <auteur>
            <nom>Ramzi Abbès</nom>
            <email>ramzi.abbes@univ-lyon2.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Malek Boualem</nom>
            <email>malek.boualem@orange-ftgroup.com</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">ICAR-CNRS/ Lyon 2</affiliation>
          <affiliation affiliationId="2">Orange Labs – France Telecom R&amp;D</affiliation>
        </affiliations>
        <titre>Dissymétrie entre l'indexation des documents et le traitement des requêtes pour la recherche d’information en langue arabe</titre>
        <type>court</type>
        <pages/>
        <resume>Les moteurs de recherches sur le web produisent des résultats comparables et assez satisfaisants pour la recherche de documents écrits en caractères latins. Cependant, ils présentent de sérieuses lacunes dès que l'ont s'intéresse à des langues peu dotées ou des langues sémitiques comme l'arabe. Dans cet article nous présentons une étude analytique et qualitative de la recherche d’information en langue arabe en mettant l'accent sur l'insuffisance des outils de recherche actuels, souvent mal adaptés aux spécificités de la langue arabe. Pour argumenter notre analyse, nous présentons des résultats issus d’observations et de tests autour de certains phénomènes linguistiques de l’arabe écrit. Pour la validation des ces observations, nous avons testé essentiellement le moteur de recherche Google.</resume>
        <mots_cles>recherche d’information, langue arabe, indexation, lemmatisation, Google</mots_cles>
        <title/>
        <abstract>Web search engines provide quite good results for Latin characters-based languages. However, they still show many weaknesses when searching in other languages such as Arabic. This paper discusses a qualitative analysis of information retrieval in Arabic language, highlighting some of the numerous limitations of available search engines, mainly when they are not properly adapted to the Arabic language specificities. To argue our analysis, we present some results based on quite sufficient observations and tests on various Arabic linguistic phenomena. To validate these observations, we essentially have tested the Google search engine.</abstract>
        <keywords>information retrieval, Arabic, indexation, lemmatization, Google</keywords>
      </article>
    </articles>
  </conference>
  <conference>
    <edition>
      <acronyme>TALN'2009</acronyme>
      <titre>conférence sur le Traitement Automatique des Langues Naturelles</titre>
      <ville>Senlis</ville>
      <pays>France</pays>
      <dateDebut>2009-06-24</dateDebut>
      <dateFin>2009-06-26</dateFin>
      <presidents>
        <nom>Adeline Nazarenko</nom>
        <nom>Thierry Poibeau</nom>
      </presidents>
      <typeArticles>
        <type id="long">Papiers longs</type>
        <type id="position">Prise de position</type>
        <type id="court">Papiers courts</type>
        <type id="démonstration">Démonstrations</type>
      </typeArticles>
      <statistiques>
        <acceptations id="long" soumissions="108">29</acceptations>
        <acceptations id="position" soumissions="6">3</acceptations>
        <acceptations id="court" soumissions="108">46</acceptations>
      </statistiques>
      <siteWeb>http://lipn.univ-paris13.fr/taln09/</siteWeb>
      <meilleurArticle>
        <articleId>taln-2009-long-019</articleId>
      </meilleurArticle>
    </edition>
    <articles>
      <article id="taln-2009-long-001" session="">
        <auteurs>
          <auteur>
            <nom>Nabil Hathout</nom>
            <email>Nabil.Hathout@univ-tlse2.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université de Toulouse</affiliation>
        </affiliations>
        <titre>Acquisition morphologique à partir d’un dictionnaire informatisé</titre>
        <type>long</type>
        <pages/>
        <resume>L’article propose un modèle linguistique et informatique permettant de faire émerger la structure morphologique dérivationnelle du lexique à partir des régularités sémantiques et formelles des mots qu’il contient. Ce modèle est radicalement lexématique. La structure morphologique est constituée par les relations que chaque mot entretient avec les autres unités du lexique et notamment avec les mots de sa famille morphologique et de sa série dérivationnelle. Ces relations forment des paradigmes analogiques. La modélisation a été testée sur le lexique du français en utilisant le dictionnaire informatisé TLFi.</resume>
        <mots_cles>Morphologie dérivationnelle, morphologie lexématique, similarité morphologique, analogie formelle</mots_cles>
        <title/>
        <abstract>The paper presents a linguistic and computational model aiming at making the morphological structure of the lexicon emerge from the formal and semantic regularities of the words it contains. The model is word-based. The proposed morphological structure consists of (1) binary relations that connect each headword with words that are morphologically related, and especially with the members of its morphological family and its derivational series, and of (2) the analogies that hold between the words. The model has been tested on the lexicon of French using the TLFi machine readable dictionary.</abstract>
        <keywords>Derivational morphology, word-based morphology, morphological relatedness, formal analogy</keywords>
      </article>
      <article id="taln-2009-long-002" session="">
        <auteurs>
          <auteur>
            <nom>Joseph Le Roux</nom>
            <email>jleroux@computing.dcu.ie</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">NCLT, Dublin City University</affiliation>
        </affiliations>
        <titre>Analyse déductive pour les grammaires d’interaction</titre>
        <type>long</type>
        <pages/>
        <resume>Nous proposons un algorithme d’analyse pour les grammaires d’interaction qui utilise le cadre formel de l’analyse déductive. Cette approche donne un point de vue nouveau sur ce problème puisque les méthodes précédentes réduisaient ce dernier à la réécriture de graphes et utilisaient des techniques de résolution de contraintes. D’autre part, cette présentation permet de décrire le processus de manière standard et d’exhiber les sources d’indéterminisme qui rendent ce problème difficile.</resume>
        <mots_cles>Analyse syntaxique, grammaires d’interaction</mots_cles>
        <title/>
        <abstract>We propose a parsing algorithm for Interaction Grammars using the deductive parsing framework. This approach brings new perspectives on this problem, departing from previous methods relying on constraint-solving techniques to interpret it as a graph-rewriting problem. Furthermore, this presentation allows a standard description of the algorithm and a fine-grained inspection of the sources of non-determinism.</abstract>
        <keywords>Parsing, Interaction Grammars</keywords>
      </article>
      <article id="taln-2009-long-003" session="">
        <auteurs>
          <auteur>
            <nom>Alexis Nasr</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Frédéric Béchet</nom>
            <email/>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIF - CNRS - Université Aix Marseille</affiliation>
          <affiliation affiliationId="2">LIA - Université d’Avignon</affiliation>
        </affiliations>
        <titre>Analyse syntaxique en dépendances de l’oral spontané</titre>
        <type>long</type>
        <pages/>
        <resume>Cet article décrit un modèle d’analyse syntaxique de l’oral spontané axé sur la reconnaissance de cadres valenciels verbaux. Le modèle d’analyse se décompose en deux étapes : une étape générique, basée sur des ressources génériques du français et une étape de réordonnancement des solutions de l’analyseur réalisé par un modèle spécifique à une application. Le modèle est évalué sur le corpus MEDIA.</resume>
        <mots_cles>Analyse syntaxique, reconnaissance automatique de la parole</mots_cles>
        <title/>
        <abstract>We describe in this paper a syntactic parser for spontaneous speech geared towards the identification of verbal subcategorization frames. The parser proceeds in two stages. The first stage is based on generic syntactic ressources for French. The second stage is a reranker which is specially trained for a given application. The parser is evaluated on the MEDIA corpus.</abstract>
        <keywords>Syntactic parsing, automatic speech recognition</keywords>
      </article>
      <article id="taln-2009-long-004" session="">
        <auteurs>
          <auteur>
            <nom>Marie Candito</nom>
            <email>mcandito@linguist.jussieu.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Benoît Crabbé</nom>
            <email>bcrabbe@linguist.jussieu.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Pascal Denis</nom>
            <email>pascal.denis@inria.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>François Guérin</nom>
            <email>francois.guerin@inria.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université Paris 7/INRIA (Alpage), 30 rue du Château des Rentiers, 75013 Paris</affiliation>
          <affiliation affiliationId="2">INRIA (Alpage), Domaine de Voluceau Rocquencourt - B.P. 105 78153 Le Chesnay</affiliation>
        </affiliations>
        <titre>Analyse syntaxique du français : des constituants aux dépendances</titre>
        <type>long</type>
        <pages/>
        <resume>Cet article présente une technique d’analyse syntaxique statistique à la fois en constituants et en dépendances. L’analyse procède en ajoutant des étiquettes fonctionnelles aux sorties d’un analyseur en constituants, entraîné sur le French Treebank, pour permettre l’extraction de dépendances typées. D’une part, nous spécifions d’un point de vue formel et linguistique les structures de dépendances à produire, ainsi que la procédure de conversion du corpus en constituants (le French Treebank) vers un corpus cible annoté en dépendances, et partiellement validé. D’autre part, nous décrivons l’approche algorithmique qui permet de réaliser automatiquement le typage des dépendances. En particulier, nous nous focalisons sur les méthodes d’apprentissage discriminantes d’étiquetage en fonctions grammaticales.</resume>
        <mots_cles>Analyseur syntaxique statistique, analyse en constituants/dépendances, étiquetage en fonctions grammaticales</mots_cles>
        <title/>
        <abstract>This paper describes a technique for both constituent and dependency parsing. Parsing proceeds by adding functional labels to the output of a constituent parser trained on the French Treebank in order to further extract typed dependencies. On the one hand we specify on formal and linguistic grounds the nature of the dependencies to output as well as the conversion algorithm from the French Treebank to this dependency representation. On the other hand, we describe a class of algorithms that allows to perform the automatic labeling of the functions from the output of a constituent based parser. We specifically focus on discriminative learning methods for functional labelling.</abstract>
        <keywords>Statistical parsing, constituent/dependency parsing, grammatical function labeling</keywords>
      </article>
      <article id="taln-2009-long-005" session="">
        <auteurs>
          <auteur>
            <nom>Erwan Moreau</nom>
            <email>erwan.moreau@lipn.univ-paris13.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Isabelle Tellier</nom>
            <email>isabelle.tellier@univ-orleans.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Antonio Balvet</nom>
            <email/>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Grégoire Laurence</nom>
            <email/>
            <affiliationId>4</affiliationId>
          </auteur>
          <auteur>
            <nom>Antoine Rozenknop</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Thierry Poibeau</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIPN, université de Paris 13</affiliation>
          <affiliation affiliationId="2">LIFO, université d’Orléans</affiliation>
          <affiliation affiliationId="3">UMR STL 8163, université de Lille</affiliation>
          <affiliation affiliationId="4">LIFL, Inria Lille-nord Europe</affiliation>
        </affiliations>
        <titre>Annotation fonctionnelle de corpus arborés avec des Champs Aléatoires Conditionnels</titre>
        <type>long</type>
        <pages/>
        <resume>L’objectif de cet article est d’évaluer dans quelle mesure les “fonctions syntaxiques” qui figurent dans une partie du corpus arboré de Paris 7 sont apprenables à partir d’exemples. La technique d’apprentissage automatique employée pour cela fait appel aux “Champs Aléatoires Conditionnels” (Conditional Random Fields ou CRF), dans une variante adaptée à l’annotation d’arbres. Les expériences menées sont décrites en détail et analysées. Moyennant un bon paramétrage, elles atteignent une F1-mesure de plus de 80%.</resume>
        <mots_cles>fonctions syntaxiques, Conditional Random Fields, corpus arborés</mots_cles>
        <title/>
        <abstract>The purpose of this paper is to evaluatewhether the "syntactic functions" present in a part of the Paris 7 Treebank are learnable from examples. The learning technic used is the one of "Conditional Random Fields" (CRF), in an original variant adapted to tree labelling. The conducted experiments are extensively described and analyzed. With good parameters, a F1-mesure value of over 80% is reached.</abstract>
        <keywords>syntactic functions, Conditional Random Fields, Treebanks</keywords>
      </article>
      <article id="taln-2009-long-006" session="">
        <auteurs>
          <auteur>
            <nom>Emmanuel Morin</nom>
            <email>emmanuel.morin@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université de Nantes, LINA - UMR CNRS 6241, 2 rue de la Houssinière, BP 92208, 44322 Nantes Cedex 03</affiliation>
        </affiliations>
        <titre>Apport d’un corpus comparable déséquilibré à l’extraction de lexiques bilingues</titre>
        <type>long</type>
        <pages/>
        <resume>Les principaux travaux en extraction de lexiques bilingues à partir de corpus comparables reposent sur l’hypothèse implicite que ces corpus sont équilibrés. Cependant, les différentes méthodes computationnelles associées sont relativement insensibles à la taille de chaque partie du corpus. Dans ce contexte, nous étudions l’influence que peut avoir un corpus comparable déséquilibré sur la qualité des terminologies bilingues extraites à travers différentes expériences. Nos résultats montrent que sous certaines conditions l’utilisation d’un corpus comparable déséquilibré peut engendrer un gain significatif dans la qualité des lexiques extraits.</resume>
        <mots_cles>Multilinguisme, corpus comparable, extraction de lexiques bilingues</mots_cles>
        <title/>
        <abstract>The main work in bilingual lexicon extraction from comparable corpora is based on the implicit hypothesis that corpora are balanced. However, the different related approaches are relatively insensitive to sizes of each part of the comparable corpus. Within this context, we study the influence of unbalanced comparable corpora on the quality of bilingual terminology extraction through different experiments. Our results show the conditions under which the use of an unbalanced comparable corpus can induce a significant gain in the quality of extracted lexicons.</abstract>
        <keywords>Multilingualism, comparable corpus, bilingual lexicon extraction</keywords>
      </article>
      <article id="taln-2009-long-007" session="">
        <auteurs>
          <auteur>
            <nom>Eric Charton</nom>
            <email>eric.charton@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Juan-Manuel Torres-Moreno</nom>
            <email>juan-manuel.torres@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIA / Université d’Avignon, 339 chemin des Meinajariès, 84911 Avignon</affiliation>
        </affiliations>
        <titre>Classification d’un contenu encyclopédique en vue d’un étiquetage par entités nommées</titre>
        <type>long</type>
        <pages/>
        <resume>On utilise souvent des ressources lexicales externes pour améliorer les performances des systèmes d’étiquetage d’entités nommées. Les contenus de ces ressources lexicales peuvent être variés : liste de noms propres, de lieux, de marques. On note cependant que la disponibilité de corpus encyclopédiques exhaustifs et ouverts de grande taille tels que Worldnet ou Wikipedia, a fait émerger de nombreuses propositions spécifiques d’exploitation de ces contenus par des systèmes d’étiquetage. Un problème demeure néanmoins ouvert avec ces ressources : celui de l’adaptation de leur taxonomie interne, complexe et composée de dizaines de milliers catégories, aux exigences particulières de l’étiquetage des entités nommées. Pour ces dernières, au plus de quelques centaines de classes sémantiques sont requises. Dans cet article nous explorons cette difficulté et proposons un système complet de transformation d’un arbre taxonomique encyclopédique en une système à classe sémantiques adapté à l’étiquetage d’entités nommées.</resume>
        <mots_cles>Etiquetage, Entités nommées, classification, taxonomie</mots_cles>
        <title/>
        <abstract>The advent of Wikipedia and WordNet aroused new interest in labeling by named entity aided by external resources. The availability of these large, multilingual, comprehensive and open digital encyclopaedic corpora suggests the development of labeling solutions that exploit the knowledge contained in these corpora. The mapping of a word sequence to an encyclopedic document is possible, however the classification of encyclopaedic entities and their related labels, is not yet fully resolved. The inconsistency of an open encyclopaedic corpus such as Wikipedia, makes sometimes difficult establishing a relationship between its entities and a restricted taxonomy. In this article we explore this problem and propose a complete system to meet this need.</abstract>
        <keywords>Named entity recognition, classification, taxonomie</keywords>
      </article>
      <article id="taln-2009-long-008" session="">
        <auteurs>
          <auteur>
            <nom>Philippe Langlais</nom>
            <email>felipe@iro.umontreal.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Département d’Informatique et de Recherche Opérationnelle, Université de montreal, C.P. 6128 Suc. Centre-Ville, Montréal, H3C3J7, Qc, Canada</affiliation>
        </affiliations>
        <titre>Étude quantitative de liens entre l’analogie formelle et la morphologie constructionnelle</titre>
        <type>long</type>
        <pages/>
        <resume>Plusieurs travaux ont récemment étudié l’apport de l’apprentissage analogique dans des applications du traitement automatique des langues comme la traduction automatique, ou la recherche d’information. Il est souvent admis que les relations analogiques de forme entre les mots capturent des informations de nature morphologique. Le but de cette étude est de présenter une analyse des points de rencontre entre l’analyse morphologique et les analogies de forme. C’est à notre connaissance la première étude de ce type portant sur des corpus de grande taille et sur plusieurs langues. Bien que notre étude ne soit pas dédiée à une tâche particulière du traitement des langues, nous montrons cependant que le principe d’analogie permet de segmenter des mots en morphèmes avec une bonne précision.</resume>
        <mots_cles>Apprentissage analogique, analogie formelle, analyse morphologique</mots_cles>
        <title/>
        <abstract>Several studies recently showed the interest of analogical learning for Natural Language processing tasks such as Machine Translation and Information Retrieval. It is often admitted that formal analogies between words capture morphological information. The purpose of this study os to quantify the correlations between morphological analysis and formal analogies. This is to our knowledge the first attempt to conduct such a quantitative analysis on large datasets and for several languages. Although this paper was not geared toward tackling a specific natural language processing task, we show that segmenting a word token into morphemes can be accomplished with a good precision by a simple strategy relying solely on formal analogy.</abstract>
        <keywords>Analogical Learning, Formal Analogies, Morphological Analysis</keywords>
      </article>
      <article id="taln-2009-long-009" session="">
        <auteurs>
          <auteur>
            <nom>Thi-Ngoc-Diep Do</nom>
            <email>thi-ngoc-diep.do@imag.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Viet-Bac Le</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Brigitte Bigi</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Laurent Besacier</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Eric Castelli</nom>
            <email/>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire LIG, GETALP, Grenoble, France</affiliation>
          <affiliation affiliationId="2">Centre MICA, CNRS/UMI-2954, Hanoi, Vietnam</affiliation>
        </affiliations>
        <titre>Exploitation d’un corpus bilingue pour la création d’un système de traduction probabiliste Vietnamien - Français</titre>
        <type>long</type>
        <pages/>
        <resume>Cet article présente nos premiers travaux en vue de la construction d’un système de traduction probabiliste pour le couple de langue vietnamien-français. La langue vietnamienne étant considérée comme une langue peu dotée, une des difficultés réside dans la constitution des corpus parallèles, indispensable à l’apprentissage des modèles. Nous nous concentrons sur la constitution d’un grand corpus parallèle vietnamien-français. La méthode d’identification automatique des paires de documents parallèles fondée sur la date de publication, les mots spéciaux et les scores d’alignements des phrases est appliquée. Cet article présente également la construction d’un premier système de traduction automatique probabiliste vietnamienfrançais et français-vietnamien à partir de ce corpus et discute l’opportunité d’utiliser des unités lexicales ou sous-lexicales pour le vietnamien (syllabes, mots, ou leurs combinaisons). Les performances du système sont encourageantes et se comparent avantageusement à celles du système de Google.</resume>
        <mots_cles>traduction probabiliste, corpus bilingue, alignement de documents, table de traduction</mots_cles>
        <title/>
        <abstract>This paper presents our first attempt at constructing a Vietnamese-French statistical machine translation system. Since Vietnamese is considered as an under-resourced language, one of the difficulties is building a large Vietnamese-French parallel corpus, which is indispensable to train the models. We concentrate on building a large Vietnamese-French parallel corpus. The document alignment method based on publication date, special words and sentence alignment result is applied. The paper also presents an application of the obtained parallel corpus to the construction of a Vietnamese-French statistical machine translation system, where the use of different units for Vietnamese (syllables, words, or their combinations) is discussed. The performance of the system is encouraging and it compares favourably to that of Google Translate.</abstract>
        <keywords>statistical machine translation, bilingual corpus, document alignment, phrase table</keywords>
      </article>
      <article id="taln-2009-long-010" session="">
        <auteurs>
          <auteur>
            <nom>Emmanuel Prochasson</nom>
            <email>emmanuel.prochasson@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Emmanuel Morin</nom>
            <email>emmanuel.morin@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université de Nantes, LINA - UMR CNRS 6241, 2 rue de la Houssinière, BP 92208, 44322 Nantes Cedex 03</affiliation>
        </affiliations>
        <titre>Influence des points d’ancrage pour l’extraction lexicale bilingue à partir de corpus comparables spécialisés</titre>
        <type>long</type>
        <pages/>
        <resume>L’extraction de lexiques bilingues à partir de corpus comparables affiche de bonnes performances pour des corpus volumineux mais chute fortement pour des corpus d’une taille plus modeste. Pour pallier cette faiblesse, nous proposons une nouvelle contribution au processus d’alignement lexical à partir de corpus comparables spécialisés qui vise à renforcer la significativité des contextes lexicaux en s’appuyant sur le vocabulaire spécialisé du domaine étudié. Les expériences que nous avons réalisées en ce sens montrent qu’une meilleure prise en compte du vocabulaire spécialisé permet d’améliorer la qualité des lexiques extraits.</resume>
        <mots_cles>Corpus comparable, extraction de lexiques bilingues, points d’ancrage</mots_cles>
        <title/>
        <abstract>Bilingual lexicon extraction from comparable corpora gives good results for large corpora but drops significantly for small size corpora. In order to compensate this weakness, we suggest a new contribution dedicated to the lexical alignment from specialized comparable corpora that strengthens the representativeness of the lexical contexts based on domainspecific vocabulary. The experiments carried out in this way show that taking better account the specialized vocabulary induces a significant improvement in the quality of extracted lexicons.</abstract>
        <keywords>Comparable corpus, bilingual lexicon extraction, anchor points</keywords>
      </article>
      <article id="taln-2009-long-011" session="">
        <auteurs>
          <auteur>
            <nom>Stéphane Huet</nom>
            <email>huetstep@iro.umontreal.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Julien Bourdaillet</nom>
            <email>bourdaij@iro.umontreal.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Philippe Langlais</nom>
            <email>felipe@iro.umontreal.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">DIRO - Université de Montréal, C.P. 6128, succursale centre-ville, H3C 3J7, Montréal, Québec, Canada</affiliation>
        </affiliations>
        <titre>Intégration de l’alignement de mots dans le concordancier bilingue TransSearch</titre>
        <type>long</type>
        <pages/>
        <resume>Malgré les nombreuses études visant à améliorer la traduction automatique, la traduction assistée par ordinateur reste la solution préférée des traducteurs lorsqu’une sortie de qualité est recherchée. Dans cet article, nous présentons nos travaux menés dans le but d’améliorer le concordancier bilingue TransSearch. Ce service, accessible sur le Web, repose principalement sur un alignement au niveau des phrases. Dans cette étude, nous discutons et évaluons l’intégration d’un alignement statistique au niveau des mots. Nous présentons deux nouvelles problématiques essentielles au succès de notre nouveau prototype : la détection des traductions erronées et le regroupement des variantes de traduction similaires.</resume>
        <mots_cles>alignement au niveau des mots, concordancier bilingue, traduction automatique</mots_cles>
        <title/>
        <abstract>Despite the impressive amount of recent studies devoted to improving the state of the art of machine translation, computer assisted translation tools remain the preferred solution of human translators when publication quality is of concern. In this paper, we present our ongoing efforts conducted within a project which aims at improving the commercial bilingual concordancer TransSearch. The core technology of this Web-based service mainly relies on sentence-level alignment. In this study, we discuss and evaluate the embedding of statistical word-level alignment. Two novel issues that are essential to the success of our new prototype are tackled: detecting erroneous translations and grouping together similar translations.</abstract>
        <keywords>word-level alignment, bilingual concordancer, machine translation</keywords>
      </article>
      <article id="taln-2009-long-012" session="">
        <auteurs>
          <auteur>
            <nom>Agata Jackiewicz</nom>
            <email>Agata.Jackiewicz@paris-sorbonne.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Thierry Charnois</nom>
            <email>Thierry.Charnois@info.unicaen.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Stéphane Ferrari</nom>
            <email>Stephane.Ferrari@info.unicaen.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LALIC – Université Paris-Sorbonne</affiliation>
          <affiliation affiliationId="2">GREYC – Université de Caen Basse-Normandie</affiliation>
        </affiliations>
        <titre>Jugements d'évaluation et constituants périphériques</titre>
        <type>long</type>
        <pages/>
        <resume>L’article présente une étude portant sur des constituants détachés à valeur axiologique. Dans un premier temps, une analyse linguistique sur corpus met en évidence un ensemble de patrons caractéristiques du phénomène. Ensuite, une expérimentation informatique est proposée sur un corpus de plus grande taille afin de permettre l’observation des patrons en vue d’un retour sur le modèle linguistique. Ce travail s’inscrit dans un projet mené à l’interface de la linguistique et du TAL, qui se donne pour but d’enrichir, d’adapter au français et de formaliser le modèle général Appraisal de l’évaluation dans la langue.</resume>
        <mots_cles>jugement d’évaluation, constituants extra-prédicatifs, constructions et lexiques subjectifs, Appraisal, implémentation informatique, portraits et biographies dans la presse de spécialité et la presse d’information</mots_cles>
        <title/>
        <abstract>In this paper, we present a study about peripheral constituent expressing some axiological value. First, a linguistic corpus analysis highlights some characteristic patterns for this phenomenon. Then, a computational experiment is carried out on a larger corpus in order to enable the observation of these patterns and to get a feedback on the linguistic model. This work takes part in a project at the intersection of Linguistics and NLP, which aims at enhancing, adapting to French language and formalizing the Appraisal generic model of the evaluation in language.</abstract>
        <keywords>evaluative judgement, peripheral constituent, evaluative constructions and lexicon, Appraisal, computational implementation, biographies and portraits in specialized press and newspapers</keywords>
      </article>
      <article id="taln-2009-long-013" session="">
        <auteurs>
          <auteur>
            <nom>François Portet</nom>
            <email>portet@imag.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Albert Gatt</nom>
            <email>a.gatt@abdn.ac.uk</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Jim Hunter</nom>
            <email>j.hunter@abdn.ac.uk</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Ehud Reiter</nom>
            <email>e.reiter@abdn.ac.uk</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Somayajulu Sripada</nom>
            <email>yaji.sripada@abdn.ac.uk</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Department of Computing Science, University of Aberdeen, Écosse</affiliation>
          <affiliation affiliationId="2">LIG, UMR 5217, Université de Grenoble, France</affiliation>
        </affiliations>
        <titre>Le projet BabyTalk : génération de texte à partir de données hétérogènes pour la prise de décision en unité néonatale</titre>
        <type>long</type>
        <pages/>
        <resume>Notre société génère une masse d’information toujours croissante, que ce soit en médecine, en météorologie, etc. La méthode la plus employée pour analyser ces données est de les résumer sous forme graphique. Cependant, il a été démontré qu'un résumé textuel est aussi un mode de présentation efficace. L'objectif du prototype BT-45, développé dans le cadre du projet Babytalk, est de générer des résumés de 45 minutes de signaux physiologiques continus et d'événements temporels discrets en unité néonatale de soins intensifs (NICU). L'article présente l'aspect génération de texte de ce prototype. Une expérimentation clinique a montré que les résumés humains améliorent la prise de décision par rapport à l'approche graphique, tandis que les textes de BT-45 donnent des résultats similaires à l’approche graphique. Une analyse a identifié certaines des limitations de BT-45 mais en dépit de cellesci, notre travail montre qu'il est possible de produire automatiquement des résumés textuels efficaces de données complexes.</resume>
        <mots_cles>Traitement automatique des langues naturelles, Génération de texte, Analyse de données, Unité de soins intensifs, Systèmes d'aide à la décision</mots_cles>
        <title/>
        <abstract>Nowadays large amount of data is produced every day in medicine, meteorology and other areas and the most common approach to analyse such data is to present it graphically. However, it has been shown that textual summarisation is also an effective approach. As part of the BabyTalk project, the prototype BT-45 was developed to generate summaries of 45 minutes of continuous physiological signals and discrete temporal events in a neonatal intensive care unit (NICU). The paper presents its architecture with an emphasis on its natural language generation part. A clinical experiment showed that human textual summaries led to better decision making than graphical presentation, whereas BT-45 texts led to similar results as visualisations. An analysis identified some of the reasons for the BT-45 texts inferiority, but, despite these deficiencies, our work shows that it is possible for computer systems to generate effective textual summaries of complex data.</abstract>
        <keywords>Natural language processing, Natural language generation, Intelligent data analysis, Intensive care unit, Decision support systems</keywords>
      </article>
      <article id="taln-2009-long-014" session="">
        <auteurs>
          <auteur>
            <nom>Bruno Cartoni</nom>
            <email>cartonib@gmail.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université de Genève</affiliation>
        </affiliations>
        <titre>Les adjectifs relationnels dans les lexiques informatisés : formalisation et exploitation dans un contexte multilingue</titre>
        <type>long</type>
        <pages/>
        <resume>Dans cet article, nous nous intéressons aux adjectifs dits relationnels et à leur statut en traitement automatique des langues naturelles (TALN). Nous montrons qu’ils constituent une « sous-classe » d’adjectifs rarement explicitée et donc rarement représentée dans les lexiques sur lesquels reposent les applications du TALN, alors qu’ils jouent un rôle important dans de nombreuses applications. Leur formation morphologique est source d’importantes divergences entre différentes langues, et c’est pourquoi ces adjectifs sont un véritable défi pour les applications informatiques multilingues. Dans une partie plus pratique, nous proposons une formalisation de ces adjectifs permettant de rendre compte de leurs liens avec leur base nominale. Nous tentons d’extraire ces informations dans les lexiques informatisés existants, puis nous les exploitons pour traduire les adjectifs relationnels préfixés de l’italien en français.</resume>
        <mots_cles>Adjectifs relationnels, ressources lexicales, morphologie constructionnelle</mots_cles>
        <title/>
        <abstract>This article focuses on a particular type of adjectives, relational adjectives, and especially on the way they are processed in natural language processing systems. We show that this class of adjectives is barely recorded in an explicit manner in computer lexicons. There is an important discrepancy in the way those adjectives are morphologically constructed in different languages, and therefore, they are a real challenge for multilingual computing applications. On a more practical side, we propose a formalisation for the adjectives that shows their semantic link with their nominal base. We make an attempt to extract this kind of information in existing machine lexica, and we exploit their semantic links in the translation of prefixed relational adjectives from Italian into French.</abstract>
        <keywords>Relational adjectives, lexical resources, constructional morphology</keywords>
      </article>
      <article id="taln-2009-long-015" session="">
        <auteurs>
          <auteur>
            <nom>Marc Plantevit</nom>
            <email>Marc.Plantevit@info.unicaen.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Thierry Charnois</nom>
            <email>Thierry.Charnois@info.unicaen.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">GREYC – CNRS UMR 6072, Université de Caen – Bd Mal. Juin, 14032 CAEN Cedex</affiliation>
        </affiliations>
        <titre>Motifs séquentiels pour l’extraction d’information : illustration sur le problème de la détection d’interactions entre gènes</titre>
        <type>long</type>
        <pages/>
        <resume>Face à la prolifération des publications en biologie et médecine (plus de 18 millions de publications actuellement recensées dans PubMed), l’extraction d’information automatique est devenue un enjeu crucial. Il existe de nombreux travaux dans le domaine du traitement de la langue appliquée à la biomédecine ("BioNLP"). Ces travaux se distribuent en deux grandes tendances. La première est fondée sur les méthodes d’apprentissage automatique de type numérique qui donnent de bons résultats mais ont un fonctionnement de type "boite noire". La deuxième tendance est celle du TALN à base d’analyses (lexicales, syntaxiques, voire sémantiques ou discursives) coûteuses en temps de développement des ressources nécessaires (lexiques, grammaires, etc.). Nous proposons dans cet article une approche basée sur la découverte de motifs séquentiels pour apprendre automatiquement les ressources linguistiques, en l’occurrence les patrons linguistiques qui permettent l’extraction de l’information dans les textes. Plusieurs aspects méritent d’être soulignés : cette approche permet de s’affranchir de l’analyse syntaxique de la phrase, elle ne nécessite pas de ressources en dehors du corpus d’apprentissage et elle ne demande que très peu d’intervention manuelle. Nous illustrons l’approche sur le problème de la détection d’interactions entre gènes et donnons les résultats obtenus sur des corpus biologiques qui montrent l’intérêt de ce type d’approche.</resume>
        <mots_cles>Extraction d’information, fouille de textes, motifs séquentiels, interactions entre gènes</mots_cles>
        <title/>
        <abstract>The proliferation of publications in biology andmedicine (more than 18million publications currently listed in PubMed) has lead to the crucial need of automatic information extraction. There are many work in the field of natural language processing applied to biomedicine (BioNLP). Two types of approaches tackle this problem. On the one hand, machine learning based approaches give good results but run as a "black box". On the second hand, NLP based approaches are highly time consuming for developing the resources (lexicons, grammars, etc.). In this paper, we propose an approach based on sequential pattern mining to automatically discover linguistic patterns that allow the information extraction in texts. This approach allows to overcome sentence parsing and it does not require resources outside the training data set. We illustrate the approach on the problem of detecting interactions between genes and give the results obtained on biological corpora that show the relevance of this type of approach.</abstract>
        <keywords>Information extraction, text mining, sequential patterns, gene interactions</keywords>
      </article>
      <article id="taln-2009-long-016" session="">
        <auteurs>
          <auteur>
            <nom>Aurélien Max</nom>
            <email>aurelien.max@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Rafik Maklhoufi</nom>
            <email>rafik.makhloufi@utt.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Philippe Langlais</nom>
            <email>felipe@iro.umontreal.ca</email>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIMSI-CNRS et Université Paris-Sud 11, Orsay, France</affiliation>
          <affiliation affiliationId="2">Université de Technologie de Troyes, France</affiliation>
          <affiliation affiliationId="3">DIRO, Université de Montréal, Canada</affiliation>
        </affiliations>
        <titre>Prise en compte de dépendances syntaxiques pour la traduction contextuelle de segments</titre>
        <type>long</type>
        <pages/>
        <resume>Dans un système standard de traduction statistique basé sur les segments, le score attribué aux différentes traductions d’un segment ne dépend pas du contexte dans lequel il apparaît. Plusieurs travaux récents tendent à montrer l’intérêt de prendre en compte le contexte source lors de la traduction, mais ces études portent sur des systèmes traduisant vers l’anglais, une langue faiblement fléchie. Dans cet article, nous décrivons nos expériences sur la prise en compte du contexte source dans un système statistique traduisant de l’anglais vers le français, basé sur l’approche proposée par Stroppa et al. (2007). Nous étudions l’impact de différents types d’indices capturant l’information contextuelle, dont des dépendances syntaxiques typées. Si les mesures automatiques d’évaluation de la qualité d’une traduction ne révèlent pas de gains significatifs de notre système par rapport à un système à l’état de l’art ne faisant pas usage du contexte, une évaluation manuelle conduite sur 100 phrases choisies aléatoirement est en faveur de notre système. Cette évaluation fait également ressortir que la prise en compte de certaines dépendances syntaxiques est bénéfique à notre système.</resume>
        <mots_cles>Traduction automatique statistique, contexte source, dépendances syntaxiques</mots_cles>
        <title/>
        <abstract>In standard phrase-based Statistical Machine Translation (PBSMT) systems, the score associated with each translation of a phrase does not depend on its context. While several works have shown the potential gain of exploiting source context, they all considered English, a morphologically poor language, as the target language. In this article, we describe experiments on exploiting the source context in an English -&gt; French PBSMT system, inspired by the work of Stroppa et al. (2007). We report a study on the impact of various types of features that capture contextual information, including syntactic dependencies. While automatic metrics do not show significative gains relative to a baseline system, a manual evaluation of 100 randomly selected sentences concludes that our context-aware system performs consistently better. This evaluation also shows that some types of syntactic dependencies can participate to the gains observed.</abstract>
        <keywords>Statistical Machine Translation, source context, syntactic dependencies</keywords>
      </article>
      <article id="taln-2009-long-017" session="">
        <auteurs>
          <auteur>
            <nom>Maud Ehrmann</nom>
            <email>Maud.Ehrmann@xrce.xerox.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Caroline Hagège</nom>
            <email>Caroline.Hagege@xrce.xerox.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Xerox Research Center Europe - XRCE, 6, Chemin de Maupertuis, 38240 Meylan</affiliation>
        </affiliations>
        <titre>Proposition de caractérisation et de typage des expressions temporelles en contexte</titre>
        <type>long</type>
        <pages/>
        <resume>Nous assistons actuellement en TAL à un regain d’intérêt pour le traitement de la temporalité véhiculée par les textes. Dans cet article, nous présentons une proposition de caractérisation et de typage des expressions temporelles tenant compte des travaux effectués dans ce domaine tout en cherchant à pallier les manques et incomplétudes de certains de ces travaux. Nous explicitons comment nous nous situons par rapport à l’existant et les raisons pour lesquelles parfois nous nous en démarquons. Le typage que nous définissons met en évidence de réelles différences dans l’interprétation et le mode de résolution référentielle d’expressions qui, en surface, paraissent similaires ou identiques. Nous proposons un ensemble des critères objectifs et linguistiquement motivés permettant de reconnaître, de segmenter et de typer ces expressions. Nous verrons que cela ne peut se réaliser sans considérer les procès auxquels ces expressions sont associées et un contexte parfois éloigné.</resume>
        <mots_cles>Temporalité, typage et caractérisation des expressions temporelles</mots_cles>
        <title/>
        <abstract>Temporal processing in texts is a topic of renewed interest in NLP. In this paper we present a new way of typing temporal expressions that takes into account both the state of the art of this domain and that also tries to be more precise and accurate that some of the current proposals. We explain into what extent our proposal is compatible and comparable with the state-of-the art and why sometimes we stray from it. The typing system that we define highlights real differences in the interpretation and reference calculus of these expressions. At the same time, by offering objective criteria, it fulfils the necessity of high inter-agreement between annotators. After having defined what we consider as temporal expressions, we will show that tokenization, characterization and typing of those expressions can only be done having into account processes to which these expressions are linked.</abstract>
        <keywords>temporal processing, temporal expressions characterization and typing</keywords>
      </article>
      <article id="taln-2009-long-018" session="">
        <auteurs>
          <auteur>
            <nom>Yves Bestgen</nom>
            <email>yves.bestgen@psp.ucl.ac.be</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CECL / PSOR – Université catholique de Louvain, Place du Cardinal Mercier, 10 — B-1348 Louvain-la-Neuve — Belgique</affiliation>
        </affiliations>
        <titre>Quel indice pour mesurer l'efficacité en segmentation de textes?</titre>
        <type>long</type>
        <pages/>
        <resume>L'évaluation de l'efficacité d'algorithmes de segmentation thématique est généralement effectuée en quantifiant le degré d'accord entre une segmentation hypothétique et une segmentation de référence. Les indices classiques de précision et de rappel étant peu adaptés à ce domaine, WindowDiff (Pevzner, Hearst, 2002) s'est imposé comme l'indice de référence. Une analyse de cet indice montre toutefois qu'il présente plusieurs limitations. L'objectif de ce rapport est d'évaluer un indice proposé par Bookstein, Kulyukin et Raita (2002), la distance de Hamming généralisée, qui est susceptible de remédier à celles-ci. Les analyses montrent que celui-ci conserve tous les avantages de WindowDiff sans les limitations. De plus, contrairement à WindowDiff, il présente une interprétation simple puisqu'il correspond à une vraie distance entre les deux segmentations à comparer.</resume>
        <mots_cles>Segmentation thématique, évaluation, distance de Hamming généralisée, WindowDiff</mots_cles>
        <title/>
        <abstract>The evaluation of thematic segmentation algorithms is generally carried out by quantifying the degree of agreement between a hypothetical segmentation and a gold standard. The traditional indices of precision and recall being little adapted to this field, WindowDiff (Pevzner, Hearst, 2002) has become the standard for this kind of assessment. An analysis of this index shows however that it presents several limitations. The objective of this report is to evaluate an index developed by Bookstein, Kulyukin and Raita (2002), the Generalized Hamming Distance, which is likely to overcome these limitations. The analyzes show that it preserves all the advantages of WindowDiff without its limitations. Moreover, contrary to WindowDiff, it presents a simple interpretation since it corresponds to a true distance between the two segmentations.</abstract>
        <keywords>Thematic segmentation, evaluation, generalized Hamming distance, WindowDiff</keywords>
      </article>
      <article id="taln-2009-long-019" session="">
        <auteurs>
          <auteur>
            <nom>Marion Laignelet</nom>
            <email>marion.laignelet@univ-tlse2.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>François Rioult</nom>
            <email>Francois.Rioult@info.unicaen.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CLLE-ERSS / Université de Toulouse 2 Le Mirail</affiliation>
          <affiliation affiliationId="2">GREYC (CNRS UMR6072), Université de Caen Basse-Normandie</affiliation>
        </affiliations>
        <titre>Repérer automatiquement les segments obsolescents à l’aide d’indices sémantiques et discursifs</titre>
        <type>long</type>
        <pages/>
        <resume>Cet article vise la description et le repérage automatique des segments d’obsolescence dans les documents de type encyclopédique. Nous supposons que des indices sémantiques et discursifs peuvent permettre le repérage de tels segments. Pour ce faire, nous travaillons sur un corpus annoté manuellement par des experts sur lequel nous projetons des indices repérés automatiquement. Les techniques statistiques de base ne permettent pas d’expliquer ce phénomène complexe. Nous proposons l’utilisation de techniques de fouille de données pour le caractériser et nous évaluons le pouvoir prédictif de nos indices. Nous montrons, à l’aide de techniques de classification supervisée et de calcul de l’aire sous la courbe ROC, que nos hypothèses sont pertinentes.</resume>
        <mots_cles>repérage automatique de l’obsolescence, indices sémantiques et discursifs, textes encyclopédiques, classification supervisée, aire sous la courbe ROC</mots_cles>
        <title/>
        <abstract>This paper deals with the description and automatic tracking of obsolescence in encyclopedic type of documents. We suppose that semantic and discursive cues may allow the tracking of these segments. For that purpose, we have worked on an expert manually annotated corpus, on which we have projected automatically tracked cues. Basic statistic techniques can not account for this complex phenomenon. We propose the use of techniques of data mining to characterize it, and we evaluate the predictive power of our cues. We show, using techniques of supervised classification and area under the ROC curve, that our hypotheses are relevant.</abstract>
        <keywords>automatic tracking of obsolescence, semantic and discursive cues, encyclopedic type of documents, supervised classification, area under the ROC curve</keywords>
      </article>
      <article id="taln-2009-long-020" session="">
        <auteurs>
          <auteur>
            <nom>Michel Généreux</nom>
            <email>Michel.Genereux@lipn.univ-paris13.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Aurélien Bossard</nom>
            <email>Aurelien.Bossard@lipn.univ-paris13.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire d’Informatique de Paris-Nord (CNRS UMR 7030 et Université Paris 13), 99, av. J.-B. Clément – 93430 Villetaneuse</affiliation>
        </affiliations>
        <titre>Résumé automatique de textes d’opinions</titre>
        <type>long</type>
        <pages/>
        <resume>Le traitement des langues fait face à une demande croissante en matière d’analyse de textes véhiculant des critiques ou des opinions. Nous présentons ici un système de résumé automatique tourné vers l’analyse d’articles postés sur des blogues, où sont exprimées à la fois des informations factuelles et des prises de position sur les faits considérés. Nous montrons qu’une approche classique à base de traits de surface est tout à fait efficace dans ce cadre. Le système est évalué à travers une participation à la campagne d’évaluation internationale TAC (Text Analysis Conference) où notre système a réalisé des performances satisfaisantes.</resume>
        <mots_cles>résumé automatique, analyse de textes subjectifs, évaluation automatique</mots_cles>
        <title/>
        <abstract>There is currently a growing need concerning the analysis of texts expressing opinions or judgements. In this paper, we present a summarization system that is specifically designed to process blog posts, where factual information is mixed with opinions. We show that a classical approach based on surface cues is efficient to summarize this kind of texts. The system is evaluated through a participation to TAC (Text Analysis Conference), an international evaluation framework for automatic summarization, in which our system obtained good results.</abstract>
        <keywords>automatic summarization, analysis of subjective texts, automatic evaluation</keywords>
      </article>
      <article id="taln-2009-long-021" session="">
        <auteurs>
          <auteur>
            <nom>Ingrid Falk</nom>
            <email>ingrid.falk@loria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Claire Gardent</nom>
            <email>claire.gardent@loria.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Evelyne Jacquey</nom>
            <email>evelyne.jacquey@atilf.fr</email>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Fabienne Venant</nom>
            <email>fabienne.venant@loria.fr</email>
            <affiliationId>4</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">INRIA / Université Nancy 2</affiliation>
          <affiliation affiliationId="2">CNRS / LORIA, Nancy</affiliation>
          <affiliation affiliationId="3">CNRS / ATILF, Nancy</affiliation>
          <affiliation affiliationId="4">Université Nancy 2</affiliation>
        </affiliations>
        <titre>Sens, synonymes et définitions</titre>
        <type>long</type>
        <pages/>
        <resume>Cet article décrit une méthodologie visant la réalisation d’une ressource sémantique en français centrée sur la synonymie. De manière complémentaire aux travaux existants, la méthode proposée n’a pas seulement pour objectif d’établir des liens de synonymie entre lexèmes, mais également d’apparier les sens possibles d’un lexème avec les ensembles de synonymes appropriés. En pratique, les sens possibles des lexèmes proviennent des définitions du TLFi et les synonymes de cinq dictionnaires accessibles à l’ATILF. Pour évaluer la méthode d’appariement entre sens d’un lexème et ensemble de synonymes, une ressource de référence a été réalisée pour 27 verbes du français par quatre lexicographes qui ont spécifié manuellement l’association entre verbe, sens (définition TLFi) et ensemble de synonymes. Relativement à ce standard étalon, la méthode d’appariement affiche une F-mesure de 0.706 lorsque l’ensemble des paramètres est pris en compte, notamment la distinction pronominal / non-pronominal pour les verbes du français et de 0.602 sans cette distinction.</resume>
        <mots_cles/>
        <title/>
        <abstract>We present a method for grouping the synonyms of a word into sets representing the possible meanings of that word. The possible meanings are given by the definitions of a general dictionary for French, the TLFi (Trésor de la langue française informatisé) and the method is applied to the synonyms of 5 synonym dictionnaries. To evaluate the method, we manually constructed a gold standard where for each (word, definition) pair, 4 lexicographers specified the set of synonyms they judge adequate. The method scores an F-measure of 0.602 when no distinction is made between pronominal and non-pronominal use and 0.706 when it is.</abstract>
        <keywords/>
      </article>
      <article id="taln-2009-long-022" session="">
        <auteurs>
          <auteur>
            <nom>Étienne Ailloud</nom>
            <email>ailloud@cl.uzh.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Manfred Klenner</nom>
            <email>klenner@cl.uzh.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Institute of Computational Linguistics, Zurich University, Zurich, Switzerland</affiliation>
        </affiliations>
        <titre>Vers des contraintes plus linguistiques en résolution de coréférences</titre>
        <type>long</type>
        <pages/>
        <resume>Nous proposons un modèle filtrant de résolution de coréférences basé sur les notions de transitivité et d’exclusivité linguistique. À partir de l’hypothèse générale que les chaînes de coréférence demeurent cohérentes tout au long d’un texte, notre modèle assure le respect de certaines contraintes linguistiques (via des filtres) quant à la coréférence, ce qui améliore la résolution globale. Le filtrage a lieu à différentes étapes de l’approche standard (c-à-d. par apprentissage automatique), y compris avant l’apprentissage et avant la classification, accélérant et améliorant ce processus.</resume>
        <mots_cles>Résolution de coréférences, apprentissage automatique, linguistique informatique par contraintes</mots_cles>
        <title/>
        <abstract>We propose a filter model of coreference resolution that is based on the notions of transitivity and linguistic exclusivity. Starting from the general assumption that coreference sets remain coherent throughout a text, our model enforces the checking of some compatibility criteria (filters) between coreference candidates, thereby improving resolution performance. This filtering is achieved at different stages of the workflow of machine-learning-based coreference resolution, including at the standard learning and testing steps, where it may help reduce the computational load and better distribute the actual occurrences to be learned.</abstract>
        <keywords>Coreference resolution, Machine learning, Constraint-based NLP</keywords>
      </article>
      <article id="taln-2009-long-023" session="">
        <auteurs>
          <auteur>
            <nom>Lionel Nicolas</nom>
            <email>lnicolas@i3s.unice.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Benoît Sagot</nom>
            <email>benoit.sagot@inria.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Miguel A. Molinero</nom>
            <email>mmolinero@udc.es</email>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Jacques Farré</nom>
            <email>jf@i3s.unice.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Éric de La Clergerie</nom>
            <email>Eric.De_La_Clergerie@inria.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Équipe RL, Laboratoire I3S, UNSA+CNRS, France</affiliation>
          <affiliation affiliationId="2">Projet ALPAGE, INRIA Rocquencourt + Paris 7, France</affiliation>
          <affiliation affiliationId="3">Grupo LYS, Univ. de A Coruña, España</affiliation>
        </affiliations>
        <titre>Trouver et confondre les coupables : un processus sophistiqué de correction de lexique</titre>
        <type>long</type>
        <pages/>
        <resume>La couverture d’un analyseur syntaxique dépend avant tout de la grammaire et du lexique sur lequel il repose. Le développement d’un lexique complet et précis est une tâche ardue et de longue haleine, surtout lorsque le lexique atteint un certain niveau de qualité et de couverture. Dans cet article, nous présentons un processus capable de détecter automatiquement les entrées manquantes ou incomplètes d’un lexique, et de suggérer des corrections pour ces entrées. La détection se réalise au moyen de deux techniques reposant soit sur un modèle statistique, soit sur les informations fournies par un étiqueteur syntaxique. Les hypothèses de corrections pour les entrées lexicales détectées sont générées en étudiant les modifications qui permettent d’améliorer le taux d’analyse des phrases dans lesquelles ces entrées apparaissent. Le processus global met en oeuvre plusieurs techniques utilisant divers outils tels que des étiqueteurs et des analyseurs syntaxiques ou des classifieurs d’entropie. Son application au Lefff , un lexique morphologique et syntaxique à large couverture du français, nous a déjà permis de réaliser des améliorations notables.</resume>
        <mots_cles>Acquisition et correction lexicale, lexique à large couverture, fouille d’erreurs, étiqueteur syntaxique, classifieur d’entropie, analyseur syntaxique</mots_cles>
        <title/>
        <abstract>The coverage of a parser depends mostly on the quality of the underlying grammar and lexicon. The development of a lexicon both complete and accurate is an intricate and demanding task, overall when achieving a certain level of quality and coverage. We introduce an automatic process able to detect missing or incomplete entries in a lexicon, and to suggest corrections hypotheses for these entries. The detection of dubious lexical entries is tackled by two techniques relying either on a specific statistical model, or on the information provided by a part-of-speech tagger. The generation of correction hypotheses for the detected entries is achieved by studying which modifications could improve the parse rate of the sentences in which the entries occur. This process brings together various techniques based on different tools such as taggers, parsers and entropy classifiers. Applying it on the Lefff , a large-coverage morphological and syntactic French lexicon, has already allowed us to perfom noticeable improvements.</abstract>
        <keywords>Lexical acquisition and correction, wide coverage lexicon, error mining, tagger, entropy classifier, syntactic parser</keywords>
      </article>
      <article id="taln-2009-long-024" session="">
        <auteurs>
          <auteur>
            <nom>François Trouilleux</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LRL, Université Blaise-Pascal</affiliation>
        </affiliations>
        <titre>Un analyseur de surface non déterministe pour le français</titre>
        <type>long</type>
        <pages/>
        <resume>Les analyseurs syntaxiques de surface à base de règles se caractérisent par un processus en deux temps : désambiguïsation lexicale, puis reconnaissance de patrons. Considérant que ces deux étapes introduisent une certaine redondance dans la description linguistique et une dilution des heuristiques dans les différents processus, nous proposons de définir un analyseur de surface qui fonctionne sur une entrée non désambiguïsée et produise l’ensemble des analyses possibles en termes de syntagmes noyau (chunks). L’analyseur, implanté avec NooJ, repose sur la définition de patrons étendus qui annotent des séquences de syntagmes noyau. Les résultats obtenus sur un corpus de développement d’environ 22 500 mots, avec un rappel proche de 100 %, montrent la faisabilité de l’approche et signalent quelques points d’ambiguïté à étudier plus particulièrement pour améliorer la précision.</resume>
        <mots_cles>Analyse syntaxique de surface, automates à états finis, déterminisme, désambiguïsation</mots_cles>
        <title/>
        <abstract>Rule-based chunkers are characterized by a two-tier process : part-of-speech disambiguation, and pattern matching. Considering that these two stages introduce some redundancy in the linguistic description and a dilution of heuristics over the different processes, we propose to define a chunker which parses a non-disambiguated input, and produces all possible analysis in terms of chunks. The parser, implemented with NooJ, relies on the definition of extended patterns, which annotate sequences of chunks. The results obtained on an approx. 22500 word corpus, with almost 100 % recall, demonstrate the feasability of the approach, and signal which ambiguities should be further studied in order to improve precision.</abstract>
        <keywords>Chunking, finite-state automata, determinism, disambiguation</keywords>
      </article>
      <article id="taln-2009-long-025" session="">
        <auteurs>
          <auteur>
            <nom>Aurélien Bossard</nom>
            <email>aurelien.bossard@lipn.univ-paris13.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIPN - UMR 7030, CNRS - Université Paris 13, F-93430 Villetaneuse, France</affiliation>
        </affiliations>
        <titre>Une approche mixte-statistique et structurelle - pour le résumé automatique de dépêches</titre>
        <type>long</type>
        <pages/>
        <resume>Les techniques de résumé automatique multi-documents par extraction ont récemment évolué vers des méthodes statistiques pour la sélection des phrases à extraire. Dans cet article, nous présentons un système conforme à l’« état de l’art » — CBSEAS — que nous avons développé pour les tâches Opinion (résumés d’opinions issues de blogs) et Update (résumés de dépêches et mise à jour du résumé à partir de nouvelles dépêches sur le même événement) de la campagne d’évaluation TAC 2008, et montrons l’intérêt d’analyses structurelles et linguistiques des documents à résumer. Nous présentons également notre étude sur la structure des dépêches et l’impact de son intégration à CBSEAS.</resume>
        <mots_cles>Résumé automatique, structure de documents</mots_cles>
        <title/>
        <abstract>Automatic multi-document summarization techniques have recently evolved into statistical methods for selecting the sentences that will be used to generate the summary. In this paper, we present a system in accordance with « State-of-the-art » — CBSEAS — that we have developped for the « Opinion Task » (automatic summaries of opinions from blogs) and the « Update Task » (automatic summaries of newswire articles and information update) of the TAC 2008 evaluation campaign, and show the interest of structural and linguistic analysis of the documents to summarize .We also present our study on news structure and its integration to CBSEAS impact.</abstract>
        <keywords>Automatic summarization, document structure</keywords>
      </article>
      <article id="taln-2009-long-026" session="">
        <auteurs>
          <auteur>
            <nom>Caroline Brun</nom>
            <email>Caroline.Brun@xrce.xerox.com</email>
            <affiliationId>5</affiliationId>
          </auteur>
          <auteur>
            <nom>Nicolas Dessaigne</nom>
            <email>Nicolas.Dessaigne@arisem.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Maud Ehrmann</nom>
            <email>Maud.Ehrmann@xrce.xerox.com</email>
            <affiliationId>5</affiliationId>
          </auteur>
          <auteur>
            <nom>Baptiste Gaillard</nom>
            <email>Baptiste.gaillard@fr.thalesgroup.com</email>
            <affiliationId>4</affiliationId>
          </auteur>
          <auteur>
            <nom>Sylvie Guillemin-Lanne</nom>
            <email>sylvie.guillemin-lanne@temis.com</email>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Guillaume Jacquet</nom>
            <email>Guillaume.Jacquet@xrce.xerox.com</email>
            <affiliationId>5</affiliationId>
          </auteur>
          <auteur>
            <nom>Aaron Kaplan</nom>
            <email>Aaron.Kaplan@xrce.xerox.com</email>
            <affiliationId>5</affiliationId>
          </auteur>
          <auteur>
            <nom>Marianna Kucharski</nom>
            <email>marianna.kucharski@temis.com</email>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Claude Martineau</nom>
            <email>claude.martineau@univ-mlv.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Aurélie Migeotte</nom>
            <email>Aurelie.Migeotte@arisem.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Takuya Nakamura</nom>
            <email>takuya.nakamura@univ-mlv.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Stavroula Voyatzi</nom>
            <email>stavroula.voyatzi@univ-mlv.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Arisem – 1-5 rue Carnot- 91883 Massy cedex</affiliation>
          <affiliation affiliationId="2">IGM-LabInfo Université Paris-Est – 77454 Marne-la-Vallée Cedex 2</affiliation>
          <affiliation affiliationId="3">Temis –Tour Gamma B -193-197 rue de Bercy, 75582 Paris Cedex</affiliation>
          <affiliation affiliationId="4">Thales Communication – 1-5 Avenue Carnot, 91883 Massy</affiliation>
          <affiliation affiliationId="5">XRCE – 6 chemin de Maupertuis, 38240 Meylan</affiliation>
        </affiliations>
        <titre>Une expérience de fusion pour l’annotation d'entités nommées</titre>
        <type>long</type>
        <pages/>
        <resume>Nous présentons une expérience de fusion d’annotations d’entités nommées provenant de différents annotateurs. Ce travail a été réalisé dans le cadre du projet Infom@gic, projet visant à l’intégration et à la validation d’applications opérationnelles autour de l’ingénierie des connaissances et de l’analyse de l’information, et soutenu par le pôle de compétitivité Cap Digital « Image, MultiMédia et Vie Numérique ». Nous décrivons tout d’abord les quatre annotateurs d’entités nommées à l’origine de cette expérience. Chacun d’entre eux fournit des annotations d’entités conformes à une norme développée dans le cadre du projet Infom@gic. L’algorithme de fusion des annotations est ensuite présenté ; il permet de gérer la compatibilité entre annotations et de mettre en évidence les conflits, et ainsi de fournir des informations plus fiables. Nous concluons en présentant et interprétant les résultats de la fusion, obtenus sur un corpus de référence annoté manuellement.</resume>
        <mots_cles>Entités nommées, fusion d’annotations, UIMA</mots_cles>
        <title/>
        <abstract>In this paper, we present an experiment aimed at merging named entity annotations provided by different annotators. This work has been performed as part of the Infom@gic project, whose goal is the integration and validation of knowledge engineering and information analysis applications, and which is supported by the pole of competitiveness Cap Digital « Image, MultiMédia et Vie Numérique ». We first describe the four annotators, which provide named entity annotations that conform to guidelines defined in the Infom@gic project. Then we present an algorithm for merging the different annotations. It uses information about the compatibility of various annotations and can point out conflicts, and thus yields annotations that are more reliable than those of any single annotator. We conclude by describing and interpreting the merging results obtained on a manually annotated reference corpus.</abstract>
        <keywords>Named entities, fusion of annotations, UIMA</keywords>
      </article>
      <article id="taln-2009-long-027" session="">
        <auteurs>
          <auteur>
            <nom>Stéphanie Léon</nom>
            <email>stephanie.leon@lirmm.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIRMM, 161 rue Ada, 34392 Montpellier Cedex 5</affiliation>
        </affiliations>
        <titre>Un système modulaire d’acquisition automatique de traductions à partir du Web</titre>
        <type>long</type>
        <pages/>
        <resume>Nous présentons une méthode de Traduction Automatique d’Unités Lexicales Complexes (ULC) pour la construction de ressources bilingues français/anglais, basée sur un système modulaire qui prend en compte les propriétés linguistiques des unités sources (compositionnalité, polysémie, etc.). Notre système exploite les différentes « facettes » du Web multilingue pour valider des traductions candidates ou acquérir de nouvelles traductions. Après avoir collecté une base d’ULC en français à partir d’un corpus de pages Web, nous passons par trois phases de traduction qui s’appliquent à un cas linguistique, avec une méthode adaptée : les traductions compositionnelles non polysémiques, les traductions compositionnelles polysémiques et les traductions non compositionnelles et/ou inconnues. Notre évaluation sur un vaste échantillon d’ULC montre que l’exploitation du Web pour la traduction et la prise en compte des propriétés linguistiques au sein d’un système modulaire permet une acquisition automatique de traductions avec une excellente précision.</resume>
        <mots_cles>Traduction Automatique, Unités Lexicales Complexes, Désambiguïsation lexicale, World Wide Web, Terminologie</mots_cles>
        <title/>
        <abstract>We present a method of automatic translation (French/English) of Complex Lexical Units (CLU) for aiming at extracting a bilingual lexicon. Our modular system is based on linguistic properties (compositionality, polysemy, etc.). Different aspects of the multilingual Web are used to validate candidate translations and collect new terms. We first build a French corpus of Web pages to collect CLU. Three adapted processing stages are applied for each linguistic property : compositional and non polysemous translations, compositional polysemous translations and non compositional translations. Our evaluation on a sample of CLU shows that our technique based on the Web can reach a very high precision.</abstract>
        <keywords>Automatic translation, Complex lexical units, Lexical disambiguisation, World Wide Web, Terminology</keywords>
      </article>
      <article id="taln-2009-long-028" session="">
        <auteurs>
          <auteur>
            <nom>Philippe Blache</nom>
            <email>blache@lpl-aix.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire Parole et Langage, CNRS &amp; Université de Provence</affiliation>
        </affiliations>
        <titre>Des relations d’alignement pour décrire l’interaction des domaines linguistiques : vers des Grammaires Multimodales</titre>
        <type>long</type>
        <pages/>
        <resume>Un des problèmes majeurs de la linguistique aujourd’hui réside dans la prise en compte de phénomènes relevant de domaines et de modalités différentes. Dans la littérature, la réponse consiste à représenter les relations pouvant exister entre ces domaines de façon externe, en termes de relation de structure à structure, s’appuyant donc sur une description distincte de chaque domaine ou chaque modalité. Nous proposons dans cet article une approche différente permettant représenter ces phénomènes dans un cadre formel unique, permettant de rendre compte au sein d’une même grammaire tous les phénomènes concernés. Cette représentation précise de l’interaction entre domaines et modalités s’appuie sur la définition de relations d’alignement.</resume>
        <mots_cles>Multimodalité, interaction entre domaines, grammaire, corpus multimodaux</mots_cles>
        <title/>
        <abstract>Linguistics is now faced with the question of representing information coming from different domains or modalities. The classical answer consists in representing separately each of these domains, building for each of them an independent structure, and then representing domain interaction in terms of relation between structures. We propose n this paper a different approach in which all information is represented within a unique and homogeneous framework, making it possible to represent into a same grammar all interaction phenomena. This precise representation of interaction relies on the definition of a new notion of alignment relations.</abstract>
        <keywords>Multimodality, domains interaction, grammar, multimodal corpora</keywords>
      </article>
      <article id="taln-2009-long-029" session="">
        <auteurs>
          <auteur>
            <nom>Karën Fort</nom>
            <email>karen.fort@inist.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Maud Ehrmann</nom>
            <email>maud.ehrmann@xrce.xerox.com</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Adeline Nazarenko</nom>
            <email>adeline.nazarenko@lipn.univ-paris13.fr</email>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">INIST, 2 allée du Parc de Brabois, 54500 Vandoeuvre-lès-Nancy</affiliation>
          <affiliation affiliationId="2">XRCE, 6 Chemin de Maupertuis, 38240 Meylan</affiliation>
          <affiliation affiliationId="3">LIPN, Université Paris 13 &amp; CNRS, 99 av. J.B. Clément, 93430 Villetaneuse</affiliation>
        </affiliations>
        <titre>Vers une méthodologie d’annotation des entités nommées en corpus ?</titre>
        <type>long</type>
        <pages/>
        <resume>La tâche, aujourd’hui considérée comme fondamentale, de reconnaissance d’entités nommées, présente des difficultés spécifiques en matière d’annotation. Nous les précisons ici, en les illustrant par des expériences d’annotation manuelle dans le domaine de la microbiologie. Ces problèmes nous amènent à reposer la question fondamentale de ce que les annotateurs doivent annoter et surtout, pour quoi faire. Nous identifions pour cela les applications nécessitant l’extraction d’entités nommées et, en fonction des besoins de ces applications, nous proposons de définir sémantiquement les éléments à annoter. Nous présentons ensuite un certain nombre de recommandations méthodologiques permettant d’assurer un cadre d’annotation cohérent et évaluable.</resume>
        <mots_cles>annotation, reconnaissance d’entités nommées</mots_cles>
        <title/>
        <abstract>Today, the named entity recognition task is considered as fundamental, but it involves some specific difficulties in terms of annotation. We list them here, with illustrations taken from manual annotation experiments in microbiology. Those issues lead us to ask the fundamental question of what the annotators should annotate and, even more important, for which purpose. We thus identify the applications using named entity recognition and, according to the real needs of those applications, we propose to semantically define the elements to annotate. Finally, we put forward a number of methodological recommendations to ensure a coherent and reliable annotation scheme.</abstract>
        <keywords>annotation, named entities extraction</keywords>
      </article>
      <article id="taln-2009-position-001" session="">
        <auteurs>
          <auteur>
            <nom>Sylwia Ozdowska</nom>
            <email>sozdowska@computing.dcu.ie</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">National Centre for Language Technology – Dublin City University, Glasnevin, Dublin 9, Ireland</affiliation>
        </affiliations>
        <titre>Données bilingues pour la TAS français-anglais : impact de la langue source et direction de traduction originales sur la qualité de la traduction</titre>
        <type>position</type>
        <pages/>
        <resume>Dans cet article, nous prenons position par rapport à la question de la qualité des données bilingues destinées à la traduction automatique statistique en terme de langue source et direction de traduction originales à l’égard d’une tâche de traduction français-anglais. Nous montrons que l’entraînement sur un corpus contenant des textes qui ont été à l’origine traduits du français vers l’anglais améliore la qualité de la traduction. Inversement, l’entraînement sur un corpus contenant exclusivement des textes dont la langue source originale n’est ni le français ni l’anglais dégrade la traduction.</resume>
        <mots_cles>Traduction automatique statistique, corpus bilingue, direction de la traduction, langue source, langue cible</mots_cles>
        <title/>
        <abstract>In this paper, we argue about the quality of bilingual data for statistical machine translation in terms of the original source language and translation direction in the context of a French to English translation task. We show that data containing original French and English translated from French improves translation quality. Conversely, using data comprising exclusively French and English translated from several other languages results in a clear-cut decrease in translation quality.</abstract>
        <keywords>Statistical machine translation, bilingual corpus, translation direction, source language, target language</keywords>
      </article>
      <article id="taln-2009-position-002" session="">
        <auteurs>
          <auteur>
            <nom>Marianna Apidianaki</nom>
            <email>mapidianaki@computing.dcu.ie</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">National Centre for Language Technology, Dublin City University</affiliation>
        </affiliations>
        <titre>La place de la désambiguïsation lexicale dans la Traduction Automatique Statistique</titre>
        <type>position</type>
        <pages/>
        <resume>L’étape de la désambiguïsation lexicale est souvent esquivée dans les systèmes de Traduction Automatique Statistique (Statistical Machine Translation (SMT)) car considérée comme non nécessaire à la sélection de traductions correctes. Le débat autour de cette nécessité est actuellement assez vif. Dans cet article, nous présentons les principales positions sur le sujet. Nous analysons les avantages et les inconvénients de la conception actuelle de la désambiguïsation dans le cadre de la SMT, d’après laquelle les sens des mots correspondent à leurs traductions dans des corpus parallèles. Ensuite, nous présentons des arguments en faveur d’une analyse plus poussée des informations sémantiques induites à partir de corpus parallèles et nous expliquons comment les résultats d’une telle analyse pourraient être exploités pour une évaluation plus flexible et concluante de l’impact de la désambiguïsation dans la SMT.</resume>
        <mots_cles>Désambiguïsation lexicale, Traduction Automatique Statistique, sélection lexicale</mots_cles>
        <title/>
        <abstract>Word Sense Disambiguation (WSD) is often omitted in Statistical Machine Translation (SMT) systems, as it is considered unnecessary for lexical selection. The discussion on the need ofWSD is currently very active. In this article we present the main positions on the subject. We analyze the advantages and weaknesses of the current conception of WSD in SMT, according to which the senses of ambiguous words correspond to their translations in a parallel corpus. Then we present some arguments towards a more thorough analysis of the semantic information induced from parallel corpora and we explain how the results of this analysis could be exploited for a more flexible and conclusive evaluation of the impact of WSD on SMT.</abstract>
        <keywords>Word Sense Disambiguation, Statistical Machine Translation, lexical selection</keywords>
      </article>
      <article id="taln-2009-position-003" session="">
        <auteurs>
          <auteur>
            <nom>Marianne Laurent</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Ghislain Putois</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Philippe Bretier</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Thierry Moudenc</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Orange Labs, Lannion, 2 avenue Pierre Marzin, 22307 Lannion Cedex</affiliation>
        </affiliations>
        <titre>Nouveau paradigme d’évaluation des systèmes de dialogue homme-machine</titre>
        <type>position</type>
        <pages/>
        <resume>L’évaluation des systèmes de dialogue homme-machine est un problème difficile et pour lequel ni les objectifs ni les solutions proposées ne font aujourd’hui l’unanimité. Les approches ergonomiques traditionnelles soumettent le système de dialogue au regard critique de l’utilisateur et tente d’en capter l’expression, mais l’absence d’un cadre objectivable des usages de ces utilisateurs empêche une comparaison entre systèmes différents, ou entre évolutions d’un même système. Nous proposons d’inverser cette vision et de mesurer le comportement de l’utilisateur au regard du système de dialogue. Aussi, au lieu d’évaluer l’adéquation du système à ses utilisateurs, nous mesurons l’adéquation des utilisateurs au système. Ce changement de paradigme permet un changement de référentiel qui n’est plus les usages des utilisateurs mais le cadre du système. Puisque le système est complètement défini, ce paradigme permet des approches quantitatives et donc des évaluations comparatives de systèmes.</resume>
        <mots_cles>Évaluation, Dialogue</mots_cles>
        <title/>
        <abstract>Evaluation of a human-machine dialogue system is a difficult problem for which neither the objectives nor the proposed solutions gather a unanimous support. Traditional approaches in the ergonomics field evaluate the system by describing how it fits the user in the user referential of practices. However, the user referential is even more complicated to formalise, and one cannot ground a common use context to enable the comparison of two systems, even if they are merely an evolution of the same service. We propose to shift the point of view on the evaluation problem : instead of evaluating the system in interaction with the user in the user’s referential, we will now measure the user’s adequacy to the system in the system referential. This is our Copernician revolution : for the evaluation purpose, our system is no longer user-centric, because the user referential is not properly objectifiable, while the system referential is completely known by design.</abstract>
        <keywords>Evaluation, Dialogue</keywords>
      </article>
      <article id="taln-2009-demo-001" session="">
        <auteurs>
          <auteur>
            <nom>Francis Brunet-Manquat</nom>
            <email>Francis.Brunet-Manquat@imag.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Jérôme Goulian</nom>
            <email>Jerome.Goulian@imag.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIG-GETALP, BP 53 38041 Grenoble cedex 9, FRANCE</affiliation>
        </affiliations>
        <titre>ACOLAD un environnement pour l’édition de corpus de dépendances</titre>
        <type>démonstration</type>
        <pages/>
        <resume>Dans cette démonstration, nous présentons le prototype d’un environnement open-source pour l’édition de corpus de dépendances. Cet environnement, nommé ACOLAD (Annotation de COrpus Linguistique pour l’Analyse de dépendances), propose des services manuels de segmentation et d’annotation multi-niveaux (segmentation en mots et en syntagmes minimaux (chunks), annotation morphosyntaxique des mots, annotation syntaxique des chunks et annotation syntaxique des dépendances entre mots ou entre chunks).</resume>
        <mots_cles>dépendances, chunk, édition</mots_cles>
        <title/>
        <abstract/>
        <keywords/>
      </article>
      <article id="taln-2009-demo-002" session="">
        <auteurs>
          <auteur>
            <nom>Houda Bouamor</nom>
            <email>Houda.Bouamor@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Aurélien Max</nom>
            <email>Aurelien.Max@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Anne Vilnat</nom>
            <email>Anne.Vilnat@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIMSI-CNRS et Université Paris-Sud 11 Orsay, France</affiliation>
        </affiliations>
        <titre>Amener des utilisateurs à créer et évaluer des paraphrases par le jeu</titre>
        <type>démonstration</type>
        <pages/>
        <resume>Dans cet article, nous présentons une application sur le web pour l’acquisition de paraphrases phrastiques et sous-phrastiques sous forme de jeu. L’application permet l’acquisition à la fois de paraphrases et de jugements humains multiples sur ces paraphrases, ce qui constitue des données particulièrement utiles pour les applications du TAL basées sur les phénomènes paraphrastiques.</resume>
        <mots_cles>Paraphrase, acquisition de données, évaluation de données</mots_cles>
        <title/>
        <abstract>In this article, we present a web application presented as a game for acquiring sentencial and phrasal paraphrases. It can be used both to acquire paraphrases and important quantities of human evaluations of their quality. These are particularly useful for NLP applications relying on paraphrasing.</abstract>
        <keywords>Paraphrasing, data acquisition, data evaluation</keywords>
      </article>
      <article id="taln-2009-demo-003" session="">
        <auteurs>
          <auteur>
            <nom>Adrien Lardilleux</nom>
            <email>Adrien.Lardilleux@info.unicaen.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Yves Lepage</nom>
            <email>Yves.Lepage@info.unicaen.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">GREYC, Université de Caen Basse-Normandie</affiliation>
        </affiliations>
        <titre>anymalign : un outil d’alignement sous-phrastique libre pour les êtres humains</titre>
        <type>démonstration</type>
        <pages/>
        <resume>Nous présentons anymalign, un aligneur sous-phrastique grand public. Ses résultats ont une qualité qui rivalise avec le meilleur outil du domaine, GIZA++. Il est rapide et simple d’utilisation, et permet de produire dictionnaires et autres tables de traduction en une seule commande. À notre connaissance, c’est le seul outil au monde permettant d’aligner un nombre quelconque de langues simultanément. Il s’agit donc du premier aligneur sousphrastique réellement multilingue.</resume>
        <mots_cles>alignement sous-phrastique, multilinguisme, table de traduction</mots_cles>
        <title/>
        <abstract>We present anymalign, a sub-sentential aligner oriented towards end users. It produces results that are competitive with the best known tool in the domain, GIZA++. It is fast and easy to use, and allows dictionaries or translation tables to be produced in a single command. To our knowledge, it is the only tool in the world capable of aligning any number of languages simultaneously. It is therefore the first truly multilingual sub-sentential aligner.</abstract>
        <keywords>sub-sentential alignment, multilinguism, translation table</keywords>
      </article>
      <article id="taln-2009-demo-004" session="">
        <auteurs>
          <auteur>
            <nom>Jean Charlet</nom>
            <email>Jean.Charlet@spim.jussieu.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Sylvie Szulman</nom>
            <email>Sylvie.Szulman@lipn.univ-paris13.fr</email>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Nathalie Aussenac-Gilles</nom>
            <email/>
            <affiliationId>4</affiliationId>
          </auteur>
          <auteur>
            <nom>Adeline Nazarenko</nom>
            <email/>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Nathalie Hernandez</nom>
            <email/>
            <affiliationId>4</affiliationId>
          </auteur>
          <auteur>
            <nom>Nadia Nadah</nom>
            <email/>
            <affiliationId>5</affiliationId>
          </auteur>
          <auteur>
            <nom>Éric Sardet</nom>
            <email/>
            <affiliationId>6</affiliationId>
          </auteur>
          <auteur>
            <nom>Jean Delahousse</nom>
            <email/>
            <affiliationId>7</affiliationId>
          </auteur>
          <auteur>
            <nom>Guy Pierra</nom>
            <email/>
            <affiliationId>6</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">INSERM UMR_S 872, Eq. 20, Paris</affiliation>
          <affiliation affiliationId="2">Université Pierre et Marie Curie ; AP-HP, Paris</affiliation>
          <affiliation affiliationId="3">LIPN - UMR 7030, Université Paris 13 - CNRS</affiliation>
          <affiliation affiliationId="4">CNRS/IRIT et Université de Toulouse</affiliation>
          <affiliation affiliationId="5">Heudiasyc CNRS/UMR 6599, Université de Technologie de Compiègne</affiliation>
          <affiliation affiliationId="6">LISI-ENSMA et CRITT-Informatique, Poitiers</affiliation>
          <affiliation affiliationId="7">MONDECA, Paris</affiliation>
        </affiliations>
        <titre>Apport des outils de TAL à la construction d’ontologies : propositions au sein de la plateforme DaFOE</titre>
        <type>démonstration</type>
        <pages/>
        <resume>La construction d’ontologie à partir de textes fait l’objet d’études depuis plusieurs années dans le domaine de l’ingénierie des ontologies. Un cadre méthodologique en quatre étapes (constitution d’un corpus de documents, analyse linguistique du corpus, conceptualisation, opérationnalisation de l’ontologie) est commun à la plupart des méthodes de construction d’ontologies à partir de textes. S’il existe plusieurs plateformes de traitement automatique de la langue (TAL) permettant d’analyser automatiquement les corpus et de les annoter tant du point de vue syntaxique que statistique, il n’existe actuellement aucune procédure généralement acceptée, ni a fortiori aucun ensemble cohérent d’outils supports, permettant de concevoir de façon progressive, explicite et traçable une ontologie de domaine à partir d’un ensemble de ressources informationnelles relevant de ce domaine. Le but de ce court article est de présenter les propositions développées, au sein du projet ANR DaFOE 4app, pour favoriser l’émergence d’un tel ensemble d’outils.</resume>
        <mots_cles>Ontologie, construction d’ontologie, TALN</mots_cles>
        <title/>
        <abstract>The concept of ontologies, appeared in the nineties, constitute a key point to represent and share the meaning carried out by formal symbols. Thus, the building of such an ontology is quite difficult. A way to do so is to use preexistent elements (textual corpus, taxonomies, norms or other ontologies) and operate them as a basis to define the ontology field. However, there is neither accepted process nor set of tools to progressively built ontologies from the available resources in a traceable and explicit way. We report in this paper several propositions developed within the framework of the ANR DaFOE4App project to support emergence of such tools.</abstract>
        <keywords>Ontology, Ontology building, NLP</keywords>
      </article>
      <article id="taln-2009-demo-005" session="">
        <auteurs>
          <auteur>
            <nom>Davy Weissenbacher</nom>
            <email>Davy.Weissenbacher@manchester.ac.uk</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Elisa Pieri</nom>
            <email>Elisa.Pieri@manchester.ac.uk</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Sophia Ananiadou</nom>
            <email>Sophia.Ananiadou@manchester.ac.uk</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Brian Rea</nom>
            <email>Brian.Rea@manchester.ac.uk</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Farida Vis</nom>
            <email>Farida.Vis@manchester.ac.uk</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Yuwei Lin</nom>
            <email>Yuwei.Lin@manchester.ac.uk</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Rob Procter</nom>
            <email>Rob.Procter@manchester.ac.uk</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Peter Halfpenny</nom>
            <email>Peter.Halfpenny@manchester.ac.uk</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">National Centre for Text Mining, University of Manchester, 131 Princess Street, M1 7DN UK</affiliation>
          <affiliation affiliationId="2">National Centre for e-Social Science, University of Manchester, Oxford Rd, Manchester M13 9PL,UK</affiliation>
        </affiliations>
        <titre>ASSIST : un moteur de recherche spécialisé pour l’analyse des cadres d’expériences</titre>
        <type>démonstration</type>
        <pages/>
        <resume>L’analyse qualitative des données demande au sociologue un important travail de sélection et d’interprétation des documents. Afin de faciliter ce travail, cette communauté c’est dotée d’outils informatique mais leur fonctionnalités sont encore limitées. Le projet ASSIST est une étude exploratoire pour préciser les modules de traitement automatique des langues (TAL) permettant d’assister le sociologue dans son travail d’analyse. Nous présentons le moteur de recherche réalisé et nous justifions le choix des composants de TAL intégrés au prototype.</resume>
        <mots_cles>Recherche d’information, Extraction d’information, Terminologie</mots_cles>
        <title/>
        <abstract>Qualitative data analysis requiers from the sociologist an important work of selection and interpretation of the documents. To facilitate this work, several software have been created but their functionalities are still limitated. The ASSIST project is a preliminary work to define the natural language processing modules for helping the sociologist. We present the search engine realised and justify the NLP modules integrated in the prototype.</abstract>
        <keywords>Information Retrieval, Information Extraction, Terminology</keywords>
      </article>
      <article id="taln-2009-demo-006" session="">
        <auteurs>
          <auteur>
            <nom>Ivan Šmilauer</nom>
            <email>smilauer@cetlef.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">INALCO, LaLIC-CERTAL, 49bis avenue de la Belle Gabrielle, 75012 Paris</affiliation>
        </affiliations>
        <titre>CETLEF.fr - diagnostic automatique des erreurs de déclinaison tchèque dans un outil ELAO</titre>
        <type>démonstration</type>
        <pages/>
        <resume>CETLEF.fr – une application Web dynamique – propose des exercices de déclinaison tchèque avec un diagnostic automatique des erreurs. Le diagnostic a nécessité l'élaboration d'un modèle formel spécifique de la déclinaison contenant un classement des types paradigmatiques et des règles pour la réalisation des alternances morphématiques. Ce modèle est employé pour l'annotation des formes requises, nécessaire pour le diagnostic, mais également pour une présentation didactique sur la plateforme apprenant. Le diagnostic est effectué par comparaison d'une production erronée avec des formes hypothétiques générées à partir du radical de la forme requise et des différentes désinences casuelles. S'il existe une correspondance, l'erreur est interprétée d'après les différences dans les traits morphologiques de la forme requise et de la forme hypothétique. La majorité des erreurs commises peut être interprétée à l'aide de cette technique.</resume>
        <mots_cles>morphologie flexionnelle, déclinaison tchèque, acquisition d'une langue étrangère, diagnostic des erreurs et feedback, ELAO</mots_cles>
        <title/>
        <abstract>CETLEF.fr – a dynamic Web application – contains fill-in-the-blank exercises on Czech declension with an automatic error diagnosis. The diagnosis rendered necessary the definition of a specific formal model of nominal inflection containing a classification of the paradigms and the rules for the realization of morphemic alternations. This model has been employed for the morphological annotation of required forms, necessary for the error diagnosis as well as for a didactic presentation on the learning platform. Diagnosis is carried out by the comparison of an erroneous production with hypothetical forms generated from the radical of the required form and various haphazard endings. If a correspondence is found, the error is interpreted according to the differences in the morphological features of the required form and the hypothetical form. The majority of errors can be interpreted with the aid of this technique.</abstract>
        <keywords>inflectional morphology, Czech declension, second language acquisition, error diagnosis and feedback, CALL</keywords>
      </article>
      <article id="taln-2009-demo-007" session="">
        <auteurs>
          <auteur>
            <nom>Georges Fafiotte</nom>
            <email>georges.fafiotte@imag.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Achille Falaise</nom>
            <email>achille.falaise@imag.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Jérôme Goulian</nom>
            <email>jerome.goulian@imag.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIG-GETALP, UJF Grenoble 1 BP 53, 38041 – GRENOBLE cedex 9</affiliation>
        </affiliations>
        <titre>CIFLI-SurviTra, deux facettes : démonstrateur de composants de TA fondée sur UNL, et phrasebook multilingue</titre>
        <type>démonstration</type>
        <pages/>
        <resume>CIFLI-SurviTra ("Survival Translation" assistant) est une plate-forme destinée à favoriser l'ingénierie et la mise au point de composants UNL de TA, à partir d'une mémoire de traduction formée de livres de phrases multilingues avec variables lexicales. SurviTra est aussi un phrasebook digital multilingue, assistant linguistique pour voyageurs monolingues (français, hindi, tamoul, anglais) en situation de "survie linguistique". Le corpus d’un domaine-pilote ("Restaurant") a été structuré et construit : sous-domaines de phrases alignées et classes lexicales de locutions quadrilingues, graphes UNL, dictionnaires UW++/français et UW++/hindi par domaines. L’approche, générique, est applicable à d’autres langues. Le prototype d’assistant linguistique (application Web, à interface textuelle) peut évoluer vers une application UNL embarquée sur SmartPhone, avec Traitement de Parole et multimodalité.</resume>
        <mots_cles>TA via UNL, démonstrateur de composants UNL, assistant linguistique sur le Web, phrasebook digital multilingue, mémoire de traduction, collecte collaborative de corpus</mots_cles>
        <title/>
        <abstract/>
        <keywords/>
      </article>
      <article id="taln-2009-demo-008" session="">
        <auteurs>
          <auteur>
            <nom>Stefanos Petrakis</nom>
            <email>petrakis@ifi.uzh.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Manfred Klenner</nom>
            <email>klenner@ifi.uzh.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Étienne Ailloud</nom>
            <email>ailloud@ifi.uzh.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Angela Fahrni</nom>
            <email>angela.fahrni@swissonline.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Institute of Computational Linguistics, University of Zurich, Switzerland</affiliation>
        </affiliations>
        <titre>Composition multilingue de sentiments</titre>
        <type>démonstration</type>
        <pages/>
        <resume>Nous présentons ici PolArt, un outil multilingue pour l’analyse de sentiments qui aborde la composition des sentiments en appliquant des transducteurs en cascade. La compositionnalité est assurée au moyen de polarités préalables extraites d’un lexique et des règles de composition appliquées de manière incrémentielle.</resume>
        <mots_cles>Analyse de sentiments</mots_cles>
        <title/>
        <abstract>We introduce PolArt, a multilingual tool for sentiment detection that copes with sentiment composition through the application of cascaded transducers. Compositionality is enabled by prior polarities taken from a polarity lexicon and the compositional rules applied incrementally.</abstract>
        <keywords>Sentiment Detection</keywords>
      </article>
      <article id="taln-2009-demo-009" session="">
        <auteurs>
          <auteur>
            <nom>Motasem Alrahabi</nom>
            <email>motasem.alrahabi@paris4.sorbonne.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Jean-Pierre Desclés</nom>
            <email>jean-pierre.descles@paris-sorbonne.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire LaLIC – Université Paris-Sorbonne</affiliation>
        </affiliations>
        <titre>EXCOM : Plate-forme d'annotation sémantique de textes multilingues</titre>
        <type>démonstration</type>
        <pages/>
        <resume>Nous proposons une plateforme d‟annotation sémantique, appelée « EXCOM ». Basée sur la méthode de l‟ « Exploration Contextuelle », elle permet, à travers une diversité de langues, de procéder à des annotations automatiques de segments textuels par l'analyse des formes de surface dans leur contexte. Les textes sont traités selon des « points de vue » discursifs dont les valeurs sont organisées dans une « carte sémantique ». L‟annotation se base sur un ensemble de règles linguistiques, écrites par un analyste, qui permettent d‟identifier les représentations textuelles sous-jacentes aux différentes catégories de la carte. Le système offre, à travers deux types d‟interfaces (développeur ou utilisateur), une chaîne de traitements automatiques de textes qui comprend la segmentation, l‟annotation et d‟autres fonctionnalités de post-traitement. Les documents annotés peuvent être utilisés, par exemple, pour des systèmes de recherche d‟information, de veille, de classification ou de résumé automatique.</resume>
        <mots_cles>Plate-forme, Annotation automatique, Exploration Contextuelle, analyse sémantique, marqueurs discursifs, carte sémantique, multilinguisme</mots_cles>
        <title/>
        <abstract>We propose a platform for semantic annotation, called “EXCOM”. Based on the “Contextual Exploration” method, it enables, across a great range of languages, to perform automatic annotations of textual segments by analyzing surface forms in their context. Texts are approached through discursive “points of view”, of which values are organized into a “semantic map”. The annotation is based on a set of linguistic rules, manually constructed by an analyst, and that enables to automatically identify the textual representations underlying the different semantic categories of the map. The system provides through two sorts of user-friendly interfaces (analyst or end-user) a complete pipeline of automatic text processing which consists of segmentation, annotation and other post-processing functionalities. Annotated documents can be used, for instance, for information retrieval systems, classification or automatic summarization.</abstract>
        <keywords>Platform, automatic annotation, Contextual Exploration, semantic analysis, discoursive markers, Semantic Map, multilingual approach</keywords>
      </article>
      <article id="taln-2009-demo-010" session="">
        <auteurs>
          <auteur>
            <nom>Antoine Widlöcher</nom>
            <email>antoine.widlocher@info.unicaen.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Yann Mathet</nom>
            <email>yann.mathet@info.unicaen.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire GREYC, CNRS UMR 6072, Université de Caen</affiliation>
        </affiliations>
        <titre>La plate-forme d’annotation Glozz</titre>
        <type>démonstration</type>
        <pages/>
        <resume/>
        <mots_cles>Linguistique de corpus, Annotation, Plate-forme logicielle</mots_cles>
        <title/>
        <abstract/>
        <keywords>Corpus Linguistics, Annotation, Software Framework</keywords>
      </article>
      <article id="taln-2009-demo-011" session="">
        <auteurs>
          <auteur>
            <nom>Blin Raoul</nom>
            <email>blin@ehess.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CRLAO - CNRS , 54 Raspail 75006 Paris</affiliation>
        </affiliations>
        <titre>SAGACE-v3.3 ; Analyseur de corpus pour langues non flexionnelles</titre>
        <type>démonstration</type>
        <pages/>
        <resume>Nous présentons la dernière version du logiciel SAGACE, analyseur de corpus pour langues faiblement flexionnelles (par exemple japonais ou chinois). Ce logiciel est distribué avec un lexique où les catégories sont exprimées à l'aide de systèmes de traits.</resume>
        <mots_cles>corpus, lexique, analyseur, japonais, chinois</mots_cles>
        <title/>
        <abstract>We present a software program named SAGACE, designed to search for and extract word strings from a large corpus. It has been conceived for poor flexional languages, such as Japanese or Chinese. It is associated with a lexicon where categories are expressed with feature systems.</abstract>
        <keywords>corpus, lexicon, analyzer, japanese, chinese</keywords>
      </article>
      <article id="taln-2009-demo-012" session="">
        <auteurs>
          <auteur>
            <nom>Nicolas Hernandez</nom>
            <email>Nicolas.Hernandez@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Fabien Poulard</nom>
            <email>Fabien.Poulard@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Stergos Afantenos</nom>
            <email>Stergos.Afantenos@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Matthieu Vernier</nom>
            <email>Matthieu.Vernier@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Jérôme Rocheteau</nom>
            <email>Jerome.Rocheteau@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LINA (CNRS - UMR 6241), 2 rue de la Houssinière – B.P. 92208, 44322 NANTES Cedex 3</affiliation>
        </affiliations>
        <titre>Apache UIMA pour le Traitement Automatique des Langues</titre>
        <type>démonstration</type>
        <pages/>
        <resume>L’objectif de la démonstration est d’une part de faire un retour d’expérience sur la solution logicielle Apache UIMA comme infrastructure de développement d’applications distribuées de TAL, et d’autre part de présenter les développements réalisés par l’équipe TALN du LINA pour permettre à la communauté de s’approprier ce « framework ».</resume>
        <mots_cles>Apache UIMA, Applications du TAL, Infrastructure logicielle</mots_cles>
        <title/>
        <abstract>Our objectives are twofold : First, based on some common use cases, we will discuss the interest of using UIMA as a middleware solution for developing Natural Language Processing systems. Second, we will present various preprocessing tools we have developed in order to facilitate the access to the framework for the French community.</abstract>
        <keywords>Apache UIMA, NLP applications, Middleware</keywords>
      </article>
      <article id="taln-2009-demo-013" session="">
        <auteurs>
          <auteur>
            <nom>Jérôme Lehuen</nom>
            <email>Jerome.Lehuen@lium.univ-lemans.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Thierry Lemeunier</nom>
            <email>Thierry.Lemeunier@lium.univ-lemans.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIUM - Université du Maine, Avenue Laënnec, 72085 Le Mans Cedex 9</affiliation>
        </affiliations>
        <titre>Un Analyseur Sémantique pour le DHM</titre>
        <type>démonstration</type>
        <pages/>
        <resume/>
        <mots_cles/>
        <title/>
        <abstract/>
        <keywords/>
      </article>
      <article id="taln-2009-demo-014" session="">
        <auteurs>
          <auteur>
            <nom>Jacques Vergne</nom>
            <email>Jacques.Vergne@info.unicaen.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">GREYC – Université de Caen, campus 2 - BP 5186, F-14032 CAEN cedex</affiliation>
        </affiliations>
        <titre>Un chunker multilingue endogène</titre>
        <type>démonstration</type>
        <pages/>
        <resume>Le chunking consiste à segmenter un texte en chunks, segments sous-phrastiques qu'Abney a défini approximativement comme des groupes accentuels. Traditionnellement, le chunking utilise des ressources monolingues, le plus souvent exhaustives, quelquefois partielles : des mots grammaticaux et des ponctuations, qui marquent souvent des débuts et fins de chunk. Mais cette méthode, si l'on veut l'étendre à de nombreuses langues, nécessite de multiplier les ressources monolingues. Nous présentons une nouvelle méthode : le chunking endogène, qui n'utilise aucune ressource hormis le texte analysé lui-même. Cette méthode prolonge les travaux de Zipf : la minimisation de l'effort de communication conduit les locuteurs à raccourcir les mots fréquents. On peut alors caractériser un chunk comme étant la période des fonctions périodiques correllées longueur et effectif des mots sur l'axe syntagmatique. Cette méthode originale présente l'avantage de s'appliquer à un grand nombre de langues d'écriture alphabétique, avec le même algorithme, sans aucune ressource.</resume>
        <mots_cles>chunking, multilingue, endogène, longueur des mots, effectif des mots</mots_cles>
        <title/>
        <abstract>Chunking is segmenting a text into chunks, sub-sentential segments, that Abney approximately defined as stress groups. Chunking usually uses monolingual resources, most often exhaustive, sometimes partial : function words and punctuations, which often mark beginnings and ends of chunks. But, to extend this method to other languages, monolingual resources have to be multiplied. We present a new method : endogenous chunking, which uses no other resource than the text to be parsed itself. The idea of this method comes from Zipf : to make the least communication effort, speakers are driven to shorten frequent words. A chunk then can be characterised as the period of the periodic correlated functions length and frequency of words on the syntagmatic axis. This original method takes its advantage to be applied to a great number of languages of alphabetic script, with the same algorithm, without any resource.</abstract>
        <keywords>chunking, multilingual, endogenous, word length, word frequency</keywords>
      </article>
      <article id="taln-2009-court-001" session="">
        <auteurs>
          <auteur>
            <nom>Djamé Seddah</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Marie Candito</nom>
            <email/>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Benoît Crabbé</nom>
            <email/>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LALIC et INRIA (Alpage), Université Paris-Sorbonne</affiliation>
          <affiliation affiliationId="2">UFRL et INRIA (Alpage), Université Paris 7</affiliation>
        </affiliations>
        <titre>Adaptation de parsers statistiques lexicalisés pour le français : Une évaluation complète sur corpus arborés</titre>
        <type>court</type>
        <pages/>
        <resume>Cet article présente les résultats d’une évaluation exhaustive des principaux analyseurs syntaxiques probabilistes dit “lexicalisés” initialement conçus pour l’anglais, adaptés pour le français et évalués sur le CORPUS ARBORÉ DU FRANÇAIS (Abeillé et al., 2003) et le MODIFIED FRENCH TREEBANK (Schluter &amp; van Genabith, 2007). Confirmant les résultats de (Crabbé &amp; Candito, 2008), nous montrons que les modèles lexicalisés, à travers les modèles de Charniak (Charniak, 2000), ceux de Collins (Collins, 1999) et le modèle des TIG Stochastiques (Chiang, 2000), présentent des performances moindres face à un analyseur PCFG à Annotation Latente (Petrov et al., 2006). De plus, nous montrons que le choix d’un jeu d’annotations issus de tel ou tel treebank oriente fortement les résultats d’évaluations tant en constituance qu’en dépendance non typée. Comparés à (Schluter &amp; van Genabith, 2008; Arun &amp; Keller, 2005), tous nos résultats sont state-of-the-art et infirment l’hypothèse d’une difficulté particulière qu’aurait le français en terme d’analyse syntaxique probabiliste et de sources de données.</resume>
        <mots_cles>Analyse syntaxique probabiliste, corpus arborés, évaluation, analyse du français</mots_cles>
        <title/>
        <abstract>This paper presents complete investigation results on the statistical parsing of French by bringing a complete evaluation on French data of the main based probabilistic lexicalized (Charniak, Collins, Chiang) and unlexicalized (Berkeley) parsers designed first on the Penn Treebank. We adapted the parsers on the two existing treebanks of French (Abeillé et al., 2003; Schluter &amp; van Genabith, 2007). To our knowledge, all the results reported here are state-of-the-art for the constituent parsing of French on every available treebank and invalidate the hypothesis of French being particularly difficult to parse. Regarding the algorithms, the comparisons show that lexicalized parsing models are outperformed by the unlexicalized Berkeley parser. Regarding the treebanks, we observe that a tag set with specific features has direct influences over evaluation results depending on the parsing model.</abstract>
        <keywords>Probabilistic parsing, treebanks, evaluation, French parsing</keywords>
      </article>
      <article id="taln-2009-court-002" session="">
        <auteurs>
          <auteur>
            <nom>Fiammetta Namer</nom>
            <email>fiammetta.namer@univ-nancy2.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">UMR 7118 « ATILF » - Nancy Université</affiliation>
        </affiliations>
        <titre>Analyse automatique des noms déverbaux composés : pourquoi et comment faire interagir analogie et système de règles</titre>
        <type>court</type>
        <pages/>
        <resume>Cet article aborde deux problèmes d’analyse morpho-sémantique du lexique : (1) attribuer automatiquement une définition à des noms et verbes morphologiquement construits inconnus des dictionnaires mais présents dans les textes ; (2) proposer une analyse combinant règles et analogie, deux techniques généralement contradictoires. Les noms analysés sont apparemment suffixés et composés (HYDROMASSAGE). La plupart d’entre eux, massivement attestés dans les documents (journaux, Internet) sont absents des dictionnaires. Ils sont souvent reliés à des verbes (HYDROMASSER) également néologiques. Le nombre de ces noms et verbes est estimé à 5.400. L’analyse proposée leur attribue une définition par rapport à leur base, et enrichit un lexique de référence pour le TALN au moyen de cette base, si elle est néologique. L’implémentation des contraintes linguistiques qui régissent ces formations est reproductible dans d’autres langues européennes où sont rencontrés les mêmes types de données dont l’analyse reflète le même raisonnement que pour le français.</resume>
        <mots_cles>Analyse morphologique, Annotation sémantique, Composition savante, Noms déverbaux, Règles, Analogie</mots_cles>
        <title/>
        <abstract>This paper addresses two morpho-semantic parsing issues: (1) to automatically provide morphologically complex unknown nouns and verbs with a definition; (2) to propose a methodology combining both rules and analogy, which are techniques usually seen as inconsistent with eachother. The analysed nouns look like both suffixed and compounded (HYDROMASSAGE). Most of them are not stored in dictionaries, although they are very frequent in newspapers or online documents. They are often related to verbs (HYDROMASSER), also lacking from dictionaries. The estimated amount of these nouns and verbs is 5,400. The proposed analysis assigns them a definition calculated according to their base meaning, and it increases the existing reference lexicon content with this base, from the moment that it is a new-coined form. The implementation of linguistic constraints which govern this word formations is reproducible in other West-European languages, where the same data type is found, subject to the same kind of analysis.</abstract>
        <keywords>Morphological parsing, Semantic annotation, Neo-classical compounds, Deverbal nouns, Rules, Analogy</keywords>
      </article>
      <article id="taln-2009-court-003" session="">
        <auteurs>
          <auteur>
            <nom>Jonathan Marchand</nom>
            <email>Jonathan.Marchand@loria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Bruno Guillaume</nom>
            <email>Bruno.Guillaume@loria.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Guy Perrier</nom>
            <email>Guy.Perrier@loria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LORIA / Université Nancy 2</affiliation>
          <affiliation affiliationId="2">LORIA / INRIA Nancy Grand-Est</affiliation>
        </affiliations>
        <titre>Analyse en dépendances à l’aide des grammaires d’interaction</titre>
        <type>court</type>
        <pages/>
        <resume>Cet article propose une méthode pour extraire une analyse en dépendances d’un énoncé à partir de son analyse en constituants avec les grammaires d’interaction. Les grammaires d’interaction sont un formalisme grammatical qui exprime l’interaction entre les mots à l’aide d’un système de polarités. Le mécanisme de composition syntaxique est régi par la saturation des polarités. Les interactions s’effectuent entre les constituants, mais les grammaires étant lexicalisées, ces interactions peuvent se traduire sur les mots. La saturation des polarités lors de l’analyse syntaxique d’un énoncé permet d’extraire des relations de dépendances entre les mots, chaque dépendance étant réalisée par une saturation. Les structures de dépendances ainsi obtenues peuvent être vues comme un raffinement de l’analyse habituellement effectuée sous forme d’arbre de dépendance. Plus généralement, ce travail apporte un éclairage nouveau sur les liens entre analyse en constituants et analyse en dépendances.</resume>
        <mots_cles>Analyse syntaxique, grammaires de dépendances, grammaires d’interaction, polarité</mots_cles>
        <title/>
        <abstract>This article proposes a method to extract dependency structures from phrasestructure level parsing with Interaction Grammars. Interaction Grammars are a formalism which expresses interactions among words using a polarity system. Syntactical composition is led by the saturation of polarities. Interactions take place between constituents, but as grammars are lexicalized, these interactions can be translated at the level of words. Dependency relations are extracted from the parsing process : every dependency is the consequence of a polarity saturation. The dependency relations we obtain can be seen as a refinement of the usual dependency tree. Generally speaking, this work sheds new light on links between phrase structure and dependency parsing.</abstract>
        <keywords>Syntactic analysis, dependency grammars, interaction grammars, polarity</keywords>
      </article>
      <article id="taln-2009-court-004" session="">
        <auteurs>
          <auteur>
            <nom>Jean-Philippe Prost</nom>
            <email>JPProst@gmail.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIFO, Université d’Orléans</affiliation>
        </affiliations>
        <titre>Analyse relâchée à base de contraintes</titre>
        <type>court</type>
        <pages/>
        <resume>La question de la grammaticalité, et celle duale de l’agrammaticalité, sont des sujets délicats à aborder, dès lors que l’on souhaite intégrer différents degrés, tant de grammaticalité que d’agrammaticalité. En termes d’analyse automatique, les problèmes posés sont de l’ordre de la représentation des connaissances, du traitement, et bien évidement de l’évaluation. Dans cet article, nous nous concentrons sur l’aspect traitement, et nous nous penchons sur la question de l’analyse d’énoncés agrammaticaux. Nous explorons la possibilité de fournir une analyse la plus complète possible pour un énoncé agrammatical, sans l’apport d’information complémentaire telle que par le biais de mal-règles ou autre grammaire d’erreurs. Nous proposons une solution algorithmique qui permet l’analyse automatique d’un énoncé agrammatical, sur la seule base d’une grammaire modèle-théorique de bonne formation. Cet analyseur est prouvé générer une solution optimale, selon un critère numérique maximisé.</resume>
        <mots_cles>grammaticalité, analyse syntaxique, contraintes, syntaxe modèle-théorique</mots_cles>
        <title/>
        <abstract>The question of grammaticality, and the dual one of ungrammaticality, are topics delicate to address when interested in modeling different degrees, whether of grammaticality or ungrammaticality. As far as parsing is concerned, the problems are with regard to knowledge representation, processing, and obviously evaluation. In this paper, we concentrate on the processing aspect and we address the question of parsing ungrammatical utterances. We explore the possibility to provide a full parse for an ungrammatical utterance without relying on any kind of additional information, which would be provided by mal-rules or other error grammar. We propose an algorithmic solution in order to parse an ungrammatical utterance using only a model-theoretic grammar of well-formedness. The parser is proven to generate an optimal solution, according to a maximised criterion.</abstract>
        <keywords>grammaticality, syntactic parsing, constraints, Model-Theoretic Syntax</keywords>
      </article>
      <article id="taln-2009-court-005" session="">
        <auteurs>
          <auteur>
            <nom>Marie-Paule Péry-Woodley</nom>
            <email>pery@univ-tlse2.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Nicholas Asher</nom>
            <email>asher@irit.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Patrice Enjalbert</nom>
            <email>patrice.enjalbert@info.unicaen.fr</email>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Farah Benamara</nom>
            <email>benamara@irit.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Myriam Bras</nom>
            <email>bras@univ-tlse2.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Cécile Fabre</nom>
            <email>cecile.fabre@univ-tlse2.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Stéphane Ferrari</nom>
            <email>stephane.ferrari@info.unicaen.fr</email>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Lydia-Mai Ho-Dac</nom>
            <email>hodac@univ-tlse2.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Anne Le Draoulec</nom>
            <email>draoulec@univ-tlse2.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Yann Mathet</nom>
            <email>mathet@info.unicaen.fr</email>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Philippe Muller</nom>
            <email>philippe.muller@irit.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Laurent Prévot</nom>
            <email>laurent.prevot@lpl-aix.fr</email>
            <affiliationId>4</affiliationId>
          </auteur>
          <auteur>
            <nom>Josette Rebeyrolle</nom>
            <email>rebeyrol@univ-tlse2.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Ludovic Tanguy</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Marianne Vergez-Couret</nom>
            <email>vergez@univ-tlse2.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Laure Vieu</nom>
            <email>vieu@irit.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Antoine Widlöcher</nom>
            <email>awidloch@info.unicaen.fr</email>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CLLE-ERSS – Université de Toulouse UTM</affiliation>
          <affiliation affiliationId="2">IRIT – Université de Toulouse UPS</affiliation>
          <affiliation affiliationId="3">GREYC – Université de Caen</affiliation>
          <affiliation affiliationId="4">Laboratoire Parole et Langage – Université de Provence</affiliation>
        </affiliations>
        <titre>ANNODIS: une approche outillée de l'annotation de structures discursives</titre>
        <type>court</type>
        <pages/>
        <resume>Le projet ANNODIS vise la construction d’un corpus de textes annotés au niveau discursif ainsi que le développement d'outils pour l’annotation et l’exploitation de corpus. Les annotations adoptent deux points de vue complémentaires : une perspective ascendante part d'unités de discours minimales pour construire des structures complexes via un jeu de relations de discours ; une perspective descendante aborde le texte dans son entier et se base sur des indices pré-identifiés pour détecter des structures discursives de haut niveau. La construction du corpus est associée à la création de deux interfaces : la première assiste l'annotation manuelle des relations et structures discursives en permettant une visualisation du marquage issu des prétraitements ; une seconde sera destinée à l'exploitation des annotations. Nous présentons les modèles et protocoles d'annotation élaborés pour mettre en oeuvre, au travers de l'interface dédiée, la campagne d'annotation.</resume>
        <mots_cles>annotation de corpus, structures de discours, interface d'annotation</mots_cles>
        <title/>
        <abstract>The ANNODIS project has two interconnected objectives: to produce a corpus of texts annotated at discourse-level, and to develop tools for corpus annotation and exploitation. Two sets of annotations are proposed, representing two complementary perspectives on discourse organisation: a bottom-up approach starting from minimal discourse units and building complex structures via a set of discourse relations; a top-down approach envisaging the text as a whole and using pre-identified cues to detect discourse macro-structures. The construction of the corpus goes hand in hand with the development of two interfaces: the first one supports manual annotation of discourse structures, and allows different views of the texts using NLPM-based pre-processing; another interface will support the exploitation of the annotations. We present the discourse models and annotation protocols, and the interface which embodies them.</abstract>
        <keywords>corpus annotation, discourse structures, annotation tools</keywords>
      </article>
      <article id="taln-2009-court-006" session="">
        <auteurs>
          <auteur>
            <nom>Véronique Moriceau</nom>
            <email>moriceau@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Xavier Tannier</nom>
            <email>xtannier@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université Paris-Sud 11 / LIMSI-CNRS BP 133, 91403 Orsay Cedex, France</affiliation>
        </affiliations>
        <titre>Apport de la syntaxe dans un système de question-réponse : étude du système FIDJI.</titre>
        <type>court</type>
        <pages/>
        <resume>Cet article présente une série d’évaluations visant à étudier l’apport d’une analyse syntaxique robuste des questions et des documents dans un système de questions-réponses. Ces évaluations ont été effectuées sur le système FIDJI, qui utilise à la fois des informations syntaxiques et des techniques plus “traditionnelles”. La sélection des documents, l’extraction de la réponse ainsi que le comportement selon les différents types de questions ont été étudiés.</resume>
        <mots_cles>Systèmes de questions-réponses, analyse syntaxique, évaluation</mots_cles>
        <title/>
        <abstract>This paper presents some experiments aiming at estimating the contribution of a syntactic parser on both questions and documents in a question-answering system. This evaluation has been performed with the system FIDJI, which makes use of both syntactic information and more “traditional” techniques. Document selection, answer extraction as well as system behaviour on different types of questions have been experimented.</abstract>
        <keywords>Question-answering, syntactic analysis, evaluation</keywords>
      </article>
      <article id="taln-2009-court-007" session="">
        <auteurs>
          <auteur>
            <nom>Dominique Laurent</nom>
            <email>dlaurent@synapse-fr.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Sophie Nègre</nom>
            <email>sophie.negre@synapse-fr.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Patrick Séguéla</nom>
            <email>patrick.seguela@synapse-fr.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Synapse Développement</affiliation>
        </affiliations>
        <titre>Apport des cooccurrences à la correction et à l'analyse syntaxique</titre>
        <type>court</type>
        <pages/>
        <resume>Le correcteur grammatical Cordial utilise depuis de nombreuses années les cooccurrences pour la désambiguïsation sémantique. Un dictionnaire de cooccurrences ayant été constitué pour les utilisateurs du logiciel de correction et d'aides à la rédaction, la grande richesse de ce dictionnaire a incité à l'utiliser intensivement pour la correction, spécialement des homonymes et paronymes. Les résultats obtenus sont spectaculaires sur ces types d'erreurs mais la prise en compte des cooccurrences a également été utilisée avec profit pour la pure correction orthographique et pour le rattachement des groupes en analyse syntaxique.</resume>
        <mots_cles>cooccurrences, collocations, correction grammaticale</mots_cles>
        <title/>
        <abstract>For many years, the spellchecker named Cordial has been using cooccurrences for semantic disambiguation. Since a dictionary of co-occurrences had been established for users of the spellchecker and of the writing aid, the richness of this dictionary led us to use it intensively for the correction, especially for homonyms and paronyms. The results are impressive on this kind of errors but taking into account the cooccurrences proved to be very profitable for pure spellchecking and for the attachment of groups in syntactic parsing.</abstract>
        <keywords>collocation, grammar checking</keywords>
      </article>
      <article id="taln-2009-court-008" session="">
        <auteurs>
          <auteur>
            <nom>Daoud Daoud</nom>
            <email>daoud@batelco.jo</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Mohammad Daoud</nom>
            <email>Mohammad.Daoud@imag.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Princess Sumaya University for Technology, P.O.Box 1438 Al-Jubaiha, 11941 Jordan</affiliation>
          <affiliation affiliationId="2">Laboratoire LIG - Université Joseph Fourier 85, rue de la Bibliothèque,, 38041 Grenoble, France</affiliation>
        </affiliations>
        <titre/>
        <type>court</type>
        <pages/>
        <resume/>
        <mots_cles/>
        <title>Arabic Disambiguation Using Dependency Grammar</title>
        <abstract>In this paper, we present a new approach to disambiguation Arabic using a joint rule-based model which is conceptualized using Dependency Grammar. This approach helps in highly accurate analysis of sentences. The analysis produces a semantic net like structure expressed by means of Universal Networking Language (UNL) - a recently proposed interlingua. Extremely varied and complex phenomena of Arabic language have been addressed.</abstract>
        <keywords>Dependency Grammar, Arabic Language, Disambiguation, EnCo, UNL</keywords>
      </article>
      <article id="taln-2009-court-009" session="">
        <auteurs>
          <auteur>
            <nom>Claude de Loupy</nom>
            <email>loupy@syllabs.com</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Michaël Bagur</nom>
            <email>bagur@syllabs.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Helena Blancafort</nom>
            <email>blancafort@syllabs.com</email>
            <affiliationId>1</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Syllabs – 15, rue Jean Baptiste Berlier, 75013 Paris</affiliation>
          <affiliation affiliationId="2">MoDyCo – Université Paris 10, 200 Av. de la République, Nanterre</affiliation>
          <affiliation affiliationId="3">Universitat Pompeu Fabra, Roc Boronat 138, 08018 Barcelona</affiliation>
        </affiliations>
        <titre>Association automatique de lemmes et de paradigmes de flexion à un mot inconnu</titre>
        <type>court</type>
        <pages/>
        <resume>La maintenance et l’enrichissement des lexiques morphosyntaxiques sont souvent des tâches fastidieuses. Dans cet article nous présentons la mise en place d’une procédure de guessing de flexion afin d’aider les linguistes dans leur travail de lexicographes. Le guesser développé ne fait pas qu’évaluer l’étiquette morphosyntaxique comme c’est généralement le cas. Il propose pour un mot français inconnu, un ou plusieurs candidats-lemmes, ainsi que les paradigmes de flexion associés (formes fléchies et étiquettes morphosyntaxiques). Dans cet article, nous décrivons le modèle probabiliste utilisé ainsi que les résultats obtenus. La méthode utilisée permet de réduire considérablement le nombre de règles à valider, permettant ainsi un gain de temps important.</resume>
        <mots_cles>guesser, lexiques morphosyntaxiques, aide aux linguistes, induction des règles de flexion</mots_cles>
        <title/>
        <abstract>Lexicon maintenance and lexicon enrichment is a labour-intensive task. In this paper, we present preliminary work on an inflectional guessing procedure for helping the linguist in lexicographic tasks. The guesser presented here does not only output morphosyntactic tags, but also suggests for an unknown French word one or more lemma candidates as well as their corresponding inflectional rules and morphosyntactic tags that the linguist has to validate. In this article, we present the probabilistic model we used as well as obtained results. The method allows a drastic reduction of the number of rules to validate.</abstract>
        <keywords>guesser, morphosyntactic lexica, aide to the linguist, induction of inflection rules</keywords>
      </article>
      <article id="taln-2009-court-010" session="">
        <auteurs>
          <auteur>
            <nom>Matthieu Vernier</nom>
            <email>Matthieu.Vernier@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Laura Monceaux</nom>
            <email>Laura.Monceaux@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Béatrice Daille</nom>
            <email>Beatrice.Daille@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Estelle Dubreil</nom>
            <email>Estelle.Dubreil@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LINA / CNRS UMR 6241, Université de Nantes</affiliation>
        </affiliations>
        <titre>Catégorisation sémantico-discursive des évaluations exprimées dans la blogosphère</titre>
        <type>court</type>
        <pages/>
        <resume>Les blogs constituent un support d’observations idéal pour des applications liées à la fouille d’opinion. Toutefois, ils imposent de nouvelles problématiques et de nouveaux défis au regard des méthodes traditionnelles du domaine. De ce fait, nous proposons une méthode automatique pour la détection et la catégorisation des évaluations localement exprimées dans un corpus de blogs multi-domaine. Celle-ci rend compte des spécificités du langage évaluatif décrites dans deux théories linguistiques. L’outil développé au sein de la plateforme UIMA vise d’une part à construire automatiquement une grammaire du langage évaluatif, et d’autre part à utiliser cette grammaire pour la détection et la catégorisation des passages évaluatifs d’un texte. La catégorisation traite en particulier l’aspect axiologique de l’évaluation, sa configuration d’énonciation et sa modalité dans le discours.</resume>
        <mots_cles>fouille d’opinion, langage évaluatif, catégorisation des évaluations</mots_cles>
        <title/>
        <abstract>Blogs are an ideal observation for applications related to the opinion mining task. However, they impose new problems and new challenges in this field. Therefore, we propose a method for automatic detection and classification of appraisal locally expressed in a multi-domain blogs corpus. It reflects the specific aspects of appraisal language described in two linguistic theories. The tool developed within the UIMA platform aims both to automatically build a grammar of the appraisal language, and the other part to use this grammar for the detection and categorization of evaluative segments in a text. Categorization especially deals with axiological aspect of an evaluative segments, enunciative configuration and its attitude in discourse.</abstract>
        <keywords>opinion mining, appraisal language, appraisal classification</keywords>
      </article>
      <article id="taln-2009-court-011" session="">
        <auteurs>
          <auteur>
            <nom>Stéphanie Weiser</nom>
            <email>sweiser@u-paris10.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Martin Coste</nom>
            <email>martin.coste@mondeca.com</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Florence Amardeilh</nom>
            <email>florence.amardeilh@mondeca.com</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">MoDyCo – CNRS, Université Paris Ouest Nanterre La Défense – 200, avenue de la République, 92001 Nanterre</affiliation>
          <affiliation affiliationId="2">Mondeca – 3, cité Nollez, 75018 Paris</affiliation>
        </affiliations>
        <titre>Chaîne de traitement linguistique : du repérage d'expressions temporelles au peuplement d'une ontologie de tourisme</titre>
        <type>court</type>
        <pages/>
        <resume>Cet article présente la chaîne de traitement linguistique réalisée pour la mise en place d'une plateforme touristique sur Internet. Les premières étapes de cette chaîne sont le repérage et l'annotation des expressions temporelles présentes dans des pages Web. Ces deux tâches sont effectuées à l'aide de patrons linguistiques. Elles soulèvent de nombreux questionnements auxquels nous tentons de répondre, notamment au sujet de la définition des informations à extraire, du format d'annotation et des contraintes. L'étape suivante consiste en l'exploitation des données annotées pour le peuplement d'une ontologie du tourisme. Nous présentons les règles d'acquisition nécessaires pour alimenter la base de connaissance du projet. Enfin, nous exposons une évaluation du système d'annotation. Cette évaluation permet de juger aussi bien le repérage des expressions temporelles que leur annotation.</resume>
        <mots_cles>Annotation, expressions temporelles, ontologies, base de connaissance, tourisme</mots_cles>
        <title/>
        <abstract>This paper presents the linguistic data processing sequence built for a tourism web portal. The first steps of this sequence are the detection and the annotation of the temporal expressions found in the web pages. These tasks are performed using linguistic patterns. They lead to many questions which we try to answer, such as the definition of information to detect, annotation format and constraints. In the next step this annotated data is used to populate a tourism ontology. We present the acquisition rules which are necessary to enrich the portal knowledge base. Then we present an evaluation of our annotation system. This evaluation is able to judge the detection of the temporal expressions and their annotation.</abstract>
        <keywords>Annotation, temporal expressions, ontologies, knowledge base, tourism</keywords>
      </article>
      <article id="taln-2009-court-012" session="">
        <auteurs>
          <auteur>
            <nom>Yue Ma</nom>
            <email>Yue.Ma@lipn.univ-paris13.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Laurent Audibert</nom>
            <email>Laurent.Audibert@lipn.univ-paris13.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire d’Informatique de l’université Paris-Nord (LIPN) - UMR 7030, Université Paris 13 - CNRS, 99, avenue Jean-Baptiste Clément - F-93430 Villetaneuse, France</affiliation>
        </affiliations>
        <titre>Détection des contradictions dans les annotations sémantiques</titre>
        <type>court</type>
        <pages/>
        <resume>L’annotation sémantique a pour objectif d’apporter au texte une représentation explicite de son interprétation sémantique. Dans un précédent article, nous avons proposé d’étendre les ontologies par des règles d’annotation sémantique. Ces règles sont utilisées pour l’annotation sémantique d’un texte au regard d’une ontologie dans le cadre d’une plate-forme d’annotation linguistique automatique. Nous présentons dans cet article une mesure, basée sur la valeur de Shapley, permettant d’identifier les règles qui sont sources de contradiction dans l’annotation sémantique. Par rapport aux classiques mesures de précision et de rappel, l’intérêt de cette mesure est de ne pas nécessiter de corpus manuellement annoté, d’être entièrement automatisable et de permettre l’identification des règles qui posent problème.</resume>
        <mots_cles>Annotation sémantique, valeur de Shapley, plate-forme d’annotation</mots_cles>
        <title/>
        <abstract>The semantic annotation has the objective to bring to a text an explicit representation of its semantic interpretation. In a preceding article, we suggested extending ontologies by semantic annotation rules. These rules are used for the semantic annotation of a text with respect to an ontology within the framework of an automated linguistic annotation platform. We present in this article a measure, based on the Shapley value, allowing to identify the rules which are sources of contradictions in the semantic annotation. With regard to the classic measures, precision and recall, the interest of this measure is without the requirement of manually annotated corpus, completely automated and its ability to identify rules which raise problems.</abstract>
        <keywords>Semantic annotation, Shapley value, annotation platform</keywords>
      </article>
      <article id="taln-2009-court-013" session="">
        <auteurs>
          <auteur>
            <nom>Marc Le Tallec</nom>
            <email>Marc.letallec@univ-tours.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Jeanne Villaneau</nom>
            <email/>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Jean-Yves Antoine</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Agata Savary</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Arielle Syssau-Vaccarella</nom>
            <email/>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université François Rabelais Tours – LI</affiliation>
          <affiliation affiliationId="2">Université Européenne de Bretagne – VALORIA</affiliation>
          <affiliation affiliationId="3">Université Montpellier 3</affiliation>
        </affiliations>
        <titre>Détection des émotions à partir du contenu linguistique d’énoncés oraux : application à un robot compagnon pour enfants fragilisés</titre>
        <type>court</type>
        <pages/>
        <resume>Le projet ANR Emotirob aborde la question de la détection des émotions sous un cadre original : concevoir un robot compagnon émotionnel pour enfants fragilisés. Notre approche consiste à combiner détection linguistique et prosodie. Nos expériences montrent qu'un sujet humain peut estimer de manière fiable la valence émotionnelle d'un énoncé à partir de son contenu propositionnel. Nous avons donc développé un premier modèle de détection linguistique qui repose sur le principe de compositionnalité des émotions : les mots simples ont une valence émotionnelle donnée et les prédicats modifient la valence de leurs arguments. Après une description succincte du système logique de compréhension dont les sorties sont utilisées pour le calcul global de l'émotion, cet article présente la construction d'une norme émotionnelle lexicale de référence, ainsi que d'une ontologie de classes émotionnelles de prédicats, pour des enfants de 5 et 7 ans.</resume>
        <mots_cles>Emotion, valence émotionnelle, norme lexicale émotionnelle, robot compagnon, compréhension de parole</mots_cles>
        <title/>
        <abstract>Project ANR Emotirob aims at detecting emotions from an original point of view: realizing an emotional companion robot for weakened children. In our approach, linguistic detection and prosodie are combined. Our experiments show that human beings can estimate the emotional value of an utterance from its propositional content in a reliable way. So we have implemented a first model of linguistic detection, based on the principle that emotions can be compound: lexical words have an emotional value while predicates can modify emotional values of their arguments. This paper presents a short description of the logical understanding system, the outputs of which are used for the final emotional value calculus. Then, the creation of a lexical emotional reference standard is presented with an ontology of emotional predicate classes for children, aged between 5 and 7.</abstract>
        <keywords>Emotion, Emotional valency, Emotional lexical standard, companion robot, spoken language understanding</keywords>
      </article>
      <article id="taln-2009-court-014" session="">
        <auteurs>
          <auteur>
            <nom>Nuria Gala</nom>
            <email>nuria.gala@lif.univ-mrs.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Véronique Rey</nom>
            <email>veronique.rey@univ-provence.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Laurent Tichit</nom>
            <email>tichit@iml.univ-mrs.fr</email>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIF, Marseille, CNRS UMR 6166</affiliation>
          <affiliation affiliationId="2">SHADYC, Marseille, CNRS et EHESS, UMR 8562</affiliation>
          <affiliation affiliationId="3">IML, Marseille, CNRS UMR 6206</affiliation>
        </affiliations>
        <titre>Dispersion sémantique dans des familles morpho-phonologiques : éléments théoriques et empiriques</titre>
        <type>court</type>
        <pages/>
        <resume>Traditionnellement, la morphologie lexicale a été diachronique et a permis de proposer le concept de famille de mots. Ce dernier est repris dans les études en synchronie et repose sur une forte cohérence sémantique entre les mots d’une même famille. Dans cet article, nous proposons une approche en synchronie fondée sur la notion de continuité à la fois phonologique et sémantique. Nous nous intéressons, d’une part, à la morpho-phonologie et, d’autre part, à la dispersion sémantique des mots dans les familles. Une première étude (Gala &amp; Rey, 2008) montrait que les familles de mots obtenues présentaient des espaces sémantiques soit de grande cohésion soit de grande dispersion. Afin de valider ces observations, nous présentons ici une méthode empirique qui permet de pondérer automatiquement les unités de sens d’un mot et d’une famille. Une expérience menée auprès de 30 locuteurs natifs valide notre approche et ouvre la voie pour une étude approfondie du lexique sur ces bases phonologiques et sémantiques.</resume>
        <mots_cles>morpho-phonologie lexicale, traitement automatique des familles dérivationnelles, espaces sémantiques</mots_cles>
        <title/>
        <abstract>Traditionally, lexical morphology has been diachronic and has established the notion of word families. This notion is reused in synchronic studies and implies strong semantic coherence within the words of a same family. In this paper, we propose an approach in synchrony which highlights phonological and semantic continuity. Our interests go on morphophonology and on the semantic dispersion of words in a family. A first study (Gala &amp; Rey, 2008) showed that the semantic spaces of the families displayed either a strong semantic cohesion or a strong dispersion. In order to validate this observation, we present here a corpus-based method that automatically weights the semantic units of a word and a family. An experience carried out with 30 native speakers validates our approach and allows us to foresee a thorough study of the lexicon based on phonological and semantic basis.</abstract>
        <keywords>lexicalmorpho-phonology, derivational families processing, semantic spaces</keywords>
      </article>
      <article id="taln-2009-court-015" session="">
        <auteurs>
          <auteur>
            <nom>Kévin Séjourné</nom>
            <email>kevin.sejourne@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université de Paris Sud XI, Limsi/CNRS</affiliation>
        </affiliations>
        <titre>Exploitation d’une structure pour les questions enchaînées</titre>
        <type>court</type>
        <pages/>
        <resume>Nous présentons des travaux réalisés dans le domaine des systèmes de questions réponses (SQR) utilisant des questions enchaînées. La recherche des documents dans un SQR est perturbée par l’absence des éléments utiles à la recherche dans les questions liées, éléments figurant dans les échanges précédents. Les récentes campagnes d’évaluation montrent que ce problème est sous-estimé, et n’a pas fait l’objet de technique dédiée. Afin d’améliorer la recherche des documents dans un SQR nous utilisons une méthode récente d’organisation des informations liées aux interactions entre questions. Celle-ci se base sur l’exploitation d’une structure de données adaptée à la transmission des informations des questions liées jusqu’au moteur d’interrogation. Le moteur d’interrogation doit alors être adapté afin de tirer partie de cette structure de données.</resume>
        <mots_cles>Question réponse enchaînée</mots_cles>
        <title/>
        <abstract>We present works realized in the field of the questions answering (QA) using chained questions. The documents search in QA system is disrupted because useful elements are missing for search using bound questions. Recents evaluation campaigns show this problem as underestimated, and this problem wasn’t solve by specific techniques. To improve documents search in a QA we use a recent information organization method for bound questions to the interactions between questions. This methode is bases on the operation of a special data structure. This data structure transmit informations from bound questions to the interrogation engine. Then the interrogation engine must be improve to take advantage of this data structure.</abstract>
        <keywords>chained question answering</keywords>
      </article>
      <article id="taln-2009-court-016" session="">
        <auteurs>
          <auteur>
            <nom>Alexandre Denis</nom>
            <email>alexandre.denis@loria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Matthieu Quignard</nom>
            <email>matthieu.quignard@loria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">UMR 7503 LORIA/CNRS – Campus scientifique, 56 506 Vandoeuvre-lès-Nancy Cedex</affiliation>
        </affiliations>
        <titre>Exploitation du terrain commun pour la production d’expressions référentielles dans les systèmes de dialogue</titre>
        <type>court</type>
        <pages/>
        <resume>Cet article présente un moyen de contraindre la production d’expressions référentielles par un système de dialogue en fonction du terrain commun. Cette capacité, fondamentale pour atteindre la compréhension mutuelle, est trop souvent oubliée dans les systèmes de dialogue. Le modèle que nous proposons s’appuie sur une modélisation du processus d’ancrage (grounding process) en proposant un raffinement du statut d’ancrage appliqué à la description des référents. Il décrit quand et comment ce statut doit être révisé en fonction des jugements de compréhension des deux participants ainsi que son influence dans le choix d’une description partagée destinée à la génération d’une expression référentielle.</resume>
        <mots_cles>Compréhension mutuelle, processus d’ancrage, référence, génération</mots_cles>
        <title/>
        <abstract>This paper presents a way to constraint the production of referring expressions by a dialogue system according to the common ground. This ability – fundamental for reaching mutual understanding – is often neglected in dialogue system design. The proposed model is based on a view of the grounding process and offering a refinement of the grounding status concerning the referent description. It explains how and when this status should be revised with respect to how participants evaluate their understanding and how this status may help to choose a shared description with which a referring expression can be generated.</abstract>
        <keywords>Mutual understanding, grounding process, referring expression, generation</keywords>
      </article>
      <article id="taln-2009-court-017" session="">
        <auteurs>
          <auteur>
            <nom>Younès Bahou</nom>
            <email>bahou_younes@yahoo.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Amine Bayoudhi</nom>
            <email>bayoudhi.amine@gmail.com</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Lamia Hadrich Belguith</nom>
            <email>l.belguith@fsegs.rnu.tn</email>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire LARIS-MIRACL, FSEGS – Université de Sfax, Tunisie.</affiliation>
        </affiliations>
        <titre>Gestion de dialogue oral Homme-machine en arabe</titre>
        <type>court</type>
        <pages/>
        <resume>Dans le présent papier, nous présentons nos travaux sur la gestion du dialogue oral arabe Homme-machine. Ces travaux entrent dans le cadre de la réalisation du serveur vocal interactif SARF (Bahou et al., 2008) offrant des renseignements sur le transport ferroviaire tunisien en langue arabe standard moderne. Le gestionnaire de dialogue que nous proposons est basé sur une approche structurelle et est composé de deux modèles à savoir, le modèle de tâche et le modèle de dialogue. Le premier modèle permet de i) compléter et vérifier l’incohérence des structures sémantiques représentant les sens utiles des énoncés, ii) générer une requête vers l’application et iii) récupérer le résultat et de formuler une réponse à l’utilisateur en langage naturel. Quant au modèle de dialogue, il assure l’avancement du dialogue avec l’utilisateur et l’identification de ses intentions. L’interaction entre ces deux modèles est assurée grâce à un contexte du dialogue permettant le suivi et la mise à jour de l’historique du dialogue.</resume>
        <mots_cles>gestion du dialogue Homme-machine, dialogue oral arabe, modèle de tâche, modèle de dialogue</mots_cles>
        <title/>
        <abstract>In this paper, we present our research work on Human-machine Arabic oral dialogue management. This work enters in the context of SARF system (Bahou et al., 2008) an interactive vocal server that provides information on Tunisian railway using modern standard Arabic. The dialogue manager that we propose is based on a structural approach and consists of two models namely, the task model and the dialogue model. The first model is used to i) complete and verify the incoherence of semantic structures representing the useful meaning of utterances, ii) generate a query to the application and iii) get back the results and formulate an answer to the user in natural language. As for the dialogue model, it assures the dialogue progress with the user and the identification of her or his intentions. The interaction between these two models is assured by a dialogue context that allows monitoring and updating the dialogue history.</abstract>
        <keywords>Human-machine dialogue management, Arabic oral dialogue, task model, dialogue model</keywords>
      </article>
      <article id="taln-2009-court-018" session="">
        <auteurs>
          <auteur>
            <nom>Lionel Clément</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Kim Gerdes</nom>
            <email/>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Renaud Marlet</nom>
            <email/>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université Bordeaux 1, LaBRI</affiliation>
          <affiliation affiliationId="2">ILPGA, LPP, Sorbonne Nouvelle</affiliation>
          <affiliation affiliationId="3">INRIA, LaBRI</affiliation>
        </affiliations>
        <titre>Grammaires d’erreur – correction grammaticale avec analyse profonde et proposition de corrections minimales</titre>
        <type>court</type>
        <pages/>
        <resume>Nous présentons un système de correction grammatical ouvert, basé sur des analyses syntaxiques profondes. La spécification grammaticale est une grammaire hors-contexte équipée de structures de traits plates. Après une analyse en forêt partagée où les contraintes d’accord de traits sont relâchées, la détection d’erreur minimise globalement les corrections à effectuer et des phrases alternatives correctes sont automatiquement proposées.</resume>
        <mots_cles>Correcteur grammatical, analyse syntaxique, forêt partagée</mots_cles>
        <title/>
        <abstract>We present an open system for grammar checking, based on deep parsing. The grammatical specification is a contex-free grammar with flat feature structures. After a sharedforest analysis where feature agreement constraints are relaxed, error detection globally minimizes the number of fixes and alternate correct sentences are automatically proposed.</abstract>
        <keywords>Grammar checker, parsing, shared forest</keywords>
      </article>
      <article id="taln-2009-court-019" session="">
        <auteurs>
          <auteur>
            <nom>André Bittar</nom>
            <email>andre.bittar@linguist.jussieu.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Laurence Danlos</nom>
            <email>laurence.danlos@linguist.jussieu.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">ALPAGE, INRIA Rocquencourt, 78150 Le Chesnay, Université Paris Diderot, 75013 Paris</affiliation>
        </affiliations>
        <titre>Intégration des constructions à verbe support dans TimeML</titre>
        <type>court</type>
        <pages/>
        <resume>Le langage TimeML a été conçu pour l’annotation des informations temporelles dans les textes, notamment les événements, les expressions de temps et les relations entre les deux. Des consignes d’annotation générales ont été élaborées afin de guider l’annotateur dans cette tâche, mais certains phénomènes linguistiques restent à traiter en détail. Un problème commun dans les tâches de TAL, que ce soit en traduction, en génération ou en compréhension, est celui de l’encodage des constructions à verbe support. Relativement peu d’attention a été portée, jusqu’à maintenant, sur ce problème dans le cadre du langage TimeML. Dans cet article, nous proposons des consignes d’annotation pour les constructions à verbe support.</resume>
        <mots_cles>TimeML, verbes support, discours, sémantique</mots_cles>
        <title/>
        <abstract>TimeML is a markup language developed for the annotation of temporal information in texts, in particular events, temporal expressions and the relations which hold between the two. General annotation guidelines have been developed to guide the annotator in this task, but certain linguistic phenomena have yet to be dealt with in detail. A common problem in NLP tasks, whether in translation, generation or understanding, is that of the encoding of light verb constructions. Relatively little attention has been paid to this problem, until now, in the TimeML framework. In this article, we propose annotation guidelines for light verb constructions.</abstract>
        <keywords>TimeML, light verbs, discourse, semantics</keywords>
      </article>
      <article id="taln-2009-court-020" session="">
        <auteurs>
          <auteur>
            <nom>Benoît Sagot</nom>
            <email>benoit.sagot@inria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Elsa Tolone</nom>
            <email>elsa.tolone@univ-paris-est.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Alpage, INRIA / Paris 7, 30 rue du Ch. des rentiers, 75013 Paris, France</affiliation>
          <affiliation affiliationId="2">IGM, Université Paris-Est, 77454 Marne-la-Vallée Cedex, France</affiliation>
        </affiliations>
        <titre>Intégrer les tables du Lexique-Grammaire à un analyseur syntaxique robuste à grande échelle</titre>
        <type>court</type>
        <pages/>
        <resume>Dans cet article, nous montrons comment nous avons converti les tables du Lexique-Grammaire en un format TAL, celui du lexique Lefff, permettant ainsi son intégration dans l’analyseur syntaxique FRMG. Nous présentons les fondements linguistiques de ce processus de conversion et le lexique obtenu. Nous validons le lexique obtenu en évaluant l’analyseur syntaxique FRMG sur le corpus de référence de la campagne EASy selon qu’il utilise les entrées verbales du Lefff ou celles des tables des verbes du Lexique-Grammaire ainsi converties.</resume>
        <mots_cles>Lexiques syntaxiques, Lexique-Grammaire, analyse syntaxique</mots_cles>
        <title/>
        <abstract>In this paper, we describe how we converted the lexicon-grammar tables into an NLP format, that of the Lefff lexicon, which allowed us to integrate it into the FRMG parser. We decribe the linguistic basis of this conversion process, and the resulting lexicon.We validate the resulting lexicon by evaluating the FRMG parser on the EASy reference corpus depending on the set of verbal entries it relies on, namely those of the Lefff or those of the converted lexicon-grammar verb tables.</abstract>
        <keywords>Syntactic lexica, Lexicon-Grammar, parsing</keywords>
      </article>
      <article id="taln-2009-court-021" session="">
        <auteurs>
          <auteur>
            <nom>Cédric Messiant</nom>
            <email>cedric.messiant@lipn.univ-paris13.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Takuya Nakamura</nom>
            <email>takuya.nakamura@univ-mlv.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Stavroula Voyatzi</nom>
            <email>voyatzi@univ-mlv.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire d’Informatique de Paris-Nord, CNRS UMR 7030 et Université Paris 13, 99, avenue Jean-Baptiste Clément, F-93430 Villetaneuse France</affiliation>
          <affiliation affiliationId="2">Laboratoire d’Informatique Gaspard-Monge, CNRS UMR 8049 IGM-LabInfo et Université de Marne-la-Vallée, 5 Bd Descartes, Champs-sur-Marne, 77454 Marne-la-Vallée Cedex 2</affiliation>
        </affiliations>
        <titre>La complémentarité des approches manuelle et automatique en acquisition lexicale</titre>
        <type>court</type>
        <pages/>
        <resume>Les ressources lexicales sont essentielles pour obtenir des systèmes de traitement des langues performants. Ces ressources peuvent être soit construites à la main, soit acquises automatiquement à partir de gros corpus. Dans cet article, nous montrons la complémentarité de ces deux approches. Pour ce faire, nous utilisons l’exemple de la sous-catégorisation verbale en comparant un lexique acquis par des méthodes automatiques (LexSchem) avec un lexique construit manuellement (Le Lexique-Grammaire). Nous montrons que les informations acquises par ces deux méthodes sont bien distinctes et qu’elles peuvent s’enrichir mutuellement.</resume>
        <mots_cles>verbe, syntaxe, lexique, sous-catégorisation</mots_cles>
        <title/>
        <abstract>Lexical resources are essentially created to obtain efficient text-processing systems. These resources can be constructed either manually or automatically from large corpora. In this paper, we show the complementarity of these two types of approaches, comparing an automatically constructed lexicon (LexSchem) to a manually constructed one (Lexique-Grammaire), on examples of verbal subcategorization. The results show that the information retained by these two resources is in fact different and that they can be mutually enhanced.</abstract>
        <keywords>verb, syntax, lexicon, subcategorization</keywords>
      </article>
      <article id="taln-2009-court-022" session="">
        <auteurs>
          <auteur>
            <nom>Dominique Laurent</nom>
            <email>dlaurent@synapse-fr.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Sophie Nègre</nom>
            <email>sophie.negre@synapse-fr.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Patrick Séguéla</nom>
            <email>patrick.seguela@synapse-fr.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Synapse Développement</affiliation>
        </affiliations>
        <titre>L'analyseur syntaxique Cordial dans Passage</titre>
        <type>court</type>
        <pages/>
        <resume>Cordial est un analyseur syntaxique et sémantique développé par la société Synapse Développement. Largement utilisé par les laboratoires de TALN depuis plus de dix ans, cet analyseur participe à la campagne Passage ("Produire des Annotations Syntaxiques à Grande Échelle"). Comment fonctionne cet analyseur ? Quels résultats a-t-il obtenu lors de la première phase d'évaluation de cette campagne ? Au-delà de ces questions, cet article montre en quoi les contraintes industrielles façonnent les outils d'analyse automatique du langage naturel.</resume>
        <mots_cles>Analyse syntaxique, analyse sémantique, évaluation, Passage</mots_cles>
        <title/>
        <abstract>Cordial is a syntactic and semantic parser developed by Synapse Développement. Widely used by the laboratories of NLP for over ten years, this analyzer is involved in the Passage campaign ("Producing Syntactic Annotations on a Large Scale"). How does this parser work? What were the results obtained during the first phase of the evaluation? Beyond these issues, this article shows how the industrial constraints condition the tools for Natural Language Procesing.</abstract>
        <keywords>Parsing, semantic analysis, evaluation</keywords>
      </article>
      <article id="taln-2009-court-023" session="">
        <auteurs>
          <auteur>
            <nom>Antoine Widlöcher</nom>
            <email>antoine.widlocher@info.unicaen.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Yann Mathet</nom>
            <email>yann.mathet@info.unicaen.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire GREYC, CNRS UMR 6072, Université de Caen</affiliation>
        </affiliations>
        <titre>La plate-forme Glozz : environnement d’annotation et d’exploration de corpus</titre>
        <type>court</type>
        <pages/>
        <resume>La nécessité d’une interaction systématique entre modèles, traitements et corpus impose la disponibilité d’annotations de référence auxquelles modèles et traitements pourront être confrontés. Or l’établissement de telles annotations requiert un cadre formel permettant la représentation d’objets linguistiques variés, et des applications permettant à l’annotateur de localiser sur corpus et de caractériser les occurrences des phénomènes observés. Si différents outils d’annotation ont vu le jour, ils demeurent souvent fortement liés à un modèle théorique et à des objets linguistiques particuliers, et ne permettent que marginalement d’explorer certaines structures plus récemment appréhendées expérimentalement, notamment à granularité élevée et en matière d’analyse du discours. La plate-forme Glozz répond à ces différentes contraintes et propose un environnement d’exploration de corpus et d’annotation fortement configurable et non limité a priori au contexte discursif dans lequel elle a initialement vu le jour.</resume>
        <mots_cles>Linguistique de corpus, Annotation, Plate-forme logicielle</mots_cles>
        <title/>
        <abstract>The need for a systematic confrontation between models and corpora make it necessary to have - and consequently, to produce - reference annotations to which linguistic models could be compared. Creating such annotations requires both a formal framework which copes with various linguistic objects, and specific manual annotation tools, in order to make it possible to locate, identify and feature linguistic phenomena in texts. Though several annotation tools do already exist, they are mostly dedicated to a given theory and to a given set of structures. The Glozz platform, described in this paper, tries to address all of these needs, and provides a highly versatile corpus exploration and annotation framework.</abstract>
        <keywords>Corpus Linguistics, Annotation, Software Framework</keywords>
      </article>
      <article id="taln-2009-court-024" session="">
        <auteurs>
          <auteur>
            <nom>Pierre-André Buvet</nom>
            <email>pabuvet@ldi.univparis13.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Emmanuel Cartier</nom>
            <email>ecartier@ldi.univparis13.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Fabrice Issac</nom>
            <email>fissac@ldi.univparis13.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Yassine Madiouni</nom>
            <email>ymadiouni@ldi.univparis13.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Michel Mathieu-Colas</nom>
            <email>mmathieu-colas@ldi.univparis13.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Salah Mejri</nom>
            <email>smejri@ldi.univparis13.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CNRS LDI UMR 7187 – Université Paris 13, 99 avenue Jean-Baptiste Clément, 93430 Villetaneuse</affiliation>
        </affiliations>
        <titre>Morfetik, ressource lexicale pour le TAL</titre>
        <type>court</type>
        <pages/>
        <resume>Le traitement automatique des langues exige un recensement lexical aussi rigoureux que possible. Dans ce but, nous avons développé un dictionnaire morphologique du français, conçu comme le point de départ d’un système modulaire (Morfetik) incluant un moteur de flexion, des interfaces de consultation et d’interrogation et des outils d’exploitation. Nous présentons dans cet article, après une brève description du dictionnaire de base (lexique des mots simples), quelques-uns des outils informatiques liés à cette ressource : un moteur de recherche des lemmes et des formes fléchies ; un moteur de flexion XML et MySQL ; des outils NLP permettant d’exploiter le dictionnaire ainsi généré ; nous présentons notamment un analyseur linguistique développé dans notre laboratoire. Nous comparons dans une dernière partie Morfetik avec d’autres ressources analogues du français : Morphalou, Lexique3 et le DELAF.</resume>
        <mots_cles>dictionnaire morphologique du français, CMLF, analyse linguistique des textes</mots_cles>
        <title/>
        <abstract>Automatic language processing requires as rigorous a lexical inventory as possible. For this purpose, we have developed a morphological dictionary for French, conceived as the starting point of a modular system (Morfetik) which includes an inflection generator, user interfaces and operating tools. In this paper, we briefly describe the basic dictionary (lexicon of simple words) and detail some of the computing tools based on the dictionary. The computing tools built on this resource include: a lemma / inflected forms search engine; an XML and MySQL engine to build the inflected forms; the generated dictionary can then be used by various NLP Tools; in this article, we present the use of the dictionary in a linguistic analyser developed at the laboratory. Finally, we compare Morfetik to similar resources : Morphalou, Lexique3 and DELAF.</abstract>
        <keywords>French morphological dictionary, XML, CMLF, Linguistical analysis, Morfetik</keywords>
      </article>
      <article id="taln-2009-court-025" session="">
        <auteurs>
          <auteur>
            <nom>Fabien Poulard</nom>
            <email>Fabien.Poulard@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Stergos Afantenos</nom>
            <email>Stergos.Afantenos@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Nicolas Hernandez</nom>
            <email>Nicolas.Hernandez@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LINA (CNRS - UMR 6241), 2 rue de la Houssinière – B.P. 92208, 44322 NANTES Cedex 3</affiliation>
        </affiliations>
        <titre>Nouvelles considérations pour la détection de réutilisation de texte</titre>
        <type>court</type>
        <pages/>
        <resume>Dans cet article nous nous intéressons au problème de la détection de réutilisation de texte. Plus particulièrement, étant donné un document original et un ensemble de documents candidats — thématiquement similaires au premier — nous cherchons à classer ceux qui sont dérivés du document original et ceux qui ne le sont pas. Nous abordons le problème selon deux approches : dans la première, nous nous intéressons aux similarités discursives entre les documents, dans la seconde au recouvrement de n-grams hapax. Nous présentons le résultat d’expérimentations menées sur un corpus de presse francophone construit dans le cadre du projet ANR PIITHIE.</resume>
        <mots_cles>réutilisation de texte, recouvrement de n-grams hapax, similarités discursives, corpus journalistique francophone</mots_cles>
        <title/>
        <abstract>In this article we are interested in the problem of text reuse. More specifically, given an original document and a set of candidate documents—which are thematically similar to the first one — we are interested in classifying them into those that have been derived from the original document and those that are not. We are approaching the problem in two ways : firstly we are interested in the discourse similarities between the documents, and secondly we are interested in the overlap of n-grams that are hapax. We are presenting the results of the experiments that we have performed on a corpus constituted from articles of the French press which has been created in the context of the PIITHIE project funded by the French National Agency for Research (Agence National de la Recherche, ANR).</abstract>
        <keywords>text reuse, hapax n-grams overlap, discourse similarities, french journalistic corpus</keywords>
      </article>
      <article id="taln-2009-court-026" session="">
        <auteurs>
          <auteur>
            <nom>Anne Garcia-Fernandez</nom>
            <email>annegf@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Sophie Rosset</nom>
            <email>vilnat@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Anne Vilnat</nom>
            <email>rosset@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIMSI-CNRS / Université Paris Sud 11 Orsay</affiliation>
        </affiliations>
        <titre>Collecte et analyses de réponses naturelles pour les systèmes de questions-réponses</titre>
        <type>court</type>
        <pages/>
        <resume>Notre travail se situe dans le cadre des systèmes de réponse a une question et à pour but de fournir une réponse en langue naturelle aux questions posées en langue naturelle. Cet article présente une expérience permettant d’analyser les réponses de locuteurs du français à des questions que nous leur posons. L’expérience se déroule à l’écrit comme à l’oral et propose à des locuteurs français des questions relevant de différents types sémantiques et syntaxiques. Nous mettons en valeur une large variabilité dans les formes de réponses possibles en langue française. D’autre part nous établissons un certain nombre de liens entre formulation de question et formulation de réponse. Nous proposons d’autre part une comparaison des réponses selon la modalité oral / écrit. Ces résultats peuvent être intégrés à des systèmes existants pour produire une réponse en langue naturelle de façon dynamique.</resume>
        <mots_cles>systèmes de réponse à une question, expérience, variations linguistiques, réponse en langue naturelle</mots_cles>
        <title/>
        <abstract>Situated within the domain of interactive question-answering, our work is to increase the naturalness of natural language answers. This paper presents an experiment aiming at observing the formulation of answers by French speakers. The system asked simple questions to which the humans had to answer. Two modalities were used : text (web) and speech (phone). We present and analyze the collected corpus. Within the large variability of answer forms in French, we point some links between the answer form and the question form. Moreover we present a preliminary study on the observed variation between modalities. We expect these results to be integrable in existing systems to dynamically produce adpated natural language answers.</abstract>
        <keywords>question-answering systems, experimentation, linguistics variations, natural language answer</keywords>
      </article>
      <article id="taln-2009-court-027" session="">
        <auteurs>
          <auteur>
            <nom>Vincent Claveau</nom>
            <email>IRISA-CNRS, Campus de Beaulieu, 35042 Rennes cedex</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">vincent.claveau@irisa.fr</affiliation>
        </affiliations>
        <titre>La /fOnetizasjc/ comme un problème de translittération</titre>
        <type>court</type>
        <pages/>
        <resume>La phonétisation est une étape essentielle pour le traitement de l’oral. Dans cet article, nous décrivons un système automatique de phonétisation de mots isolés qui est simple, portable et performant. Il repose sur une approche par apprentissage ; le système est donc construit à partir d’exemples de mots et de leur représentation phonétique. Nous utilisons pour cela une technique d’inférence de règles de réécriture initialement développée pour la translittération et la traduction. Pour évaluer les performances de notre approche, nous avons utilisé plusieurs jeux de données couvrant différentes langues et divers alphabets phonétiques, tirés du challenge Pascal Pronalsyl. Les très bons résultats obtenus égalent ou dépassent ceux des meilleurs systèmes de l’état de l’art.</resume>
        <mots_cles>Phonétisation, phonémisation, inférence de règles de réécriture, challenge Pronalsyl, conversion graphème-phonème, translittération</mots_cles>
        <title/>
        <abstract>Phonetizing is a crucial step to process oral documents. In this paper, a new word-based phonetization approach is proposed ; it is automatic, simple, portable and efficient. It relies on machine learning ; thus, the system is built from examples of words with their phonetic representations. More precisely, it makes the most of a technique inferring rewriting rules initially developed for transliteration and translation. In order to evaluate the performances of this approach, we used several datasets from the Pronalsyl Pascal challenge, including different languages. The obtained results equal or outperform those of the best known systems.</abstract>
        <keywords>Phonetization, phonemization, inference of rewriting rule, Pronalsyl challenge, grapheme-phoneme conversion, transliteration</keywords>
      </article>
      <article id="taln-2009-court-028" session="">
        <auteurs>
          <auteur>
            <nom>Josep Maria Crego</nom>
            <email>jmcrego@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Aurélien Max</nom>
            <email>amax@limsi.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>François Yvon</nom>
            <email>yvon@limsi.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIMSI-CNRS, Orsay</affiliation>
          <affiliation affiliationId="2">Université Paris-Sud 11, Orsay</affiliation>
        </affiliations>
        <titre>Plusieurs langues (bien choisies) valent mieux qu’une : traduction statistique multi-source par renforcement lexical</titre>
        <type>court</type>
        <pages/>
        <resume>Les systèmes de traduction statistiques intègrent différents types de modèles dont les prédictions sont combinées, lors du décodage, afin de produire les meilleures traductions possibles. Traduire correctement des mots polysémiques, comme, par exemple, le mot avocat du français vers l’anglais (lawyer ou avocado), requiert l’utilisation de modèles supplémentaires, dont l’estimation et l’intégration s’avèrent complexes. Une alternative consiste à tirer parti de l’observation selon laquelle les ambiguïtés liées à la polysémie ne sont pas les mêmes selon les langues source considérées. Si l’on dispose, par exemple, d’une traduction vers l’espagnol dans laquelle avocat a été traduit par aguacate, alors la traduction de ce mot vers l’anglais n’est plus ambiguë. Ainsi, la connaissance d’une traduction français!espagnol permet de renforcer la sélection de la traduction avocado pour le système français!anglais. Dans cet article, nous proposons d’utiliser des documents en plusieurs langues pour renforcer les choix lexicaux effectués par un système de traduction automatique. En particulier, nous montrons une amélioration des performances sur plusieurs métriques lorsque les traductions auxiliaires utilisées sont obtenues manuellement.</resume>
        <mots_cles>Traduction automatique statistique, désambiguïsation lexicale, réévaluation de listes d’hypothèses</mots_cles>
        <title/>
        <abstract>Statistical Machine Translation (SMT) systems integrate various models that exploit all available features during decoding to produce the best possible translation hypotheses. Correctly translating polysemous words, such as the French word avocat into English (lawyer or avocado) requires integrating complex models. Such translation lexical ambiguities, however, depend on the language pair considered. If one knows, for instance, that avocat was translated into Spanish as aguacate, then translating it into English is no longer ambiguous (avocado). Thus, in this example, the knowledge of the Spanish translation allows to reinforce the choice of the appropriate English word for the French!English system. In this article, we present an approach in which documents available in several languages are used to reinforce the lexical choices made by a SMT system. In particular, we show that gains can be obtained on several metrics when using auxiliary translations produced by human translators.</abstract>
        <keywords>Statistical Machine Translation, Word Sense Disambiguation, N-best list rescoring</keywords>
      </article>
      <article id="taln-2009-court-029" session="">
        <auteurs>
          <auteur>
            <nom>Jean-Leon Bouraoui</nom>
            <email>bouraoui@irit.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Philippe Boissière</nom>
            <email>boissier@irit.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Mustapha Mojahid</nom>
            <email>mojahid@irit.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Nadine Vigouroux</nom>
            <email>vigourou@irit.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Aurélie Lagarrigue</nom>
            <email>alagarri@irit.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Frédéric Vella</nom>
            <email>vella@irit.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Jean-Luc Nespoulous</nom>
            <email>nespoulo@univ-tlse2.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">IRIT (Institut de Recherche en Informatique de Toulouse), UMR CNRS 5505, Université Paul Sabatier, 118, Route de Narbonne, F-31062 Toulouse Cedex</affiliation>
          <affiliation affiliationId="2">OCTOGONE, (E.A 4156) / Laboratoire Jacques Lordat, Université de Toulouse II - Le Mirail Pavillon de la Recherche, 5, Allées Antonio-Machado, F-31058 Toulouse Cedex</affiliation>
        </affiliations>
        <titre>Problématique d'analyse et de modélisation des erreurs en production écrite. Approche interdisciplinaire</titre>
        <type>court</type>
        <pages/>
        <resume>L'objectif du travail présenté ici est la modélisation de la détection et la correction des erreurs orthographiques et dactylographiques, plus particulièrement dans le contexte des handicaps langagiers. Le travail est fondé sur une analyse fine des erreurs d’écriture commises. La première partie de cet article est consacrée à une description précise de la faute. Dans la seconde partie, nous analysons l’erreur (1) en déterminant la nature de la faute (typographique, orthographique, ou grammaticale) et (2) en explicitant sa conséquence sur le niveau de perturbation linguistique (phonologique, orthographique, morphologique ou syntaxique). Il résulte de ce travail un modèle général des erreurs (une grille) que nous présenterons, ainsi que les résultats statistiques correspondants. Enfin, nous montrerons sur des exemples, l’utilité de l’apport de cette grille, en soumettant ces types de fautes à quelques correcteurs. Nous envisageons également les implications informatiques de ce travail.</resume>
        <mots_cles>Typologie et analyse d’erreurs textuelles, assistance à la saisie de textes</mots_cles>
        <title/>
        <abstract>The aim of our work is modeling the detection and the correction of spelling and typing errors, especially in the linguistic disabilities context. The work is based on a fine analysis of clerical errors committed. The first part of this article is devoted to a detailed description of error. In the second part, we analyze error in (1) determining the nature of the fault (typographical, spelling, or grammar) and (2) by explaining its consequences on the level of linguistic disturbance (phonological, orthographic, morphological and syntactic). The outcome of this work is a general model of errors (a grid) that we present, as well as the corresponding statistical results. Finally, we show on examples, the usefulness of this grid, by submitting these types of errors to a few spellcheckers. We also envisage the computer implications of this work.</abstract>
        <keywords>Typology and analyze of textual errors, writing assitance systems</keywords>
      </article>
      <article id="taln-2009-court-030" session="">
        <auteurs>
          <auteur>
            <nom>Rémy Kessler</nom>
            <email>remy.kessler@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Nicolas Béchet</nom>
            <email>nicolas.bechet@lirmm.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Juan-Manuel Torres-Moreno</nom>
            <email>juan-manuel.torres@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Mathieu Roche</nom>
            <email>mathieu.roche@lirmm.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Marc El-Bèze</nom>
            <email>marc.elbeze@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIA / Université d’Avignon, 339 chemin des Meinajariès, 84911 Avignon</affiliation>
          <affiliation affiliationId="2">LIRMM - UMR 5506, CNRS - Université Montpellier 2 - France</affiliation>
        </affiliations>
        <titre>Profilage de candidatures assisté par Relevance Feedback</titre>
        <type>court</type>
        <pages/>
        <resume>Le marché d’offres d’emploi et des candidatures sur Internet connaît une croissance exponentielle. Ceci implique des volumes d’information (majoritairement sous la forme de texte libre) qu’il n’est plus possible de traiter manuellement. Une analyse et catégorisation assistées nous semble pertinente en réponse à cette problématique. Nous proposons E-Gen, système qui a pour but l’analyse et catégorisation assistés d’offres d’emploi et des réponses des candidats. Dans cet article nous présentons plusieurs stratégies, reposant sur les modèles vectoriel et probabiliste, afin de résoudre la problématique du profilage des candidatures en fonction d’une offre précise. Nous avons évalué une palette de mesures de similarité afin d’effectuer un classement pertinent des candidatures au moyen des courbes ROC. L’utilisation d’une forme de relevance feedback a permis de surpasser nos résultats sur ce problème difficile et sujet à une grande subjectivité.</resume>
        <mots_cles>Classification, recherche d’information, Ressources humaines, modèle probabiliste, mesures de similarité, Relevance Feedback</mots_cles>
        <title/>
        <abstract>The market of online job search sites has grown exponentially. This implies volumes of information (mostly in the form of free text) manually impossible to process. An analysis and assisted categorization seems relevant to address this issue. We present E-Gen, a system which aims to perform assisted analysis and categorization of job offers and the responses of candidates. This paper presents several strategies based on vectorial and probabilistic models to solve the problem of profiling applications according to a specific job offer. We have evaluated a range of measures of similarity to rank candidatures by using ROC curves. Relevance feedback approach allows surpass our previous results on this task, difficult and higly subjective.</abstract>
        <keywords>Classification,Information Retrieval, Human Ressources, Probabilistic Model, Similarity measure, Relevance Feedback</keywords>
      </article>
      <article id="taln-2009-court-031" session="">
        <auteurs>
          <auteur>
            <nom>Thierry Hamon</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Natalia Grabar</nom>
            <email/>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIPN – UMR 7030, Université Paris 13 – CNRS, 99 av. J-B Clément, F-93430 Villetaneuse, France</affiliation>
          <affiliation affiliationId="2">Centre de Recherche des Cordeliers, Université Pierre et Marie Curie - Paris6, UMR_S 872, Paris, F-75006 France ; Université Paris Descartes, UMR_S 872, Paris, F-75006 France ; INSERM, U872, Paris, F-75006 France ; HEGP AP-HP</affiliation>
        </affiliations>
        <titre>Profilage sémantique endogène des relations de synonymie au sein de Gene Ontology</titre>
        <type>court</type>
        <pages/>
        <resume>Le calcul de la similarité sémantique entre les termes repose sur l’existence et l’utilisation de ressources sémantiques. Cependant de telles ressources, qui proposent des équivalences entre entités, souvent des relations de synonymie, doivent elles-mêmes être d’abord analysées afin de définir des zones de fiabilité où la similarité sémantique est plus forte. Nous proposons une méthode d’acquisition de synonymes élémentaires grâce à l’exploitation des terminologies structurées au travers l’analyse de la structure syntaxique des termes complexes et de leur compositionnalité. Les synonymes acquis sont ensuite profilés grâce aux indicateurs endogènes inférés automatiquement à partir de ces mêmes terminologies (d’autres types de relations, inclusions lexicales, productivité, forme des composantes connexes). Dans le domaine biomédical, il existe de nombreuses terminologies structurées qui peuvent être exploitées pour la constitution de ressources sémantiques. Le travail présenté ici exploite une de ces terminologies, Gene Ontology.</resume>
        <mots_cles>Terminologie, distance sémantique, relations sémantiques, synonymie</mots_cles>
        <title/>
        <abstract>Computing the semantic similarity between terms relies on existence and usage of semantic resources. However, these resources, often composed of equivalent units, or synonyms, must be first analyzed and weighted in order to define within them the reliability zones where the semantic similarity shows to be stronger. We propose a method for acquisition of elementary synonyms which is based on exploitation of structured terminologies, analysis of syntactic structure of complex (multi-unit) terms and their compositionality. The acquired synonyms are then profiled thanks to endogenous indicators (other types of relations, lexical inclusions, productivity, form of connected components), which are automatically inferred within the same terminologies. In the biomedical area, several structured terminologies have been built and can be exploited for the construction of semantic resources. The work we present in this paper, is applied to terms of one of these terminologies, i.e. the Gene Ontology.</abstract>
        <keywords>Terminology, semantic distance, semantic relations, synonymy</keywords>
      </article>
      <article id="taln-2009-court-032" session="">
        <auteurs>
          <auteur>
            <nom>Fériel Ben Fraj</nom>
            <email>Feriel.BenFraj@riadi.rnu.tn</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Chiraz Ben Othmane Zribi</nom>
            <email>Chiraz.BenOthmane@riadi.rnu.tn</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Mohamed Ben Ahmed</nom>
            <email>Mohamed.BenAhmed@riadi.rnu.tn</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire RIADI  Université La Manouba, ENSI, La Manouba, Tunisie</affiliation>
        </affiliations>
        <titre>Quels attributs discriminants pour une analyse syntaxique par classification de textes en langue arabe ?</titre>
        <type>court</type>
        <pages/>
        <resume>Dans le cadre dune approche déterministe et incrémentale danalyse syntaxique par classification de textes en langue arabe, nous avons prévu de prendre en considération un ensemble varié dattributs discriminants afin de mieux assister la procédure de classification dans ses prises de décisions à travers les différentes étapes danalyse. Ainsi, en plus des attributs morpho-syntaxiques du mot en cours danalyse et des informations contextuelles des mots lavoisinant, nous avons ajouté des informations compositionnelles extraites du fragment de larbre syntaxique déjà construit lors de létape précédente de lanalyse en cours. Ce papier présente notre approche danalyse syntaxique par classification et vise lexposition dune justification expérimentale de lapport de chaque type dattributs discriminants et spécialement ceux compositionnels dans ladite analyse syntaxique.</resume>
        <mots_cles>analyse syntaxique incrémentale, langue arabe, apprentissage automatique, classification, attributs discriminants</mots_cles>
        <title/>
        <abstract>For parsing Arabic texts in a deterministic and incremental classification approach, we suggest that varying discriminative attributes is helpful in disambiguation between structures to classify. Thats why; we consider morpho-syntactic information of the current analyzed word and its surrounding context. In addition, we add a new information type: the compositional one. It consists of the portion of the syntactic tree already constructed until the previous analysis step. In this paper, we expose our parsing approach with classification basis and we justify the utility of the different discriminative attributes and especially the compositional ones.</abstract>
        <keywords>incremental parsing, Arabic language, machine learning, classification, discriminative attributes</keywords>
      </article>
      <article id="taln-2009-court-033" session="">
        <auteurs>
          <auteur>
            <nom>Richard Beaufort</nom>
            <email>richard.beaufort@uclouvain.be</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Anne Dister</nom>
            <email>anne.dister@uclouvain.be</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Hubert Naets</nom>
            <email>hubert.naets@uclouvain.be</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Kévin Macé</nom>
            <email>kevin.mace@uclouvain.be</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Cédrick Fairon</nom>
            <email>cedrick.fairon@uclouvain.be</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CENTAL / Université Catholique de Louvain, 1 Place Blaise Pascal, B-1348 Louvain-la-Neuve, Belgique</affiliation>
        </affiliations>
        <titre>Recto /Verso Un système de conversion automatique ancienne / nouvelle orthographe à visée linguistique et didactique</titre>
        <type>court</type>
        <pages/>
        <resume>Cet article présente Recto /Verso, un système de traitement automatique du langage dédié à l’application des rectifications orthographiques de 1990. Ce système a été développé dans le cadre de la campagne de sensibilisation réalisée en mars dernier par le Service et le Conseil de la langue française et de la politique linguistique de la Communauté française de Belgique. Nous commençons par rappeler les motivations et le contenu de la réforme proposée, et faisons le point sur les principes didactiques retenus dans le cadre de la campagne. La plus grande partie de l’article est ensuite consacrée à l’implémentation du système. Nous terminons enfin par une première analyse de l’impact de la campagne sur les utilisateurs.</resume>
        <mots_cles>Rectifications orthographiques de 1990, conversion ancienne / nouvelle orthographe, objectifs didactiques, machines à états finis</mots_cles>
        <title/>
        <abstract>This paper presents Recto /Verso, a natural language processing system dedicated to the application of the 1990 French spelling rectifications. This system was developed for supporting the awareness-raising campaign promoted last March by the Superior council of the French language in Belgium. We first remind the motivations and the content of the reform, and we draw up the didactic principles followed during the campaign. The most important part of this paper is then focused on the system’s implementation.We finally end by a short analysis of the campaign’s impact on the users.</abstract>
        <keywords>1990 French spelling rectifications, ancient / new spelling conversion, didactic purposes, finite-state machines</keywords>
      </article>
      <article id="taln-2009-court-034" session="">
        <auteurs>
          <auteur>
            <nom>Véronique Malaisé</nom>
            <email>vmalaise@few.vu.nl</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Luit Gazendam</nom>
            <email/>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Willemijn Heeren</nom>
            <email/>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Roeland Ordelman</nom>
            <email/>
            <affiliationId>3</affiliationId>
            <affiliationId>4</affiliationId>
          </auteur>
          <auteur>
            <nom>Hennie Brugman</nom>
            <email/>
            <affiliationId>5</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">VU University Amsterdam</affiliation>
          <affiliation affiliationId="2">Telematica Instituut, Enschede</affiliation>
          <affiliation affiliationId="3">University of Twente, Enschede</affiliation>
          <affiliation affiliationId="4">Netherlands Institute for Sound and Vision, Hilversum</affiliation>
          <affiliation affiliationId="5">MPI for Psycholinguistics, Nijmegen</affiliation>
        </affiliations>
        <titre/>
        <type>court</type>
        <pages/>
        <resume>L’accès aux documents multimédia, dans une archive audiovisuelle, dépend en grande partie de la quantité et de la qualité des métadonnées attachées aux documents, notamment la description de leur contenu. Cependant, l’annotation manuelle des collections est astreignante pour le personnel. De nombreuses archives évoluent vers des méthodes d’annotation (semi-)automatiques pour la création et/ou l’amélioration des métadonnées. Le project CATCH-CHOICE, fondé par NWO, s’est penché sur l’extraction de mots clés à partir de resources textuelles liées aux programmes TV destinés à être archivés (péritextes), en collaboration avec les archives audiovisuelles néerlandaises, Sound and Vision. Cet article se penche sur la question de l’adéquation des transcriptions de Reconnaissance Automatique de la Parole développés dans le projet CATCH-CHoral pour la génération automatique de mots-clés : les mots-clés extraits de ces ressources sont évalués par rapport à des annotations manuelles et par rapport à des mots-clés générés à partir de péritextes décrivant les programmes télévisuels.</resume>
        <mots_cles>Extraction de mots clés, Reconnaissance Automatique de la Parole, Documents Audiovisuels</mots_cles>
        <title>Relevance of ASR for the Automatic Generation of Keywords Suggestions for TV programs</title>
        <abstract>Semantic access to multimedia content in audiovisual archives is to a large extent dependent on quantity and quality of the metadata, and particularly the content descriptions that are attached to the individual items. However, the manual annotation of collections puts heavy demands on resources. A large number of archives are introducing (semi) automatic annotation techniques for generating and/or enhancing metadata. The NWO funded CATCH-CHOICE project has investigated the extraction of keywords from textual resources related to TV programs to be archived (context documents), in collaboration with the Dutch audiovisual archives, Sound and Vision. This paper investigates the suitability of Automatic Speech Recognition transcripts produced in the CATCH-CHoral project for generating such keywords, which we evaluate against manual annotations of the documents, and against keywords automatically generated from context documents describing the TV programs’ content.</abstract>
        <keywords>Keyword extraction, Automatic Speech Recognition, Audiovisual Documents</keywords>
      </article>
      <article id="taln-2009-court-035" session="">
        <auteurs>
          <auteur>
            <nom>Florian Boudin</nom>
            <email>florian.boudin@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Juan-Manuel Torres-Moreno</nom>
            <email>juan-manuel.torres@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIA / Université d’Avignon, 339 chemin des Meinajariès, 84911 Avignon</affiliation>
          <affiliation affiliationId="2">École Polytechnique de Montréal, CP 6079 Succ. Centre-ville, Montréal</affiliation>
        </affiliations>
        <titre>Résumé automatique multi-document et indépendance de la langue : une première évaluation en français</titre>
        <type>court</type>
        <pages/>
        <resume>Le résumé automatique de texte est une problématique difficile, fortement dépendante de la langue et qui peut nécessiter un ensemble de données d’apprentissage conséquent. L’approche par extraction peut aider à surmonter ces difficultés. (Mihalcea, 2004) a démontré l’intérêt des approches à base de graphes pour l’extraction de segments de texte importants. Dans cette étude, nous décrivons une approche indépendante de la langue pour la problématique du résumé automatique multi-documents. L’originalité de notre méthode repose sur l’utilisation d’une mesure de similarité permettant le rapprochement de segments morphologiquement proches. De plus, c’est à notre connaissance la première fois que l’évaluation d’une approche de résumé automatique multi-document est conduite sur des textes en français.</resume>
        <mots_cles>Résumé automatique de texte, Approches à base de graphes, Extraction d’information</mots_cles>
        <title/>
        <abstract>Automatic text summarization is a difficult task, highly language-dependent and which may require a large training dataset. Recently, (Mihalcea, 2004) has shown that graph-based approaches applied to the sentence extraction issue can achieve good results. In this paper, we describe a language-independent approach for automatic multi-document text summarization. The main originality of our approach is the use of an hybrid similarity measure during the graph building process that can identify morphologically similar words. Moreover, this is as far as we know, the first time that the evaluation of a summarization approach is conducted on French documents.</abstract>
        <keywords>Text summarization, Graph-Based approaches, Information Extraction</keywords>
      </article>
      <article id="taln-2009-court-036" session="">
        <auteurs>
          <auteur>
            <nom>Laurent Bozzi</nom>
            <email>Laurent.Bozzi@edf.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Philippe Suignard</nom>
            <email>Philippe.Suignard@edf.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Claire Waast-Richard</nom>
            <email>Claire.Waast-Richard@edf.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">EDF R&amp;D, 1, avenue du Général de Gaulle, 92141 Clamart Cedex</affiliation>
        </affiliations>
        <titre>Segmentation et classification non supervisée de conversations téléphoniques automatiquement retranscrites</titre>
        <type>court</type>
        <pages/>
        <resume>Cette étude porte sur l’analyse de conversations entre des clients et des téléconseillers d’EDF. Elle propose une chaîne de traitements permettant d’automatiser la détection des sujets abordés dans chaque conversation. L’aspect multi-thématique des conversations nous incite à trouver une unité de documents entre le simple tour de parole et la conversation entière. Cette démarche enchaîne une étape de segmentation de la conversation en thèmes homogènes basée sur la notion de cohésion lexicale, puis une étape de text-mining comportant une analyse linguistique enrichie d’un vocabulaire métier spécifique à EDF, et enfin une classification non supervisée des segments obtenus. Plusieurs algorithmes de segmentation ont été évalués sur un corpus de test, segmenté et annoté manuellement : le plus « proche » de la segmentation de référence est C99. Cette démarche, appliquée à la fois sur un corpus de conversations transcrites à la main, et sur les mêmes conversations décodées par un moteur de reconnaissance vocale, aboutit quasiment à l’obtention des 20 mêmes classes thématiques.</resume>
        <mots_cles>audio-mining, text mining, segmentation, classification, catégorisation, reconnaissance vocale, données textuelles, conversations téléphoniques, centre d’appel</mots_cles>
        <title/>
        <abstract>This study focuses on the analysis of conversations and between clients and EDF agent. It offers a range of treatments designed to automate the detection of the topics covered in each conversation. As the conversations are multi-thematic we have to find a document unit, between the simple turn of speech and the whole conversation. The proposed approach starts with a step of segmentation of the conversation (based on lexical cohesion), and then a stage of text-mining, including a language enriched by a vocabulary specific to EDF, and finally a clustering of the segments. Several segmentation algorithms were tested on a test corpus, manually annotated and segmented : the "closest" to the reference segmentation is C99. This approach, applied to both a corpus of conversations transcribed manually, and on the same conversations decoded by a voice recognition engine, leads to almost obtain the same 200 clusters.</abstract>
        <keywords>audio-mining, text mining, segmentation, clustering, categorization, voice recognition, textual data, phone conversation, call center</keywords>
      </article>
      <article id="taln-2009-court-037" session="">
        <auteurs>
          <auteur>
            <nom>Sopheap Seng</nom>
            <email>Sopheap.Seng@imag.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Laurent Besacier</nom>
            <email>Laurent.Besacier@imag.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Brigitte Bigi</nom>
            <email>Brigitte.Bigi@imag.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Eric Castelli</nom>
            <email>Eric.Castelli@mica.edu.vn</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire LIG/GETALP, Grenoble France</affiliation>
          <affiliation affiliationId="2">Laboratoire MICA, CNRS/UMI-2954, Hanoi Vietnam</affiliation>
        </affiliations>
        <titre>Segmentation multiple d’un flux de données textuelles pour la modélisation statistique du langage</titre>
        <type>court</type>
        <pages/>
        <resume>Dans cet article, nous traitons du problème de la modélisation statistique du langage pour les langues peu dotées et sans segmentation entre les mots. Tandis que le manque de données textuelles a un impact sur la performance des modèles, les erreurs introduites par la segmentation automatique peuvent rendre ces données encore moins exploitables. Pour exploiter au mieux les données textuelles, nous proposons une méthode qui effectue des segmentations multiples sur le corpus d’apprentissage au lieu d’une segmentation unique. Cette méthode basée sur les automates d’état finis permet de retrouver les n-grammes non trouvés par la segmentation unique et de générer des nouveaux n-grammes pour l’apprentissage de modèle du langage. L’application de cette approche pour l’apprentissage des modèles de langage pour les systèmes de reconnaissance automatique de la parole en langue khmère et vietnamienne s’est montrée plus performante que la méthode par segmentation unique, à base de règles.</resume>
        <mots_cles>segmentation multiple, langue non segmentée, modélisation statistique du langage</mots_cles>
        <title/>
        <abstract>In this article we deal with the problem of statistical language modelling for under-resourced language with a writing system without word boundary delimiters. While the lack of text resources has an impact on the performance of language models, the errors produced by the word segmentation makes those data less usable. To better exploit the text resources, we propose a method to make multiples segmentations on the training corpus instead of a unique segmentation. This method based on finite state machine allows obtaining the n-grams not found by the unique segmentation and generate new n-grams. We use this approach to train the language models for automatic speech recognition systems of Khmer and Vietnamese languages and it proves better performance than the unique segmentation method.</abstract>
        <keywords>multiple segmentation, unsegmented language, statistical language modeling</keywords>
      </article>
      <article id="taln-2009-court-038" session="">
        <auteurs>
          <auteur>
            <nom>Mathieu Lafourcade</nom>
            <email>lafourcade@lirmm.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Alain Joubert</nom>
            <email>joubert@lirmm.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Stéphane Riou</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIRMM – Université Montpellier 2 - CNRS, Laboratoire d’Informatique, Robotique et Microélectronique de Montpellier, 161, rue Ada, F-34392 Montpellier cedex 5</affiliation>
        </affiliations>
        <titre>Sens et usages d’un terme dans un réseau lexical évolutif</titre>
        <type>court</type>
        <pages/>
        <resume>L’obtention d’informations lexicales fiables est un enjeu primordial en TALN, mais cette collecte peut s’avérer difficile. L’approche présentée ici vise à pallier les écueils de cette difficulté en faisant participer un grand nombre de personnes à un projet contributif via des jeux accessibles sur le web. Ainsi, les joueurs vont construire le réseau lexical, en fournissant de plusieurs manières possibles des associations de termes à partir d'un terme cible et d'une consigne correspondant à une relation typée. Le réseau lexical ainsi produit est de grande taille et comporte une trentaine de types de relations. A partir de cette ressource, nous abordons la question de la détermination des différents sens et usages d’un terme. Ceci est réalisé en analysant les relations entre ce terme et ses voisins immédiats dans le réseau et en calculant des cliques ou des quasi-cliques. Ceci nous amène naturellement à introduire la notion de similarité entre cliques, que nous interprétons comme une mesure de similarité entre ces différents sens et usages. Nous pouvons ainsi construire pour un terme son arbre des usages, qui est une structure de données exploitable en désambiguïsation de sens. Nous présentons quelques résultats obtenus en soulignant leur caractère évolutif.</resume>
        <mots_cles>Traitement Automatique du Langage Naturel, réseau lexical évolutif, relations typées pondérées, similarité entre sens et usages, arbre des usages</mots_cles>
        <title/>
        <abstract>Obtaining reliable lexical information is an essential task in NLP, but it can prove a difficult task. The approach we present here aims at lessening the difficulty: it consists in having people take part in a collective project by offering them playful applications accessible on the web. The players themselves thus build the lexical network, by supplying (in various possible ways) associations between terms from a target term and an instruction concerning a typed relation. The lexical network thus obtained is large and it includes about thirty types of relations. From this network, we then discuss the question of meaning and word usage determination for a term, by searching relations between this term and its neighbours in the network and by computing cliques or quasicliques. This leads us to introduce the notion of similarity between cliques, which can be interpreted as a measure of similarity between these different meanings and word usages. We are thus able to build the tree of word usages for a term: it is a data structure that can be used to disambiguate meaning. Finally, we briefly present some of the results obtained, putting the emphasis on their evolutionary aspect.</abstract>
        <keywords>Natural Language Processing, evolutionary lexical network, typed and weighted relations, meaning and word usage similarity, tree of word usages</keywords>
      </article>
      <article id="taln-2009-court-039" session="">
        <auteurs>
          <auteur>
            <nom>Jean-Leon Bouraoui</nom>
            <email>bouraoui@irit.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Nadine Vigouroux</nom>
            <email>vigourou@irit.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">IRIT – Paul Sabatier, 118, route de Narbonne, 31062 Toulouse, France</affiliation>
        </affiliations>
        <titre>Traitement automatique de disfluences dans un corpus linguistiquement contraint</titre>
        <type>court</type>
        <pages/>
        <resume>Cet article présente un travail de modélisation et de détection des phénomènes de disfluence. Une des spécificité de ce travail est le cadre dans lequel il se situe: le contrôle de la navigation aérienne. Nous montrons ce que ce cadre particulier implique certains choix concernant la modélisation et l'implémentation. Ainsi, nous constatons que la modélisation fondée sur la syntaxe, souvent utilisée dans le traitement des langues naturelles, n'est pas la plus appropriée ici. Nous expliquons la façon dont l'implémentation a été réalisée. Dans une dernière partie, nous présentons la validation de ce dispositif, effectuée sur 400 énoncés.</resume>
        <mots_cles>Dialogue oral spontané, Analyse linguistique de corpus, Compréhension robuste, Contrôle Aérien, Phraséologie, Disfluences, Modèles de langage, Traitement Automatique du Langage Naturel</mots_cles>
        <title/>
        <abstract>This article presents a work of modeling and detection of phenomena disfluences. One of the specificity of this work is its framework: the air traffic control. We show that this particular framework implies certain choices about modeling and implementation. Thus, we find that modeling based on the syntax, often used in natural language processing, is not the most appropriate here. We explain how the implementation has been completed. In a final section, we present the validation of this device, made of 400 utterances.</abstract>
        <keywords>Spontaneous speech dialog, corpus linguistic analysis, robust understanding, Air Traffic Control, phraseology, disfluencies, language models, Natural Language Processing</keywords>
      </article>
      <article id="taln-2009-court-040" session="">
        <auteurs>
          <auteur>
            <nom>Laura Kallmeyer</nom>
            <email>lk@sfs.uni-tuebingen.de</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Wolfgang Maier</nom>
            <email>wo.maier@uni-tuebingen.de</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Yannick Parmentier</nom>
            <email>parmenti@loria.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">SFB 441 / Universität Tübingen, Nauklerstr. 35, D-72074 Tübingen</affiliation>
          <affiliation affiliationId="2">LORIA / Nancy Université, Campus Scientifique, BP 239, F-54506 Vandoeuvre-Lès-Nancy Cedex</affiliation>
        </affiliations>
        <titre>Un Algorithme d’Analyse de Type Earley pour Grammaires à Concaténation d’Intervalles</titre>
        <type>court</type>
        <pages/>
        <resume>Nous présentons ici différents algorithmes d’analyse pour grammaires à concaténation d’intervalles (Range Concatenation Grammar, RCG), dont un nouvel algorithme de type Earley, dans le paradigme de l’analyse déductive. Notre travail est motivé par l’intérêt porté récemment à ce type de grammaire, et comble un manque dans la littérature existante.</resume>
        <mots_cles>Analyse syntaxique déductive, grammaires à concaténation d’intervalles</mots_cles>
        <title/>
        <abstract>We present several different parsing algorithms for Range Concatenation Grammar (RCG), inter alia an entirely novel Earley-style algorithm, using the deductive parsing framework. Our work is motivated by recent interest in range concatenation grammar in general and fills a gap in the existing literature.</abstract>
        <keywords>Deductive parsing, range concatenation grammar</keywords>
      </article>
      <article id="taln-2009-court-041" session="">
        <auteurs>
          <auteur>
            <nom>Jérôme Lehuen</nom>
            <email>Jerome.Lehuen@lium.univ-lemans.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Thierry Lemeunier</nom>
            <email>Thierry.Lemeunier@lium.univ-lemans.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIUM - Université du Maine, Avenue Laënnec, 72085 Le Mans Cedex 9</affiliation>
        </affiliations>
        <titre>Un Analyseur Sémantique pour le DHM Modélisation – Réalisation – Évaluation</titre>
        <type>court</type>
        <pages/>
        <resume>Cet article décrit un modèle de langage dédié au dialogue homme-machine, son implémentation en CLIPS, ainsi qu’une évaluation comparative. Notre problématique n’est ni d’analyser des grands corpus, ni de proposer une grammaire à grande couverture. Notre objectif est de produire des représentations sémantiques utilisables par un module de dialogue à partir d’énoncés oraux courts, le plus souvent agrammaticaux. Une démarche pragmatique nous a conduit à fonder l’analyse sur des principes simples mais efficaces dans le cadre que nous nous sommes fixé. L’algorithme retenu s’inspire de l’analyse tabulaire. L’évaluation que nous présentons repose sur le corpus MEDIA qui a fait l’objet d’une annotation sémantique manuelle de référence pour une campagne d’évaluation d’analyseurs sémantiques pour le dialogue. Les résultats que nous obtenons place notre analyseur dans le trio de tête des systèmes évalués lors de la campagne de juin 2005, et nous confortent dans nos choix d’algorithme et de représentation des connaissances.</resume>
        <mots_cles>Analyse sémantique tabulaire, contexte dialogique, évaluation</mots_cles>
        <title/>
        <abstract>This article describes a natural language model dedicated to man-machine dialogue, its implementation in CLIPS, as well as a comparative evaluation. Our problematic is neither to analyze large corpora nor to propose a large-coverage grammar. Our objective is to produce semantic representations usable for a dialog module from short oral utterances that are rather often ungrammatical. A pragmatic approach leads us to base parsing on simple but efficient principles within the man-machine dialog framework. Chart parsing influences the algorithm we have chosen. The evaluation that we present here uses the MEDIA corpora. It has been manually annotated and represents a standard usable in an evaluation campaign for semantic parsers dedicated to the dialog. With the results that we obtain our parser is in the three bests of the systems evaluated in the June 2005 campaign. It confirms our choices of algorithm and of knowledge representation.</abstract>
        <keywords>Semantic chart parsing, dialogue context, evaluation</keywords>
      </article>
      <article id="taln-2009-court-042" session="">
        <auteurs>
          <auteur>
            <nom>Silvia Fernández Sabido</nom>
            <email>silvia.fernandez@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Juan-Manuel Torres-Moreno</nom>
            <email>juan-manuel.torres@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire Informatique d’Avignon, BP 1228 84911 Avignon</affiliation>
          <affiliation affiliationId="2">Laboratoire de Physique de Matériaux, UHP-Nancy, 54506 Vandoeuvre</affiliation>
        </affiliations>
        <titre>Une approche exploratoire de compression automatique de phrases basée sur des critères thermodynamiques</titre>
        <type>court</type>
        <pages/>
        <resume>Nous présentons une approche exploratoire basée sur des notions thermodynamiques de la Physique statistique pour la compression de phrases. Nous décrivons le modèle magnétique des verres de spins, adapté à notre conception de la problématique. Des simulations Métropolis Monte-Carlo permettent d’introduire des fluctuations thermiques pour piloter la compression. Des comparaisons intéressantes de notre méthode ont été réalisées sur un corpus en français.</resume>
        <mots_cles>Compression de phrases, Résumé automatique, Résumé par extraction, Enertex, Mécanique statistique</mots_cles>
        <title/>
        <abstract>We present an exploratory approach based on thermodynamic concepts of Statistical Physics for sentence compression.We describe the magnetic model of spin glasses, well suited to our conception of problem. The Metropolis Monte-Carlo simulations allow to introduce thermal fluctuations to drive the compression. Interesting comparisons of our method were performed on a French text corpora.</abstract>
        <keywords>Sentence Compression, Automatic Summarization, Extraction Summarization, Enertex, Statistical Mechanics</keywords>
      </article>
      <article id="taln-2009-court-043" session="">
        <auteurs>
          <auteur>
            <nom>Sebastián Peña Saldarriaga</nom>
            <email>sebastian.pena-saldarriaga@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Emmanuel Morin</nom>
            <email>Emmanuel.Morin@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Christian Viard-Gaudin</nom>
            <email>Christian.Viard-Gaudin@univ-nantes.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LINA - UMR CNRS 6241, Université de Nantes</affiliation>
          <affiliation affiliationId="2">IRCCyN - UMR CNRS 6597, Université de Nantes</affiliation>
        </affiliations>
        <titre>Un nouveau schéma de pondération pour la catégorisation de documents manuscrits</titre>
        <type>court</type>
        <pages/>
        <resume>Les schémas de pondération utilisés habituellement en catégorisation de textes, et plus généralement en recherche d’information (RI), ne sont pas adaptés à l’utilisation de données liées à des textes issus d’un processus de reconnaissance de l’écriture. En particulier, les candidats-mot à la reconnaissance ne pourraient être exploités sans introduire de fausses occurrences de termes dans le document. Dans cet article nous présentons un nouveau schéma de pondération permettant d’exploiter les listes de candidats-mot. Il permet d’estimer le pouvoir discriminant d’un terme en fonction de la probabilité a posteriori d’un candidat-mot dans une liste de candidats. Les résultats montrent que le taux de classification de documents fortement dégradés peut être amélioré en utilisant le schéma proposé.</resume>
        <mots_cles>Catégorisation de textes, écriture en-ligne, n-best candidats, pondération</mots_cles>
        <title/>
        <abstract>The traditional weighting schemes used in information retrieval, and especially in text categorization cannot exploit information intrinsic to texts obtained through an on-line handwriting recognition process. In particular, top n (n &gt; 1) candidates could not be used without introducing false occurrences of spurious terms thus making the resulting text noisier. In this paper, we propose an improved weighting scheme for text categorization, that estimates a term importance from the posterior probabilities of the top n candidates. The experimental results show that the categorization rate of poorly recognized texts increases when our weighting model is applied.</abstract>
        <keywords>Text categorization, on-line handwriting, n-best candidates, weighting</keywords>
      </article>
      <article id="taln-2009-court-044" session="">
        <auteurs>
          <auteur>
            <nom>Yves Scherrer</nom>
            <email>yves.scherrer@unige.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LATL, Université de Genève, Rue de Candolle 5, 1211 Genève 4, Suisse</affiliation>
        </affiliations>
        <titre>Un système de traduction automatique paramétré par des atlas dialectologiques</titre>
        <type>court</type>
        <pages/>
        <resume>Contrairement à la plupart des systèmes de traitement du langage, qui s’appliquent à des langues écrites et standardisées, nous présentons ici un système de traduction automatique qui prend en compte les spécificités des dialectes. En général, les dialectes se caractérisent par une variation continue et un manque de données textuelles en qualité et quantité suffisantes. En même temps, du moins en Europe, les dialectologues ont étudié en détail les caractéristiques linguistiques des dialectes. Nous soutenons que des données provenant d’atlas dialectologiques peuvent être utilisées pour paramétrer un système de traduction automatique. Nous illustrons cette idée avec le prototype d’un système de traduction basé sur des règles, qui traduit de l’allemand standard vers les différents dialectes de Suisse allemande. Quelques exemples linguistiquement motivés serviront à exposer l’architecture de ce système.</resume>
        <mots_cles>Traduction automatique, dialectes, langues proches, langues germaniques</mots_cles>
        <title/>
        <abstract>Most natural language processing systems apply to written, standardized language varieties. In contrast, we present a machine translation system that takes into account some specificites of dialects : dialect areas show continuous variation along all levels of linguistic analysis, and textual data is often not available in sufficient quality and quantity. At the same time, many European dialect areas are well studied by dialectologists. We argue that data from dialectological atlases can be used to parametrize a machine translation system. We illustrate this idea by presenting the prototype of a rule-based machine translation system that translates from Standard German into the Swiss German dialect continuum. Its architecture is explained with some linguistically motivated examples.</abstract>
        <keywords>Machine translation, dialects, closely related languages, Germanic languages</keywords>
      </article>
      <article id="taln-2009-court-045" session="">
        <auteurs>
          <auteur>
            <nom>Jean-Cédric Chappelier</nom>
            <email>jean-cedric.chappelier@epfl.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Emmanuel Eckard</nom>
            <email>emmanuel.eckard@epfl.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire d’Intelligence Artificielle, École polytechnique fédérale de Lausanne, Suisse</affiliation>
        </affiliations>
        <titre>Utilisation de PLSI en recherche d’information Représentation des requêtes</titre>
        <type>court</type>
        <pages/>
        <resume>Le modèle PLSI (« Probabilistic Latent Semantic Indexing ») offre une approche de l’indexation de documents fondée sur des modèles probabilistes de catégories sémantiques latentes et a conduit à des applications dans différents domaines. Toutefois, ce modèle rend impossible le traitement de documents inconnus au moment de l’apprentissage, problème particulièrement sensible pour la représentation des requêtes dans le cadre de la recherche d’information. Une méthode, dite de « folding-in », permet dans une certaine mesure de contourner ce problème, mais présente des faiblesses. Cet article introduit nouvelle une mesure de similarité document-requête pour PLSI, fondée sur lesmodèles de langue, où le problème du « folding-in » ne se pose pas. Nous comparons cette nouvelle similarité aux noyaux de Fisher, l’état de l’art en la matière. Nous présentons aussi une évaluation de PLSI sur un corpus de recherche d’information de près de 7500 documents et de plus d’un million d’occurrences de termes provenant de la collection TREC–AP, une taille considérable dans le cadre de PLSI.</resume>
        <mots_cles/>
        <title/>
        <abstract>The PLSI model (“Probabilistic Latent Semantic Indexing”) offers a document indexing scheme based on probabilistic latent category models. It entailed applications in diverse fields, notably in information retrieval (IR). Nevertheless, PLSI cannot process documents not seen during parameter inference, a major liability for queries in IR. A method known as “folding-in” allows to circumvent this problem up to a point, but has its own weaknesses. The present paper introduces a new document-query similarity measure for PLSI based on language models that entirely avoids the problem a query projection.We compare this similarity to Fisher kernels, the state of the art similarities for PLSI. Moreover, we present an evaluation of PLSI on a particularly large training set of almost 7500 document and over one million term occurrence large, created from the TREC–AP collection.</abstract>
        <keywords/>
      </article>
      <article id="taln-2009-court-046" session="">
        <auteurs>
          <auteur>
            <nom>Olivier Ferret</nom>
            <email>olivier.ferret@cea.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CEA, LIST, 18 route du Panorama, BP6, Fontenay-aux-Roses, F-92265 France</affiliation>
        </affiliations>
        <titre>Utiliser des sens de mots pour la segmentation thématique ?</titre>
        <type>court</type>
        <pages/>
        <resume>La segmentation thématique est un domaine de l’analyse discursive ayant donné lieu à de nombreux travaux s’appuyant sur la notion de cohésion lexicale. La plupart d’entre eux n’exploitent que la simple récurrence lexicale mais quelques uns ont néanmoins exploré l’usage de connaissances rendant compte de cette cohésion lexicale. Celles-ci prennent généralement la forme de réseaux lexicaux, soit construits automatiquement à partir de corpus, soit issus de dictionnaires élaborés manuellement. Dans cet article, nous examinons dans quelle mesure une ressource d’une nature un peu différente peut être utilisée pour caractériser la cohésion lexicale des textes. Il s’agit en l’occurrence de sens de mots induits automatiquement à partir de corpus, à l’instar de ceux produits par la tâche «Word Sense Induction and Discrimination » de l’évaluation SemEval 2007. Ce type de ressources apporte une structuration des réseaux lexicaux au niveau sémantique dont nous évaluons l’apport pour la segmentation thématique.</resume>
        <mots_cles>Segmentation thématique, désambiguïsation sémantique</mots_cles>
        <title/>
        <abstract>Many topic segmenters rely on lexical cohesion. Most of them only exploit lexical recurrence but some of them makes use of knowledge sources about lexical cohesion. These sources are generally lexical networks built either by hand or automatically from corpora. In this article, we study to what extent a new source of knowledge about lexical cohesion can be used for topic segmentation. This source is a set of word senses that were automatically discriminated from corpora, as the word senses resulting from the Word Sense Induction and Discrimination task of the SemEval 2007 evaluation. Such a resource is a way to structurate lexical networks at a semantic level. The impact of this structuring on topic segmentation is evaluated in this article.</abstract>
        <keywords>Topic segmentation, word sense disambiguation</keywords>
      </article>
    </articles>
  </conference>
  <conference>
    <edition>
      <acronyme>RECITAL'2010</acronyme>
      <titre>Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues</titre>
      <ville>Montréal</ville>
      <pays>Canada</pays>
      <dateDebut>2010-07-19</dateDebut>
      <dateFin>2010-07-23</dateFin>
      <presidents>
        <nom>Alexandre Patry</nom>
        <nom>Philippe Langlais</nom>
        <nom>Aurélien Max</nom>
      </presidents>
      <typeArticles>
        <type id="long">Papiers longs</type>
      </typeArticles>
      <statistiques>
        <acceptations id="long" soumissions="16">11</acceptations>
      </statistiques>
      <siteWeb>http://www.groupes.polymtl.ca/taln2010/recital.php</siteWeb>
      <meilleurArticle>
        <articleId>recital-2010-long-001</articleId>
      </meilleurArticle>
    </edition>
    <articles>
      <article id="recital-2010-long-001" session="Analyse textuelle">
        <auteurs>
          <auteur>
            <nom>Audrey Laroche</nom>
            <email>audrey.laroche@umontreal.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">OLST, Dép. de linguistique et de traduction, Université de Montréal</affiliation>
        </affiliations>
        <titre>Attribution d’auteur au moyen de modèles de langue et de modèles stylométriques</titre>
        <type>long</type>
        <pages/>
        <resume>Dans une tâche consistant à trouver l’auteur (parmi 53) de chacun de 114 textes, nous analysons la performance de modèles de langue et de modèles stylométriques sous les angles du rappel et du nombre de paramètres. Le modèle de mots bigramme à lissage de Kneser-Ney modifié interpolé est le plus performant (75 % de bonnes réponses au premier rang). Parmi les modèles stylométriques, une combinaison de 7 paramètres liés aux parties du discours produit les meilleurs résultats (rappel de 25 % au premier rang). Dans les deux catégories de modèles, le rappel maximal n’est pas atteint lorsque le nombre de paramètres est le plus élevé.</resume>
        <mots_cles>Attribution d’auteur, modèle de langue, stylométrie, n-grammes, vecteurs de traits</mots_cles>
        <title/>
        <abstract>In a task consisting of attributing the proper author (among 53) of each of 114 texts, we analyze the performance of language models and stylometric models from the point of view of recall and the number of parameters. The best performance is obtained with a bigram word model using interpolated modified Kneser-Ney smoothing (first-rank recall of 75 %). The best of the stylometric models, which combines 7 parameters characterizing the proportion of the different parts of speech in a text, has a firstrank recall of 25 % only. In both types of models, the maximal recall is not reached when the number of parameters is highest.</abstract>
        <keywords>Authorship attribution, language model, stylometry, n-grams, feature vectors</keywords>
      </article>
      <article id="recital-2010-long-002" session="Parole">
        <auteurs>
          <auteur>
            <nom>Hyeran Lee</nom>
            <email>hlee1@univ-montp3.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Philippe Gambette</nom>
            <email/>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Elsa Maillé</nom>
            <email/>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Constance Thuillier</nom>
            <email/>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Praxiling - Université Montpellier 3 / CNRS, 17 rue Abbé de l'Épée 34090 Montpellier France</affiliation>
          <affiliation affiliationId="2">LIRMM - Université Montpellier 2 / CNRS, 34095 Montpellier Cedex 5</affiliation>
          <affiliation affiliationId="3">ISTR - Université Claude Bernard Lyon 1, 69622 Villeurbanne Cedex</affiliation>
        </affiliations>
        <titre>Densidées : calcul automatique de la densité des idées dans un corpus oral</titre>
        <type>long</type>
        <pages/>
        <resume>La densité des idées, qui correspond au ratio entre le nombre de propositions sémantiques et le nombre de mots dans un texte reflète la qualité informative des propositions langagières d’un texte. L'apparition de la maladie d'Alzheimer a été reliée à une dégradation de la densité des idées, ce qui explique l'intérêt pour un calcul automatique de cette mesure. Nous proposons une méthode basée sur un étiquetage morphosyntaxique et des règles d'ajustement, inspirée du logiciel CPIDR. Cette méthode a été validée sur un corpus de quarante entretiens oraux transcrits et obtient de meilleurs résultats pour le français que CPIDR pour l’anglais. Elle est implémentée dans le logiciel libre Densidées disponible sur http://code.google.com/p/densidees.</resume>
        <mots_cles>densité des idées, analyse prédicative, étiquetage sémantique, psycholinguistique</mots_cles>
        <title/>
        <abstract>Idea density, which is the ratio of semantic propositions divided by the number of words in a text, reflects the informative quality of the sentences of a text. A decreasing idea density has been identified as one of the symptoms of Alzheimer’s disease, which explains the interest in an automatic calculation of idea density. We propose a method based on part-of-speech tagging followed by adjustment rules inspired from the CPIDR software. This method was validated on a corpus of 40 transcribed conversations in French and obtains better results in French than CPIDR in English. It is implemented in the free software Densidées available at http://code.google.com/p/densidees.</abstract>
        <keywords>idea density, propositional analysis, semantic tagging, psycholinguistics</keywords>
      </article>
      <article id="recital-2010-long-003" session="Segmentation">
        <auteurs>
          <auteur>
            <nom>Li-Chi Wu</nom>
            <email>lucielichi@gmail.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">SYLED, Université Sorbonne Nouvelle Paris III, 13 rue de Santeuil, 75005 Paris, France</affiliation>
        </affiliations>
        <titre>Outils de segmentation du chinois et textométrie</titre>
        <type>long</type>
        <pages/>
        <resume>La segmentation en mots est une première étape possible dans le traitement automatique de la langue chinoise. Les systèmes de segmentation se sont beaucoup développés depuis le premier apparu dans les années 1980. Il n’existe cependant aucun outil standard aujourd’hui. L’objectif de ce travail est de faire une comparaison des différents outils de segmentation en s’appuyant sur une analyse statistique. Le but est de définir pour quel type de texte chacun d’eux est le plus performant. Quatre outils de segmentation et deux corpus avec des thèmes distincts ont été choisis pour cette étude. À l’aide des outils textométriques Lexico3 et mkAlign, nous avons centré notre analyse sur le nombre de syllabes du chinois. Les données quantitatives ont permis d’objectiver des différences entre les outils. Le système Hylanda s’avère performant dans la segmentation des termes spécialisés et le système Stanford est plus indiqué pour les textes généraux. L’étude de la comparaison des outils de segmentation montre le statut incontournable de l’analyse textométrique aujourd’hui, celle-ci permettant d’avoir accès rapidement à la recherche d’information.</resume>
        <mots_cles>Textométrie, comparaison des segmenteurs chinois, nombre de syllabes</mots_cles>
        <title/>
        <abstract>Chinese word segmentation is the first step in Chinese natural language processing. The system of segmentation has considerably developed since the first automatic system of segmentation of the 1980’s. However, till today there are no standard tools. The aim of this paper is to compare various tools of segmentation by through statistical analysis. Our goal is to identify the kind of texts for which these segmentation tools are the most effective. This study chose four segmentation tools and two corpora, marked by distinct themes. Using two textometric toolboxes, Lexico3 and mkAlign, we focused on the number of syllables in Chinese. The quantitative data allowed us to objectify disparities between tools. The Hylanda system turns out to be effective in the segmentation of specialized terms and the Stanford system is more appropriate for general texts. The comparative study of segmenters shows the undeniable status of textometrical analysis which is able to quickly access information retrieval.</abstract>
        <keywords>Textometry, comparison of Chinese segmenters, number of syllables</keywords>
      </article>
      <article id="recital-2010-long-004" session="Résumé/Extraction">
        <auteurs>
          <auteur>
            <nom>Mani Ezzat</nom>
            <email>mani.ezzat@arisem.com</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Er-Tim, Inalco, 75343 Paris</affiliation>
          <affiliation affiliationId="2">Arisem, Thales, 91300 Massy</affiliation>
        </affiliations>
        <titre>Acquisition de grammaires locales pour l’extraction de relations entre entités nommées</titre>
        <type>long</type>
        <pages/>
        <resume>La constitution de ressources linguistiques est une tâche cruciale pour les systèmes d’extraction d’information fondés sur une approche symbolique. Ces systèmes reposent en effet sur des grammaires utilisant des informations issues de dictionnaires électroniques ou de réseaux sémantiques afin de décrire un phénomène linguistique précis à rechercher dans les textes. La création et la révision manuelle de telles ressources sont des tâches longues et coûteuses en milieu industriel. Nous présentons ici un nouvel algorithme produisant une grammaire d’extraction de relations entre entités nommées, de manière semi-automatique à partir d’un petit ensemble de phrases représentatives. Dans un premier temps, le linguiste repère un jeu de phrases pertinentes à partir d’une analyse des cooccurrences d’entités repérées automatiquement. Cet échantillon n’a pas forcément une taille importante. Puis, un algorithme permet de produire une grammaire en généralisant progressivement les éléments lexicaux exprimant la relation entre entités. L’originalité de l’approche repose sur trois aspects : une représentation riche du document initial permettant des généralisations pertinentes, la collaboration étroite entre les aspects automatiques et l’apport du linguiste et sur la volonté de contrôler le processus en ayant toujours affaire à des données lisibles par un humain.</resume>
        <mots_cles>relation, entité nommée, grammaire</mots_cles>
        <title/>
        <abstract>Building linguistics resources is a vital task for information extraction systems based on a symbolic approach : cascaded patterns use information from digital dictionaries or semantic networks to describe a precise linguistic phenomenon in texts. The manual elaboration and revision of such patterns is a long and costly process in an industrial environment. This work presents a semi-automatic method for creating patterns that detect relations between named entities in corpora. The process is made of two different phases. The result of the first phase is a collection of sentences containing the relevant relation. This collection isn’t necessairly big. During the second phase, an algorithm automatically produces the recognition grammar by generalizing the actual content of the different relevant sentences. This method is original from three different points of view : it uses a rich description of the linguistic content to allow accurate generalizations, it is based on a close collaboration between an automatic process and a linguist and, lastly, the output of the acquisition process is always readable and modifiable by the end user.</abstract>
        <keywords>relation, named entity, pattern</keywords>
      </article>
      <article id="recital-2010-long-005" session="Traduction">
        <auteurs>
          <auteur>
            <nom>Houda Bouamor</nom>
            <email>houda.bouamor@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIMSI-CNRS, groupe ILES, Université Paris-Sud 11, Orsay, France</affiliation>
        </affiliations>
        <titre>Construction d’un corpus de paraphrases d’énoncés par traduction multiple multilingue</titre>
        <type>long</type>
        <pages/>
        <resume>Les corpus de paraphrases à large échelle sont importants dans de nombreuses applications de TAL. Dans cet article nous présentons une méthode visant à obtenir un corpus parallèle de paraphrases d’énoncés en français. Elle vise à collecter des traductions multiples proposées par des contributeurs volontaires francophones à partir de plusieurs langues européennes. Nous formulons l’hypothèse que deux traductions soumises indépendamment par deux participants conservent généralement le sens de la phrase d’origine, quelle que soit la langue à partir de laquelle la traduction est effectuée. L’analyse des résultats nous permet de discuter cette hypothèse.</resume>
        <mots_cles>corpus monolingue parallèle, paraphrases, traductions multiples</mots_cles>
        <title/>
        <abstract>Large scale paraphrase corpora are important for a variety of natural language processing applications. In this paper, we present an approach which collects multiple translations from several languages proposed by volunteers in order to obtain a parallel corpus of paraphrases in French. We hypothesize that two translations proposed independently by two volunteers usually retain the meaning of the original sentence, regardless of the language from which the translation is done. The analysis of results allows us to discuss this hypothesis.</abstract>
        <keywords>monolingual parallel corpora, paraphrases, multiple translations</keywords>
      </article>
      <article id="recital-2010-long-006" session="Poster">
        <auteurs>
          <auteur>
            <nom>Adila Amaria Bouabdallah</nom>
            <email>amaria@info.univ-angers.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LERIA - Université d’Angers, 2 Bd Lavoisier 49000 Angers, France</affiliation>
        </affiliations>
        <titre>Ces noms qui cachent des événements : un premier repérage</titre>
        <type>long</type>
        <pages/>
        <resume>La détection des informations temporelles est cruciale pour le traitement automatique des textes, qu’il s’agisse de modélisation linguistique, d’applications en compréhension du langage ou encore de tâches de recherche documentaire ou d’extraction d’informations. De nombreux travaux ont été dédiés à l’analyse temporelle des textes, et plus précisément l’annotation des expressions temporelles ou des événements sous leurs différentes formes : verbales, adjectivales ou nominales. Dans cet article, nous décrivons une méthode pour la détection des syntagmes nominaux dénotant des événements. Notre approche est basée sur l’implémentation d’un test linguistique simple proposé par les linguistes pour cette tâche. Nous avons expérimenté notre méthode sur deux corpus différents ; le premier est composé d’articles de presse et le second est beaucoup plus grand, utilisant une interface pour interroger automatiquement le moteur de recherche Yahoo. Les résultats obtenus ont montré que cette méthode se révèle plus pertinente pour un plus large corpus.</resume>
        <mots_cles>Repérage des événements nominaux, annotation temporelle</mots_cles>
        <title/>
        <abstract>The detection of temporal information is a crucial task for automatic text processing. It is not only used in linguistics for the modelization of phenomenon and reasoning implying time entities but also in numerous applications in language comprehension, information retrieval, question-answering and information extraction. Many studies have been devoted to the temporal analysis of texts, and more precisely to the tagging of temporal entities and relations occurring in texts. Among these lasts, the various avatars of events in their multiples occurring forms has been tackled by numerous works. In this article we describe a method for the detection of noun phrases denoting events. Our approach is based on the implementation of a simple linguistic test proposed by linguists for this task. Our method is applied on two different corpora ; the first is composed of newspaper articles and the second, a much larger one, rests on an interface for automatically querying the Yahoo search engine. Primary results are encouraging and increasing the size of the learning corpora should allow for a real statistical validation of the results.</abstract>
        <keywords>Nominal event recognition, temporal annotation</keywords>
      </article>
      <article id="recital-2010-long-007" session="Poster">
        <auteurs>
          <auteur>
            <nom>Baptiste Chardon</nom>
            <email>chardon@irit.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Synapse Développement, 33 rue Maynard, 31000 Toulouse</affiliation>
          <affiliation affiliationId="2">IRIT, UMR5505, 31000 Toulouse</affiliation>
        </affiliations>
        <titre>Catégorisation automatique d'adjectifs d'opinion à partir d'une ressource linguistique générique</titre>
        <type>long</type>
        <pages/>
        <resume>Cet article décrit un processus d’annotation manuelle de textes d’opinion, basé sur un schéma fin d'annotation indépendant de la langue et du corpus. Ensuite, à partir d'une partie de ce schéma, une méthode de construction automatique d'un lexique d'opinion à partir d'un analyseur syntaxique et d'une ressource linguistique est décrite. Cette méthode consiste à construire un arbre de décision basé sur les classes de concepts de la ressource utilisée. Dans un premier temps, nous avons étudié la couverture du lexique d'opinion obtenu par comparaison avec l’annotation manuelle effectuée sur un premier corpus de critiques de restaurants. La généricité de ce lexique a été mesurée en le comparant avec un second lexique, généré à partir d'un corpus de commentaires de films. Dans un second temps, nous avons évalué l'utilisabilité du lexique au travers d'une tâche extrinsèque, la reconnaissance de la polarité de commentaires d'internautes.</resume>
        <mots_cles>Analyse d'opinion, Extension de lexique, Annotation d'opinions</mots_cles>
        <title/>
        <abstract>This paper introduces a manual annotation process of opinion texts, based on a fine-featured annotation scheme, independent of language and corpus. Then, from a part of this scheme, a method to build automatically an opinion lexicon from a syntactic analyser and a linguistic resource is described. This method consists in building a decision tree from the classes of the resource. The coverage of the lexicon has been determined by comparing it to the gold annotation of a restaurants review corpus. Its genericity was determined by comparing it to another lexicon generated from a different domain corpus (movie reviews). Eventually, the usefulness of the lexicon has been measured with an extrinsic task, the recognition of the polarity of reviews.</abstract>
        <keywords>Opinion mining, Lexicon extension, Opinion annotation</keywords>
      </article>
      <article id="recital-2010-long-008" session="Poster">
        <auteurs>
          <auteur>
            <nom>Mohamed Hédi Maâloul</nom>
            <email>mohamed.maaloul@lpl-aix.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Iskandar keskes</nom>
            <email>iskandarkeskes@gmail.com</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire LPL, 5 avenue Pasteur - BP 80975, 13604 Aix-en-Provence, France</affiliation>
          <affiliation affiliationId="2">Laboratoire MIRACL, Route de Tunis Km 10, BP 242, 3021 – Sfax, Tunisie</affiliation>
        </affiliations>
        <titre>Résumé automatique de documents arabes basé sur la technique RST</titre>
        <type>long</type>
        <pages/>
        <resume>Dans cet article, nous nous intéressons au résumé automatique de textes arabes. Nous commençons par présenter une étude analytique réalisée sur un corpus de travail qui nous a permis de déduire, suite à des observations empiriques, un ensemble de relations et de frames (règles ou patrons) rhétoriques; ensuite nous présentons notre méthode de production de résumés pour les textes arabes. La méthode que nous proposons se base sur la Théorie de la Structure Rhétorique (RST) (Mann et al., 1988) et utilise des connaissances purement linguistiques. Le principe de notre proposition s’appuie sur trois piliers. Le premier pilier est le repérage des relations rhétoriques entres les différentes unités minimales du texte dont l’une possède le statut de noyau – segment de texte primordial pour la cohérence – et l’autre a le statut noyau ou satellite – segment optionnel. Le deuxième pilier est le dressage et la simplification de l’arbre RST. Le troisième pilier est la sélection des phrases noyaux formant le résumé final, qui tiennent en compte le type de relation rhétoriques choisi pour l’extrait.</resume>
        <mots_cles>Théorie de la Structure Rhétorique, Relations rhétoriques, Marqueurs linguistiques, Résumé automatique de textes arabes</mots_cles>
        <title/>
        <abstract>In this paper, we focus on automatic summarization of Arabic texts. We start by presenting an analytical study carried out on a study corpus which enabled us to deduce, following empirical observations, a set of relations and rhetorical frames; then we present our proposed method to produce summaries for Arabic texts. This method is based bases on the Rhetorical Structure Theory (RST) technique (Mann and Al., 1988) and uses purely linguistic knowledge. The principle of the proposed method is based on three pillars. The first pillar is the location of the rhetorical relations between the various minimal units of the text of which one has the status of nucleus - text segment necessary to maintain coherence - and the other has the status of nucleus or satellite - optional segment. The second pillar is the representation and the simplification of RST-tree that is considered most descriptive. The third pillar is the selection of the nucleus sentences forming the final summary, which hold in account the type of rhetorical relations chosen.</abstract>
        <keywords>Rhetorical Structure Theory, Rhetorical relations, Linguistic markers, Automatic summarization of Arabic texts</keywords>
      </article>
      <article id="recital-2010-long-009" session="Poster">
        <auteurs>
          <auteur>
            <nom>Hee-Jin Ro</nom>
            <email>heejinro@yahoo.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire LaLIC (Langues, logiques, Informatique et Cognition), Université Paris-Sorbonne, Maison de la Recherche, 28 rue Serpente, 75006 Paris</affiliation>
        </affiliations>
        <titre>Inférences aspecto-temporelles analysées avec la Logique Combinatoire</titre>
        <type>long</type>
        <pages/>
        <resume>Ce travail s’inscrit dans une recherche centrée sur une approche de l’Intelligence Artificielle (IA) et de la linguistique computationnelle. Il permet d’intégrer différentes techniques formelles de la Logique Combinatoire avec des types (Curry) et sa programmation fonctionnelle (Haskell) avec une théorie énonciative du temps et de l’aspect. Nous proposons des calculs formels de valeurs aspectotemporelles (processus inaccompli présent, processus inaccompli passé, événement passé et étatrésultant présent) associées à des représentations de significations verbales sous forme de schèmes applicatifs.</resume>
        <mots_cles>Logique Combinatoire, Référentiel énonciatif, Schème sémantico-cognitif, Grammaire Applicative et Cognitive, Haskell</mots_cles>
        <title/>
        <abstract>This work is in line with the research centered on an approach of the Artificial Intelligence and the Computational linguistic. It allows integrating different formal technologies of the Combinatory Logic with types (Curry) and their functional programme (Haskell) with an enunciative theory of the tense and the aspect. We propose formal calculus of aspecto-temporal values (present unaccomplished process, past unaccomplished process, past event and present résultative-state) associated with representations of verbal meanings in the form of applicative schemes.</abstract>
        <keywords>Combinatory Logic, Enunciative Frame of reference, Semantic-cognitive scheme, Applicative and Cognitive Grammar, Haskell</keywords>
      </article>
      <article id="recital-2010-long-010" session="Poster">
        <auteurs>
          <auteur>
            <nom>Selja Seppälä</nom>
            <email>seppala2@etu.unige.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">TIM/ISSCO, ETI, Université de Genève, Suisse</affiliation>
        </affiliations>
        <titre>Automatiser la rédaction de définitions terminographiques : questions et traitements</titre>
        <type>long</type>
        <pages/>
        <resume>Dans cet article, nous présentons une analyse manuelle de corpus de contextes conceptuels afin (i) de voir dans quelle mesure les méthodes de TALN existantes sont en principe adéquates pour automatiser la rédaction de définitions terminographiques, et (ii) de dégager des question précises dont la résolution permettrait d’automatiser davantage la production de définitions. Le but est de contribuer à la réflexion sur les enjeux de l’automatisation de cette tâche, en procédant à une série d’analyses qui nous mènent, étape par étape, à examiner l’adéquation des méthodes d’extraction de définitions et de contextes plus larges au travail terminographique de rédaction des définitions. De ces analyses émergent des questions précises relatives à la pertinence des informations extraites et à leur sélection. Des propositions de solutions et leurs implications pour le TALN sont examinées.</resume>
        <mots_cles>Terminologie, définitions terminographiques, sélection des traits, pertinence des traits, extraction de définitions, contextes conceptuels, traitement automatique des définitions.</mots_cles>
        <title/>
        <abstract>A manual corpus analysis of conceptual contexts is presented in order (i) to indicatively evaluate to what extent NLP methods can in principle be used to automate the production of terminographic definitions, and (ii) to identify central questions to be answered if one wants to further automate the task. The objective is to contribute to reflection on the challenges faced by the automation of this task. Through a series of analyses, the adequacy of extraction methods for defining or knowledge-rich contexts is examined in the light of the terminographic activity of definition writing. Precise questions emerge from these analyses relating to the relevance and the selection of the extracted information. Some solutions are proposed and their implications to NLP reviewed.</abstract>
        <keywords>Terminology, terminographic definitions, feature selection, feature relevance, definition extraction, conceptual contexts, definition processing.</keywords>
      </article>
      <article id="recital-2010-long-011" session="Poster">
        <auteurs>
          <auteur>
            <nom>Benoît Trouvilliez</nom>
            <email>btrouvilliez@onyme.com</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Centre de Recherche en Informatique de Lens, Université d’Artois, Rue Jean Souvraz, 62300 Lens, France</affiliation>
          <affiliation affiliationId="2">Onyme SARL, 165 Avenue de Bretagne, 59000 Lille, France</affiliation>
        </affiliations>
        <titre>Représentation vectorielle de textes courts d’opinions, Analyse de traitements sémantiques pour la fouille d’opinions par clustering</titre>
        <type>long</type>
        <pages/>
        <resume>Avec le développement d’internet et des sites d’échanges (forums, blogs, sondages en ligne, ...), l’exploitation de nouvelles sources d’informations dans le but d’en extraire des opinions sur des sujets précis (film, commerce,...) devient possible. Dans ce papier, nous présentons une approche de fouille d’opinions à partir de textes courts. Nous expliquons notamment en quoi notre choix d’utilisation de regroupements autour des idées exprimées nous a conduit à opter pour une représentation implicite telle que la représentation vectorielle. Nous voyons également les différents traitements sémantiques intégrés à notre chaîne de traitement (traitement de la négation, lemmatisation, stemmatisation, synonymie ou même polysémie des mots) et discutons leur impact sur la qualité des regroupements obtenus.</resume>
        <mots_cles>représentation des textes, représentation vectorielle, traitement de textes courts, regroupements d’opinions</mots_cles>
        <title/>
        <abstract>With the internet and sharing web sites developement (forums, blogs, online surveys, ...), new data source exploitation in order to extract opinions about various subjects (film, business, ...) becomes possible. In this paper, we show an opinion mining approach from short texts. We explain how our choice of using opinions clustering have conducted us to use an implicit representation like vectorial representation. We present different semantic process that we have incorporated into our process chain (negation process, lemmatisation, stemmatisation, synonymy or polysemy) and we discut their impact on the cluster quality.</abstract>
        <keywords>text representation, vectorial representation, short text processing, opinion clustering</keywords>
      </article>
    </articles>
  </conference>
  <conference>
    <edition>
      <acronyme>RECITAL'2011</acronyme>
      <titre>Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues</titre>
      <ville>Montpellier</ville>
      <pays>France</pays>
      <dateDebut>2011-06-27</dateDebut>
      <dateFin>2011-07-01</dateFin>
      <presidents>
        <nom>Cédric Lopez</nom>
      </presidents>
      <typeArticles>
        <type id="long">Papiers longs</type>
        <type id="court">Papiers courts</type>
      </typeArticles>
      <statistiques>
        <acceptations id="long" soumissions="8">5</acceptations>
        <acceptations id="court" soumissions="6">4</acceptations>
      </statistiques>
      <siteWeb>http://www.lirmm.fr/~lopez/TALN2011/</siteWeb>
      <meilleurArticle>
        <articleId>recital-2011-long-004</articleId>
      </meilleurArticle>
    </edition>
    <articles>
      <article id="recital-2011-long-001" session="Fouille de textes et applications">
        <auteurs>
          <auteur>
            <nom>Fanny Lalleman</nom>
            <email>fanny.lalleman@univ-tlse2.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CLLE &amp; CNRS, 5, allées Antonio Machado 31058 Toulouse Cedex 9</affiliation>
          <affiliation affiliationId="2">Orange Labs, 2, Avenue Pierre Marzin 22307 Lannion Cedex</affiliation>
        </affiliations>
        <auteurs/>
        <titre>Analyse de l’ambiguïté des requêtes utilisateurs par catégorisation thématique</titre>
        <type>long</type>
        <pages/>
        <resume>Dans cet article, nous cherchons à identifier la nature de l’ambiguïté des requêtes utilisateurs issues d’un moteur de recherche dédié à l’actualité, 2424actu.fr, en utilisant une tâche de catégorisation. Dans un premier temps, nous verrons les différentes formes de l’ambiguïté des requêtes déjà décrites dans les travaux de TAL. Nous confrontons la vision lexicographique de l’ambiguïté à celle décrite par les techniques de classification appliquées à la recherche d’information. Dans un deuxième temps, nous appliquons une méthode de catégorisation thématique afin d’explorer l’ambiguïté des requêtes, celle-ci nous permet de conduire une analyse sémantique de ces requêtes, en intégrant la dimension temporelle propre au contexte des news. Nous proposons une typologie des phénomènes d’ambiguïté basée sur notre analyse sémantique. Enfin, nous comparons l’exploration par catégorisation à une ressource comme Wikipédia, montrant concrètement les divergences des deux approches.</resume>
        <mots_cles>recherche d’information, ambiguïté, classification de requêtes</mots_cles>
        <title/>
        <abstract>In this paper, we try to identify the nature of ambiguity of user queries from a search engine dedicated to news, 2424actu.fr, using a categorization task. At first, we see different forms of ambiguity queries already described in the works of NLP. We confront lexicographical vision of the ambiguity to that described by classification techniques applied to information retrieval. In a second step, we apply a method of categorizing themes to explore the ambiguity of queries, it allow us to conduct a semantic analysis of these applications by integrating temporal context-specific news. We propose a typology of phenomena of ambiguity based on our semantic analysis. Finally, we compare the exploration by categorization with a resource as Wikipedia, showing concretely the differences between these two approaches.</abstract>
        <keywords>Information retrieval, ambiguity, classification queries</keywords>
      </article>
      <article id="recital-2011-long-002" session="Fouille de textes et applications">
        <auteurs>
          <auteur>
            <nom>Boutheina Smine</nom>
            <email>Boutheina.Smine@etudiants.univ-paris4.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Rim Faiz</nom>
            <email>Rim.Faiz@ihec.rnu.tn</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Jean-Pierre Desclés</nom>
            <email>Jean-pierre.Descles@paris4.sorbonne.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LaLIC, Université Paris-Sorbonne, 28 rue Serpente, 75006 Paris, France</affiliation>
          <affiliation affiliationId="2">LaRODEC, IHEC de Carthage, 2016 Carthage Présidence, Tunisie</affiliation>
        </affiliations>
        <titre>Extraction Automatique d'Informations Pédagogiques Pertinentes à partir de Documents Textuels</titre>
        <type>long</type>
        <pages/>
        <resume>Plusieurs utilisateurs ont souvent besoin d'informations pédagogiques pour les intégrer dans leurs ressources pédagogiques, ou pour les utiliser dans un processus d'apprentissage. Une indexation de ces informations s'avère donc utile en vue d'une extraction des informations pédagogiques pertinentes en réponse à une requête utilisateur. La plupart des systèmes d'extraction d'informations pédagogiques existants proposent une indexation basée sur une annotation manuelle ou semi-automatique des informations pédagogiques, tâche qui n'est pas préférée par les utilisateurs. Dans cet article, nous proposons une approche d'indexation d'objets pédagogiques (Définition, Exemple, Exercice, etc.) basée sur une annotation sémantique par Exploration Contextuelle des documents. L'index généré servira à une extraction des objets pertinents répondant à une requête utilisateur sémantique. Nous procédons, ensuite, à un classement des objets extraits selon leur pertinence en utilisant l'algorithme Rocchio. Notre objectif est de mettre en valeur une indexation à partir de contextes sémantiques et non pas à partir de seuls termes linguistiques.</resume>
        <mots_cles>extraction d’informations, objets pédagogiques, carte sémantique, exploration contextuelle, algorithme Rocchio</mots_cles>
        <title/>
        <abstract>Different users need pedagogical information in order to use them in their resources or in a learning process. Indexing this information is therefore useful for extracting relevant pedagogical information in response to a user request. Several searching systems of pedagogical information propose manual or semi-automatic annotations to index documents, which is a complex task for users. In this article, we propose an approach to index pedagogical objects (Definition, Exercise, Example, etc.) based on automatic annotation of documents using Contextual Exploration. Then, we use the index to extract relevant pedagogical objects as response to the user's requests. We proceed to sort the extracted objects according to their relevance. Our objective is to reach the relevant objects using a contextual semantic analysis of the text.</abstract>
        <keywords>Information retrieval, pedagogical objects, semantic map, Contextual Exploration, Rocchio algorithm</keywords>
      </article>
      <article id="recital-2011-long-003" session="Extraction d’informations/de relations">
        <auteurs>
          <auteur>
            <nom>Nikola Tulechki</nom>
            <email>nikola.tulechki@univ-tlse2.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CLLE-ERSS, Université de Toulouse-Le Mirail, CNRS</affiliation>
        </affiliations>
        <titre>Des outils de TAL en support aux experts de sûreté industrielle pour l’exploitation de bases de données de retour d’expérience</titre>
        <type>long</type>
        <pages/>
        <resume>Cet article présente des applications d’outils et méthodes du traitement automatique des langues (TAL) à la maîtrise du risque industriel grâce à l’analyse de données textuelles issues de volumineuses bases de retour d’expérience (REX). Il explicite d’abord le domaine de la gestion de la sûreté, ses aspects politiques et sociaux ainsi que l’activité des experts en sûreté et les besoins qu’ils expriment. Dans un deuxième temps il présente une série de techniques, comme la classification automatique de documents, le repérage de subjectivité, et le clustering, adaptées aux données REX visant à répondre à ces besoins présents et à venir, sous forme d’outils, en support à l’activité des experts.</resume>
        <mots_cles>REX, rapport d’incident, risque, sûreté industrielle, signaux faibles, classification automatique, clustering, recherche d’information, similarité, subjectivité</mots_cles>
        <title/>
        <abstract>This article presents a series of natural language processing (NLP) techniques, applied to the domain of industrial risk management and the analysis of large collections of textual feedback data. First we describe the socio-political aspects of the risk mangement domain, the activity of the investigators working with this data. We then present present applications of NLP techniques like automatic text classification, clustering and opinion extraction, responding to different needs stated by the investigators.</abstract>
        <keywords>risk management, incident report, industrial safety, weak signals, automatic classification, information retrieval, similarity, clustering, subjectivity</keywords>
      </article>
      <article id="recital-2011-long-004" session="Discours">
        <auteurs>
          <auteur>
            <nom>Charlotte Roze</nom>
            <email>charlotteroze@linguist.jussieu.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Alpage, INRIA Paris–Rocquencourt &amp; Université Paris 7</affiliation>
        </affiliations>
        <titre>Vers une algèbre des relations de discours pour la comparaison de structures discursives</titre>
        <type>long</type>
        <pages/>
        <resume>Nous proposons une méthodologie pour la construction de règles de déduction de relations de discours, destinées à être intégrées dans une algèbre de ces relations. La construction de ces règles a comme principal objectif de pouvoir calculer la fermeture discursive d’une structure de discours, c’est-à-dire de déduire toutes les relations que la structure contient implicitement. Calculer la fermeture des structures discursives peut permettre d’améliorer leur comparaison, notamment dans le cadre de l’évaluation de systèmes d’analyse automatique du discours. Nous présentons la méthodologie adoptée, que nous illustrons par l’étude d’une règle de déduction.</resume>
        <mots_cles>Relation de discours, fermeture discursive, évaluation, déduction</mots_cles>
        <title/>
        <abstract>We propose a methodology for the construction of discourse relations inference rules, to be integrated into an algebra of these relations. The construction of these rules has as main objective to allow for the calculation of the discourse closure of a structure, i.e. deduce all the relations implicitly contained in the structure. Calculating the closure of discourse structures improves their comparison, in particular within the evaluation of discourse parsing systems. We present the adopted methodology, which we illustrate by the study of a rule.</abstract>
        <keywords>Discourse relation, discourse closure, evaluation, inference</keywords>
      </article>
      <article id="recital-2011-long-005" session="Traduction et Alignement">
        <auteurs>
          <auteur>
            <nom>Prajol Shrestha</nom>
            <email>Prajol.Shrestha@etu.univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LINA, Université de Nantes, France</affiliation>
        </affiliations>
        <titre/>
        <type>long</type>
        <pages/>
        <resume>Les corpus comparables monolingues, alignés non pas au niveau des documents mais au niveau d’unités textuelles plus fines (paragraphe, phrases, etc.), sont utilisés dans diverses applications de traitement automatique des langues comme par exemple en détection de plagiat. Mais ces types de corpus ne sont pratiquement pas disponibles et les chercheurs sont donc obligés de les construire et de les annoter manuellement, ce qui est un travail très fastidieux et coûteux en temps. Dans cet article, nous présentons une méthode, composée de deux étapes, qui permet de réduire ce travail d’annotation de segments de texte. Cette méthode est évaluée lors de l’alignement de paragraphes provenant de dépêches en langue anglaise issues de diverses sources. Les résultats obtenus montrent un apport considérable de la méthode en terme de réduction de temps d’annotation. Nous présentons aussi des premiers résultats obtenus à l’aide de simples traitements automatiques (recouvrement de mots, de racines, mesure cosinus) pour tenter de diminuer encore la charge de travail humaine.</resume>
        <mots_cles>corpus comparable monolingue, alignement, similarité</mots_cles>
        <title>Alignment of Monolingual Corpus by Reduction of the Search Space</title>
        <abstract>Monolingual comparable corpora annotated with alignments between text segments (paragraphs, sentences, etc.) based on similarity are used in a wide range of natural language processing applications like plagiarism detection, information retrieval, summarization and so on. The drawback wanting to use them is that there aren’t many standard corpora which are aligned. Due to this drawback, the corpus is manually created, which is a time consuming and costly task. In this paper, we propose a method to significantly reduce the search space for manual alignment of the monolingual comparable corpus which in turn makes the alignment process faster and easier. This method can be used in making alignments on different levels of text segments. Using this method we create our own gold corpus aligned on the level of paragraph, which will be used for testing and building our algorithms for automatic alignment. We also present some experiments for the reduction of search space on the basis of stem overlap, word overlap, and cosine similarity measure which help us automatize the process to some extent and reduce human effort for alignment.</abstract>
        <keywords>monolingual comparable corpus, alignment, similarity</keywords>
      </article>
      <article id="recital-2011-court-001" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Prajol Shrestha</nom>
            <email>prajol.shrestha@etu.univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LINA-UFR Sciences, 44322 Nantes Cedex 3</affiliation>
        </affiliations>
        <titre/>
        <type>court</type>
        <pages/>
        <resume>Cet article concerne la détermination de la similarité entre des textes courts (phrases, paragraphes, ...). Ce problème est souvent abordé dans la littérature à l’aide de méthodes supervisées ou de ressources externes comme le thesaurus Wordnet ou le British National Corpus. Les méthodes que nous proposons sont non supervisées et n’utilisent pas de connaissances à priori. La première méthode que nous présentons est basée sur le modèle vectoriel de Salton auquel nous avons apporté des modifications pour prendre en compte le contexte, le sens et la relation entre les mots des textes. Dans un deuxième temps, nous testons les mesures de Dice et de ressemblance pour résoudre ce problème ainsi que l’utilisation de la racinisation. Enfin, ces différentes méthodes sont évaluées et comparées aux résultats obtenus dans la littérature.</resume>
        <mots_cles>Similarité, Modèle Vectoriel, Mesure de Similarité</mots_cles>
        <title>Corpus-Based methods for Short Text Similarity</title>
        <abstract>This paper presents corpus-based methods to find similarity between short text (sentences, paragraphs, ...) which has many applications in the field of NLP. Previous works on this problem have been based on supervised methods or have used external resources such as WordNet, British National Corpus etc. Our methods are focused on unsupervised corpus-based methods. We present a new method, based on Vector Space Model, to capture the contextual behavior, senses and correlation, of terms and show that this method performs better than the baseline method that uses vector based cosine similarity measure. The performance of existing document similarity measures, Dice and Resemblance, are also evaluated which in our knowledge have not been used for short text similarity. We also show that the performance of the vector-based baseline method is improved when using stems instead of words and using the candidate sentences for computing the parameters rather than some external resource.</abstract>
        <keywords>Similarity, Vector Space Model, Similarity metric</keywords>
      </article>
      <article id="recital-2011-court-002" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Inga Gheorghita</nom>
            <email>inga.gheorghita@atilf.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">ATILF-CNRS, Nancy-Université (UMR 7118), France</affiliation>
          <affiliation affiliationId="2">XILOPIX, 37 rue de la Plaine, 75020 Paris, France</affiliation>
        </affiliations>
        <titre>Ressources lexicales au service de recherche et d’indexation des images</titre>
        <type>court</type>
        <pages/>
        <resume>Cet article présente une méthodologie d’utilisation du Trésor de la Langue Française informatisée (TLFi) pour l’indexation et la recherche des images fondée sur l’annotation textuelle. Nous utilisons les définitions du TLFi pour la création automatique et l’enrichissement d’un thésaurus à partir des mots-clés de la requête de recherche et des mots-clés attribués à l’image lors de l’indexation. Plus précisement il s’agit d’associer, de façon automatisé, à chaque mot-clé de l’image une liste des mots extraits de ses définitions TLFi pour un domaine donné, en construisant ainsi un arbre hiérarchique. L’approche proposée permet une catégorisation très précise des images, selon les domaines, une indexation de grandes quantités d’images et une recherche rapide.</resume>
        <mots_cles>TLFi, indexation, recherche, images, thésaurus</mots_cles>
        <title/>
        <abstract>This article presents a methodology for using the “Trésor de la Langue Française informatisée” (TLFi) for indexing and searching images based on textual annotation. We use the definitions of TLFi for automatic creation and enrichment of a thesaurus based on keywords from the search query and the keywords assigned to the image during indexing. More specifically it is automatically to associate, to each keyword of the image a list of words from their TLFi’s definitions for a given area, thus building a hierarchical tree. The proposed approach allows a very accurate categorization of images, depending on the fields, a indexing of large amounts of images and a quick search.</abstract>
        <keywords>TLFi, indexing, search, images, thesaurus</keywords>
      </article>
      <article id="recital-2011-court-003" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Mathias Lambert</nom>
            <email>Mathias.Lambert@paris-sorbonne.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université Paris IV-Sorbonne, Laboratoire STIH (LaLIC) - 28 rue Serpente, 75006 Paris</affiliation>
        </affiliations>
        <titre>Repérer les phrases évaluatives dans les articles de presse à partir d’indices et de stéréotypes d’écriture</titre>
        <type>court</type>
        <pages/>
        <resume>Ce papier présente une méthode de recherche des phrases évaluatives dans les articles de presse économique et financière à partir de marques et d’indices stéréotypés, propres au style journalistique, apparaissant de manière concomitante à l’expression d’évaluation(s) dans les phrases. Ces marques et indices ont été dégagés par le biais d’une annotation manuelle. Ils ont ensuite été implémentés, en vue d’une phase-test d’annotation automatique, sous forme de grammaires DCG/GULP permettant, par filtrage, de matcher les phrases les contenant. Les résultats de notre première tentative d’annotation automatique sont présentés dans cet article. Enfin les perspectives offertes par cette méthode relativement peu coûteuse en ressources (à base d’indices non intrinsèquement évaluatifs) font l’objet d’une discussion.</resume>
        <mots_cles>Opinion, évaluation, repérage de phrases évaluatives, presse économique et financière, style journalistique, indices/marques/stéréotypes d’écriture</mots_cles>
        <title/>
        <abstract>This paper presents a method to locate evaluative sentences in financial and economic newspapers, relying on marks and stereotyped signs. Peculiar to journalese, these are present concomitantly with the expression of evaluation(s) in sentences. These marks or signs have been found by means of a manual annotation. Then, in preparation for an automatic annotation phase, they have been implemented in the form of DCG/GULP grammars which, by filtering, allows to locate the sentences containing them. The results of our first automatic annotation attempt are shown in this article. Furthermore, the prospects offered by this method, which relies on nonintrinsically evaluative marks and therefore does not require long lists of lexical resources, are discussed.</abstract>
        <keywords>Opinion, appraisal, detection of evaluative sentences, financial and economic newspapers, journalese, writing signs/marks/stereotypes</keywords>
      </article>
      <article id="recital-2011-court-004" session="Boosters">
        <auteurs>
          <auteur>
            <nom>Adrien Barbaresi</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">ICAR, ENS LYON</affiliation>
        </affiliations>
        <titre>La complexité linguistique Méthode d’analyse</titre>
        <type>court</type>
        <pages/>
        <resume>La complexité linguistique regroupe différents phénomènes dont il s’agit de modéliser le rapport. Le travail en cours que je décris ici propose une réflexion sur les approches linguistiques et techniques de cette notion et la mise en application d’un balayage des textes qui s’efforce de contribuer à leur enrichissement. Ce traitement en surface effectué suivant une liste de critères qui représentent parfois des approximations de logiques plus élaborées tente de fournir une image ``raisonnable'' de la complexité.</resume>
        <mots_cles>Complexité, lisibilité, allemand, analyse de surface</mots_cles>
        <title/>
        <abstract>Linguistic complexity includes various linguistic phenomena which interaction is to be modeled. The ongoing work described here tackles linguistic and technical approaches of this idea as well as an implementation of a parsing method which is part of text enrichment techniques. This chunk parsing is performed according to a list of criteria that may consist in logical approximations of more sophisticated processes in order to provide a ``reasonable'' image of complexity.</abstract>
        <keywords>Complexity, lisibility, German, chunk parsing</keywords>
      </article>
    </articles>
  </conference>
  <conference>
    <edition>
      <acronyme>RECITAL'2012</acronyme>
      <titre>Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues</titre>
      <ville>Grenoble</ville>
      <pays>France</pays>
      <dateDebut>2012-06-04</dateDebut>
      <dateFin>2012-06-08</dateFin>
      <presidents>
        <nom>Didier Schwab</nom>
        <nom>Jorge-Mauricio Molina-Mejia</nom>
      </presidents>
      <typeArticles>
        <type id="long">Papiers longs</type>
      </typeArticles>
      <statistiques>
        <acceptations id="long" soumissions="42">28</acceptations>
      </statistiques>
      <siteWeb>http://www.jeptaln2012.org/</siteWeb>
      <meilleurArticle>
        <articleId>recital-2012-long-001</articleId>
      </meilleurArticle>
    </edition>
    <articles>
      <article id="recital-2012-long-001" session="Orale 1">
        <auteurs>
          <auteur>
            <nom>Pierre Magistry</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Alpage, INRIA Paris-Rocquencourt &amp; Université Paris Diderot</affiliation>
        </affiliations>
        <titre>Segmentation non supervisée : le cas du mandarin</titre>
        <type>long</type>
        <pages>1-13</pages>
        <resume>Dans cet article, nous présentons un système de segmentation non supervisée que nous évaluons sur des données en mandarin. Notre travail s’inspire de l’hypothèse de Harris (1955) et suit Kempe (1999) et Tanaka-Ishii (2005) en se basant sur la reformulation de l’hypothèse en termes de variation de l’entropie de branchement. Celle-ci se révèle être un bon indicateur des frontières des unités linguistiques. Nous améliorons le système de (Jin et Tanaka-Ishii, 2006) en ajoutant une étape de normalisation qui nous permet de reformuler la façon dont sont prises les décisions de segmentation en ayant recours à la programmation dynamique. Ceci nous permet de supprimer la plupart des seuils de leur modèle tout en obtenant de meilleurs résultats, qui se placent au niveau de l’état de l’art (Wang et al., 2011) avec un système plus simple que ces derniers. Nous présentons une évaluation des résultats sur plusieurs corpus diffusés pour le Chinese Word Segmentation bake-off II (Emerson, 2005) et détaillons la borne supérieure que l’on peut espérer atteindre avec une méthode non-supervisée. Pour cela nous utilisons ZPAR en apprentissage croisé (Zhang et Clark, 2010) comme suggéré dans (Huang et Zhao, 2007; Zhao et Kit, 2008)</resume>
        <mots_cles>Apprentissage non-supervisé, segmentation, chinois, mandarin</mots_cles>
        <title>Unsupervized Word Segmentation</title>
        <abstract>In this paper, we present an unsupervised segmentation system tested on Mandarine Chinese. Following Harris’s Hypothesis in Kempe (1999) and Tanaka-Ishii (2005) reformulation, we base our work on the Variation of Branching Entropy. We improve on (Jin et Tanaka-Ishii, 2006) by adding normalization and Viterbi-decoding. This enables us to remove most of the thresholds and parameters from their model and to reach near state-of-the-art results (Wang et al., 2011) with a simpler system. We provide evaluation on different corpora available from the Segmentation bake-off II (Emerson, 2005) and define a more precise topline for the task using cross-trained supervised system available off-the-shelf (Zhang et Clark, 2010; Zhao et Kit, 2008; Huang et Zhao, 2007)</abstract>
        <keywords>Unsupervized machine learning, segmentation, Mandarin Chinese</keywords>
      </article>
      <article id="recital-2012-long-002" session="Orale 1">
        <auteurs>
          <auteur>
            <nom>Matthias Tauveron</nom>
            <email>matthias.tauveron@etu.unistra.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Fonctionnements Discursifs et Traduction, UR LiLPa, Université de Strasbourg</affiliation>
        </affiliations>
        <titre>Incrémentation lexicale dans les textes : une auto-organisation</titre>
        <type>long</type>
        <pages>15-28</pages>
        <resume>Nous proposons une étude dynamique du lexique, en décrivant la manière dont il s’organise progressivement du début à la fin d’un texte. Pour ce faire, nous nous focalisons sur la co-occurrence généralisée, en formant un graphe qui représente tous les lemmes du texte et synthétise leurs relations mutuelles de co-occurrence. L’étude d’un corpus de 40 textes montre que ces relations évoluent d’une manière auto-organisée : la forme - et l’identité - du graphe de co-occurrence restent stables après une phase d’organisation terminée avant la 1ère moitié du texte. Ensuite, il n’évolue plus : les nouveaux mots et les nouvelles relations de co-occurrence s’inscrivent peu à peu dans le réseau, sans modifier la forme d’ensemble de la structure. La relation de co-occurrence généralisée dans un texte apparaît donc comme la construction rapide d’un système, qui est ensuite assez souple pour canaliser un flux d’information sans changer d’identité.</resume>
        <mots_cles>Texte, lexique, co-occurrence généralisée, auto-organisation</mots_cles>
        <title>Lexical Incrementation within Texts: a Self-Organization</title>
        <abstract>We propose here a dynamic study of lexicon: we describe how it is organized progressively from the beginning to the end of a given text. We focus on the “generalized co-occurrence”, forming a graph that represents all the lemmas of the text and their mutual co-occurrence relations. The study of a corpus of 40 texts shows that these relations have a self-organized evolution: the shape and the identity of the graph of cooccurrence become stable after a period of organization finished before the first half of the text. Then they no longer change: new words and new co-occurrence relations gradually take place in the network without changing its overall shape. We show that the evolution of the “generalized co-occurrence” is the quick construction of a system, which is then flexible enough to channel the flow of information without changing its identity.</abstract>
        <keywords>Text, lexicon, generalized co-occurrence, self-organization</keywords>
      </article>
      <article id="recital-2012-long-003" session="Orale 1">
        <auteurs>
          <auteur>
            <nom>Alexander Panchenko</nom>
            <email>alexander.panchenko@student.uclouvain.be</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Center for Natural Language Processing (CENTAL), Université catholique de Louvain, College Erasme, 1 place Blaise Pascal, B-1348 Louvain-la-Neuve (Belgium</affiliation>
        </affiliations>
        <titre>Etude des mesures de similarité hétérogènes pour l’extraction de relations sémantiques</titre>
        <type>long</type>
        <pages>29-42</pages>
        <resume>L’article évalue un éventail de mesures de similarité qui ont pour but de prédire les scores de similarité sémantique et les relations sémantiques qui s’établissent entre deux termes, et étudie les moyens de combiner ces mesures. Nous présentons une analyse comparative à grande échelle de 34 mesures basées sur des réseaux sémantiques, le Web, des corpus, ainsi que des définitions. L’article met en évidence les forces et les faiblesses de chaque approche en contexte de l’extraction de relations. Enfin, deux techniques de combinaison de mesures sont décrites et testées. Les résultats montrent que les mesures combinées sont plus performantes que toutes les mesures simples et aboutissent à une corrélation de 0,887 et une Precision(20) de 0,979.</resume>
        <mots_cles>Similarité sémantique, Relations sémantiques, Similarité distributionnelle</mots_cles>
        <title>A Study of Heterogeneous Similarity Measures for Semantic Relation Extraction</title>
        <abstract>This paper evaluates a wide range of heterogeneous semantic similarity measures on the task of predicting semantic similarity scores and the task of predicting semantic relations that hold between two terms, and investigates ways to combine these measures. We present a large-scale benchmarking of 34 knowledge-, web-, corpus-, and definition-based similarity measures. The strengths and weaknesses of each approach regarding relation extraction are discussed. Finally, we describe and test two techniques for measure combination. These combined measures outperform all single measures, achieving a correlation of 0.887 and Precision(20) of 0.979.</abstract>
        <keywords>Semantic Similarity, Semantic Relations, Distributional Similarity</keywords>
      </article>
      <article id="recital-2012-long-004" session="Orale 2">
        <auteurs>
          <auteur>
            <nom>Luong Ngoc Quang</nom>
            <email>Ngoc-Quang.Luong@imag.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire LIG, GETALP, Grenoble, France</affiliation>
        </affiliations>
        <titre>Intégration de paramètres lexicaux, syntaxiques et issus du système de traduction automatique pour améliorer l’estimation des mesures de confiance au niveau des mots</titre>
        <type>long</type>
        <pages>43-56</pages>
        <resume>L’estimation des mesures de confiance (MC) au niveau des mots consiste à prédire leur exactitude dans la phrase cible générée par un système de traduction automatique. Ceci permet d’estimer la fiabilité d'une sortie de traduction et de filtrer les segments trop mal traduits pour une post-édition. Nous étudions l’impact sur le calcul des MC de différents paramètres : lexicaux, syntaxiques et issus du système de traduction. Nous présentons la méthode permettant de labelliser automatiquement nos corpus (mot correct ou incorrect), puis le classifieur à base de champs aléatoires conditionnels utilisé pour intégrer les différents paramètres et proposer une classification appropriée des mots. Nous avons effectué des expériences préliminaires, avec l’ensemble des paramètres, où nous mesurons la précision, le rappel et la F-mesure. Finalement nous comparons les résultats avec notre système de référence. Nous obtenons de bons résultats pour la classification des mots considérés comme corrects (F-mesure : 86.7%), et encourageants pour ceux estimés comme mal traduits (F-mesure : 36,8%).</resume>
        <mots_cles>Système de traduction automatique, mesure de confiance, estimation de la confiance, champs aléatoires conditionnels</mots_cles>
        <title>Integrating Lexical, Syntactic and System-based Features to Improve Word Confidence Estimation in SMT</title>
        <abstract>Confidence Estimation at word level is the task of predicting the correct and incorrect words in the target sentence generated by a MT system. It helps to conclude the reliability of a given translation as well as to filter out sentences that are not good enough for post-editing. This paper investigates various types of features to circumvent this issue, including lexical, syntactic and system-based features. A method to set training label for each word in the hypothesis is also presented. A classifier based on conditional random fields (CRF) is employed to integrate features and determine the word’s appropriate label. We conducted our preliminary experiment with all features, tracked precision, recall and F-score and we compared with our baseline system. Experimental results of the full combination of all features yield the very encouraging precision, recall and F-score for Good label (F-score: 86.7%), and acceptable scores for Bad label (F-score: 36.8%).</abstract>
        <keywords>Machine translation, confidence measure, confidence estimation, conditional random fields</keywords>
      </article>
      <article id="recital-2012-long-005" session="Orale 2">
        <auteurs>
          <auteur>
            <nom>Gabriel Bernier-Colborne</nom>
            <email>gabriel.bernier-colborne@umontreal.ca</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Observatoire de linguistique Sens-Texte, Université de Montréal</affiliation>
        </affiliations>
        <titre>Application d’un algorithme de traduction statistique à la normalisation de textos</titre>
        <type>long</type>
        <pages>71-79</pages>
        <resume>Ce travail porte sur l’application d’une technique de traduction statistique au problème de la normalisation de textos. La méthode est basée sur l’algorithme de recherche vorace décrit dans (Langlais et al., 2007). Une première normalisation est générée, puis nous appliquons itérativement une fonction qui génère des nouvelles hypothèses à partir de la normalisation courante, et maximisons une fonction de score. Cette méthode fournit une réduction du taux d’erreurs moyen par phrase de 33 % sur le corpus de test, et une augmentation du score BLEU de plus de 30 %. Nous mettons l’accent sur les fonctions qui génèrent la normalisation initiale et sur les opérations permettant de générer des nouvelles hypothèses.</resume>
        <mots_cles>Traduction statistique, normalisation de textos, algorithme de recherche vorace, modèle de langue</mots_cles>
        <title>Applying a Statistical Machine Translation Algorithm to SMS Text Message Normalization</title>
        <abstract>We report on the application of a statistical machine translation algorithm to the problem of SMS text message normalization. The technique is based on a greedy search algorithm described in (Langlais et al., 2007). A first normalization is generated, then a function that generates new hypotheses is applied iteratively to a current best guess, while maximizing a scoring function. This method leads to a drop in word error rate of 33% on a held-out test set, and a BLEU score gain of over 30%. We focus on the methods of generating the initial normalization and the operations that allow us to generate new hypotheses.</abstract>
        <keywords>Machine translation, SMS, text message, normalization, greedy search algorithm, language model</keywords>
      </article>
      <article id="recital-2012-long-006" session="Orale 2">
        <auteurs>
          <auteur>
            <nom>Marion Baranes</nom>
            <email>marion.baranes@viavoo.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Alpage, INRIA Paris-Rocquencourt &amp; Université Paris Diderot, 175 rue du Chevaleret, 75013 Paris</affiliation>
          <affiliation affiliationId="2">viavoo, 69 rue Danjou, 92100 Boulogne Billancourt</affiliation>
        </affiliations>
        <titre>Vers la correction automatique de textes bruités: Architecture générale et détermination de la langue d’un mot inconnu</titre>
        <type>long</type>
        <pages>95-108</pages>
        <resume>Dans ce papier, nous introduisons le problème que pose la correction orthographique sur des corpus de qualité très dégradée tels que les messages publiés sur les forums, les sites d’avis ou les réseaux sociaux. Nous proposons une première architecture de correction qui a pour objectif d’éviter au maximum la sur-correction. Nous présentons, par ailleurs l’implémentation et les résultats d’un des modules de ce système qui a pour but de détecter si un mot inconnu, dans une phrase de langue connue, est un mot qui appartient à cette langue ou non.</resume>
        <mots_cles>Correction automatique, détection de langue, données produite par l’utilisateur</mots_cles>
        <title>Towards Automatic Spell-Checking of Noisy Texts : General Architecture and Language Identification for Unknown Words</title>
        <abstract>This paper deals with the problem of spell checking on degraded-quality corpora such as blogs, review sites and social networks. We propose a first architecture of correction which aims at reducing overcorrection, and we describe its implementation. We also report and discuss the results obtained thanks to the module that detects whether an unknown word from a sentence in a known language belongs to this language or not.</abstract>
        <keywords>Spelling correction, language identification, User-Generated Content</keywords>
      </article>
      <article id="recital-2012-long-007" session="Orale 2">
        <auteurs>
          <auteur>
            <nom>Carlos Ramisch</nom>
            <email>Carlos.Ramisch@imag.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIG-GETALP, Grenoble, France</affiliation>
          <affiliation affiliationId="2">INF-UFRGS, Porto Alegre, Brésil</affiliation>
        </affiliations>
        <titre>Une plate-forme générique et ouverte pour l’acquisition des expressions polylexicales</titre>
        <type>long</type>
        <pages>137-149</pages>
        <resume>Cet article présente et évalue une plate-forme ouverte et flexible pour l’acquisition automatique d’expressions polylexicales (EPL) à partir des corpus monolingues. Nous commençons par une motivation pratique suivie d’une discussion théorique sur le comportement et les défis posés par les EPL dans les applications de TAL. Ensuite, nous décrivons les modules de notre plate-forme, leur enchaînement et les choix d’implémentation. L’évaluation de la plate-forme a été effectuée à travers une applications : la lexicographie assistée par ordinateur. Cette dernière peut bénéficier de l’acquisition d’EPL puisque les expressions acquises automatiquement à partir des corpus peuvent à la fois accélérer la création et améliorer la qualité et la couverture des ressources lexicales. Les résultats prometteurs encouragent une recherche plus approfondie sur la manière optimale d’intégrer le traitement des EPL dans de nombreuses applications de TAL, notamment dans les systèmes traduction automatique.</resume>
        <mots_cles>Expressions polylexicales, extraction lexicale, lexique, mesures d’association, corpus, lexicographie</mots_cles>
        <title>An Open and Generic Framework for the Acquisition of Multiword Expressions</title>
        <abstract>In this paper, we present and evaluate an open and flexible methodological framework for the automatic acquisition of multiword expressions (MWEs) from monolingual textual corpora. We start with a pratical motivation followed by a theoretical discussion of the behaviour and of the challenges that MWEs pose for NLP applications. Afterwards, we describe the modules of our framework, the overall pipeline and the design choices of the tool implementing the framework. The evaluation of the framework was performed extrinsically based on an application : computerassisted lexicography. This application can benefit from MWE acquisition because the expressions acquired automatically from corpora can both speed up the creation and improve the quality and the coverage of the lexical resources. The promising results of previous and ongoing experiments encourage further investigation about the optimal way to integrate MWE treatment into NLP applications, and particularly into machine translation systems.</abstract>
        <keywords>Multiword expression, lexical extraction, lexicon, association measures, corpus, lexicography</keywords>
      </article>
      <article id="recital-2012-long-008" session="Orale 3">
        <auteurs>
          <auteur>
            <nom>Aurélie Merlo</nom>
            <email>aurelie.merlo@etu.univ-lille3.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">STL UMR 8163, rue du Barreau BP 60149 59653 Villeneuve d’Ascq Cedex</affiliation>
        </affiliations>
        <titre>Système de prédiction de néologismes formels : le cas des N suffixés par -IER dénotant des artefacts</titre>
        <type>long</type>
        <pages>57-70</pages>
        <resume>Nous présentons ici un système de prédiction de néologismes formels avec pour exemple la génération automatique de néologismes nominaux suffixés par -IER dénotant des artefacts (saladier, brassière, thonier). L’objectif de cet article est double. Il s’agira (i) de mettre en évidence les contraintes de la suffixation par -IER afin de les implémenter dans un système de génération morphologique puis (ii) de montrer qu’il est possible de prédire les néologismes formels. Ce système de prédiction permettrait ainsi de compléter automatiquement les lexiques pour le Traitement Automatique des Langues (TAL).</resume>
        <mots_cles>morphologie constructionnelle, néologie, génération morphologique, incomplétude lexicale</mots_cles>
        <title>Prediction Device of Formal Neologisms : the Case of -IER Suffixed Nouns Denoting Artifacts</title>
        <abstract>We’ll introduce here a device that can predict neologisms using an example the automatical generation of nominal neologisms suffixed by -IER denoting artifacts (saladier, brassière, thonier). The aim of this article is double. We will first address the - IER suffixation constraints in order to take them into account on the implementation of our morphological generator. Second, we will describe our method to predict formal neologisms. Such a method will permit to automatically enrich NLP lexicons.</abstract>
        <keywords>constructional morphology, neology, morphological generation, lexical incompleteness</keywords>
      </article>
      <article id="recital-2012-long-009" session="Orale 3">
        <auteurs>
          <auteur>
            <nom>Boris Karlov</nom>
            <email>bnkarlov@gmail.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Ophélie Lacroix</nom>
            <email>ophelie.lacroix@univ-nantes.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université de Tver, 33, rue Zheliabov, 170000, Tver, Russie</affiliation>
          <affiliation affiliationId="2">LINA, 2, rue de la Houssinière 44322 Nantes Cedex 3</affiliation>
        </affiliations>
        <titre>Prémices d’une analyse syntaxique par transition pour des structures de dépendance non-projectives</titre>
        <type>long</type>
        <pages>81-94</pages>
        <resume>L’article présente une extension de l’analyseur traditionnel en dépendances par transitions adapté aux dépendances discontinues et les premiers résultats de son entraînement sur un corpus de structures de dépendances de phrases en français. Les résultats des premières expérimentations vont servir de base pour le choix des traits des configurations de calcul bien adaptés aux dépendances discontinues pour améliorer l’apprentissage des dépendances tête.</resume>
        <mots_cles>analyse syntaxique par transitions, structure de dépendance non-projective, grammaire catégorielle de dépendance</mots_cles>
        <title>Beginnings of a Transition-Based Parsing for Non-Projectives Dependency Structures</title>
        <abstract>This paper presents an extension of the traditional transition-based dependency parser adapted to discontinuous dependencies and the first results of its training on a dependency tree corpus of French. The first experimental results will be useful for the choice of parsing configuration features well adapted to discontinuous dependencies in order to ameliorate learning of head dependencies.</abstract>
        <keywords>transition-based parsing, non-projective dependency structure, dependency categorial grammar</keywords>
      </article>
      <article id="recital-2012-long-010" session="Orale 3">
        <auteurs>
          <auteur>
            <nom>Julie Belião</nom>
            <email>julie@beliao.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LPP - Université Paris Sorbonne Nouvelle (ILPGA) - CNRS - UMR 7018</affiliation>
          <affiliation affiliationId="2">MoDyCo - Université Paris Ouest Nanterre La Défense - CNRS - UMR 7114</affiliation>
        </affiliations>
        <titre>Création d’un multi-arbre à partir d’un texte balisé : l’exemple de l’annotation d’un corpus d’oral spontané</titre>
        <type>long</type>
        <pages>109-122</pages>
        <resume>Dans cette étude, nous nous intéressons au problème de l’analyse d’un corpus annoté de l’oral. Le système d’annotation considéré est celui introduit par l’équipe des syntacticiens du projet Rhapsodie. La principale problématique qui sous-tend un tel projet est que la base écrite sur laquelle on travaille est en réalité une transcription de l’oral, balisée par les annotateurs de manière à délimiter un ensemble de structures arborescentes. Un tel système introduit plusieurs structures, en particulier macro et micro-syntaxiques. Du fait de leur étroite imbrication, il s’est avéré difficile de les analyser de façon indépendante et donc de travailler sur l’aspect macro-syntaxique indépendamment de l’aspect micro-syntaxique. Cependant, peu d’études jusqu’à présent considèrent ces problèmes conjointement et de manière automatisée. Dans ce travail, nous présentons nos efforts en vue de produire un outil de parsing capable de rendre compte à la fois de l’information micro et macro-syntaxique du texte annoté. Pour ce faire, nous proposons une représentation partant de la notion de multi-arbre et nous montrons comment une telle structure peut être générée à partir de l’annotation et utilisée à des fins d’analyse.</resume>
        <mots_cles>Arbres syntaxiques, unité illocutoire, unités rectionnelles, micro-syntaxe, macrosyntaxe, entassement</mots_cles>
        <title>Creating a Multi-Tree from a Tagged Text : Annotating Spoken French</title>
        <abstract>This study focuses on automatic analysis of annotated transcribed speech. The annotation system considered has been recently introduced to address the several limitations of classical syntactic annotations when faced to natural speech transcriptions. It introduces many different components such as embedding, piles, kernels, pre-kernels, discursive markers etc.. All those components are tightly coupled in a complex tree structure and can hardly be considered separately because of their close intrication. Hence, a joint analysis is required but no analysis tool to handle them all together was available yet. In this study, we introduce such an automatic parser of annotated transcriptions of speech and present the corresponding framework based on multi-trees. This framework permits to jointly handle separate aspects of speech such as macro and micro syntactic levels, which are traditionnaly considered separately. Several applications are proposed, including analysis of the transcribed speech by classical parsers designed for written language.</abstract>
        <keywords>Syntactic trees, illocutionary unit, microsyntax, macrosyntax, piles</keywords>
      </article>
      <article id="recital-2012-long-011" session="Orale 3">
        <auteurs>
          <auteur>
            <nom>Noémi Boubel</nom>
            <email>noemi.boubel@uclouvain.be</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">UCLouvain, Cental, Place Blaise Pascal, 1, B-1348 Louvain-la-Neuve, Belgique</affiliation>
        </affiliations>
        <titre>Construction automatique d’un lexique de modifieurs de polarité</titre>
        <type>long</type>
        <pages>123-136</pages>
        <resume>La recherche présentée 1 s’inscrit dans le domaine de la fouille d’opinion, domaine qui consiste principalement à déterminer la polarité d’un texte ou d’une phrase. Dans cette optique, le contexte autour d’un mot polarisé joue un rôle essentiel, car il peut modifier la polarité initiale de ce terme. Nous avons choisi d’approfondir cette question et de détecter précisément ces modifieurs de polarité. Une étude exploratoire, décrite dans des travaux antérieurs, nous a permis d’extraire automatiquement des adverbes qui jouent un rôle sur la polarité des adjectifs auxquels ils sont associés et de préciser leur impact. Nous avons ensuite amélioré le système d’extraction afin de construire automatiquement un lexique de structures lexico-syntaxiques modifiantes associées au type d’impact qu’elles ont sur un terme polarisé. Nous présentons ici le fonctionnement du système actuel ainsi que l’évaluation du lexique obtenu.</resume>
        <mots_cles>fouille d’opinion, modifieurs de valence affective, modifieurs de polarité</mots_cles>
        <title>Automatic Construction of a Contextual Valence Shifters Lexicon</title>
        <abstract>The research presented in this paper takes place in the field of Opinion Mining, which is mainly devoted to assigning a positive or negative label to a text or a sentence. The context of a highly polarized word plays an essential role, as it can modify its original polarity. The present work addresses this issue and focuses on the detection of polarity shifters. In a previous study, we have automatically extracted adverbs impacting the polarity of the adjectives they are associated to and qualified their influence. The extraction system has then been improved to automatically build a lexicon of contextual valence shifters. This lexicon contains lexico-syntactic patterns combined with the type of influence they have on the valence of the polarized item. The purpose of this paper is to show how the current system works and to present the evaluation of the created lexicon.</abstract>
        <keywords>opinion mining, contextual valence shifters</keywords>
      </article>
      <article id="recital-2012-long-012" session="Poster 1">
        <auteurs>
          <auteur>
            <nom>Ahmed Hamdi</nom>
            <email>ahmed.hamdi@lif.univ-mrs.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Aix Marseille Université, LIF-CNRS, Marseille</affiliation>
        </affiliations>
        <titre>Apport de la diacritisation dans l’analyse morphosyntaxique de l’arabe</titre>
        <type>long</type>
        <pages>247-254</pages>
        <resume>Ce travail s’inscrit dans le cadre de l’analyse morphologique et syntaxique automatique de la langue arabe. Nous nous intéressons au traitement de la diacritisation et à son apport pour l’analyse morphologique. En effet, la plupart des analyseurs morphologiques et des étiqueteurs morphosyntaxiques existants ignorent les diacritiques présents dans le texte à analyser et commettent des erreurs qui pourraient être évitées. Dans cet article, nous proposons une méthode qui prend en considération les diacritiques lors de l’analyse, et nous montrons que cette prise en compte permet de diminuer considérablement le taux d’erreur de l’analyse morphologique selon le taux de diacritiques du texte traité.</resume>
        <mots_cles>diacritisation, traitement automatique, analyse morphosyntaxique, langue arabe</mots_cles>
        <title>Apport of Diacritization in Arabic Morpho-Syntactic Analysis</title>
        <abstract>This work is concerned with the automatic morphological and syntactical analysis of the Arabic language. It focuses on diacritization and on its contribution to morphological analysis. Most of existing morphological analyzers and syntactical taggers do not take diacritics into account; as a consequence, they make mistakes that could have been avoided. In this paper, we propose a method which process diacritics. We show that doing so reduces considerably the morphological error rate, depending on the diacritics rate in the input text.</abstract>
        <keywords>diacritization, computer processing, morpho-syntaxic analysis, Arabic language</keywords>
      </article>
      <article id="recital-2012-long-013" session="Poster 1">
        <auteurs>
          <auteur>
            <nom>Alexandre Baudrillart</nom>
            <email>alexandre.baudrillart@u-grenoble3.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université Stendhal Grenoble 3, BP 25, 38040 Grenoble Cedex 9, France</affiliation>
          <affiliation affiliationId="2">Université de Lyon, CNRS INSA-Lyon, UMR5205 F-69621, France</affiliation>
        </affiliations>
        <titre>Extraction d’indicateurs de construction collective de connaissances dans la formation en ligne</titre>
        <type>long</type>
        <pages>337-350</pages>
        <resume>Dans le cadre d’apprentissages humains assistés par des environnements informatiques, les techniques de TAL ne sont que rarement employées ou restreintes à des tâches ou des domaines spécifiques comme l’ALAO (Apprentissage de la Langue Assisté par Ordinateur) où elles sont omniprésentes mais ne concernent que certaines dimensions du TAL. Nous cherchons à explorer les possibilités ou les performances des techniques voire des méthodes de TAL pour des systèmes moins spécifiques dès lors qu’une dimension de réseau et de collectivité est présente. Plus particulièrement, notre objectif est d’obtenir des indicateurs sur la construction collective de connaissances, et ses modalités. Ce papier présente la problématique de notre thèse, son contexte, nos motivations ainsi que nos premières réflexions.</resume>
        <mots_cles>TAL, EIAH, formation en ligne, socio-constructivisme, acquisition des connaissances, apprentissage collaboratif en ligne</mots_cles>
        <title>Collaborative Knowledge Building Indicators Extraction in Distance Learning</title>
        <abstract>Natural Language Processing techniques are still not very much used within the field of Technology Enhanced Learning. They are restricted to specific tasks or domains such as CALL (standing for Computer Assisted Language Learning) in which they are ubiquitous but do not match every linguistic aspect they could process. We are seeking to explore possibilities or performances of thoses techniques for less specific systems including a network or community aspect. More precisely, our goal is to get indicators about collective knowledge building and its modalities. This paper presents the problem and the background of our thesis problem, as well as our motivation and our first reflections.</abstract>
        <keywords>NLP, TEL, distance learning, socio-constructivism, knowledge aquisition, collaboration, CSCL</keywords>
      </article>
      <article id="recital-2012-long-014" session="Poster 1">
        <auteurs>
          <auteur>
            <nom>Andon Tchechmedjiev</nom>
            <email>andon.tchechmedjiev@imag.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIG-GETALP, Laboratoire d’Informatique de Grenoble-Groupe d’Étude pour la Traduction Automatique/Traitement Automatisé des Langues et de la Parole, Université de Grenoble</affiliation>
        </affiliations>
        <titre>État de l’art : mesures de similarité sémantique locales et algorithmes globaux pour la désambiguïsation lexicale à base de connaissances</titre>
        <type>long</type>
        <pages>295-308</pages>
        <resume>Dans cet article, nous présentons les principales méthodes non supervisées à base de connaissances pour la désambiguïsation lexicale. Elles sont composées d’une part de mesures de similarité sémantique locales qui donnent une valeur de proximité entre deux sens de mots et, d’autre part, d’algorithmes globaux qui utilisent les mesures de similarité sémantique locales pour trouver les sens appropriés des mots selon le contexte à l’échelle de la phrase ou du texte.</resume>
        <mots_cles>désambiguïsation lexicale non-supervisée, mesures de similarité sémantique à base de connaissances, algorithmes globaux de propagation de mesures locales</mots_cles>
        <title>State of the art : Local Semantic Similarity Measures and Global Algorithmes for Knowledge-based Word Sense Disambiguation</title>
        <abstract>We present the main methods for unsupervised knowledge-based word sense disambiguation. On the one hand, at the local level, we present semantic similarity measures, which attempt to quantify the semantic proximity between two word senses. On the other hand, at the global level, we present algorithms which use local semantic similarity measures to assign the appropriate senses to words depending on their context, at the scale of a text or of a corpus.</abstract>
        <keywords>unsupervised word sense disambiguation, knowledge-based semantic similarity measures, global algorithms for the propagation of local measures</keywords>
      </article>
      <article id="recital-2012-long-015" session="Poster 1">
        <auteurs>
          <auteur>
            <nom>Arnaud Kirsch</nom>
            <email>arnaud.kirsch@student.uclouvain.be</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">UCL - CenTAL - Place Blaise Pascal 1, Louvain-la-Neuve, 1348</affiliation>
        </affiliations>
        <titre>Compression textuelle sur la base de règles issues d'un corpus de sms</titre>
        <type>long</type>
        <pages>309-322</pages>
        <resume>La présente recherche cherche à réduire la taille de messages textuels sur la base de techniques de compression observées, pour la plupart, dans un corpus de sms. Ce papier explique la méthodologie suivie pour établir des règles de contraction. Il présente ensuite les 33 règles retenues, et illustre les quatre niveaux de compression proposés par deux exemples concrets, produits automatiquement par un premier prototype. Le but de cette recherche n'est donc pas de produire de "l'écrit-sms", mais d'élaborer un procédé de compression capable de produire des textes courts et compréhensibles à partir de n'importe quelle source textuelle en français. Le terme "d'essentialisation" est proposé pour désigner cette approche de réduction textuelle.</resume>
        <mots_cles>résumé automatique, compression de texte, sms, lisibilité, essentialisation</mots_cles>
        <title>Textual Compression Based on Rules Arising from a Corpus of Text Messages</title>
        <abstract>The present research seeks to reduce the size of text messages on the basis of compression techniques observed mostly in a corpus of sms. This paper explains the methodology followed to establish compression rules. It then presents the 33 considered rules, and illustrates the four suggested levels of compression with two practical examples, automatically generated by a first prototype. This research’s main purpose is not to produce "sms-language", but consists in designing a textual compression process able to generate short and understandable texts from any textual source in French. The term of "essentialization" is proposed to describe this approach of textual reduction.</abstract>
        <keywords>summarization, text compression, text messaging, readability, essentialization</keywords>
      </article>
      <article id="recital-2012-long-016" session="Poster 1">
        <auteurs>
          <auteur>
            <nom>Aurélie Joseph</nom>
            <email>joseph.aurelie@gmail.com</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LDI, 99 avenue Jean-Baptiste Clément, 93 Villetaneuse</affiliation>
          <affiliation affiliationId="2">ITESFOT, Parc d’Andron, Le Séquoia, 30470 Aimargues</affiliation>
        </affiliations>
        <titre>Pour un étiquetage automatique des séquences verbales figées : état de l’art et approche transformationnelle</titre>
        <type>long</type>
        <pages>255-266</pages>
        <resume>Cet article présente une approche permettant de reconnaitre automatiquement dans un texte des séquences verbales figées (casser sa pipe, briser la glace, prendre en compte) à partir d’une ressource. Cette ressource décrit chaque séquence en termes de possibilités et de restrictions transformationnelles. En effet, les séquences figées ne le sont pas complètement et nécessitent une description exhaustive afin de ne pas extraire seulement les formes canoniques. Dans un premier temps nous aborderons les approches traditionnelles permettant d’extraire des séquences phraséologiques. Par la suite, nous expliquerons comment est constituée notre ressource et comment celle-ci est utilisée pour un traitement automatique.</resume>
        <mots_cles>séquences verbales figées, reconnaissance automatique, étiquetage, transformations linguistiques, ressources électroniques</mots_cles>
        <title>For an Automatic Fixed Verbal Sequence Tagging: State of the Art and Transformational Approach</title>
        <abstract>This article presents a resource-based method aiming at automatically recognizing fixed verbal sequences in French (i.e casser sa pipe, briser la glace, prendre en compte) inside a text. This resource describes each sequence from the view-point of transformational possibilities and restrictions. Fixed sequences are not totally fixed and an exhaustive description is necessary to not only extract canonical forms. We will first describe some transformational approaches that are able to extract phraseological sequences. The building of the resource will be then addressed followed by our approach to automatically recognize fixed sequences in corpora.</abstract>
        <keywords>fixed verbal sequences, automatic recognition, tagging, linguistical transformations, electronic resources</keywords>
      </article>
      <article id="recital-2012-long-017" session="Poster 1">
        <auteurs>
          <auteur>
            <nom>Céline Battaïa</nom>
            <email>celine.battaia@u-grenoble3.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Groupe de Recherche sur les Enjeux de la Communication (GRESEC), Université Stendhal, Laboratoire Gresec, Grenoble 3</affiliation>
        </affiliations>
        <titre>L’analyse de l’émotion dans les forums de santé</titre>
        <type>long</type>
        <pages>267-280</pages>
        <resume>Les travaux sur l’émotion dans les forums sont nombreux en Linguistique et Psychologie. L’objectif de cette contribution est de proposer une analyse de l’émotion dans les forums de santé selon l’angle des Sciences de l’Information et de la Communication mais également selon une approche interdisciplinaire. Il s’agira ici, d’étudier l’émotion comme un critère de pertinence lorsque des personnes malades effectuent des recherches dans les forums. Ce papier introduit la méthodologie utilisée en traitement automatique de la langue afin de répondre à cette interrogation. Ainsi, le travail présenté abordera l’exploitation d’un corpus de messages de forums, la catégorisation semi-supervisée et l’utilisation du logiciel NooJ pour traiter de manière automatique les données.</resume>
        <mots_cles>émotion, forum de santé, traitement automatique de la langue, désambiguïsation lexicale</mots_cles>
        <title>Analysis of Emotion in Health Fora</title>
        <abstract>Studies about emotion in fora are numerous in Linguistics and Psychology. This contribution approaches this subject from an Information and Communication Sciences point of view, and studies emotion as a criteron of pertinence for patients in a health forum. This paper introduces the empirical step of automatic language processing in order to answer this question, and uses data processing on the corpus of forum messages, semi-supervised categorisation of messages and use of software NooJ for Natural Language Processing.</abstract>
        <keywords>emotion, health forum, automatic language processing, lexical disambiguation</keywords>
      </article>
      <article id="recital-2012-long-018" session="Poster 1">
        <auteurs>
          <auteur>
            <nom>Driss Sadoun</nom>
            <email>driss.sadoun@limsi.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIMSI/CNRS, B.P. 133 91403 Orsay Cedex, France</affiliation>
          <affiliation affiliationId="2">Université Paris-Sud, 91400 Orsay, France</affiliation>
        </affiliations>
        <titre>Peuplement d’une ontologie modélisant le comportement d’un environnement intelligent guidé par l’extraction d’instances de relations</titre>
        <type>long</type>
        <pages>281-294</pages>
        <resume>Nous présentons une approche de peuplement d’ontologie dont le but est de modéliser le comportement de composants logiciels afin de faciliter le passage de descriptions d’exigences en langue naturelle à des spécifications formelles. L’ontologie que nous cherchons à peupler a été conçue à partir des connaissances du domaine de la domotique et est initialisée à partir d’une description de la configuration physique d’un environnement intelligent. Notre méthode est guidée par l’extraction d’instances de relations permettant par là-même d’extraire les instances de concepts liés par ces relations. Nous construisons des règles d’extraction à partir d’éléments issus de l’analyse syntaxique de descriptions de besoins utilisateurs et de ressources terminologiques associées aux concepts et relations de l’ontologie. Notre approche de peuplement se distingue par sa finalité qui n’est pas d’extraire toutes les instances décrivant un domaine mais d’extraire des instances pouvant participer sans conflit à un des multiples fonctionnements décrit par des utilisateurs.</resume>
        <mots_cles>extraction de relations, peuplement d’ontologie, représentation des connaissances</mots_cles>
        <title>Population of an Ontology Modeling the Behavior of an Intelligent Environment Guided by Instance Relation Extractions</title>
        <abstract>We present an approach for ontology population, which aims at modeling the behavior of software components, for enabling a transition from natural language requirements to formal specifications. The ontology was designed based on the knowledge of the domotic domain and is initialized from a description of a physical configuration of an intelligent environment. Our method focuses on extracting relation instances which allows the extraction of concept instances linked by these relations. We built extraction rules using elements coming from syntactic analysis of user need descriptions, semantic and terminological resources linked to the knowledge contained in the ontology. Our approach for ontology population, distinguishes itself by its purpose, which is not to extract all instances describing a domain but to extract instances that can participate without any conflict to one of the mutiple operation decribed by users.</abstract>
        <keywords>relation extraction, ontology population, knowledge representation</keywords>
      </article>
      <article id="recital-2012-long-019" session="Poster 1">
        <auteurs>
          <auteur>
            <nom>Franck Dernoncourt</nom>
            <email>franck.dernoncourt@lip6.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIP6, 4 place Jussieu, 75005 Paris</affiliation>
        </affiliations>
        <titre>De l'utilisation du dialogue naturel pour masquer les QCM au sein des jeux sérieux</titre>
        <type>long</type>
        <pages>323-336</pages>
        <resume>Une des principales faiblesses des jeux sérieux à l'heure actuelle est qu'ils incorporent très souvent des questionnaires à choix multiple (QCM). Or, aucune étude n'a démontré que les QCM sont capables d'évaluer précisément le niveau de compréhension des apprenants. Au contraire, certaines études ont montré expérimentalement que permettre à l'apprenant d'entrer une phrase libre dans le programme au lieu de simplement cocher une réponse dans un QCM rend possible une évaluation beaucoup plus fine des compétences de l'apprenant. Nous proposons donc de concevoir un agent conversationnel capable de comprendre des énoncés en langage naturel dans un cadre sémantique restreint, cadre correspondant au domaine de compétence testé chez l'apprenant. Cette fonctionnalité est destinée à permettre un dialogue naturel avec l'apprenant, en particulier dans le cadre des jeux sérieux. Une telle interaction en langage naturel a pour but de masquer les QCM sous-jacents. Cet article présente notre approche.</resume>
        <mots_cles>Agent conversationnel éducatif, intelligence artificielle, jeu sérieux, questionnaire à choix multiple, système d'évaluation de réponses libres</mots_cles>
        <title>Of the Use of Natural Dialogue to Hide MCQs in Serious Games</title>
        <abstract>A major weakness of serious games at the moment is that they often incorporate multiple choice questionnaires (MCQs). However, no study has demonstrated that MCQs can accurately assess the level of understanding of a learner. On the contrary, some studies have experimentally shown that allowing the learner to input a free-text answer in the program instead of just selecting one answer in an MCQ allows a much finer evaluation of the learner's skills. We therefore propose to design a conversational agent that can understand statements in natural language within a narrow semantic context corresponding to the area of competence on which we assess the learner. This feature is intended to allow a natural dialogue with the learner, especially in the context of serious games. Such interaction in natural language aims to hide the underlying MCQs. This paper presents our approach.</abstract>
        <keywords>Educational conversational agent, artificial intelligence, serious game, multiple-choice questionnaire, automatic assessment of free-text answer</keywords>
      </article>
      <article id="recital-2012-long-020" session="Poster 2">
        <auteurs>
          <auteur>
            <nom>Jihene Jmal</nom>
            <email>fer.jmal_jihene@hotmail.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LARODEC, ISG, Université de Tunis, 2000, Le Bardo, Tunisie</affiliation>
        </affiliations>
        <titre>ResTS : Système de Résumé Automatique des Textes d’Opinions basé sur Twitter et SentiWordNet</titre>
        <type>long</type>
        <pages>233-246</pages>
        <resume>Comme le E-commerce est devenu de plus en plus populaire, le nombre de commentaires des internautes est en croissance constante. Les opinions sur le Web affectent nos choix et nos décisions. Il s’avère alors indispensable de traiter une quantité importante de critiques des clients afin de présenter à l’utilisateur l’information dont il a besoin dans la forme la plus appropriée. Dans cet article, nous présentons ResTS, un nouveau système de résumé automatique de textes d’opinions basé sur les caractéristiques des produits. Notre approche vise à transformer les critiques des utilisateurs en des scores qui mesurent le degré de satisfaction des clients pour un produit donné et pour chacune de ses caractéristiques. Ces scores sont compris entre 0 et 1 et peuvent être utilisés pour la prise de décision. Nous avons étudié les opinions véhiculées par les noms, les adjectifs, les verbes et les adverbes, contrairement aux recherches précédentes qui utilisent essentiellement les adjectifs. Les résultats expérimentaux préliminaires montrent que notre méthode est comparable aux méthodes classiques de résumé automatique basées sur les caractéristiques des produits.</resume>
        <mots_cles>Fouille d’opinion, Classification, Intensité de l’Opinion, Résumé de texte d’opinion, Popularité</mots_cles>
        <title>System of Customer Review Summarization using Twitter and SentiWordNet</title>
        <abstract>As E-commerce is becoming more and more popular, the number of customer reviews raises rapidly. Opinions on the Web affect our choices and decisions. Thus, it is more efficient to automatically process a mixture of reviews and prepare to the customer the required information in an appropriate form. In this paper, we present ResTS, a new system of feature-based opinion summarization. Our approach aims to turn the customer reviews into scores that measure the customer satisfaction for a given product and its features. These scores are between 0 and 1 and can be used for decision making and then help users in their choices. We investigated opinions extracted from nouns, adjectives, verbs and adverbs contrary to previous research which use only adjectives. Experimental results show that our method performs comparably to classic feature-based summarization methods.</abstract>
        <keywords>Opinion mining, Sentiment Classification, Opinion Strength, Feature-based Opinion Summarization, Feature Buzz Summary</keywords>
      </article>
      <article id="recital-2012-long-021" session="Poster 2">
        <auteurs>
          <auteur>
            <nom>Mathieu-Henri Falco</nom>
            <email>falco@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIMSI-CNRS, Université Paris-Sud, 91403 Orsay, France</affiliation>
        </affiliations>
        <titre>Typologie des questions à réponses multiples pour un système de question-réponse</titre>
        <type>long</type>
        <pages>191-204</pages>
        <resume>L’évaluation des systèmes de question-réponse lors des campagnes repose généralement sur la validité d’une réponse individuelle supportée par un passage (question factuelle) ou d’un groupe de réponses toutes contenues dans un même passage (questions listes). Ce cadre évaluatif empêche donc de fournir un ensemble de plusieurs réponses individuelles et ne permet également pas de fournir des réponses provenant de documents différents. Ce recoupement inter-documents peut être necessaire pour construire une réponse composée de plusieurs éléments afin d’être le plus complet possible. De plus une grande majorité de questions formulées au singulier et semblant n’attendre qu’une seule réponse se trouve être des questions possédant plusieurs réponses correctes. Nous présentons ici une typologie des questions à réponses multiples ainsi qu’un aperçu sur les problèmes posés à un système de question-réponse par ce type de question.</resume>
        <mots_cles>question-réponse, questions à réponses multiples, question liste</mots_cles>
        <title>Typology of Multiple Answer Questions for a Question-answering System</title>
        <abstract>The evaluation campaigns of question-answering systems are generally based on the validity of an individual answer supported by a passage (for a factual question) or a group of answers coming all from a same supporting passage (for a list question). This framework does not allow the possibily to answer with a set of answers, nor with answers gathered from several documents. This cross-checking can be needed for building an answer composed of several elements in order to be as accurate as possible. Besides a large majority of questions with a singular form seems to be answered with a single answer whereas they can be satisfied with many. We present here a typology of questions with multiple answers and an overview of problems encountered by a question-answering system with this kind of questions.</abstract>
        <keywords>question-answering, multiple answer questions, list question</keywords>
      </article>
      <article id="recital-2012-long-022" session="Poster 2">
        <auteurs>
          <auteur>
            <nom>Mohamed Hatmi</nom>
            <email>mohamed.hatmi@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LINA, UMR 6241,Université de Nantes</affiliation>
        </affiliations>
        <titre>Adaptation d’un système de reconnaissance d’entités nommées pour le français à l’anglais à moindre coût</titre>
        <type>long</type>
        <pages>151-161</pages>
        <resume>La portabilité entre les langues des systèmes de reconnaissance d’entités nommées est coûteuse en termes de temps et de connaissances linguistiques requises. L’adaptation des systèmes symboliques souffrent du coût de développement de nouveaux lexiques et de la mise à jour des règles contextuelles. D’un autre côté, l’adaptation des systèmes statistiques se heurtent au problème du coût de préparation d’un nouveau corpus d’apprentissage. Cet article étudie l’intérêt et le coût associé pour porter un système existant de reconnaissance d’entités nommées pour du texte bien formé vers une autre langue. Nous présentons une méthode peu coûteuse pour porter un système symbolique dédié au français vers l’anglais. Pour ce faire, nous avons d’une part traduit automatiquement l’ensemble des lexiques de mots déclencheurs au moyen d’un dictionnaire bilingue. D’autre part, nous avons manuellement modifié quelques règles de manière à respecter la syntaxe de la langue anglaise. Les résultats expérimentaux sont comparés à ceux obtenus avec un système de référence développé pour l’anglais.</resume>
        <mots_cles>Reconnaissance d’entités nommées, approche symbolique, portabilité entre les langues</mots_cles>
        <title>Adapting a French Named Entity Recognition System to English with Minimal Costs</title>
        <abstract>Cross-language portability of Named Entity Recognition systems requires linguistic expertise and needs human effort. Adapting symbolic systems suffers from the cost of developing new lexicons and updating grammar rules. Porting statistical systems on the other hand faces the problem of the high cost of annotation of new training corpus. This paper examines the cost of adapting a rule-based Named Entity Recognition system designed for well-formed text to another language. We present a low-cost method to adapt a French rule-based Named Entity Recognition system to English. We first solve the problem of lexicon adaptation to English by simply translating the French lexical resources. We then get to the task of grammar adaptation by slightly modifying the grammar rules. Experimental results are compared to a state-of-the-art English system.</abstract>
        <keywords>Named entity recognition, symbolic approache, cross-language portability</keywords>
      </article>
      <article id="recital-2012-long-023" session="Poster 2">
        <auteurs>
          <auteur>
            <nom>Monia Ben Mlouka</nom>
            <email>mlouka@irit.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">IRIT -TCI , UMR5505, 31000 Toulouse</affiliation>
        </affiliations>
        <titre>Analyse automatique de discours en langue des signes : Représentation et traitement de l’espace de signation</titre>
        <type>long</type>
        <pages>219-232</pages>
        <resume>En langue des signes, l’espace est utilisé pour localiser et faire référence à certaines entités dont l’emplacement est important pour la compréhension du sens. Dans cet article, nous proposons une représentation informatique de l’espace de signation et les fonctions de création et d’accès associées, afin d’analyser les gestes manuels et non manuels qui contribuent à la localisation et au référencement des signes et de matérialiser leur effet. Nous proposons une approche bi-directionnelle qui se base sur l’analyse de données de capture de mouvement de discours en langue des signes dans le but de caractériser les événements de localisation et de référencement.</resume>
        <mots_cles>Langue des signes, Espace de signation, gestes de pointage, capture de mouvement, suivi du regard</mots_cles>
        <title>Automatic Analysis of Discourse in Sign Language : Signing Space Representation and Processing</title>
        <abstract>In sign language, signing space is used to locate and refer to entities whose locations are important for understanding the meaning. In this paper, we propose a computer-based representation of the signing space and their associated functions. It aims to analyze manual and non-manual gestures, that contribute to locating and referencing signs, and to make real their effect. For that, we propose an approach based on the analysis of motion capture data of entities’ assignment and activation events in the signing space.</abstract>
        <keywords>Sign language, Signing space, pointing gestures, motion capture, gaze tracker</keywords>
      </article>
      <article id="recital-2012-long-024" session="Poster 2">
        <auteurs>
          <auteur>
            <nom>Morgane Marchand</nom>
            <email>morgane.marchand@cea.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CEA, LIST, Laboratoire Vision et Ingénierie des Contenus, Centre Nano-Innov Saclay, 91191 Gif-sur-Yvette Cedex</affiliation>
          <affiliation affiliationId="2">LIMSI-CNRS, Univ. Paris-Sud, 91403 Orsay Cedex</affiliation>
        </affiliations>
        <titre>État de l’art : l’influence du domaine sur la classification de l’opinion, Dis-moi de quoi tu parles, je te dirai ce que tu penses</titre>
        <type>long</type>
        <pages>177-190</pages>
        <resume>L’intérêt pour la fouille d’opinion s’est développé en même temps que se sont répandus les blogs, forums et autres plate-formes où les internautes peuvent librement exprimer leur opinion. La très grande quantité de données disponibles oblige à avoir recours à des traitements automatiques de fouille d’opinion. Cependant, la manière dont les gens expriment leur avis change selon ce dont ils parlent. Les distributions des mots utilisés sont différentes d’un domaine à l’autre. Aussi, il est très difficile d’obtenir un classifieur d’opinion fonctionnant sur tous les domaines. De plus, on ne peut appliquer sans adaptation sur un domaine cible un classifieur entraîné sur un domaine source différent. L’objet de cet article est de recenser les moyens de résoudre ce problème difficile.</resume>
        <mots_cles>État de l’art, Fouille d’opinion, Multi-domaines, Cross-domaines</mots_cles>
        <title>State of the Art : Influence of Domain on Opinion Classification</title>
        <abstract>The interest in opinion mining has grown concurrently with blogs, forums, and others platforms where the internauts can freely write about their opinion on every topic. As the amounts of available data are increasingly huge, the use of automatic methods for opinion mining becomes imperative. However, sentiment is expressed differently in different domains : words distributions can indeed differ significantly. An effective global opinion classifier is therefore hard to develop. Moreover, a classifier trained on a source domain can’t be used without adaptation on a target domain. This article aims to describe the state-of-the-art methods used to solve this difficult task.</abstract>
        <keywords>State of the art, Opinion mining, Multi-domain, Cross-domain</keywords>
      </article>
      <article id="recital-2012-long-025" session="Poster 2">
        <auteurs>
          <auteur>
            <nom>Mounira Manser</nom>
            <email>manser.mounira@gmail.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIM&amp;BIO (EA3969), Université Paris 13, 93017 Bobigny Cedex, France</affiliation>
        </affiliations>
        <titre>État de l’art sur l’acquisition de relations sémantiques entre termes : contextualisation des relations de synonymie</titre>
        <type>long</type>
        <pages>163-175</pages>
        <resume>L’accès au contenu des textes de spécialité est une tâche difficile à réaliser. Cela nécessite la définition de méthodes automatiques ou semi-automatiques pour identifier des relations sémantiques entre les termes que contiennent ces textes. Nous distinguons les approches de TAL permettant d’acquérir ces relations suivant deux types d’information : la structure interne des termes ou le contexte de ces termes en corpus. Afin d’améliorer la qualité des relations acquises et faciliter leur réutilisation en corpus, nous nous intéressons à la prise en compte du contexte dans une méthode d’acquisition de relations de synonymie basée sur l’utilisation de la structure interne des termes. Nous présentons les résultats d’une expérience préliminaire tenant compte de l’usage des termes dans un corpus biomédical en anglais. Nous donnons quelques pistes de travail pour définir des contraintes sémantiques sur les relations de synonymie acquises.</resume>
        <mots_cles>Acquisition de relations, Synonymie, Relations sémantiques, Terminologie, Domaine Biomédical, Corpus de spécialité</mots_cles>
        <title>State of the Art on the Acquisition of Semantic Relations between Terms : Contextualisation of the Synonymy Relations</title>
        <abstract>Accessing to the context of specialised texts is a crucial but difficult task. It requires automatic or semi-automatic methods dedicated to the identification of semantic relations between terms appearing in the texts. NLP approaches for acquiring semantic relations between terms can be distinguished according to the type of information : the internal structure of the terms and the term context. In order to improve the quality of the acquired synonymy relations and their reusability in other corpora, we aim at taking into account the context into an approach based on the internal structure of the terms. We present the results of a preliminary experiment taking into account the use of the terms in a English biomedical corpora. This experiment will be helpful to add semantic constraints to the already acquired synonymy relations.</abstract>
        <keywords>Relation Acquisition, Synonymy, Semantic Relations, Terminology, Biomedical Domain, Specialised corpora</keywords>
      </article>
      <article id="recital-2012-long-026" session="Poster 2">
        <auteurs>
          <auteur>
            <nom>Noémie-Fleur Sandillon-Rezer</nom>
            <email>nfsr@labri.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CNRS, Esplanade des Arts et Métiers, 33402 Talence</affiliation>
          <affiliation affiliationId="2">LaBRI, 351 Cours de la Libération, 33405 Talence</affiliation>
        </affiliations>
        <titre>Extraction de PCFG et analyse de phrases pré-typées</titre>
        <type>long</type>
        <pages>205-218</pages>
        <resume>Cet article explique la chaîne de traitement suivie pour extraire une grammaire PCFG à partir du corpus de Paris VII. Dans un premier temps cela nécessite de transformer les arbres syntaxiques du corpus en arbres de dérivation d’une grammaire AB, ce que nous effectuons en utilisant un transducteur d’arbres généralisé ; il faut ensuite extraire de ces arbres une PCFG. Le transducteur d’arbres généralisé est une variation des transducteurs d’arbres classiques et c’est l’extraction de la grammaire à partir des arbres de dérivation qui donnera l’aspect probabiliste à la grammaire. La PCFG extraite est utilisée via l’algorithme CYK pour l’analyse de phrases.</resume>
        <mots_cles>Extraction de grammaire, grammaire de Lambek, PCFG, transducteur d’arbre, algorithme CYK</mots_cles>
        <title>PCFG Extraction and Pre-typed Sentences Analysis</title>
        <abstract>This article explains the way we extract a PCFG from the Paris VII treebank. Firslty, we need to transform the syntactic trees of the corpus into derivation trees. The transformation is done with a generalized tree transducer, a variation of the usual top-down tree transducers, and gives as result some derivation trees for an AB grammar. Secondely, we have to extract a PCFG from the derivation trees. For this, we assume that the derivation trees are representative of the grammar. The extracted grammar is used, via the CYK algorithm, for sentence analysis.</abstract>
        <keywords>Grammar Extraction, Lambek grammar, PCFG, tree transducer, CYK Algorithm</keywords>
      </article>
    </articles>
  </conference>
  <conference>
    <edition>
      <acronyme>RECITAL'2013</acronyme>
      <titre>Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues</titre>
      <ville>Sables d'Olonne</ville>
      <pays>France</pays>
      <dateDebut>2013-06-17</dateDebut>
      <dateFin>2012-06-21</dateFin>
      <presidents>
        <nom>Florian Boudin</nom>
        <nom>Loïc Barrault</nom>
      </presidents>
      <typeArticles>
        <type id="long">Articles longs</type>
      </typeArticles>
      <statistiques>
        <acceptations id="long" soumissions="25">18</acceptations>
      </statistiques>
      <siteWeb>http://www.taln2013.org/</siteWeb>
      <meilleurArticle>
        <articleId/>
      </meilleurArticle>
    </edition>
    <articles>
      <article id="recital-2013-long-001" session="Orale">
        <auteurs>
          <auteur>
            <nom>Dhouha Bouamor</nom>
            <email>dhouha.bouamor@cea.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CEA-LIST, LVIC, F91191 Gif sur Yvette Cedex, France</affiliation>
          <affiliation affiliationId="2">LIMSI-CNRS, F-91403 Orsay, France</affiliation>
          <affiliation affiliationId="3">Univ. Paris Sud, Orsay, France</affiliation>
        </affiliations>
        <titre>Acquisition de lexique bilingue d’expressions polylexicales: Une application à la traduction automatique statistique</titre>
        <type>long</type>
        <pages>1-13</pages>
        <resume>Cet article décrit une méthode permettant d’acquérir un lexique bilingue d’expressions polylexicales (EPLS) à partir d’un corpus parallèle français-anglais. Nous identifions dans un premier temps les EPLS dans chaque partie du corpus parallèle. Ensuite, nous proposons un algorithme d’alignement assurant la mise en correspondance bilingue d’EPLS. Pour mesurer l’apport du lexique construit, une évaluation basée sur la tâche de Traduction Automatique Statistique (TAS) est menée. Nous étudions les performances de trois stratégies dynamiques et d’une stratégie statique pour intégrer le lexique bilingue d’expressions polylexicales dans un système de TAS. Les expériences menées dans ce cadre montrent que ces unités améliorent significativement la qualité de traduction.</resume>
        <mots_cles>Expression polylexicale, alignement bilingue, traduction automatique statistique</mots_cles>
        <title>Mining a Bilingual Lexicon of MultiWord Expressions : A Statistical Machine Translation Evaluation Perspective</title>
        <abstract>This paper describes a method aiming to construct a bilingual lexicon of MultiWord Expressions (MWES) from a French-English parallel corpus. We first extract monolingual MWES from each part of the parallel corpus. The second step consists in acquiring bilingual correspondences of MWEs. In order to assess the quality of the mined lexicon, a Statistical Machine Translation (SMT) task-based evaluation is conducted. We investigate the performance of three dynamic strategies and of one static strategy to integrate the mined bilingual MWES lexicon in a SMT system. Experimental results show that such a lexicon significantly improves the quality of translation.</abstract>
        <keywords>MultiWord expression, bilingual alignment, statistical machine translation</keywords>
      </article>
      <article id="recital-2013-long-002" session="Poster">
        <auteurs>
          <auteur>
            <nom>Guiyao Ke</nom>
            <email>guiyao.ke@univ-ubs.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">IRISA, UMR 6074</affiliation>
          <affiliation affiliationId="2">Université de Bretagne Sud, 56000 Vannes</affiliation>
        </affiliations>
        <titre>Quelques variations sur les mesures de comparabilité quantitatives et évaluations sur des corpus comparables Français-Anglais synthétiques</titre>
        <type>long</type>
        <pages>15-27</pages>
        <resume>Dans la suite des travaux de (Li et Gaussier, 2010) nous abordons dans cet article l'analyse d'une famille de mesures quantitatives de comparabilité pour la construction ou l'évaluation des corpus comparables. Après avoir rappelé la définition de la mesure de comparabilité proposée par (Li et Gaussier, 2010), nous développons quelques variantes de cette mesure basées principalement sur la prise en compte des fréquences d'occurrences des entrées lexicales et du nombre de leurs traductions. Nous comparons leurs avantages et inconvénients respectifs dans le cadre d'expérimentations basées sur la dégradation progressive du corpus parallèle Europarl par remplacement de blocs selon la méthodologie suivie par (Li et Gaussier, 2010). L'impact sur ces mesures des taux de couverture des dictionnaires bilingues vis-à-vis des blocs considérés est également examiné.</resume>
        <mots_cles>Corpus comparables, Mesures de comparabilité, Évaluation</mots_cles>
        <title>Some variations on quantitative comparability measures and evaluations on synthetic French-English comparable corpora</title>
        <abstract>Following the pioneering work by (Li et Gaussier, 2010) we address in this paper the analysis of a family of quantitative measures of comparability dedicated to the construction or evaluation of comparable corpora. After recalling the definition of the comparability measure proposed by (Li et Gaussier, 2010), we develop some variants of this measure based primarily on the consideration of the occurrence frequency of lexical entries and the number of their translations. We compare the respective advantages and disadvantages of these variants in the context of an experiments based on the progressive degradation of the Europarl parallel corpus, by replacing blocks according to the methodology followed by (Li et Gaussier, 2010). The impact of the coverage of bilingual dictionaries on these measures is also discussed.</abstract>
        <keywords>Comparable corpora, Comparability measures, Evaluation</keywords>
      </article>
      <article id="recital-2013-long-003" session="Orale">
        <auteurs>
          <auteur>
            <nom>Noémie-Fleur Sandillon-Rezer</nom>
            <email>nfsr@labri.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CNRS, Esplanade des Arts et Métiers, 33402 Talence</affiliation>
          <affiliation affiliationId="2">LaBRI, 351 Cours de la Libération, 33405 Talence</affiliation>
        </affiliations>
        <titre>Inférence grammaticale guidée par clustering</titre>
        <type>long</type>
        <pages>28-41</pages>
        <resume>Dans cet article, nous nous focalisons sur la manière d’utiliser du clustering hiérarchique pour apprendre une grammaire AB à partir d’arbres de dérivation partiels. Nous décrirons brièvement les grammaires AB ainsi que les arbres de dérivation dont nous nous servons comme entrée pour l’algorithme, puis la manière dont nous extrayons les informations des corpus arborés pour l’étape de clustering. L’algorithme d’unification, dont le pivot est le cluster, sera décrit et les résultats analysés en détails.</resume>
        <mots_cles>grammaires catégorielles, clustering hiérarchique, inférence grammaticale</mots_cles>
        <title>Clustering for categorial grammar induction</title>
        <abstract>In this article, we describe the way we use hierarchical clustering to learn an AB grammar from partial derivation trees. We describe AB grammars and the derivation trees we use as input for the clustering, then the way we extract information from Treebanks for the clustering. The unification algorithm, based on the information extracted from our cluster, will be explained and the results discussed.</abstract>
        <keywords>categorial grammars, hierarchical clustering, grammatical inference</keywords>
      </article>
      <article id="recital-2013-long-004" session="Poster">
        <auteurs>
          <auteur>
            <nom>Aurélie Joseph</nom>
            <email>joseph.aurelie@gmail.com</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LDI, 99 avenue Jean-Baptise Clément F-93430 Villetaneuse</affiliation>
          <affiliation affiliationId="2">ITESOFT, Parc d’Andron, le Séquoia, 30470 Aimargues</affiliation>
        </affiliations>
        <titre>Améliorer l’extraction et la description d’expressions polylexicales grâce aux règles transformationnelles</titre>
        <type>long</type>
        <pages>42-55</pages>
        <resume>Cet article présente une méthodologie permettant d’extraire et de décrire des locutions verbales vis-à-vis de leur comportement transformationnel. Plusieurs objectifs sont ciblés : 1) extraire automatiquement les expressions phraséologiques et en particulier les expressions figées, 2) décrire linguistiquement le comportement des phraséologismes 3) comparer les méthodes statistiques et notre approche et enfin 4) montrer l’importance de ces expressions dans un outil de classification de textes.</resume>
        <mots_cles>expressions polylexicales, expressions figées, locution verbale, extraction, transformation, classification de textes</mots_cles>
        <title>Enhance Multiword Expressions Extraction and Description with Transformational Rules</title>
        <abstract>This paper presents a methodology to extract and describe verbal multiword expressions using their transformational behavior. Several objectives are targeted: 1) automatically extracting MWE and especially frozen expression, 2) describing linguistically their MWE behavior, 3) comparing statistical methods and our approach, and finally 4) showing the importance of MWE in a text classification tool.</abstract>
        <keywords>multiword expression, verbal phrase, extraction, transformation, text classification</keywords>
      </article>
      <article id="recital-2013-long-005" session="Poster">
        <auteurs>
          <auteur>
            <nom>Manuela Yapomo</nom>
            <email>yapomodomkem@etu.unistra.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LiLPa (Linguistique, Langues, Parole), EA 1339</affiliation>
          <affiliation affiliationId="2">ICube - Laboratoire des sciences de l’Ingénieur, de l’Informatique et de l’Imagerie, UMR 7357 Université de Strasbourg</affiliation>
        </affiliations>
        <titre>Construction de corpus multilingues : état de l’art</titre>
        <type>long</type>
        <pages>56-68</pages>
        <resume>Les corpus multilingues sont extensivement exploités dans plusieurs branches du traitement automatique des langues. Cet article présente une vue d’ensemble des travaux en construction automatique de ces corpus. Nous traitons ce sujet en donnant premièrement un aperçu de différentes perceptions de la comparabilité. Nous examinons ensuite les principales approches de calcul de similarité, de construction et d’évaluation développées dans le domaine. Nous observons que Le calcul de la similarité textuelle se fait généralement sur la base de statistiques de corpus, de la structure de ressources ontologiques ou de la combinaison de ces deux approches. Dans un cadre multilingue avec l’utilisation d’un dictionnaire multilingue ou d’un traducteur automatique, de nombreux problèmes apparaissent. L’exploitation d’une ressource ontologique multilingue semble être une solution. En classification, la problématique de l’ajout de documents à la base initiale sans affecter la qualité des clusters demeure ouverte.</resume>
        <mots_cles>corpus multilingues, comparabilité, similarité textuelle translingue, classification</mots_cles>
        <title>Multilingual document clustering : state of the art</title>
        <abstract>Multilingual corpora are extensively exploited in several branches of natural language processing. This paper presents an overview of works in the automatic construction of such corpora. We address this topic by first providing an overview of different perceptions of comparability. We then examine the main approaches to similarity computation, construction and evaluation developed in the field. We notice that the measurement of the textual similarity is usually based on corpus statistics or the structure of ontological resources or on a combination of these two approaches. In a multilingual framework, with the use of a multilingual dictionary or a machine translator, many problems arise. The exploitation of a multilingual ontological ressource seems to be a worthy option. In clustering, the problem of adding documents to the initial base without affecting the quality of clusters remains open.</abstract>
        <keywords>multilingual corpora, comparability, crosslingual textual similarity, classification</keywords>
      </article>
      <article id="recital-2013-long-006" session="Poster">
        <auteurs>
          <auteur>
            <nom>Dhaou Ghoul</nom>
            <email>Dhaou.Ghoul@gmail.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">STIH, 1, rue Victor Cousin 75005 Paris</affiliation>
        </affiliations>
        <titre>Développement de ressources pour l’entrainement et l’utilisation de l’étiqueteur morphosyntaxique TreeTagger sur l’arabe</titre>
        <type>long</type>
        <pages>69-82</pages>
        <resume>Dans cet article, nous présentons les étapes du développement de ressources pour l’entraînement et l’utilisation d’un nouvel outil de l’étiquetage morphosyntaxique de la langue arabe. Nous avons mis en oeuvre un système basé sur l'étiqueteur stochastique TreeTagger, réputé pour son efficacité et la généricité de son architecture. Pour ce faire, nous avons commencé par la constitution de notre corpus de travail. Celui-ci nous a d'abord servi à réaliser l'étape de segmentation lexicale. Dans un second temps, ce corpus a permis d'effectuer l'entrainement de TreeTagger, grâce à un premier étiquetage réalisé avec l'étiqueteur ASVM 1.0, suivi d'une phase de correction manuelle. Nous détaillons ainsi les prétraitements requis, et les différentes étapes de la phase d'apprentissage avec cet outil. Nous terminons par une évaluation sommaire des résultats, à la fois qualitative et quantitative. Cette évaluation, bien que réalisée sur un corpus de test de taille modeste, montre que nos premiers résultats sont encourageants.</resume>
        <mots_cles>TALN, langue arabe, corpus d'apprentissage, étiquetage morphosyntaxique, segmentation de l'arabe, arbre de décision, lexique, jeux d’étiquette, TreeTagger, ASVM 1.0</mots_cles>
        <title>Development of resources for training and the use of the tagger TreeTagger on Arabic</title>
        <abstract>In this paper, we present the steps of the development of resources for training and the use of a new tool for the part-of-speech tagging of Arabic. We implemented a tagging system based on TreeTagger, a generic stochastic tagging tool, very popular for its efficiency. First of all, we began by gathering a working corpus, large enough to ensure a general linguistic coverage. This corpus has been used to implement the tokenization process, as well as to train TreeTagger. We first present our method of tokenization, then we describe all the steps of the preprocessing and training process, using ASVM 1.0 to yield a raw POS tagging that was subsequently manually corrected. Finally, we implemented a straightforward evaluation of the outputs, both in a quantitative and qualitative way, on a small test corpus. Though restricted, this evaluation showed really encouraging results.</abstract>
        <keywords>NLP, Arabic language, training corpus, POS tagging, tokenization, decision tree, lexicon, tagsets, TreeTagger, ASVM 1.0</keywords>
      </article>
      <article id="recital-2013-long-007" session="Poster">
        <auteurs>
          <auteur>
            <nom>Amel Ziani</nom>
            <email>Z_amel1911@live.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Nabiha Azizi</nom>
            <email>yamina.tlili@univ-annaba.org</email>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Yamina Tlili Guiassa</nom>
            <email>nabiha.azizi@univ-annaba.org</email>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Département d’informatique, Université Badji Mokhtar Annaba (Algérie)</affiliation>
          <affiliation affiliationId="2">Lri laboratory : laboratoire de recherche en informatique</affiliation>
          <affiliation affiliationId="3">Labged laboratory : Laboratoire de gestion électronique des documents</affiliation>
        </affiliations>
        <titre>Détection de polarité d’opinions dans les forums en langue arabe par fusion de plusieurs SVM</titre>
        <type>long</type>
        <pages>83-95</pages>
        <resume>Cet article décrit notre contribution sur la détection de polarité d’opinions en langue arabe par apprentissage supervisé. En effet le système proposé comprend trois phases: le prétraitement du corpus, l’extraction des caractéristiques et la classification. Pour la deuxième phase, nous utilisons vingt caractéristiques dont les principales sont l’émotivité, la réflexivité, l’adressage et la polarité. La phase de classification représente dans notre travail la combinaison des plusieurs classifieurs SVMs (Machine à Vecteur de Support) pour résoudre le problème multi classes. Nous avons donc analysés les deux stratégies de SVM multi classes qui sont : « un contre tous » et « un contre un » afin de comparer les résultats et améliorer la performance du système global.</resume>
        <mots_cles>Fouille d’opinions, apprentissage supervisé, Machine à Vecteur de Support (SVM), combinaison des classifieurs</mots_cles>
        <title>Polarity Opinion Detection in Arabic Forums by Fusing Multiple SVMs</title>
        <abstract>This article describes our contribution on the polarity’s detection of opinions in Arabian language by supervised training. Indeed the proposed system consists of three phases: the pretreatment of the corpus, the extraction of the features and the classification. For the second phase, we use twenty features of which the main are emotionalism, the reflexivity, the adressage and the polarity. The phase of classification represents in our work the combination of the several SVMs (Support Vector Machine),to solve the multi class problem. We analyzed the two strategies of the SVMs multi class that are: "one against all" and "one against one" in order to compare the results and to improve the performance of the global system.</abstract>
        <keywords>Opinion Mining, supervised training, Support Vector Machine (SVM), classifiers combination</keywords>
      </article>
      <article id="recital-2013-long-008" session="Poster">
        <auteurs>
          <auteur>
            <nom>Adrien Bougouin</nom>
            <email>adrien.bougouin@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LINA - UMR CNRS 6241, Université de Nantes, France</affiliation>
        </affiliations>
        <titre>État de l’art des méthodes d’extraction automatique de termes-clés</titre>
        <type>long</type>
        <pages>96-109</pages>
        <resume>Cet article présente les principales méthodes d’extraction automatique de termes-clés. La tâche d’extraction automatique de termes-clés consiste à analyser un document pour en extraire les expressions (phrasèmes) les plus représentatives de celui-ci. Les méthodes d’extraction automatique de termes-clés sont réparties en deux catégories : les méthodes supervisées et les méthodes non supervisées. Les méthodes supervisées réduisent la tâche d’extraction de termes-clés à une tâche de classification binaire (tous les phrasèmes sont classés parmi les termesclés ou les non termes-clés). Cette classification est possible grâce à une phase préliminaire d’apprentissage, phase qui n’est pas requise par les méthodes non-supervisées. Ces dernières utilisent des caractéristiques (traits) extraites du document analysé (et parfois d’une collection de documents de références) pour vérifier des propriétés permettant d’identifier ses termes-clés.</resume>
        <mots_cles>extraction de termes-clés, méthodes supervisées, méthodes non-supervisées, état de l’art</mots_cles>
        <title>State of the Art of Automatic Keyphrase Extraction Methods</title>
        <abstract>This article presents the state of the art of the automatic keyphrase extraction methods. The aim of the automatic keyphrase extraction task is to extract the most representative terms of a document. Automatic keyphrase extraction methods can be divided into two categories : supervised methods and unsupervised methods. For supervised methods, the task is reduced to a binary classification where terms are classified as keyphrases or non keyphrases. This classification requires a learning step which is not required by unsupervised methods. The unsupervised methods use features extracted from the analysed document (sometimes a document collection) to check properties which allow keyphrase identification.</abstract>
        <keywords>keyphrase extraction, supervised methods, unsupervised methods, state of the art</keywords>
      </article>
      <article id="recital-2013-long-009" session="Poster">
        <auteurs>
          <auteur>
            <nom>Ophélie Lacroix</nom>
            <email>ophelie.lacroix@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LINA - Université de Nantes, 2 Rue de la Houssinière, 44322 Nantes Cedex 3</affiliation>
        </affiliations>
        <titre>Influence de l’étiquetage syntaxique des têtes sur l’analyse en dépendances discontinues du français</titre>
        <type>long</type>
        <pages>110-123</pages>
        <resume>Dans cet article nous souhaitons mettre en évidence l’utilité d’un étiquetage syntaxique appliqué en amont d’une analyse syntaxique en dépendances. Les règles de la grammaire catégorielle de dépendances du français utilisées pour l’analyse gèrent les dépendances discontinues et les relations syntaxiques à longue distance. Une telle méthode d’analyse génère un nombre conséquent de structures de dépendances et emploie un temps d’analyse trop important. Nous voulons alors montrer qu’une méthode locale d’étiquetage peut diminuer l’ampleur de ces difficultés et par la suite aider à résoudre le problème global de désambiguïsation d’analyse en dépendances. Nous adaptons alors une méthode d’étiquetage aux catégories de la grammaire catégorielle de dépendance. Nous obtenons ainsi une pré-sélection des têtes des dépendances permettant de réduire l’ambiguïté de l’analyse et de voir que les résultats locaux d’une telle méthode permettent de trouver des relations distantes de dépendances.</resume>
        <mots_cles>Analyse syntaxique en dépendances discontinues, Étiquetage syntaxique</mots_cles>
        <title>On the Effect of Head Tagging on Parsing Discontinuous Dependencies in French</title>
        <abstract>In this paper we want to show the strong impact of syntactic tagging on syntactic dependency parsing. The rules of categorial dependency grammar used to parse French deal with discontinuous dependencies and long distance syntactic relations. Such parsing method produces a substantial number of dependency structures and takes too much parsing time. We want to show that a local tagging method can reduce these problems and help to solve the global problem of dependency parsing disambiguation. Then we adapt a tagging method to types of the categorial dependency grammar. We obtain a dependency-head pre-selection allowing to reduce parsing ambiguity and to see that we can find distant relation of dependencies through local results of such method.</abstract>
        <keywords>Discontinuous Dependency Parsing, Syntactic Tagging</keywords>
      </article>
      <article id="recital-2013-long-010" session="Poster">
        <auteurs>
          <auteur>
            <nom>Houda Saadane</nom>
            <email>houda.saadane@e.u-grenoble3.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIDILEM, Université Stendhal – Grenoble III, 1180, avenue central, F-38400 Saint Martin d’Hères</affiliation>
        </affiliations>
        <titre>Une approche linguistique pour l'extraction des connaissances dans un texte arabe</titre>
        <type>long</type>
        <pages>124-137</pages>
        <resume>Nous présentons dans cet article un système d'extraction de connaissances en arabe, fondé sur une analyse morphosyntaxique profonde. Ce système reconnaît les mots simples, les expressions idiomatiques, les mots composés et les entités nommées. L'analyse identifie aussi les relations syntaxiques de dépendance et traite les formes passives et actives. L’extraction des connaissances est propre à l’application et utilise des règles d’extraction sémantiques qui s'appuient sur le résultat de l'analyse morphosyntaxique. A ce niveau, le type de certaines entités nommées peut être révisé. L'extraction se base, dans nos expérimentations, sur une ontologie dans le domaine de la sécurité. Le RDF (Resource Description Framework) produit est ensuite traité pour regrouper les informations qui concernent un même événement ou une même entité nommée. Les informations ainsi extraites peuvent alors aider à appréhender les informations contenues dans un ensemble de textes, alimenter une base de connaissances, ou bien servir à  des outils de veille.</resume>
        <mots_cles>Analyse linguistique, fouille de textes, arabe, entités nommées, extraction d’informations, règles d’extraction, ontologie</mots_cles>
        <title>A linguistic approach for knowledge extraction from an Arabic text</title>
        <abstract>We present in this paper a knowledge extraction system for Arabic. The information extraction is based on a deep morphosyntactic analysis. It also recognizes single words, idiomatic expressions, compounds and named entities. The analysis also identifies dependency relations, verb tenses and passive/active forms. Information extraction is application-independent and uses extraction rules that rely on the result of the morphosyntactic analysis. At this level, some named entity categories can be reconsidered. This extraction is based in our experimentations on the security ontology. The Resource Description Framework (RDF) obtained is then processed to gather information concerning a single event or named entity. The information extracted can help to understand the information contained in a set of texts, to infer knowledge into a knowledge base, or be used for monitoring tools.</abstract>
        <keywords>Linguistic analysis, Text Mining, Arabic, named entities, information extraction, extraction rules, ontology</keywords>
      </article>
      <article id="recital-2013-long-011" session="Poster">
        <auteurs>
          <auteur>
            <nom>Sylvain Hatier</nom>
            <email>sylvain.hatier@u-grenoble3.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIDILEM, BP 25, 38040 Grenoble Cedex 09</affiliation>
        </affiliations>
        <titre>Extraction des mots simples du lexique scientifique transdisciplinaire dans les écrits de sciences humaines : une première expérimentation</titre>
        <type>long</type>
        <pages>138-149</pages>
        <resume>Nous présentons dans cet article les premiers résultats de nos travaux sur l'extraction de mots simples appartenant au lexique scientifique transdisciplinaire sur un corpus analysé morpho-syntaxiquement composé d'articles de recherche en sciences humaines et sociales. La ressource générée sera utilisée lors de l'indexation automatique de textes comme filtre d'exclusion afin d'isoler ce lexique de la terminologie. Nous comparons plusieurs méthodes d'extraction et montrons qu'un premier lexique de mots simples peut être dégagé et que la prise en compte des unités polylexicales ainsi que de la distribution seront nécessaires par la suite afin d'extraire l'ensemble de la phraséologie transdisciplinaire.</resume>
        <mots_cles>corpus, écrits scientifiques, lexique, phraséologie</mots_cles>
        <title>Extraction of academic lexicon's simple words in humanities writings</title>
        <abstract>This paper presents a first extraction of academic lexicon's simple words in french academic writings in the fields of humanities and social sciences through a corpus study of research articles using morpho-syntactic analysis. This academic lexicon resource will be used for automatic indexing as a stoplist in order to exclude this lexicon from the terminology. We try various extraction methods and show that a first simple words lexicon can be generated but that multiwords expressions and words distribution should be taken into consideration to extract academic phraseology.</abstract>
        <keywords>corpus, scientific writings, lexicon, phraseology</keywords>
      </article>
      <article id="recital-2013-long-012" session="Orale">
        <auteurs>
          <auteur>
            <nom>Marie Dubremetz</nom>
            <email>marie.dubremetz@lingfil.uu.se</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Uppsala universitet, Institutionen för lingvistik och filologi - Box 635 - 751 26 Uppsala, Suède</affiliation>
          <affiliation affiliationId="2">Université Paris Ouest La Défense - 200, Avenue de la République - 92001 Nanterre, France</affiliation>
        </affiliations>
        <titre>Vers une identification automatique du chiasme de mots</titre>
        <type>long</type>
        <pages>150-163</pages>
        <resume>Cette recherche porte sur le chiasme de mots : figure de style jouant sur la réversion (ex. « Bonnet blanc, blanc bonnet »). Elle place le chiasme dans la problématique de sa reconnaissance automatique : qu’est-ce qui le définit et comment un ordinateur peut le trouver ? Nous apportons une description formelle du phénomène. Puis nous procédons à la constitution d’une liste d’exemples contextualisés qui nous sert au test des hypothèses. Nous montrons ainsi que l’ajout de contraintes formelles (contrôle de la ponctuation et omission des mots vides) pénalise très peu le rappel et augmente significativement la précision de la détection. Nous montrons aussi que la lemmatisation occasionne peu d’erreurs pour le travail d’extraction mais qu’il n’en est pas de même pour la racinisation. Enfin nous mettons en évidence que l’utilisation d’un thésaurus apporte quelques résultats pertinents.</resume>
        <mots_cles>chiasme, rhétorique, antimétabole, figure de style</mots_cles>
        <title>Towards an automatic identification of chiasmus of words</title>
        <abstract>This article summarises the study of the rhetorical figure “chiasmus” (e.g : “Quitters never win and winners never quit.”). We address the problem of its computational identification. How can a computer identify this automatically ? For this purpose this article will provide a formal description of the phenomenon. First, we put together an annotated text for testing our hypothesis. At the end we demonstrate that the use of stopword lists and the identification of the punctuation improve the precision of the results with very little impact on the recall. We discover also that using lemmatization improves the results but stemming doesn’t. Finally we see that a French thesaurus provided us with good results on the most elaborate form of chiasmus.</abstract>
        <keywords>chiasmus, rhetoric, antimetabole, stylistic device</keywords>
      </article>
      <article id="recital-2013-long-013" session="Orale">
        <auteurs>
          <auteur>
            <nom>Maxime Lefrançois</nom>
            <email>maxime.lefrancois@inria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">WIMMICS, Inria, 2004, route des Lucioles, BP 93, 06902 Sophia Antipolis Cedex</affiliation>
        </affiliations>
        <titre>Représentation des connaissances du DEC: Concepts fondamentaux du formalisme des Graphes d’Unités</titre>
        <type>long</type>
        <pages>164-177</pages>
        <resume>Dans cet article nous nous intéressons au choix d’un formalisme de représentation des connaissances qui nous permette de représenter, manipuler, interroger et raisonner sur des connaissances linguistiques du Dictionnaire Explicatif et Combinatoire (DEC) de la Théorie Sens-Texte. Nous montrons que ni les formalismes du web sémantique ni le formalisme des Graphes conceptuels n’est adapté pour cela, et justifions l’introduction d’un nouveau formalisme dit des Graphes d’Unités. Nous introduisons la hiérarchie des Types d’Unités au coeur du formalisme, et présentons les Graphes d’Unités ainsi que la manière dont on peut les utiliser pour représenter certains aspects du DEC.</resume>
        <mots_cles>Représentation de Connaissances Linguistiques, Théorie Sens-Texte, Graphes d’Unités, Dictionnaire Explicatif et Combinatoire</mots_cles>
        <title>ECD Knowledge Representation : Fundamental Concepts of the Unit Graphs Framework</title>
        <abstract>In this paper we are interested in the choice of a knowledge representation formalism that enables the representation, manipulation, query, and reasoning over linguistic knowledge of the Explanatory and Combinatorial Dictionary (ECD) of the Meaning-Text Theory. We show that neither the semantic web formalisms nor the Conceptual Graphs Formalism suit our needs, and justify the introduction of a new formalism denoted Unit Graphs. We introduce the core of this formalism which is the Unit Types hierarchy, and present Unit Graphs and how one may use them to represent aspects of the ECD.</abstract>
        <keywords>Linguistic Knowledge Representation, Meaning-Text Theory, Unit Graphs, Explanatory and Combinatorial Dictionary</keywords>
      </article>
      <article id="recital-2013-long-014" session="Poster">
        <auteurs>
          <auteur>
            <nom>Corentin Ribeyre</nom>
            <email>corentin.ribeyre@inria.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université Paris 7 Diderot, 75013 PARIS</affiliation>
          <affiliation affiliationId="2">INRIA Paris-Rocquencourt, Rocquencourt BP 105 78153 LE CHESNAY</affiliation>
        </affiliations>
        <titre>Vers un système générique de réécriture de graphes pour l’enrichissement de structures syntaxiques</titre>
        <type>long</type>
        <pages>178-191</pages>
        <resume>Ce travail présente une nouvelle approche pour injecter des dépendances profondes (sujet des verbes à contrôle, partage du sujet en cas d’ellipses, ...) dans un corpus arboré présentant un schéma d’annotation surfacique et projectif. Nous nous appuyons sur un système de réécriture de graphes utilisant des techniques de programmation par contraintes pour produire des règles génériques qui s’appliquent aux phrases du corpus. Par ailleurs, nous testons la généricité des règles en utilisant des sorties de trois analyseurs syntaxiques différents, afin d’évaluer la dégradation exacte de l’application des règles sur des analyses syntaxiques prédites.</resume>
        <mots_cles>réécriture de graphes, évaluation de shéma d’annotations, parsing, analyse en syntaxe profonde</mots_cles>
        <title>Towards a generic graph rewriting system to enrich syntactic structures</title>
        <abstract>This work aims to present a new approach for injecting deep dependencies (subject of control verbs, subject sharing in case of ellipsis, ...) into a surfacic and projective treebank. We use a graph rewriting system with constraint programming techniques for producing generic rules which can be easily applied to a treebank. Moreover, we are testing the genericity of our rules by using output of three different parsers to evaluate how the rules behave on predicted parse trees.</abstract>
        <keywords>graph rewriting system, annotation schemes evaluation, deep syntax parsing</keywords>
      </article>
      <article id="recital-2013-long-015" session="Poster">
        <auteurs>
          <auteur>
            <nom>Mohammad Nasiruddin</nom>
            <email>mohammad.nasiruddin@imag.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire d’Informatique de Grenoble-Groupe d’Étude pour la Traduction Automatique/Traitement Automatisé des Langues et de la Parole, Univ. Grenoble Alpes</affiliation>
        </affiliations>
        <titre>État de l'art de l'induction de sens: une voie vers la désambiguïsation lexicale pour les langues peu dotées</titre>
        <type>long</type>
        <pages>192-205</pages>
        <resume>La désambiguïsation lexicale, le processus qui consiste à automatiquement identifier le ou les sens possible d'un mot polysémique dans un contexte donné, est une tâche fondamentale pour le Traitement Automatique des Langues (TAL). Le développement et l'amélioration des techniques de désambiguïsation lexicale ouvrent de nombreuses perspectives prometteuses pour le TAL. En effet, cela pourrait conduire à un changement paradigmatique en permettant de réaliser un premier pas vers la compréhension des langues naturelles. En raison du manque de ressources langagières, il est parfois difficile d'appliquer des techniques de désambiguïsation à des langues peu dotées. C'est pourquoi, nous nous intéressons ici, à enquêter sur comment avoir un début de recherche sur la désambiguïsation lexicale pour les langues peu dotées, en particulier en exploitant des techniques d'induction des sens de mots, ainsi que quelques suggestions de pistes intéressantes à explorer.</resume>
        <mots_cles>désambiguïsation lexicale, induction de sens, langues peu dotées, ressources langagières</mots_cles>
        <title>A State of the Art of Word Sense Induction: A Way Towards Word Sense Disambiguation for Under-Resourced Languages</title>
        <abstract>Word Sense Disambiguation (WSD), the process of automatically identifying the meaning of a polysemous word in a sentence, is a fundamental task in Natural Language Processing (NLP). Progress in this approach to WSD opens up many promising developments in the field of NLP and its applications. Indeed, improvement over current performance levels could allow us to take a first step towards natural language understanding. Due to the lack of lexical resources it is sometimes difficult to perform WSD for under-resourced languages. This paper is an investigation on how to initiate research in WSD for under-resourced languages by applying Word Sense Induction (WSI) and suggests some interesting topics to focus on.</abstract>
        <keywords>Word Sense Disambiguation, Word Sense Induction, under-resourced languages, lexical resources</keywords>
      </article>
      <article id="recital-2013-long-016" session="Poster">
        <auteurs>
          <auteur>
            <nom>Rahma Boujelbane</nom>
            <email>Rahma.boujelbane@gmail.com</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">ANLP_MIRACL, Sfax, Tunisie</affiliation>
          <affiliation affiliationId="2">LIF, UMR7279, 13288, Marseille, France</affiliation>
        </affiliations>
        <titre>Génération des corpus en dialecte tunisien pour la modélisation de langage d'un système de reconnaissance</titre>
        <type>long</type>
        <pages>206-216</pages>
        <resume>Ces derniers temps, vu la situation préoccupante du monde arabe, les dialectes arabes et notamment le dialecte tunisien est devenu de plus en plus utilisé dans les interviews, les journaux télévisés et les émissions de débats. Cependant, cette situation présente des conséquences négatives importantes pour le Traitement Automatique du Langage Naturel (TALN): depuis que les dialectes parlés ne sont pas officiellement écrits et n’ont pas d’orthographe standard, il est très coûteux d'obtenir des corpus adéquats à utiliser pour des outils de TALN. Par conséquent, il n’existe pas des corpus parallèles entre l’Arabe Standard Moderne(ASM) et le Dialecte Tunisien (DT). Dans ce travail, nous proposons une méthode pour la création d’un lexique bilingue ASM–DT et un processus pour la génération automatique de corpus dialectaux. Ces ressources vont servir à la construction d’un modèle de langage pour les journaux télévisés tunisiens, afin de l’intégrer dans un Système de Reconnaissance Automatique de Parole (SRAP).</resume>
        <mots_cles>Dialecte Tunisien, lexique ASM-DT, TDT: Tunisian Dialect Translator</mots_cles>
        <title>Generation of tunisian dialect corpora for adapting language models</title>
        <abstract>Lately, given the serious situation in the Arab world, the Arab dialects such as Tunisian dialect became increasingly used and represented in the interviews, news and debate programs. However, this situation presents negative consequences for Natural Language Processing (NLP): Since dialects are not officially written and have no orthographic standard, it is very costly to obtain adequate corpora to train NLP tools. Therefore, it does not even exist parallel corpora between Standard Arabic (MSA) and Tunisian Dialect(TD). In this work, we propose a method for the creation of a bilingual lexicon MSA-TD and an automatic process for generating dialectal corpora. These resources will be used to build a language model for Tunisian news, in order to integrate it into an Automatic Speech Recognition (ASR).</abstract>
        <keywords>Tunisian Dialect, MSA-TD lexicon, TDT: Tunisian Dialect Translator</keywords>
      </article>
      <article id="recital-2013-long-017" session="Orale">
        <auteurs>
          <auteur>
            <nom>Simon Leva</nom>
            <email>sleva@univ-tlse2.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Nicolas Faessel</nom>
            <email>nicolas.faessel@irit.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CLLE–ERSS : CNRS et Université de Toulouse (UMR 5263), 5 allées Antonio Machado, 31058 Toulouse Cedex 9</affiliation>
          <affiliation affiliationId="2">IRIT : CNRS et Université de Toulouse (UMR 5505), 118 route de Narbonne, 31062 Toulouse Cedex 9</affiliation>
        </affiliations>
        <titre>Détection automatique des sessions de recherche par similarité des résultats provenant d’une collection de documents externe</titre>
        <type>long</type>
        <pages>217-230</pages>
        <resume>Les utilisateurs d’un système de recherche d’information mettent en oeuvre des comportements de recherche complexes tels que la reformulation de requête et la recherche multitâche afin de satisfaire leurs besoins d’information. Ces comportements de recherche peuvent être observés à travers des journaux de requêtes, et constituent des indices permettant une meilleure compréhension des besoins des utilisateurs. Dans cette perspective, il est nécessaire de regrouper au sein d’une même session de recherche les requêtes reliées à un même besoin d’information. Nous proposons une méthode de détection automatique des sessions exploitant la collection de documents WIKIPÉDIA, basée sur la similarité des résultats renvoyés par l’interrogation de cette collection afin d’évaluer la similarité entre les requêtes. Cette méthode obtient de meilleures performances que les approches temporelle et lexicale traditionnellement employées pour la détection de sessions séquentielles, et peut être appliquée à la détection de sessions imbriquées. Ces expérimentations ont été réalisées sur des données provenant du portail OpenEdition.</resume>
        <mots_cles>Recherche d’information, détection automatique de sessions de recherche, analyse de journal de requêtes</mots_cles>
        <title>Automatic search session detection exploiting results similarity from an external document collection</title>
        <abstract>Search engines users apply complex search behaviours such as query reformulation and multitasking search to satisfy their information needs. These search behaviours may be observed through query logs, and constitute clues allowing a better understanding of users’ needs. In this perspective, it is decisive to group queries related to the same information need into a unique search session. We propose an automatic session detection method exploiting the WIKIPEDIA documents collection, based on the similarity between the results returned for each query pair to estimate the similarity between queries. This method shows better performance than both temporal and lexical approaches traditionally used for successive session detection, and can be applied as well to multitasking search session detection. These experiments were conducted on a dataset originating from the OpenEdition Web portal.</abstract>
        <keywords>Information retrieval, automatic search session detection, query log analysis</keywords>
      </article>
      <article id="recital-2013-long-018" session="Orale">
        <auteurs>
          <auteur>
            <nom>Zhen Wang</nom>
            <email>zhen.wang@geolsemantics.com</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">GEOLSemantics, 32, rue Brancion, 75015, Paris</affiliation>
          <affiliation affiliationId="2">INALCO, ERTIM, 2 rue de Lille, 75343, Paris</affiliation>
        </affiliations>
        <titre>Une approche mixte morpho-syntaxique et statistique pour la reconnaissance d'entités nommées en langue chinoise</titre>
        <type>long</type>
        <pages>231-243</pages>
        <resume>Cet article présente une approche mixte, morpho-syntaxique et statistique, pour la reconnaissance d'entités nommées en langue chinoise dans un système d'extraction automatique d'information. Le processus se divise principalement en trois étapes : la première génère des noms propres potentiels à l'aide de règles morphologiques ; la deuxième utilise un modèle de langue afin de sélectionner le meilleur résultat ; la troisième effectue la reconnaissance d'entités nommées grâce à une analyse syntaxique locale. Cette dernière permet une reconnaissance automatique d'entités nommées plus pertinente et plus complète.</resume>
        <mots_cles>Reconnaissance de noms propres, Reconnaissance d'entités nommées, Traitement automatique du chinois, Extraction d'information, Analyse syntaxique</mots_cles>
        <title>A Mixed Morpho-Syntactic and Statistical Approach to Chinese Named Entity Recognition</title>
        <abstract>This paper presents a morpho-syntactic and statistical approach for Chinese named entity recognition which is a part of an automatic system for information extraction. The process is divided into three steps : first, the generation of possible proper nouns is based on morphological rules; second a language model is used to select the best result, and last, a local syntactic parsing performs the named entity recognition. Syntactic parsing makes named entity recognition more relevant and more complete.</abstract>
        <keywords>Proper noun recogition, Named entity recognition (NER), Chinese Natural Language Processing, Information extraction, Syntactic parsing</keywords>
      </article>
    </articles>
  </conference>
  <conference>
	<edition>
		<acronyme>RECITAL'2014</acronyme>
		<titre>16e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues</titre>
		<ville>Marseille</ville>
		<pays>France</pays>
		<dateDebut>2014-07-01</dateDebut>
		<dateFin>2014-07-04</dateFin>
		<presidents>
			<president>
				<prenom>Núria</prenom>
				<nom>Gala</nom>
			</president>
			<president>
				<prenom>Klim</prenom>
				<nom>Peshkov</nom>
			</president>
		</presidents>
		<editeurs>
			<editeur>
				<prenom>Brigitte</prenom>
				<nom>Bigi</nom>
			</editeur>
		</editeurs>
		<typeArticles>
			<type id="long">Papiers longs</type>
		</typeArticles>
		<siteWeb>http://www.taln2014.org</siteWeb>
	</edition>
	<articles>
		<article id="recital-2014-long-001" session="Sémantique">
			<auteurs>
				<auteur>
					<prenom>Olivier</prenom>
					<nom>Michalon</nom>
					<email>olivier.michalon@lif.univ-mrs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIF, AMU, CNRS, 163 avenue de Luminy, 13288 Marseille Cedex 9</affiliation>
			</affiliations>
			<titre>Modélisation probabiliste de l’interface syntaxe sémantique à l’aide de grammaires hors contexte probabilistes Expériences avec FrameNet</titre>
			<type>long</type>
			<pages>1-12</pages>
			<resume>Cet article présente une méthode générative de prédiction de la structure sémantique en cadres d’une phrase à partir de sa structure syntaxique et décrit les grammaires utilisées ainsi que leurs performances. Ce système permet de prédire, pour un mot dans le contexte syntaxique d’une phrase, le cadre le plus probable. Le système génératif permet d’attribuer à ce mot un cadre et à l’ensemble de chemins des rôles sémantiques. Bien que les résultats ne soient pas encore satisfaisants, cet analyseur permet de regrouper les tâches d’analyse sémantique (sélection du cadre, sélection des actants, attribution des rôles), contrairement aux travaux précédemment publiés. De plus, il offre une nouvelle approche de l’analyse sémantique en cadres, dans la mesure où elle repose plus sur la structure syntaxique que sur les mots de la phrase.</resume>
			<mots_cles>Analyse sémantique automatique, interface syntaxe sémantique, FrameNet</mots_cles>
			<title>Probabilistic modeling of the syntax semantic interface using probabilistic context free grammars, Experiments with FrameNet</title>
			<abstract>This paper presents a generative method for predicting the frame semantic structure of a sentence from its syntactic structure and describes the grammars used with their performances. This system allows to predict, for a word in the syntactic context of a sentence, the most probable frame. The generative system allows to give a frame to a word and semantic roles to a set of pathes. Although results are not yet satisfying, this parser allows to group semantic parsing tasks (frame selection, role fillers selection, role assignment) unlike previously published works. In addition, it offers a new approach to parse semantic frames insofar as it is based more on syntactic structure rather than words of the sentence.</abstract>
			<keywords>Automatic Semantic Parsing, syntax semantic parsing, FrameNet</keywords>
		</article>
		<article id="recital-2014-long-002" session="Sémantique">
			<auteurs>
				<auteur>
					<prenom>Marianne</prenom>
					<nom>Djemaa</nom>
					<email>marianne.djemaa@inria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">INRIA, UMR-I 001, Alpage, Univ Paris Diderot, Sorbonne Paris Cité, F–75013 Paris, France</affiliation>
			</affiliations>
			<titre>Traitement FrameNet des constructions à attribut de l’objet</titre>
			<type>long</type>
			<pages>13-24</pages>
			<resume>Dans le cadre du projet ASFALDA, qui comporte une phase d’annotation sémantique d’un FrameNet français, nous cherchons à fournir un traitement linguistiquement motivé des constructions à attribut de l’objet, un exemple typique de divergence syntaxe-sémantique. Pour ce faire, nous commençons par dresser un panorama des propriétés syntaxiques et sémantiques des constructions à attribut de l’objet. Nous étudions ensuite le traitement FrameNet des verbes anglais typiques de cette construction, avant de nous positionner pour un traitement homogénéisé dans le cas du FrameNet français.</resume>
			<mots_cles>FrameNet, français, construction à attribut de l’objet, divergence syntaxe-sémantique</mots_cles>
			<title>Addressing object predicative complements in a French FrameNet</title>
			<abstract>Within the ASFALDA project, which includes the production of a French FrameNet, we try to provide a linguistically motivated treatment for a typical example of syntax-semantics mismatch : object complement construction. In order to do so, we first give an overview of syntactic and semantic properties of object complement constructions. Next, we study the way FrameNet deals with English verbs taking part in those constructions, and finally take a stance for a homogenized treatment of the construction within the French FrameNet.</abstract>
			<keywords>FrameNet, French, object complement construction, syntax-semantics mismatch</keywords>
		</article>
		<article id="recital-2014-long-003" session="Fouille de données et TAL">
			<auteurs>
				<auteur>
					<prenom>Marilyne</prenom>
					<nom>Latour</nom>
					<email>marilyne.latour@ac-grenoble.fr</email>
					<email>marilyne.latour@reportlinker.com</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Grenoble-Alpes, GRESEC, F-38040 Grenoble</affiliation>
				<affiliation affiliationId="2">ReportLinker, 4 Rue Montrochet, 69002 Lyon</affiliation>
			</affiliations>
			<titre>Expressions différenciées des besoins informationnels en Langue Naturelle : construction de profils utilisateurs en fonction des tâches de recherche d’informations</titre>
			<type>long</type>
			<pages>25-36</pages>
			<resume>Devant des collections massives et hétérogènes de données, les systèmes de RI doivent désormais pouvoir appréhender des comportements d’utilisateurs aussi variés qu’imprévisibles. L’objectif de notre approche est d’évaluer la façon dont un utilisateur verbalise un besoin informationnel à travers un énoncé de type « expression libre » ; appelé langage naturel (LN). Pour cela, nous nous situons dans un contexte applicatif, à savoir des demandes de remboursement des utilisateurs d’un moteur de recherche dédié à des études économiques en français. Nous avons recueilli via ce moteur, les demandes en LN sur 5 années consécutives totalisant un corpus de 1398 demandes. Nous avons alors comparé l’expression en tant que tel du besoin informationnel en fonction de la tâche de recherche d’informations (RI) de l’utilisateur.</resume>
			<mots_cles>Recherche informations, Besoin informationnel, Expression et interprétation des besoins, Formulation question, Langage naturel, comportement utilisateur, tâches de recherche d’informations</mots_cles>
			<title>Distinctions in the Formulation of Information Needs in Natural Language; Construction of a Task-Based User Profile Database</title>
			<abstract>With the massive and heterogeneous web document collections, IR system must analyze the behaviors of users which are unpredictable and varied. The approach described in this paper provides a description of the verbalizations of the information need in natural language. For this, we used data collected (i.e. users’ complaints in natural language) through a search engine dedicated to economic reports in French over 5 consecutive years totaling a corpus of 1398 natural language requests. Then, we compared the expression as such of the information need according to the IR task.</abstract>
			<keywords>Information Retrieval, Information Need, Query formulation and Query Expression, Query Formulation, Natural Language, User Behavior, IR task</keywords>
		</article>
		<article id="recital-2014-long-004" session="Modèles linguistiques">
			<auteurs>
				<auteur>
					<prenom>Ornella</prenom>
					<nom>Wandji Tchami</nom>
					<email>ornella.wandjitchami@univ-lille3.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CNRS UMR 8163 STL, Université Lille 3, 59653 Villeneuve d’Ascq, France</affiliation>
			</affiliations>
			<titre>Les modèles de description du verbe dans les travaux de Linguistique, Terminologie et TAL</titre>
			<type>long</type>
			<pages>37-48</pages>
			<resume>Dans le cadre de notre projet de recherche, qui a pour but l’implémentation d’un outil de simplification des emplois spécialisés de verbes dans des corpus médicaux à partir de l’analyse syntaxico-sémantique de ces verbes en contexte, nous proposons une analyse de quelques approches et travaux qui ont pour objet principal la description du verbe dans les trois domaines de recherche à l’interface desquels se situe notre projet : linguistique, TAL et terminologie. Nous décrivons plus particulièrement les travaux qui peuvent avoir une incidence sur notre étude. Cet état de l’art nous permet de mieux connaître le cadre théorique dans lequel s’intègre notre projet de recherche et d’avoir les repères et références susceptibles de contribuer à sa réalisation.</resume>
			<mots_cles>Verbe terminologique ou spécialisé, sémantique des cadres, sémantique lexicale, structure argumentale, étiquetage en rôles sémantiques</mots_cles>
			<title>The descriptive approaches of the verb in Linguistics, Terminology and NLP</title>
			<abstract>As part of our research project, which aims to implement a text simplification tool for the specialized usages of verbs in medical corpora using the syntactic and semantic analysis of these verbs in context, we propose an overview of some approaches and work whose main research object is the description of verbs, within the three research areas which interface our study is : linguistics, terminology and NLP. We pay a particular attention to studies that can have an impact on our work. This state of the art allows us to better understand the theoretical framework related to our research project. Moreover, it allows us to have benchmarks and references that might be usefull for the realization of our project.</abstract>
			<keywords>Specialized verb, Frame Semantics, lexical semantics, argumental structure, Semantic Role Labeling</keywords>
		</article>
		<article id="recital-2014-long-005" session="Méthodes numériques pour le TAL">
			<auteurs>
				<auteur>
					<prenom>Jérémie</prenom>
					<nom>Tafforeau</nom>
					<email>jeremie.tafforeau@lif.amu-mrs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIF, AMU, CNRS, 163 avenue de Luminy, 13288 Marseille Cedex 9</affiliation>
			</affiliations>
			<titre>Réseau de neurones profond pour l’étiquetage morpho-syntaxique</titre>
			<type>long</type>
			<pages>49-58</pages>
			<resume>L’analyse syntaxique et sémantique de langages non-canoniques est principalement limitée par le manque de corpus annotés. Il est donc primordial de mettre au point des systèmes robustes capables d’allier références canoniques et non-canoniques. Les méthodes exploitant la théorie des réseaux de neurones profonds ont prouvé leur efficacité dans des domaines tels que l’imagerie ou les traitements acoustiques. Nous proposons une architecture de réseau de neurones appliquée au traitement automatique des langages naturels, et plus particulièrement à l’étiquetage morpho-syntaxique. De plus, plutôt que d’extraire des représentations empiriques d’une phrase pour les injecter dans un algorithme de classification, nous nous inspirons de récents travaux portant sur l’extraction automatique de représentations vectorielles des mots à partir de corpus non-annotés. Nous souhaitons ainsi tirer profit des propriétés de linéarité et de compositionnalité de tels plongements afin d’améliorer les performances de notre système.</resume>
			<mots_cles>TALN, Étiquetage morpho-syntaxique, Apprentissage Automatique, Réseau de Neurones Profond, Plongements</mots_cles>
			<title>Deep Neural Network applied to Part-of-Speech Tagging</title>
			<abstract>Syntactic and semantic parsing of non-canonical languages is mainly restricted by the lack of labelled data sets. It is thus essential to develop strong systems capable of combining canonical and non-canonical text corpora. Deep Learning methods proved their efficiency in domains such as imaging or acoustic process.We propose neural network architecture applied to natural languages processing. Furthermore, instead of extracting from the sentence a rich set of hand-crafted features wich are the fed to a standard classification algorithm, we drew our inspiration from recent papers about the automatic extraction of word embeddings from large unlabelled data sets. On such embeddings, we expect to benefit from linearity and compositionality properties to improve our system performances.</abstract>
			<keywords>NLP, Part-of-Speech Tagging, Machine Learning, Deep Neural Network, Embeddings</keywords>
		</article>
		<article id="recital-2014-long-006" session="Lexique 3">
			<auteurs>
				<auteur>
					<prenom>Yuliya</prenom>
					<nom>Korenchuk</nom>
					<email>korenchuk@unistra.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LiLPa (Linguistique, Langues, Parole), EA 1339, Université de Strasbourg</affiliation>
				<affiliation affiliationId="2">Rebuz SAS, Strasbourg</affiliation>
			</affiliations>
			<titre>Extraction terminologique : vers la minimisation de ressources</titre>
			<type>long</type>
			<pages>59-70</pages>
			<resume>Cet article présente une méthode ayant pour objectif de minimiser l’apport extérieur nécessaire à la tâche d’extraction terminologique (ET) et de rendre cette tâche moins dépendante de la langue. Pour cela, la méthode prévoit des ressources morphologiques et morphosyntaxiques simplifiées construites directement à partir d’un corpus lemmatisé. Ces ressources endogènes servent à la création d’un système de filtres qui affinent les calculs statistiques et à la génération de patrons pour l’identification de candidats termes polylexicaux. La méthode a été testée sur deux corpus comparables en chimie et en télécommunication, en français et en anglais. La précision observée sur les 100 premiers candidats termes monolexicaux fluctue entre 71% et 87% pour le français et entre 44 % et 69 % en anglais ; celle des candidats termes polylexicaux s’élève à 69-78 % en français et 69-85 % en anglais en fonction du domaine.</resume>
			<mots_cles>extraction terminologique, ressources endogènes, apprentissage automatique</mots_cles>
			<title>Terminology Extraction: Towards Resource Minimization</title>
			<abstract>The article presents the method which aims to minimize the use of external resources for the terminology extraction task and to make this task less langage dependent. For that purpose, the method builds simplified morphological and morphosyntactic resources directly from a lemmatized corpus. These endogenous resources are used both in filters, which refine the statistical calculations, and in patterns for polylexical terms identification. The method was tested on two comparable corpora in chemistry and in telecommunication in French and in English. The precision observed on the first 100 monolexical terms fluctuates between 71% and 87% for French and between 44% and 69% in English ; for polylexical terms the precision was 69-78% in French and 69-85% in English depending on the domain.</abstract>
			<keywords>terminology extraction, endogenous resources, machine learning</keywords>
		</article>
		<article id="recital-2014-long-007" session="Langue des signes">
			<auteurs>
				<auteur>
					<prenom>Mohamed</prenom>
					<nom>Hadjadj</nom>
					<email>mohamed.hadjadj@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI, Rue John Von Neumann, 91400 Orsay, France</affiliation>
			</affiliations>
			<titre>Une description des structures de la durée en Langue des Signes Française à partir d’une grammaire formelle</titre>
			<type>long</type>
			<pages>71-80</pages>
			<resume>Dans cet article, nous abordons la problématique du fonctionnement de la temporalité en langue des signes française (LSF). Nous allons étudier plus particulièrement quelques structures portant sur la durée. Nous présenterons dans un premier temps les descriptions existantes du système aspecto-temporel de la LSF et les difficultés que nous trouvons pour modéliser ces travaux. Le but de cet article est de proposer une grammaire formelle qui prenne en compte le fonctionnement de la LSF et qui puisse faire l’objet d’un traitement de modélisation. Notre démarche consiste à étudier un corpus LSF pour établir des liens de fonction à forme afin d’obtenir des règles de grammaire qu’on peut générer dans un projet de synthèse à l’aide d’un signeur avatar.</resume>
			<mots_cles>grammaire, LSF, temporalité, modélisation LSF</mots_cles>
			<title>Description of structures of time (in French sign 
language) based on a formal grammar</title>
			<abstract>Temporality constitutes a major issue in filed of modeling french signed language (LSF). In fact, it is very difficult to model actual discriptions of the aspect-temporal systems of LSF. In this paper we present the bases of a novel formal grammar that permits the modeling of the LSF. This paper presents a study to construct this grammar. We analysed a French SL corpus to create formel rool between the signed gesture and its signification. Our objective is to obtain rules of grammar that can generate a synthesis project using a signer avatar.</abstract>
			<keywords>grammar, LSF, temporality, modeling LSF</keywords>
		</article>
		<article id="recital-2014-long-008" session="Traduction">
			<auteurs>
				<auteur>
					<prenom>Vincent</prenom>
					<nom>Letard</nom>
					<email>letard@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI, CNRS, rue John von Neumann, 91405 Orsay cedex</affiliation>
				<affiliation affiliationId="2">Université Paris-Sud, 91400 Orsay</affiliation>
			</affiliations>
			<titre>Interaction homme-machine en domaine large à l’aide du langage naturel : une amorce par mise en correspondance</titre>
			<type>long</type>
			<pages>81-91</pages>
			<resume>Cet article présente le problème de l’association entre énoncés en langage naturel exprimant des instructions opérationnelles et leurs expressions équivalentes et langage formel. Nous l’appliquons au cas du français et du langage R. Développer un assistant opérationnel apprenant, qui constitue notre objectif à long terme, requiert des moyens pour l’entraîner et l’évaluer, c’est-à-dire un système initial capable d’interagir avec l’utilisateur. Après avoir introduit la ligne directrice de ce travail, nous proposons un modèle pour représenter le problème et discutons de l’adéquation des méthodes par mise en correspondance, ou mapping, à notre tâche. Pour finir, nous montrons que, malgré des scores modestes, une approche simple semble suffisante pour amorcer un tel système interactif apprenant.</resume>
			<mots_cles>assistants interactifs, apprentissage artificiel, systèmes de question-réponse</mots_cles>
			<title>Natural Human-Machine Interaction for Manipulating Formal Language: Bootstrapping with Mapping</title>
			<abstract>We consider the problem of mapping natural language written utterances expressing operational instructions to formal language expressions, applied to French and the R programming language. Designing a learning operational assistant, which is our long term goal, requires the means to train and evaluate it, that is, a baseline system able to interact with the user. After presenting the guidelines of our work, we propose a model to represent the problem and discuss the fit of direct mapping methods to our task. Finally, we show that, while not resulting in excellent scores, a simple approach seems to be sufficient to bootstrap an interactive learning system.</abstract>
			<keywords>interactive assistants, machine learning, question answering systems</keywords>
		</article>
		<article id="recital-2014-long-009" session="Lexique 1">
			<auteurs>
				<auteur>
					<prenom>Sandrine</prenom>
					<nom>Ollinger</nom>
					<email>sandrine.ollinger@atilf.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CNRS, ATILF, UMR 7118 Nancy, F-54063, France Université de Lorraine, ATILF, UMR 7118 Nancy, F-54063, France</affiliation>
			</affiliations>
			<titre>Regroupement de structures de dérivations lexicales par raisonnement analogique</titre>
			<type>long</type>
			<pages>92-103</pages>
			<resume>Cet article propose une méthode de regroupement de structures de dérivations lexicales par raisonnement analogique. Nous présentons les caractéristiques générales d’un graphe lexical issu du Réseau Lexical du Français, dont nous exploitons par la suite les composantes faiblement connexes. Ces composantes sont regroupées en trois étapes : par isomorphisme, par similarité de relations, puis par similarité d’attributs. Les résultats du dernier regroupement sont analysés en détail.</resume>
			<mots_cles>graphe lexical, composantes connexes, analogie, raisonnement analogique, dérivation lexicale</mots_cles>
			<title>Merging structures of lexical derivations by analogical reasoning</title>
			<abstract>This paper presents a method for merging structures of lexical derivations by analogical reasoning. Following the presentation of general features of a lexical graph from the French Lexical Network, we focus on the weak connected components of this graph. This components are grouped together in three steps : by isomorphism, by relational similarity and finally by attributional similarity. The results of the last merging are analyzed in detail.</abstract>
			<keywords>lexical graph, analogy, connected components, analogical reasoning, lexical derivation</keywords>
		</article>
		<article id="recital-2014-long-010" session="Résumé automatique">
			<auteurs>
				<auteur>
					<prenom>Jérémy</prenom>
					<nom>Trione</nom>
					<email>Jeremy.trione@lif.univ-mrs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Aix-Marseille Université, CNRS, LIF UMR 7279, 13000, Marseille, France</affiliation>
			</affiliations>
			<titre>Méthodes par extraction pour le résumé automatique de conversations parlées provenant de centres d’appels</titre>
			<type>long</type>
			<pages>104-111</pages>
			<resume>Dans ce papier nous traitons des résumés automatiques de conversations parlées spontanées. Pour cela nous utilisons des conversations provenant de cas réels d’appels téléphoniques de centre d’appels issues du corpus DECODA. Nous testons des méthodes extractives classiques utilisées en résumé de texte (MMR) ainsi que des méthodes basées sur des heuristiques du dialogue dans le cadre des centres d’appels. Il s’agit de la sélection du tour de parole le plus long dans le premier quart de la conversation, dans l’ensemble de la conversation et dans le dernier quart de la conversation. L’ensemble est évalué avec la métrique ROUGE. Les résultats obtenus soulignent les limites de ces approches « classiques » et confirment la nécessité d’envisager des méthodes abstractives intégrant des informations de structures sur les conversations. En effet, ces premiers résultats montrent que les méthodes heuristiques basées sur la structure produisent des résultats comparables, voir meilleurs que des méthodes telles que MMR.</resume>
			<mots_cles>Résumé de conversations parlées, résumé par extraction, ROUGE, corpus DECODA, MMR</mots_cles>
			<title>Extraction methods for automatic summarization of spoken conversations from call centers</title>
			<abstract>In this paper we speak about automatic spoken conversation summaries. We use conversation from some real cases call from a call center extracted from the DECODA corpus. We test some extractive summary methods used in text summary (MMR) and some dialogue heuristics methods. It’s mainly to select the longest speaker turn in different part of the dialogue, the first quarter, the whole dialogue, and the last quarter of the dialogue. All the results are evaluated thanks the ROUGE software. The results show the limits of these classical approaches and suggest that we need some abstractive methods including some structural features of the conversation. In fact, these results show that the structural heuristics based methods are even or better than the classic method like MMR.</abstract>
			<keywords>spoken conversation summarization, extractive summary, ROUGE, DECODA corpus, MMR</keywords>
		</article>
		<article id="recital-2014-long-011" session="Traitement de corpus 2">
			<auteurs>
				<auteur>
					<prenom>Firas</prenom>
					<nom>Hmida</nom>
					<email>firas.hmida@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LINA UMR 6241,Université de Nantes</affiliation>
			</affiliations>
			<titre>Identification de Contextes Riches en Connaissances en corpus comparable</titre>
			<type>long</type>
			<pages>112-123</pages>
			<resume>Dans les études s’intéressant à la traduction assistée par ordinateur (TAO), l’un des objectifs consiste à repérer les passages qui focalisent l’attention des traducteurs humains. Ces passages sont considérés comme étant des contextes « riches en connaissances », car ils aident à la traduction. Certains contextes textuels ne donnent qu’une simple attestation d’un terme recherché, même s’ils le renferment. Ils ne fournissent pas d’informations qui permettent de saisir sa signification. D’autres, en revanche, contiennent des fragments de définitions, mentionnent une variante terminologique ou utilisent d’autres notions facilitant la compréhension du terme. Ce travail s’intéresse aux « contextes définitoires » qui sont utiles à l’acquisition de connaissances à partir de textes, en particulier dans la perspective de traduction terminologique assistée par ordinateur à partir de corpus comparables. En effet, l’association d’un exemple à un terme permet d’en appréhender le sens exact. Nous proposons, tout d’abord, trois hypothèses définissant la notion d’exemple définitoire. Ensuite nous évaluons sa validité grâce une méthode s’appuyant sur les Contextes Riches en Connaissances (CRC) ainsi que les relations hiérarchiques reliant les termes entre eux.</resume>
			<mots_cles>Contextes Riches en Connaissances, CRC, identification de définitions, identification d’exemples, énoncé définitoire, terminologie, traduction terminologique</mots_cles>
			<title>Knowledge-Rich Contexts Extraction from Comparable Corpora</title>
			<abstract>Some contexts provide only a simple explanation of a given term even if they contain it. However, others contain fragments of definitions, mention a terminological variant or use other concepts to make it easy the term understanding. In this work we focus on « definitory contexts » that would be valuable to a human for knowledge acquisition from texts, mainly in order to assist in terminological translation from comparable corpora. Indeed, provide the term with an example, makes it possible to understand its exact meaning. First, we specify three hypothesis defining the concept of a defnitory example. Then we evaluate its validity through a method based on the knowledge-Rich Contexts (KRCs) and hierarchical relationships between terms.</abstract>
			<keywords>Knowledge-Rich Contexts, KRCs, mining definitions, minging examples, terminology, terminological translation</keywords>
		</article>
		<article id="recital-2014-long-012" session="Traitement de corpus 2">
			<auteurs>
				<auteur>
					<prenom>Raja</prenom>
					<nom>Bensalem Bahloul</nom>
					<email>raja_ben_salem@yahoo.com</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Marwa</prenom>
					<nom>Elkarwi</nom>
					<email>marwaelkarwi89@gmail.com</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire Mir@cl, FSEGS, Université de Sfax, Sfax, Tunisie</affiliation>
				<affiliation affiliationId="2">Laboratoire Parole et Langage (LPL), CNRS, Université d’Aix-Marseille, Aix-en-Provence, France</affiliation>
			</affiliations>
			<titre>Induction d’une grammaire de propriétés à granularité variable à partir du treebank arabe ATB</titre>
			<type>long</type>
			<pages>124-135</pages>
			<resume>Dans cet article, nous présentons une démarche pour l’induction d’une grammaire de propriétés (GP) arabe en utilisant le treebank ATB. Cette démarche se base sur deux principales étapes : (1) l’induction d’une grammaire hors contexte et (2) l’induction d’une GP par la génération automatique des relations qui peuvent exister entre les unités grammaticales décrites dans la CFG. Le produit obtenu constitue une ressource ouvrant de nouvelles perspectives pour la description et le traitement de la langue arabe.</resume>
			<mots_cles>Treebanks, langue arabe, grammaire hors-contexte, grammaires de propriétés</mots_cles>
			<title>Induction of a variable granularity property grammar from the Arabic Treebank ATB</title>
			<abstract>This paper presents an approach for building an Arabic property grammar using the treebank ATB. This approach consists in two main steps: (1) inducing a context-free grammar from a treebank and (2) inducing a property grammar. So, we acquire first a context-free grammar (CFG) from the source treebank and then, we induce the property grammar by generating automatically existing relations between grammatical units described in the CFG. The result is a new resource for Arabic, opening the way to new tools and descriptions.</abstract>
			<keywords>Treebanks, Arabic language, context-free grammar, property grammars</keywords>
		</article>
	</articles>
</conference>
<conference>
    <edition>
      <acronyme>RECITAL'2008</acronyme>
      <titre>Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues</titre>
      <ville>Avignon</ville>
      <pays>France</pays>
      <dateDebut>2008-06-09</dateDebut>
      <dateFin>2008-06-13</dateFin>
      <presidents>
        <nom>Patrice Bellot</nom>
        <nom>Marie-Laure Guénot</nom>
      </presidents>
      <typeArticles>
        <type id="long">Papiers longs</type>
      </typeArticles>
      <statistiques>
        <!-- <acceptations id="" soumissions=""></acceptations> -->
      </statistiques>
      <siteWeb>http://www.lia.univ-avignon.fr/jep-taln08/</siteWeb>
      <meilleurArticle>
        <articleId/>
      </meilleurArticle>
    </edition>
    <articles>
      <article id="recital-2008-long-001" session="Session Orale">
        <auteurs>
          <auteur>
            <nom>Guillaume Bernard</nom>
            <email>gbernard@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIMSI - CNRS, B.P. 133, 91403 ORSAY, France</affiliation>
        </affiliations>
        <titre>Méthode de réordonnancement de réponses par transformation d’arbres : présentation et analyse des résultats</titre>
        <type>long</type>
        <pages/>
        <resume>Dans cet article nous présentons une évaluation et une analyse des résultats d’une méthode de réordonnancement de réponses pour un système de questions-réponses. Cette méthode propose une sélection des réponses candidates à une question en calculant un coût par transformation d’arbres. Nous présentons une analyse des résultats obtenus sur le corpus Clef 2004-2005 et nos conclusions sur les voies d’amélioration possibles pour notre système.</resume>
        <mots_cles>Recherche d’information, Traitement Automatique des Langues, systèmes de questions-réponses</mots_cles>
        <title/>
        <abstract>This paper describes an evaluation and an analysis of the results of an answers reranking method for a question-answering system. Candidate answers to a question are reordered by computing a tree transform cost. We discuss the results of our evaluation on the Clef 2004-2005 corpus and describe possible improvements to the system.</abstract>
        <keywords>Information Retrieval, Natural Language Processing, Question-Answering systems</keywords>
      </article>
      <article id="recital-2008-long-002" session="Session Orale">
        <auteurs>
          <auteur>
            <nom>André Bittar</nom>
            <email>andre.bittar@linguist.jussieu.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université Paris 7 Diderot - ALPAGE</affiliation>
        </affiliations>
        <titre>Annotation des informations temporelles dans des textes en français</titre>
        <type>long</type>
        <pages/>
        <resume>Le traitement des informations temporelles est crucial pour la compréhension de textes en langue naturelle. Le langage de spécification TimeML a été conçu afin de permettre le repérage et la normalisation des expressions temporelles et des événements dans des textes écrits en anglais. L’objectif des divers projets TimeML a été de formuler un schéma d’annotation pouvant s’appliquer à du texte libre, comme ce que l’on trouve sur le Web, par exemple. Des efforts ont été faits pour l’application de TimeML à d’autres langues que l’anglais, notamment le chinois, le coréen, l’italien, l’espagnol et l’allemand. Pour le français, il y a eu des efforts allant dans ce sens, mais ils sont encore un peu éparpillés. Dans cet article, nous détaillons nos travaux actuels qui visent à élaborer des ressources complètes pour l’annotation de textes en français selon TimeML - notamment un guide d’annotation, un corpus de référence (Gold Standard) et des modules d’annotation automatique.</resume>
        <mots_cles>Annotation temporelle, repérage des événements, TimeML</mots_cles>
        <title/>
        <abstract>The processing of temporal information is crucial for the understanding of natural language texts. The specification language TimeML was developed to facilitate the identification and normalization of temporal expressions and events in texts written in English. The aim of the various TimeML projects was to formulate an annotation scheme able to be applied to free text, such as that which is found on the Web, for example. Recently, efforts have been made to apply TimeML to languages other than English, namely Chinese, Korean, Italian, Spanish and German. Some efforts have been made in this direction with respect to French, but they remain somewhat scattered. In this paper, we detail our ongoing work, which aims to establish comprehensive resources for the annotation of French texts according to TimeML - an annotation guide, a Gold Standard corpus and modules for automatic annotation.</abstract>
        <keywords>Temporal annotation, event recognition, TimeML</keywords>
      </article>
      <article id="recital-2008-long-003" session="Session Orale">
        <auteurs>
          <auteur>
            <nom>Anne Garcia-Fernandez</nom>
            <email>annegf@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Carole Lailler</nom>
            <email>carole.lailler@lium.univ-lemans.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIMSI-CNRS – Université Paris Sud, BP 133 - 91403 Orsay Cedex</affiliation>
          <affiliation affiliationId="2">LIUM – Université du Maine, 72085 Le Mans Cedex 9</affiliation>
          <affiliation affiliationId="3"/>
        </affiliations>
        <titre>Morphosyntaxe de l'interrogation pour le système de question-réponse RITEL</titre>
        <type>long</type>
        <pages/>
        <resume>Nous proposons d'étudier le cas de l'interrogation en Dialogue Homme-Machine au sein d'un système de Question-Réponse à travers le prisme de la Grammaire Interactive. Celle-ci établit un rapport direct entre question et réponse et présuppose que la morphosyntaxe d'une interrogation dépend d'une « réponse escomptée »; l'interlocuteur humain ou machine ayant la possibilité de produire une réponse effective divergente. Nous proposons d’observer la présence des différentes formes de questions dans un corpus issu de l’utilisation du système RITEL. Et nous présentons une expérience menée sur des locuteurs natifs qui nous a permis de mettre en valeur la différence entre réponses effectives produites par nos sujets et réponses présupposées par le contenu intentionnel des questions. Les formalismes ainsi dégagés ont pour but de donner aux systèmes de DHM des fonctionnalités nouvelles comme la capacité à interpréter et à générer de la variabilité dans les énoncés produits.</resume>
        <mots_cles>Grammaire Interactive, modèles dynamiques, dialogue homme-machine, système de question-réponse, morphosyntaxe, expérimentation utilisateur, formulation de réponse</mots_cles>
        <title/>
        <abstract>In this paper we study the interrogation in Human-Computer Dialogue in order to integrate interpretative and dynamic functionnalities in a Question-Answering system using Interactive Grammar. It constitues a direct link between question and answer and assumes that the morphosyntax of interrogation depends on an anticipated answer. We propose an observation of question-answering corpora coming from the RITEL system showing the distribution of different morphosyntatic kinds of questions. Then we present an experimentation based on this observation in which French native speakers answer to those different kinds of questions. Our objective is to observe the difference between the anticipated answer and the effective one.</abstract>
        <keywords>Interactive Grammar, dynamic models, human-computer dialogue, question-answering system, morphosyntax, user experimentation, answer formulation</keywords>
      </article>
      <article id="recital-2008-long-004" session="Session Orale">
        <auteurs>
          <auteur>
            <nom>Elzbieta Gryglicka</nom>
            <email>elzbieta.gryglicka@thalesgroup.com</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire Mathématiques et Aide à la Décision – Thales R&amp;T, RD 128 Palaiseau</affiliation>
          <affiliation affiliationId="2">ALPAGE –Paris 7, 30 rue Château des Rentiers, 75 013 Paris</affiliation>
        </affiliations>
        <titre>Un système d'annotation des entités nommées du type personne pour la résolution de la référence</titre>
        <type>long</type>
        <pages/>
        <resume>Dans cet article nous présentons notre démarche pour l’annotation des expressions référentielles désignant les personnes et son utilisation pour la résolution partielle de la référence. Les choix effectués dans notre implémentation s'inspirent des travaux récents dans le domaine de l'extraction d'information et plus particulièrement de la reconnaissance des entités nommées. Nous utilisons les grammaires locales dans le but d'annoter les entités nommées du type Personne et pour construire, à partir des annotations produites, une base de connaissances extra-linguistiques. Les informations acquises par ce procédé sont ensuite utilisées pour implémenter une méthode de la résolution de la référence pour les syntagmes nominaux coréférentiels.</resume>
        <mots_cles>entités nommées, annotation, grammaires locales, Nooj, base de connaissances</mots_cles>
        <title/>
        <abstract>The aim of this paper is to describe our approach for annotating of the referential mentions that refer to the entities which are the instances of Person. Our method is inspired by the recent work in information extraction and particularly the named entities recognition and classification task. Local grammars are used to identify this category of named entities and to generate an extra-linguistic knowledge base which is further used for the process of reference resolution.</abstract>
        <keywords>named entities, entity recongnition, local grammars, Nooj, knowledge base</keywords>
      </article>
      <article id="recital-2008-long-005" session="Session Orale">
        <auteurs>
          <auteur>
            <nom>Alexis Kauffmann</nom>
            <email>alexis.kauffmann@lettres.unige.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LATL (Laboratoire d'Analyse et de Technologie du Langage) – Université de Genève, 2, rue de Candolle, 1211 Genève, Suisse</affiliation>
        </affiliations>
        <titre>Description de la structure de la phrase japonaise en vue d'une analyse syntaxique</titre>
        <type>long</type>
        <pages/>
        <resume>Nous décrivons la façon dont est formée la phrase japonaise, avec son contenu minimal, la structure des composants d'une phrase simple et l'ordre des mots dans ses composants, les différentes phrases complexes et les possibilités de changements modaux. Le but de cette description est de permettre l'analyse de la phrase japonaise selon des principes universels tout en restant fidèles aux particularités de la langue. L'analyseur syntaxique multilingue FIPS est en cours d'adaptation pour le japonais selon les règles de grammaire qui ont été définies. Bien qu'il fonctionnait alors uniquement pour des langues occidentales, les premiers résultats sont très positifs pour l'analyse des phrases simples, ce qui montre la capacité de Fips à s'adapter à des langues très différentes.</resume>
        <mots_cles>Analyse Syntaxique, Japonais, Grammaire</mots_cles>
        <title/>
        <abstract>We describe here the way the Japanese sentence is structured, with its minimal content, the structure of its components, the different kinds of complex sentences and the possible modal effects. The aim of this description is to analyse the Japanese sentence in a universal way while respecting the language properties. The syntactic parser FIPS is now being developed to analyse Japanese sentences following grammar rules coming from this syntactic description. Even though the parser used to be used for Western languages only, the first results of simple Japanese sentence analysis are very positive. This shows how FIPS can adapt easily to very different languages.</abstract>
        <keywords>Syntactic Parser, Japanese language, Grammar</keywords>
      </article>
      <article id="recital-2008-long-006" session="Session Orale">
        <auteurs>
          <auteur>
            <nom>Marc Le Tallec</nom>
            <email>marc.letallec@etu.univ-tours.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire d’Informatique – Université François-Rabelais de Tours, Campus Universitaire de Blois, 3 place Jean Jaurès, F-41000 Blois</affiliation>
        </affiliations>
        <titre>Adaptation d’un système de compréhension pour un robot compagnon</titre>
        <type>long</type>
        <pages/>
        <resume>Le projet EmotiRob, financé par l’ANR, a pour but de réaliser un robot compagnon pour des enfants fragilisés. Le projet se décompose en deux sous parties que sont le module de compréhension pour comprendre ce que dit l’enfant et un module d’interaction émotionnelle pour apporter une réponse en simulant des émotions par les mouvements du corps, les traits du visage et par l'émission de petits sons simples. Le module de compréhension dont il est question ici réutilise les travaux du système Logus. La principale difficulté est de faire évoluer le système existant d’un dialogue homme-machine finalisé vers un domaine plus large et de détecter l’état émotionnel de l’enfant. Dans un premier temps, nous présentons le projet EmotiRob et ses spécificités. Ensuite, le système de compréhension de la parole Logus, sur lequel se base ce travail, est présenté en détail. Enfin, nous présentons les adaptations du système à la nouvelle tâche EmotiRob.</resume>
        <mots_cles>Compréhension du langage, langue parlée spontanée</mots_cles>
        <title/>
        <abstract>The EmotiRob project, financed by ANR, aims at realizing a robot companion for weakened children. The project decomposes into two under parts that which are the module of understanding to include what says the child and a module of emotional interaction to bring an answer by feigning feelings by the movements of the body, lines of the face and by emission of small simple sounds. The module of understanding reuses the works of the system Logus. The main difficulty is to develop the existing system of a human-machine dialogue finalized towards a wider domain and to detect the emotional state of the child. At first, we present the EmotiRob project and its specificities. Then, the system of understanding Logus, on which bases itself this work is presented. Finally, we present the adaptations of the system to his new task EmotiRob.</abstract>
        <keywords>spoken language understanding, spontaneous language</keywords>
      </article>
      <article id="recital-2008-long-007" session="Session Orale">
        <auteurs>
          <auteur>
            <nom>Aiala Rosá</nom>
            <email>aialar@fing.edu.uy</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Facultad de Ingeniería - Universidad de la República, J. Herrera y Reissig 565, Montevideo, Uruguay</affiliation>
          <affiliation affiliationId="2">MoDyCo, UMR7114, CNRS - Université Paris X, 200, avenue de la République, Nanterre, France </affiliation>
        </affiliations>
        <titre>Identification automatique de marques d'opinion dans des textes</titre>
        <type>long</type>
        <pages/>
        <resume>Nous présentons un modèle conceptuel pour la représentation d'opinions, en analysant les éléments qui les composent et quelques propriétés. Ce modèle conceptuel est implémenté et nous en décrivons le jeu d’annotations. Le processus automatique d’annotation de textes en espagnol est effectué par application de règles contextuelles. Un premier sous-ensemble de règles a été écrit pour l'identification de quelques éléments du modèle. Nous analysons les premiers résultats de leur application.</resume>
        <mots_cles>Identification d'opinions, Fouille de textes, Traitement automatique du langage naturel</mots_cles>
        <title/>
        <abstract>We  present a model for the representation of opinions, by analyzing the elements which compose them and some properties. The model has an operating counterpart, implemented in the form of a set of tags. For the automatic application of these tags on Spanish texts, we work on the writing of contextual rules. A primary subset of rules was written for the identification of some elements of the model. We analyze the first results of their application.</abstract>
        <keywords>Opinions Identification, Text Mining, Natural Language Processing</keywords>
      </article>
      <article id="recital-2008-long-008" session="Session Orale">
        <auteurs>
          <auteur>
            <nom>Yves Scherrer</nom>
            <email>yves.scherrer@lettres.unige.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LATL, Université de Genève, Rue de Candolle 5, 1211 Genève 4, Suisse</affiliation>
        </affiliations>
        <titre>Transducteurs à fenêtre glissante pour l’induction lexicale</titre>
        <type>long</type>
        <pages/>
        <resume>Nous appliquons différents modèles de similarité graphique à la tâche de l’induction de lexiques bilingues entre un dialecte de Suisse allemande et l’allemand standard. Nous comparons des transducteurs stochastiques utilisant des fenêtres glissantes de 1 à 3 caractères, entraînés à l’aide de l’algorithme de maximisation de l’espérance avec des corpus d’entraînement de tailles différentes. Si les transducteurs à unigrammes donnent des résultats satisfaisants avec des corpus très petits, nous montrons que les transducteurs à bigrammes les dépassent à partir de 750 paires de mots d’entraînement. En général, les modèles entraînés nous ont permis d’améliorer la F-mesure de 7% à 15% par rapport à la distance de Levenshtein.</resume>
        <mots_cles>Induction lexicale, transducteurs stochastiques, langues apparentées</mots_cles>
        <title/>
        <abstract>We apply different models of graphemic similarity to the task of bilingual lexicon induction between a Swiss German dialect and Standard German. We compare stochastic transducers using sliding windows from 1 to 3 letters, trained with the Expectation-Maximisation algorithm on training corpora of different sizes. While the unigram transducers provide good results with very small corpora, we show that bigram transducers outperform them with corpora of 750 word pairs or more. Overall, the trained models show between 7% and 15% F-measure improvement over Levenshtein distance.</abstract>
        <keywords>Lexicon induction, stochastic transducers, cognate languages</keywords>
      </article>
      <article id="recital-2008-long-009" session="Poster">
        <auteurs>
          <auteur>
            <nom>Pierre Hankach</nom>
            <email>pierre.hankach@orange-ftgroup.com</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Orange Labs, 2 avenue Pierre Marzin, 22300 Lannion</affiliation>
          <affiliation affiliationId="2">Université Paris 7, 5 rue Thomas Mann, 75013 PARIS</affiliation>
        </affiliations>
        <titre>Génération intégrée localisée pour la production de documents</titre>
        <type/>
        <pages/>
        <resume>Dans cet article, nous proposons une approche intégrée localisée pour la génération. Dans cette approche, le traitement intégré des décisions linguistiques est limité à la production des propositions dont les décisions qui concernent leurs générations sont dépendantes. La génération se fait par groupes de propositions de tailles limitées avec traitement intégré des décisions linguistiques qui concernent la production des propositions qui appartiennent au même groupe. Notre approche apporte une solution pour le problème de complexité computationnelle de la génération intégrée classique. Elle fournit ainsi une alternative à la génération séparée (séquentielle ou interactive) qui présente plusieurs défauts mais qui est implémentée de manière répandue dans les systèmes de générations existants.</resume>
        <mots_cles>génération intégrée localisée, architectures de génération, SDRT, segmentation du discours</mots_cles>
        <title/>
        <abstract>In this paper, we propose a localized integrated approach for generation. In this approach, the integrated handling of linguistic decisions is limited to the production of propositions whose decisions concerning their generation are dependant. The generation is performed by groups of propositions of limited size with an integrated handling of linguistic decisions that concern the production of the propositions that belong to the same group. Our approach provides a solution for the computational complexity problem of classical integrated generation. Therefore, it provides an alternative to separated generation (sequential and interactive) that has many drawbacks but is widely implemented in today’s generation systems.</abstract>
        <keywords>localized integrated generation, generation architectures, SDRT, discourse segmentation</keywords>
      </article>
      <article id="recital-2008-long-010" session="Poster">
        <auteurs>
          <auteur>
            <nom>Mourad Mars</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Mounir Zrigui</nom>
            <email/>
            <affiliationId>2</affiliationId>
          </auteur>
          <auteur>
            <nom>Georges Antoniadis</nom>
            <email/>
            <affiliationId>3</affiliationId>
          </auteur>
          <auteur>
            <nom>Mohamed Belgacem</nom>
            <email/>
            <affiliationId>3</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université Stendhal</affiliation>
          <affiliation affiliationId="2">Faculté des Sciences de Monastir</affiliation>
          <affiliation affiliationId="3">Université Sthendal Grenoble3</affiliation>
        </affiliations>
        <titre>Un système de génération et étiquetage automatique de dictionnaires linguistiques de l'arabe</titre>
        <type/>
        <pages/>
        <resume>L'objectif de cet article est la présentation d'un système de génération automatique de dictionnaires électroniques de la langue arabe classique, développé au sein de laboratoire UTIC (unité de Monastir). Dans cet article, nous présenterons, les différentes étapes de réalisation, et notamment la génération automatique de ces dictionnaires se basant sur une théorie originale : les Conditions de Structures Morphomatiques (CSM), et les matrices lexicales. Ce système rentre dans le cadre des deux projets MIRTO et OREILLODULE réalisés dans les deux laboratoires LIDILEM de Grenoble et UTIC Monastir de Tunisie</resume>
        <mots_cles/>
        <title/>
        <abstract/>
        <keywords/>
      </article>
      <article id="recital-2008-long-011" session="Poster">
        <auteurs>
          <auteur>
            <nom>Fabien Poulard</nom>
            <email>Fabien.Poulard@univ-nantes.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université de Nantes, LINA CNRS UMR 6241, 44322 Nantes Cedex 3</affiliation>
        </affiliations>
        <titre>Analyse quantitative et qualitative de citations extraites d’un corpus journalistique</titre>
        <type/>
        <pages/>
        <resume>Dans le contexte de la détection de plagiats, le repérage de citations et de ses constituants est primordial puisqu’il peut aider à évaluer le caractère licite ou illicite d’une reprise (source citée ou non). Nous proposons ici une étude quantitative et qualitative des citations extraites d’un corpus que nous avons auparavant construit. Cette étude a pour but de tracer des axes de recherche vers une méthode de repérage automatique des citations.</resume>
        <mots_cles>citations, contruction et étude de corpus, genre journalistique</mots_cles>
        <title/>
        <abstract>In the plagiarism detection context, finding citations and their components is essential as it may help estimating legal value of a copy (with or without original source specified). We propose here a quantitative and qualitative study of citations we extracted from a corpus we previously built. This study aims at orienting our research towards an efficient automatic citations extraction method.</abstract>
        <keywords>citations, corpus creation and analysis, journalistic genre</keywords>
      </article>
      <article id="recital-2008-long-012" session="Poster">
        <auteurs>
          <auteur>
            <nom>Kévin Séjourné</nom>
            <email>kevin.sejourne@limsi.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Université de Paris Sud XI, Limsi-CNRS, BP 133, 91403 Orsay Cedex, Bâtiment 508</affiliation>
        </affiliations>
        <titre>Une structure pour les questions enchainées</titre>
        <type/>
        <pages/>
        <resume>Nous présentons des travaux réalisés dans le domaine des systèmes de questions réponses (SQR) utilisant des questions enchainées. La recherche des documents dans un SQR est perturbée par l’absence d’informations sur la valeur à accorder aux éléments de texte éventuellement utiles à la recherche d’informations qui figurent dans les questions liées. Les récentes campagnes d’évaluation montrent que ce problème est sous-estimé, et n’a pas fait l’oeuvre de technique dédiée. Afin d’améliorer la recherche des documents dans un SQR nous étudions une nouvelle méthode pour organiser les informations liées aux interactions entre questions. Celle-ci se base sur l’exploitation d’une structure de données adaptée à la transmission des informations des questions liées jusqu’au moteur d’interrogation.</resume>
        <mots_cles>Question réponse enchainée</mots_cles>
        <title/>
        <abstract>We present works realized in the field of questions answering systems(SQR) using chained questions. The search for documents in a SQR is disrupted by the absence of information on the value to be granted to the possibly useful elements of text in search of information which appear in bound questions. The recent campaigns of evaluation show that this problem is under estimated, and did not make the work of dedicated technique. To improve the search for documents in a SQR we study a new method to organize the information bound to the interactions between questions. This one is based on the operation of a structure of data adapted to the transmission of the information of bound questions up to the search engine.</abstract>
        <keywords>chained question answering</keywords>
      </article>
      <article id="recital-2008-long-013" session="Poster">
        <auteurs>
          <auteur>
            <nom>Agnès Souque</nom>
            <email>asouque@gmail.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire LIDILEM - Université Stendhal - Grenoble 3, 1491 rue des Résidences, 38040 Grenoble Cedex 09</affiliation>
        </affiliations>
        <titre>Vers une nouvelle approche de la correction grammaticale automatique</titre>
        <type/>
        <pages/>
        <resume>La correction grammaticale automatique du français est une fonctionnalité qui fait cruellement défaut à la communauté des utilisateurs de logiciels libres. Dans le but de combler cette lacune, nous avons travaillé à l’adaptation au français d’un outil initialement développé pour une langue étrangère. Ce travail nous a permis de montrer que les approches classiques du traitement automatique des langues utilisées dans le domaine ne sont pas appropriées. Pour y remédier, nous proposons de faire évoluer les formalismes des correcteurs en intégrant les principes linguistiques de la segmentation en chunks et de l’unification. Bien qu’efficace, cette évolution n’est pas suffisante pour obtenir un bon correcteur grammatical du français. Nous envisageons alors une nouvelle approche de la problématique.</resume>
        <mots_cles>correction grammaticale, syntagme, unification</mots_cles>
        <title/>
        <abstract>Free software users community is sorely lacking French grammar checking. With the aim of filling this gap, we have worked on the adaptation to French of a tool originally developped for a foreign language. Thanks to this work, we could show that classic natural language processing approaches used in grammar checking are not suitable. To remedy it, we suggest an evolution of grammar checkers that includes linguistic principles such as chunking and unification. Despite its efficiency, this evolution is not sufficient to get a good French grammr checker. We are then thinking of a new approach of the problem.</abstract>
        <keywords>grammar checking, chunk, unification</keywords>
      </article>
      <article id="recital-2008-long-014" session="Poster">
        <auteurs>
          <auteur>
            <nom>Stéphanie Weiser</nom>
            <email/>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">MoDyCo, UMR7114, CNRS – Université Paris X, 200 av. de la République, 92001 Nanterre</affiliation>
        </affiliations>
        <titre>Informations spatio-temporelles et objets touristiques dans des pages Web : repérage et annotation</titre>
        <type/>
        <pages/>
        <resume>Cet article présente un projet de repérage, d'extraction et d'annotation d'informations temporelles, d'informations spatiales et d'objets touristiques dans des pages Web afin d'alimenter la base de connaissance d'un portail touristique. Nous portons une attention particulière aux différences qui distinguent le repérage d'information dans des pages Web du repérage d’informations dans des documents structurés. Après avoir introduit et classifié les différentes informations à extraire, nous nous intéressons à la façon de lier ces informations entre elles (par exemple apparier une information d’ouverture et un restaurant) et de les annoter. Nous présentons également le logiciel que nous avons réalisé afin d'effectuer cette opération d'annotation ainsi que les premiers résultats obtenus. Enfin, nous nous intéressons aux autres types de marques que l'on trouve dans les pages Web, les marques sémiotiques en particulier, dont l'analyse peut être utile à l’interprétation des pages.</resume>
        <mots_cles>extraction d'information, annotation, informations spatio-temporelles, tourisme, pages Web</mots_cles>
        <title/>
        <abstract>This paper presents a project for the detection, extraction and annotation of temporal and spatial information and of tourism objects in order to fill the knowledge base of a tourism Web portal. We focus on the differences that exist between extraction from structured documents and extraction from Web pages. First, the different types of information to extract are presented. We then discuss methods for linking these pieces of information together – for example relating the name of a restaurant to its opening hours – and how to annotate the extracted data. The program we have developed to perform the extraction and annotation, as well as an evaluation of this program, are presented here. Finally, we focus on the semiotic marks which appear on the Web and show they also prove useful in interpreting Web pages.</abstract>
        <keywords>information extraction, annotation, spatial &amp; temporal information, tourism, Web pages</keywords>
      </article>
    </articles>
  </conference>
  <conference>
    <edition>
      <acronyme>RECITAL'2009</acronyme>
      <titre>Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues</titre>
      <ville>Senlis</ville>
      <pays>France</pays>
      <dateDebut>2009-06-24</dateDebut>
      <dateFin>2009-06-26</dateFin>
      <presidents>
        <nom>Thibault Mondary</nom>
        <nom>Aurélien Bossard</nom>
        <nom>Thierry Hamon</nom>
      </presidents>
      <typeArticles>
        <type id="long">Papiers longs</type>
      </typeArticles>
      <statistiques>
        <acceptations id="long" soumissions="15">12</acceptations>
      </statistiques>
      <siteWeb>http://lipn.univ-paris13.fr/taln09/index.php?conf=RECITAL</siteWeb>
      <meilleurArticle>
        <articleId>recital-2009-long-003</articleId>
      </meilleurArticle>
    </edition>
    <articles>
      <article id="recital-2009-long-001" session="Présentations Orales">
        <auteurs>
          <auteur>
            <nom>Pierre Gotab</nom>
            <email>pierre.gotab@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIA / Université d’Avignon, 339 chemin des Meinajariès, 84911 Avignon</affiliation>
        </affiliations>
        <titre>Apprentissage automatique et Co-training</titre>
        <type>long</type>
        <pages/>
        <resume>Dans le domaine de la classification supervisée et semi-supervisée, cet article présente un contexte favorable à l’application de méthodes statistiques de classification. Il montre l’application d’une stratégie alternative dans le cas où les données d’apprentissage sont insuffisantes, mais où de nombreuses données non étiquetées sont à notre disposition : le cotraining multi-classifieurs. Les deux vues indépendantes habituelles du co-training sont remplacées par deux classifieurs basés sur des techniques de classification différentes : icsiboost sur le boosting et LIBLINEAR sur de la régression logistique.</resume>
        <mots_cles>Apprentissage automatique, classification, co-training</mots_cles>
        <title/>
        <abstract>In the domain of supervised and semi-supervised classification, this paper describes an experimental context suitable with statistical classification. It shows an alternative method usable when learning data is unsufficient but when many unlabeled data is avaliable : the multi-classifier co-training. Two classifiers based on different classification methods replace the two independent views of the original co-training algorithm : icsiboost based on boosting and LIBLINEAR which is a logistic regression classifier.</abstract>
        <keywords>Machine learning, classification, co-training</keywords>
      </article>
      <article id="recital-2009-long-002" session="Présentations Orales">
        <auteurs>
          <auteur>
            <nom>Marianne Santaholma</nom>
            <email>Marianne.Santaholma@unige.ch</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">TIM/ISSCO/ETI – Université de Genève, 1211 Genève 4, CH</affiliation>
        </affiliations>
        <titre/>
        <type>long</type>
        <pages/>
        <resume>Nous présentons une comparaison de la performance de deux types différents de reconnaisseurs pour le japonais et l’anglais basés sur les grammaires. L’un des systèmes est dérivé à partir de règles d’une grammaire monolingue et l'autre de règles paramétrisées et multilingues. Ce dernier emploie, les mêmes règles de grammaire pour la création de modèles de langue nécessaires à la reconnaissance des langues typologiquement différentes. Nous avons effectué des expériences sur la reconnaissance dans les applications de dialogue de domaine limitée. Ces expériences montrent que les modèles de langue dérivés des règles multilingues de grammaire (1) traitent aussi bien l’un que l’autre les deux langues examinées, et (2) que leur performance est comparable à celle des reconnaisseurs dérivés de grammaires monolingues. Ceci suggère que le partage de grammaires entre langues typologiquement différentes pourrait être une solution pour rendre plus efficace le développement de systèmes de reconnaissance de la parole linguistiques.</resume>
        <mots_cles>Grammaire multilingue paramétrisé, reconnaissance de la parole</mots_cles>
        <title>Comparing Speech Recognizers Derived from Mono- and Multilingual Grammars</title>
        <abstract>This paper examines the performance of multilingual parameterized grammar rules on speech recognition. We present a performance comparison of two different types of Japanese and English grammar-based speech recognizers. One system is derived from monolingual grammar rules and the other from multilingual parameterized grammar rules. The latter one uses hence the same grammar rules for creation of the language models for these two different languages. We carried out experiments on speech recognition of limited domain dialog application. These experiments show that the language models derived from multilingual parameterized grammar rules (1) perform equally well on both tested languages, on English and Japanese, and (2) that the performance is comparable with the recognizers derived from monolingual grammars that were explicitly developed for these languages. This suggests that the sharing grammar resources between different languages could be one solution for more efficient development of rule-based speech recognizers.</abstract>
        <keywords>Parameterized multilingual grammar, speech recognition, typologically different languages</keywords>
      </article>
      <article id="recital-2009-long-003" session="Présentations Orales">
        <auteurs>
          <auteur>
            <nom>Clémentine Adam</nom>
            <email>adam@univ-tlse2.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>François Morlane-Hondère</nom>
            <email>morlanehondere@gmail.com</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CLLE / Université de Toulouse &amp; CNRS</affiliation>
        </affiliations>
        <titre>Détection de la cohésion lexicale par voisinage distributionnel : application à la segmentation thématique</titre>
        <type>long</type>
        <pages/>
        <resume>Cette étude s’insère dans le projet VOILADIS (VOIsinage Lexical pour l’Analyse du DIScours), qui a pour objectif d’exploiter des marques de cohésion lexicale pour mettre au jour des phénomènes discursifs. Notre propos est de montrer la pertinence d’une ressource, construite par l’analyse distributionnelle automatique d’un corpus, pour repérer les liens lexicaux dans les textes. Nous désignons par voisins les mots rapprochés par l’analyse distributionnelle sur la base des contextes syntaxiques qu’ils partagent au sein du corpus. Pour évaluer la pertinence de la ressource ainsi créée, nous abordons le problème du repérage des liens lexicaux à travers une application de TAL, la segmentation thématique. Nous discutons l’importance, pour cette tâche, de la ressource lexicale mobilixsée ; puis nous présentons la base de voisins distributionnels que nous utilisons ; enfin, nous montrons qu’elle permet, dans un système de segmentation thématique inspiré de (Hearst, 1997), des performances supérieures à celles obtenues avec une ressource traditionnelle.</resume>
        <mots_cles>Cohésion lexicale, ressources lexicales, analyse distributionnelle, segmentation thématique</mots_cles>
        <title/>
        <abstract>The present work takes place within the Voiladis project (Lexical neighborhood for discourse analysis), whose purpose is to exploit lexical cohesion markers in the study of various discursive phenomena. We want to show the relevance of a distribution-based lexical resource to locate interesting relations between lexical items in a text.We call neighbors lexical items that share a significant number of syntactic contexts in a given corpus. In order to evaluate the usefulness of such a resource, we address the task of topical segmentation of text, which generally makes use of some kind of lexical relations. We discuss here the importance of the particular resource used for the task of text segmentation. Using a system inspired by (Hearst, 1997), we show that lexical neighbors provide better results than a classical resource.</abstract>
        <keywords>Lexical cohesion, lexical resources, distributional analysis, text segmentation</keywords>
      </article>
      <article id="recital-2009-long-004" session="Présentations Orales">
        <auteurs>
          <auteur>
            <nom>Gaël Patin</nom>
            <email>gael.patin@inalco.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Er-Tim, Inalco, 75343 Paris</affiliation>
          <affiliation affiliationId="2">Arisem, Thales, 91300 Massy</affiliation>
        </affiliations>
        <titre>Extraction de lexique dans un corpus spécialisé en chinois contemporain</titre>
        <type>long</type>
        <pages/>
        <resume>La constitution de ressources lexicales est une tâche cruciale pour l’amélioration des performances des systèmes de recherche d’information. Cet article présente une méthode d’extraction d’unités lexicales en chinois contemporain dans un corpus spécialisé non-annoté et non-segmenté. Cette méthode se base sur une construction incrémentale de l’unité lexicale orientée par une mesure d’association. Elle se distingue des travaux précédents par une approche linguistique non-supervisée assistée par les statistiques. Les résultats de l’extraction, évalués sur un échantillon aléatoire du corpus de travail, sont honorables avec des scores de précision et de rappel respectivement de 52,6 % et 53,7 %.</resume>
        <mots_cles>corpus spécialisé, unité lexicale, lexie, extraction de lexique, chinois</mots_cles>
        <title/>
        <abstract>Building lexical resources is a vital task in improving the efficiency of information retrieval systems. This article introduces a Chinese lexical unit extraction method for untagged specialized corpora. This method is based on an incremental process driven by an association score. This work features an unsupervised statistically aided linguistic approach. The extraction results — evaluated on a random sample of the working corpus — show decent precision and recall which amount respectively to 52.6% and 53.7%.</abstract>
        <keywords>specialized corpus, lexical unit, lexicon extraction, Chinese</keywords>
      </article>
      <article id="recital-2009-long-005" session="Présentations Orales">
        <auteurs>
          <auteur>
            <nom>Claire Mouton</nom>
            <email>Claire.Mouton@cea.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">CEA/LIST/LIC2M - BP 6 92265 Fontenay-aux-Roses Cedex</affiliation>
          <affiliation affiliationId="2">Exalead S.A. - 10 place de la Madeleine - 75008 Paris</affiliation>
        </affiliations>
        <titre>Induction de sens de mots à partir de multiples espaces sémantiques</titre>
        <type>long</type>
        <pages/>
        <resume>Les mots sont souvent porteurs de plusieurs sens. Pour traiter l’information correctement, un ordinateur doit être capable de décider quel sens d’un mot est employé à chacune de ses occurrences. Ce problème non parfaitement résolu a généré beaucoup de travaux sur la désambiguïsation du sens des mots (Word Sense Disambiguation) et dans la génération d’espaces sémantiques dont un des buts est de distinguer ces différents sens. Nous nous inspirons ici de deux méthodes existantes de détection automatique des différents usages et/ou sens des mots, pour les appliquer à des espaces sémantiques issus d’une analyse syntaxique effectuée sur un très grand nombre de pages web. Les adaptations et résultats présentés dans cet article se distinguent par le fait d’utiliser non plus une seule représentation mais une combinaison de multiples espaces de forte dimensionnalité. Ces multiples représentations étant en compétition entre elles, elles participent chacune par vote à l’induction des sens lors de la phase de clustering.</resume>
        <mots_cles>espace sémantique, réduction de dimensions, Locality Sensitive Hashing, induction de sens, clustering de mots, objets multi-représentés</mots_cles>
        <title/>
        <abstract>Words can have many senses. In order to process information correctly, a computer should be able to decide which sense of a word is used in a given context. This unsolved problem has generated much research in word sense disambiguation and in the generation of semantic spaces in order to separate possible meanings. Here, we adapt two existing methods to automatically distinguish words uses and senses.We apply them to multiple semantic spaces produced by a syntactic analysis of a very large number of web pages. These adaptations and the results presented in this article differ from the original methods in that they use a combination of several high dimensional spaces instead of one single representation. Each of these competing semantic spaces takes part in a clustering phase in which they vote on sense induction.</abstract>
        <keywords>semantic space, dimensionality reduction, Locality Sensitive Hashing, Word Sense Induction, words clustering, multi-represented data</keywords>
      </article>
      <article id="recital-2009-long-006" session="Présentations Orales">
        <auteurs>
          <auteur>
            <nom>Marion Potet</nom>
            <email>Marion.Potet@imag.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Laboratoire d’informatique de Grenoble, équipe GETALP, UJF - BP 53, 38041 Grenoble Cedex 9</affiliation>
        </affiliations>
        <titre>Méta-moteur de traduction automatique : proposition d’une métrique pour le classement de traductions</titre>
        <type>long</type>
        <pages/>
        <resume>Compte tenu de l’essor du Web et du développement des documents multilingues, le besoin de traductions "à la volée" est devenu une évidence. Cet article présente un système qui propose, pour une phrase donnée, non pas une unique traduction, mais une liste de N hypothèses de traductions en faisant appel à plusieurs moteurs de traduction pré-existants. Neufs moteurs de traduction automatique gratuits et disponibles sur leWeb ont été sélectionnés pour soumettre un texte à traduire et réceptionner sa traduction. Les traductions obtenues sont classées selon une métrique reposant sur l’utilisation d’un modèle de langage. Les expériences conduites ont montré que ce méta-moteur de traduction se révèle plus pertinent que l’utilisation d’un seul système de traduction.</resume>
        <mots_cles>traduction automatique, web, modèle de langage, méta-moteur de traduction</mots_cles>
        <title/>
        <abstract>Considering the Web and multilingual documents development expansion, the need of fast translation has become an evidence. This paper presents a system that proposes, for a given sentence, a list of N translation hypotheses instead of a single translation, using several machine translation systems already existing. Nine free and available (on the Internet) automatic translation engines have been chosen to submit a text to be translated and to receive its translation. The translations obtained are evaluated individually with a language model adapted and a metric elaborated by us, and in this way classified by relevance order. The experiment have pointed out that this meta-translation engine is more useful than the use of one system for translation.</abstract>
        <keywords>automatic translation, web, language model, meta-translator</keywords>
      </article>
      <article id="recital-2009-long-007" session="Présentations Orales">
        <auteurs>
          <auteur>
            <nom>Thomas François</nom>
            <email>thomas.francois@uclouvain.be</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">Centre de Traitement Automatique du Langage, Université Catholique de Louvain</affiliation>
        </affiliations>
        <titre>Modèles statistiques pour l’estimation automatique de la difficulté de textes de FLE</titre>
        <type>long</type>
        <pages/>
        <resume>La lecture constitue l’une des tâches essentielles dans l’apprentissage d’une langue étrangère. Toutefois, la découverte d’un texte portant sur un sujet précis et qui soit adapté au niveau de chaque apprenant est consommatrice de temps et pourrait être automatisée. Des expériences montrent que, pour l’anglais, l’utilisation de classifieurs statistiques permet d’estimer automatiquement la difficulté d’un texte. Dans cet article, nous proposons une méthodologie originale comparant, pour le français langue étrangère (FLE), diverses techniques de classification (la régression logistique, le bagging et le boosting) sur deux corpus d’entraînement. Il ressort de cette analyse comparative une légère supériorité de la régression logistique multinomiale.</resume>
        <mots_cles>lisibilité, régression logistique, bagging, boosting, modèle de langue</mots_cles>
        <title/>
        <abstract>Reading is known to be an essential task in language learning, but finding the appropriate text for every learner is far from easy. In this context, automatic procedures can support the teacher’s work. Some works on English reveal that it is possible to assess the readability of texts using statistical classifiers. In this paper, we present an original approach comparing various classification techniques, namely logistic regression, bagging and boosting on two training corpora. The results show a slight superiority for multinomial logistic regression over bagging or boosting.</abstract>
        <keywords>readability, logistic regression, bagging, boosting, language model</keywords>
      </article>
      <article id="recital-2009-long-008" session="Présentations Orales">
        <auteurs>
          <auteur>
            <nom>Florent Pompigne</nom>
            <email>florent.pompigne@loria.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">ENS Cachan / INRIA Nancy - Grand-Est</affiliation>
        </affiliations>
        <titre>Modélisation des mouvements explicites dans les ACG avec le produit dépendant</titre>
        <type>long</type>
        <pages/>
        <resume/>
        <mots_cles>syntaxe, grammaires catégorielles abstraites, types dépendant, mouvements explicites, extraction</mots_cles>
        <title/>
        <abstract>Abstract Categorial Grammars (ACG) is a grammatical framework based on linear lambda-calculus. As in Muskens’ Lambda Grammars, an abstract term in this kind of categorial grammar can be realized in different directions, such as syntactic and semantic ones. This structure provides autonomy for these different processings. ACG’s architecture is independent from the logic used and so the type system is easily extensible in order to deal better with some linguistic phenomena. We will first introduce ACGs and the dependent product construction. This paper will then be concerned with the issue of overt grammatical movements, in particular extraction constraints in relative propositions, and how several close frameworks deal with it. Last we will show how to capture this phenomenon in extended ACG.</abstract>
        <keywords>Syntax, abstract categorial grammars, dependant product, overt movements, extraction</keywords>
      </article>
      <article id="recital-2009-long-009" session="Présentations Orales">
        <auteurs>
          <auteur>
            <nom>Vanessa Andréani</nom>
            <email>va@tkm.fr</email>
            <affiliationId>1</affiliationId>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">TecKnowMetrix – 4, rue Léon Béridot – ZAC Champfeuillet – 38500 Voiron – France</affiliation>
          <affiliation affiliationId="2">LIDILEM – Université Stendhal Grenoble 3 – Domaine universitaire – 1180, avenue centrale – 38400 Saint Martin d’Hères – France</affiliation>
        </affiliations>
        <titre>Normalisation des entités nommées : pour une approche mixte et orientée utilisateurs</titre>
        <type>long</type>
        <pages/>
        <resume>La normalisation intervient dans de nombreux champs du traitement de l'information. Elle permet d'optimiser les performances des applications, telles que la recherche ou l'extraction d'information, et de rendre plus fiable la constitution de ressources langagières. La normalisation consiste à ramener toutes les variantes d'un même terme ou d'une entité nommée à une forme standard, et permet de limiter l'impact de la variation linguistique. Notre travail porte sur la normalisation des entités nommées, pour laquelle nous avons mis en place un système complexe mêlant plusieurs approches. Nous en présentons ici une des composantes : une méthode endogène de délimitation et de validation de l’entité nommée normée, adaptée à des données multilingues. De plus, nous plaçons l'utilisateur au centre du processus de normalisation, dans l'objectif d'obtenir des données parfaitement fiables et adaptées à ses besoins.</resume>
        <mots_cles>normalisation, entités nommées, traitement de l'information, analyse de corpus, méthodes endogènes, système complexe</mots_cles>
        <title/>
        <abstract>Normalization is involved in many fields of information processing. It improves performances for several applications, such as information retrieval or information extraction, and makes linguistic resources constitution more reliable. Normalization consists in standardizing each variant of a term or named entity into a unique form, and this way restricts the impact of term variation. Our work applies to named entity normalization, for which we implemented a complex system that mixes several approaches. We present here one of its components: an endogenous method to mark out and validate the normalized named entities. Moreover, we place the user in the center of our normalization process, in order to obtain fully reliable data that fit his needs.</abstract>
        <keywords>normalization, named entities, information processing, corpus analysis, endogenous methods, complex system</keywords>
      </article>
      <article id="recital-2009-long-010" session="Poster">
        <auteurs>
          <auteur>
            <nom>Eric Charton</nom>
            <email>eric.charton@univ-avignon.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIA / Université d’Avignon, 339 chemin des Meinajariès, 84911 Avignon</affiliation>
        </affiliations>
        <titre>Combinaison de contenus encyclopédiques multilingues pour une reconnaissance d’entités nommées en contexte</titre>
        <type>long</type>
        <pages/>
        <resume>Dans cet article, nous présentons une méthode de transformation de Wikipédia en ressource d’information externe pour détecter et désambiguïser des entités nommées, en milieu ouvert et sans apprentissage spécifique. Nous expliquons comment nous construisons notre système, puis nous utilisons cinq éditions linguistiques de Wikipédia afin d’enrichir son lexique. Pour finir nous réalisons une évaluation et comparons les performances du système avec et sans compléments lexicaux issus des informations inter-linguistiques, sur une tâche d’extraction d’entités nommées appliquée à un corpus d’articles journalistiques.</resume>
        <mots_cles>Etiquetage d’entités nommées, ressources sémantiques</mots_cles>
        <title/>
        <abstract>In this paper, we present a way to use of Wikipedia as an external resource to disambiguate and detect named entities, without learning step. We explain how we build our system and why we used five linguistic editions of the Wikipedia corpus to increase the volume of potentially matching candidates. We finally experiment our system on a news corpus.</abstract>
        <keywords>Named entity labeling, semantic resources</keywords>
      </article>
      <article id="recital-2009-long-011" session="Poster">
        <auteurs>
          <auteur>
            <nom>Rami Ayadi</nom>
            <email>ayadi.rami@planet.tn</email>
            <affiliationId>1</affiliationId>
          </auteur>
          <auteur>
            <nom>Walid Jaoudi</nom>
            <email>walidjaouadi@yahoo.fr</email>
            <affiliationId>2</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">UTIC (monastir) – ISIMS (sfax) ,Tunisie</affiliation>
          <affiliation affiliationId="2">UTIC (Tunis) , Tunisie</affiliation>
        </affiliations>
        <titre>La distance intertextuelle pour la classification de textes en langue arabe</titre>
        <type>long</type>
        <pages/>
        <resume>Nos travaux de recherche s’intéressent à l’application de la théorie de la distance intertextuelle sur la langue arabe en tant qu’outil pour la classification de textes. Cette théorie traite de la classification de textes selon des critères de statistique lexicale, se basant sur la notion de connexion lexicale. Notre objectif est d’intégrer cette théorie en tant qu’outil de classification de textes en langue arabe. Ceci nécessite l’intégration d’une métrique pour la classification de textes au niveau d’une base de corpus lemmatisés étiquetés et identifiés comme étant des références d’époques, de genre, de thèmes littéraires et d’auteurs et ceci afin de permettre la classification de textes anonymes.</resume>
        <mots_cles>Distance intertextuelle, arabe, classification, lemmatisation, corpus, statistique lexicale</mots_cles>
        <title/>
        <abstract>Our researche works are interested in the application of the intertextual distance theory on the Arabic language as a tool for the classification of texts. This theory handles the classification of texts according to criteria of lexical statistics, and it is based on the lexical connection approach. Our objective is to integrate this theory as a tool of classification of texts in Arabic language. It requires the integration of a metrics for the classification of texts using a database of lemmatized and identified corpus which can be considered as a literature reference for times, genres, literary themes and authors and this in order to permit the classification of anonymous texts.</abstract>
        <keywords>Intertextual distance, Arabic, classification, lemmatization, corpus, lexical statistics</keywords>
      </article>
      <article id="recital-2009-long-012" session="Poster">
        <auteurs>
          <auteur>
            <nom>Sara Boutouhami</nom>
            <email>boutouhami@lipn.univ-paris13.fr</email>
            <affiliationId>1</affiliationId>
          </auteur>
        </auteurs>
        <affiliations>
          <affiliation affiliationId="1">LIPN – Université Paris-Nord, 99 Avenue J.B.Clément 93430 Villetaneuse</affiliation>
        </affiliations>
        <titre>Techniques argumentatives pour aider à générer des descriptions orientées d’un événement</titre>
        <type>long</type>
        <pages/>
        <resume>Les moyens et les formes stratégiques permettant la génération de descriptions textuelles argumentées d’une même réalité effective sont nombreux. La plupart des définitions proposées de l’argumentation partagent l’idée qu’argumenter c’est fournir les éléments en faveur d’une conclusion donnée. Or dans notre tâche qui consiste à générer des descriptions argumentées pour des accidents de la route, nous ne disposons pas uniquement d’éléments en faveur de la conclusion souhaitée mais aussi d’éléments qui vont à l’encontre de cette dernière et dont la présence est parfois obligatoire pour la compréhension de ces descriptions. Afin de remédier à ce problème, nous proposons des techniques de génération de descriptions argumentées qui présentent au mieux les éléments indésirables à l’aide de stratégies argumentatives.</resume>
        <mots_cles>Argumentation, Insinuation, Norme coutumières, Justification</mots_cles>
        <title/>
        <abstract>Strategic means and forms for the generation of textual descriptions of a same reality are numerous. Most definitions of the argumentation given in the literature share the idea that to argument we must provide elements in favor of a given conclusion. Unfortunately, in our task consisting in generating biased descriptions of a road crash, we do not have only elements in favor of the desired conclusion, but also elements that are against it and must nevertheless be present in the description unless we cannot be able to understand how the accident happened. To remedy this problem, we proposed a module in our system for generating biased descriptions that handles the task of presenting better the undesirable elements using argumentative techniques.</abstract>
        <keywords>Argumentation, Insinuation, Customary norms, Justification</keywords>
      </article>
    </articles>
  </conference>
</conferences>
